{
    "651e664812948147": {
        "terms": [
            "data science",
            "data analyst"
        ],
        "salary_min": 64896.26,
        "salary_max": 82173.125,
        "title": "Data Analytics Analyst 2",
        "company": "IQVIA",
        "desc": "The Affordability team within IQVIA\u2019s Patient Support Services (PSS) group is an exciting and dynamic healthcare practice where you can start or continue your career in the life sciences industry. \n \n  IQVIA possesses the largest and most comprehensive set of healthcare data assets in the world. Additionally, the PSS team generates client data through savings program transactions captured by our proprietary claims engine. From our position atop this wealth of information, we seek to deliver unparalleled insights and analysis to our clients. We are seeking the best, brightest, and most analytically motivated individuals to join our team. \n \n \n   The Role of a PSS Affordability Analyst:\n    Analysts begin under the guidance of more senior team members, but are quickly provided with opportunities to contribute to all aspects of client engagements. Team members will begin to assume an active ownership in running projects and mentoring more junior colleagues. The analytic rigor and creativity of our analysts\u2019 work is critical to the success of our engagement teams and our practice as a whole.\n   \n  As PSS Analyst you\u2019ll be responsible for: \n   \n \n Leveraging skills in Excel and SQL to yield trends, conclusions, and actionable recommendations for clients  \n Contributing to internal brainstorming sessions and collaborating with account management team members throughout client engagements \n Evaluating secondary data on markets, physicians, patients, and pharmacies \n Developing and working with reporting tools such as PowerBI to derive insights and forecasting \n Coordinating both directly with clients as well as through the PSS account team to manage the analytics needs of the clients for both scheduled and ad hoc requests \n Creating reports, presentations and other client deliverables \n Presenting to client audiences via teleconference, videoconference, or in face-to-face meetings \n Building professional relationships with internal and external members of the client team \n Proactively developing knowledge of the healthcare industry and client engagement methodologies  \n \n  An ideal candidate will have: \n   \n \n A strong analytical record, excellent problem-solving abilities, and strong quantitative skills \n Exceptional communication skills and be a proven team contributor \n An interest in and desire to learn about the constantly evolving healthcare industry \n Good project management, time management and organizational skills \n Excellent conversational and business English (written and oral) \n Exceptional IT literacy in PowerPoint and Excel; PowerBI and/or SQL are a plus \n A willingness or ability to travel as needed (<10% on average) \n \n \n \n  IQVIA is a leading global provider of advanced analytics, technology solutions and clinical research services to the life sciences industry. We believe in pushing the boundaries of human science and data science to make the biggest impact possible \u2013 to help our customers create a healthier world. Learn more at https://jobs.iqvia.com \n \n  We are committed to providing equal employment opportunities for all, including veterans and candidates with disabilities. https://jobs.iqvia.com/eoe \n \n  IQVIA\u2019s ability to operate and provide certain services to customers and partners necessitates IQVIA and its employees meet specific requirements regarding COVID-19 vaccination status. https://jobs.iqvia.com/covid-19-vaccine-status",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "be894d8f4dba0b69": {
        "terms": [
            "data science"
        ],
        "salary_min": 80242.266,
        "salary_max": 101604.59,
        "title": "Fraud Behavioral Insights, Customer Identity Analyst",
        "company": "Cash App",
        "desc": "Company Description\n   It all started with an idea at Block in 2013. Initially built to take the pain out of peer-to-peer payments, Cash App has gone from a simple product with a single purpose to a dynamic ecosystem, developing unique financial products, including Afterpay/Clearpay, to provide a better way to send, spend, invest, borrow and save to our 47 million monthly active customers. We want to redefine the world\u2019s relationship with money to make it more relatable, instantly available, and universally accessible.    Today, Cash App has thousands of employees working globally across office and remote locations, with a culture geared toward innovation, collaboration and impact. We\u2019ve been a distributed team since day one, and many of our roles can be done remotely from the countries where Cash App operates. No matter the location, we tailor our experience to ensure our employees are creative, productive, and happy.    Check out our locations, benefits, and more at cash.app/careers. \n \n \n \n Job Description\n   Cash App is seeking a Fraud Behavioral Insights, Customer Identity Analyst to join our dynamic Behavioral Insights Team. A Fraud Behavioral Insights, Customer Identity Fraud Analyst leverages natural analytical and problem-solving skills to analyze large structured and unstructured data sets to identify trends, anomalies and inauthentic behaviors across Cash App accounts and products. This position relies on decisions made independently, with high precision and attention to detail. \n  This position will collaborate often with teammates, as well as work with many organizations within the business, such as Data Science, Machine Learning Modeling, Engineering, Product, Business Operations, Risk Operations, Compliance, and more. You will help drive innovation and influence Cash App\u2019s fraud roadmap, programs, and processes. \n  The ideal candidate possesses exceptional analytical and communication skills. They must also enjoy ambiguous problem solving and have strong self-motivation skills. This role requires a bias toward taking action in a fast-paced and dynamic environment. \n  You Will \n \n  Become an expert on customer behaviors and authentic and inauthentic activity within Cash App\u2019s ecosystem \n  Master the Identity domain of risk-adjacent behaviors and how your domain affects Cash App \n  Learn to investigate and analyze complex data sets to resolve business challenges, identify opportunities for improvement, and provide insights and solutions \n  Leverage industry experience and expertise to analyze existing risks within product offerings that require a high level of attention to detail \n  Partner with internal business teams to identify and confirm emerging fraud trends and potential customer impact due to risk controls, both ahead of product launches and in an ongoing capacity \n  Respond promptly to internal business partners and exercise exceptional communication skills to optimize each contact \n  Must be able to take initiative, plan, organize and prioritize projects with overlapping deadlines competently; work independently and help others; must be highly motivated and detail-oriented \n  Foster a culture of professionalism, accountability, collaboration, speed, innovation, excellence, and a fun work environment while continuously elevating the quality and caliber of our risk controls \n  Other responsibilities as assigned \n \n \n \n \n Qualifications\n  \n \n  3+ years experience in risk and/or fraud detection in financial services or technology with an emphasis on KYC/CIP/onboarding processes. \n  Experience working with machine learning teams \n  Superior writing and editing skills, with the ability to produce copy requiring minimal rework; technical or procedural writing experience a plus \n  Experience working with teams across countries and time zones \n  Ability to synthesize information and make clear, concise recommendations on a course of action \n  Flexibility to adapt and able to manage multiple assignments while working independently \n \n  Even Better \n \n  Bachelor's Degree in Finance, Accounting, Mathematics, Economics, Computer Science, Information Management or Statistics \n  CFE, ACAMS, or similar accreditation \n  SQL experience \n \n  Additional Information\n   Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate\u2019s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.    Zone A: USD $103,900 - USD $126,900  Zone B: USD $96,600 - USD $118,000  Zone C: USD $88,300 - USD $107,900  Zone D: USD $77,900 - USD $95,300 \n \n  To find a location\u2019s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information. \n  Full-time employee benefits include the following: \n \n  Healthcare coverage (Medical, Vision and Dental insurance) \n  Health Savings Account and Flexible Spending Account \n  Retirement Plans including company match \n  Employee Stock Purchase Program \n  Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance \n  Paid parental and caregiving leave \n  Paid time off (including 12 paid holidays) \n  Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees) \n  Learning and Development resources \n  Paid Life insurance, AD&D, and disability benefits \n  Additional Perks such as WFH reimbursements and free access to caregiving, legal, and discounted resources \n \n  These benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans. \n  US and Canada EEOC Statement \n  We\u2019re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. \n  We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible.  Want to learn more about what we\u2019re doing to build a workplace that is fair and square? Check out our  I+D page . \n  Additionally, we consider qualified applicants with criminal histories for employment on our team, and always assess candidates on an individualized basis. \n \n \n  Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.",
        "cleaned_desc": " \n  3+ years experience in risk and/or fraud detection in financial services or technology with an emphasis on KYC/CIP/onboarding processes. \n  Experience working with machine learning teams \n  Superior writing and editing skills, with the ability to produce copy requiring minimal rework; technical or procedural writing experience a plus \n  Experience working with teams across countries and time zones \n  Ability to synthesize information and make clear, concise recommendations on a course of action \n  Flexibility to adapt and able to manage multiple assignments while working independently \n \n  Even Better \n \n  Bachelor's Degree in Finance, Accounting, Mathematics, Economics, Computer Science, Information Management or Statistics \n  CFE, ACAMS, or similar accreditation \n  SQL experience ",
        "techs": [
            "machine learning",
            "sql"
        ],
        "cleaned_techs": [
            "sql"
        ]
    },
    "0ff5fd9855b6a27d": {
        "terms": [
            "data science"
        ],
        "salary_min": 104276.79,
        "salary_max": 132037.66,
        "title": "Data Scientist",
        "company": "Vibronyx",
        "desc": "Job Title:  Data Scientist \n \n Department:  Information Technology \n \n Reports To:  Vishnu Venkatesh \n \n Work Location:  Flexible for remote and/or onsite. East coast preference. \n \n Who we are:  Vibronyx is a rapidly growing startup focused on building future-ready supply chains using AI technologies and data analytics. We are helping our department of defense and commercial clients solve some of the toughest digital supply chain challenges. \n \n The value that you will bring:  As a Data Scientist, you will help in the development of predictive technologies that will power the next generation of supply chains. Your abilities to apply machine learning and artificial neural network methodologies will underly our customers\u2019 predictive capabilities. You will be using your coding skills to discover features and train models. You will be working on teams on multiple projects concurrently that involve cross-functional disciplines across customer and internal organizations \n \n \n What will you bring to the role: \n  Apply analytical thinking and curiosity to build analytical solutions \n Communication skills that enable you to tell the story of the data \n Willingness to learn, be coached and take feedback \n Be able to embrace a prototype and fail-fast culture \n Consultative approach to problem solving with clients and internal teams \n \n \n Job Description and Duties: \n  Collaborate with various stakeholders to understand requirements, and translate those requirements into technical solutions \n Develop templates, design, develop, and integrate data and models for repeatability \n Stay current on new technologies and methods across data science and analytics, data engineering, and data visualization, share best practices to improve technical capabilities of the team \n Identify appropriate data sources to answer business question \n Explore, identify and evaluate features and models \n Extract, transform, cleanse, and organize and standardize data to make data usable for machine learning and neural networks \n Visualize accuracy and relationships within data \n Engage in best practices discussions with other team members, discuss project activities and results with team \n Build documentation and presentations of project results \n Communicate results of analyses and associated business benefits to business partners and executives including the application of statistical exploratory data analysis to communicate value of data \n \n \n Skills/Qualifications: \n  1 to 3+ years of technical experience with relevant analytics programming languages, frameworks, and visualization tools \n Ability to collect and analyze complex data \n Must be able to organize and prioritize work to meet multiple deadlines \n Must be able to communicate effectively in both oral and written form \n Must have strong time management skills \n Curiosity, learning ambition and collaborative behavior essential to this role. \n Must have demonstrated experience in working with relational databases and advanced SQL techniques \n Must have demonstrated experience in feature discovery and engineering \n Must have demonstrated experience in communicating relationships in data using statistical modeling \n Must have demonstrated experience in advanced Classification and Regression machine learning algorithms \n Must have demonstrated experience in Natural Language Processing technologies \n Must have demonstrated experience in Deep Learning technologies \n Must have demonstrated experience in Python data analysis frameworks and visualization including Pandas, Seaborn/Matplotlib, Jupyter and Keras/Tensorflow \n Must have demonstrated experience in dealing with imbalanced data \n Must have demonstrated experience in Git version control \n Showcasing your data science skills and experience through public repos is a plus \n Familiarity with AutoML tools such as PyCaret is a plus \n Knowledge of NoSQL and Graph databases is a plus \n Knowledge and familiarity with JSON, Protocol Buffers is a plus \n Knowledge and familiarity with Windows operating systems. Familiarity with Linux is a plus. \n Working knowledge of a virtual machines in a cloud environment like AWS \n Regular, predictable, full attendance is an essential function of the job \n Must be a U.S. Citizen or authorized to work in the U.S. without sponsorship and able to pass government security process to work within government systems \n \n \n Preferred Experience: \n  Experience with supply chain management data and technology is desirable (e.g. ERP, Transportation Management and Warehouse Management systems) \n Priority for current or recent DoD Common Access Card holders \n \n \n Physical Demands: \n  Willingness to travel as necessary, work the required schedule, work at the specific location required \n \n \n Education: \n  Bachelor\u2019s degree required, concentration in Data Analytics, Engineering, Operations Research, Statistics, Applied Math, Computer Science, or related quantitative certifications \n At Vibronyx \u2013 the ability to help our clients, colleagues and communities flourish is driven by the diversity of thought, talent, interests, and experiences of our valued team members. We don\u2019t just accept these differences, we cherish them. Vibronyx is proud to be an equal opportunity workplace and is an affirmative action employer. We invite you to join us and feel the VIBES: https://vibronyx.com/about/",
        "cleaned_desc": "  Apply analytical thinking and curiosity to build analytical solutions \n Communication skills that enable you to tell the story of the data \n Willingness to learn, be coached and take feedback \n Be able to embrace a prototype and fail-fast culture \n Consultative approach to problem solving with clients and internal teams \n \n \n Job Description and Duties: \n  Collaborate with various stakeholders to understand requirements, and translate those requirements into technical solutions \n Develop templates, design, develop, and integrate data and models for repeatability \n Stay current on new technologies and methods across data science and analytics, data engineering, and data visualization, share best practices to improve technical capabilities of the team \n Identify appropriate data sources to answer business question \n Explore, identify and evaluate features and models \n Extract, transform, cleanse, and organize and standardize data to make data usable for machine learning and neural networks   Visualize accuracy and relationships within data \n Engage in best practices discussions with other team members, discuss project activities and results with team \n Build documentation and presentations of project results \n Communicate results of analyses and associated business benefits to business partners and executives including the application of statistical exploratory data analysis to communicate value of data \n \n \n Skills/Qualifications: \n  1 to 3+ years of technical experience with relevant analytics programming languages, frameworks, and visualization tools \n Ability to collect and analyze complex data \n Must be able to organize and prioritize work to meet multiple deadlines \n Must be able to communicate effectively in both oral and written form \n Must have strong time management skills \n Curiosity, learning ambition and collaborative behavior essential to this role. \n Must have demonstrated experience in working with relational databases and advanced SQL techniques   Must have demonstrated experience in feature discovery and engineering \n Must have demonstrated experience in communicating relationships in data using statistical modeling \n Must have demonstrated experience in advanced Classification and Regression machine learning algorithms \n Must have demonstrated experience in Natural Language Processing technologies \n Must have demonstrated experience in Deep Learning technologies \n Must have demonstrated experience in Python data analysis frameworks and visualization including Pandas, Seaborn/Matplotlib, Jupyter and Keras/Tensorflow \n Must have demonstrated experience in dealing with imbalanced data \n Must have demonstrated experience in Git version control \n Showcasing your data science skills and experience through public repos is a plus \n Familiarity with AutoML tools such as PyCaret is a plus \n Knowledge of NoSQL and Graph databases is a plus \n Knowledge and familiarity with JSON, Protocol Buffers is a plus \n Knowledge and familiarity with Windows operating systems. Familiarity with Linux is a plus. \n Working knowledge of a virtual machines in a cloud environment like AWS ",
        "techs": [
            "analytical thinking",
            "communication skills",
            "curiosity",
            "prototype culture",
            "problem solving",
            "collaboration",
            "technical solutions",
            "templates",
            "data integration",
            "data science",
            "data analytics",
            "data engineering",
            "data visualization",
            "best practices",
            "data sources",
            "feature and model evaluation",
            "data transformation",
            "data cleansing",
            "data organization",
            "data standardization",
            "machine learning",
            "neural networks",
            "data visualization",
            "statistical exploratory data analysis",
            "analytics programming languages",
            "frameworks",
            "visualization tools",
            "complex data analysis",
            "time management",
            "relational databases",
            "advanced sql techniques",
            "feature discovery",
            "statistical modeling",
            "machine learning algorithms",
            "natural language processing technologies",
            "deep learning technologies",
            "python data analysis frameworks",
            "data visualization tools",
            "imbalanced data",
            "git version control",
            "automl tools",
            "nosql databases",
            "graph databases",
            "json",
            "protocol buffers",
            "windows operating systems",
            "cloud environment",
            "aws"
        ],
        "cleaned_techs": [
            "analytical thinking",
            "curiosity",
            "prototype culture",
            "problem solving",
            "collaboration",
            "technical solutions",
            "templates",
            "data integration",
            "data science",
            "data analytics",
            "data visualization",
            "data sources",
            "feature and model evaluation",
            "data transformation",
            "data cleansing",
            "data organization",
            "data standardization",
            "neural networks",
            "statistical exploratory data analysis",
            "analytics programming languages",
            "frameworks",
            "visualization tools",
            "complex data analysis",
            "time management",
            "relational databases",
            "advanced sql techniques",
            "feature discovery",
            "statistical modeling",
            "machine learning algorithms",
            "nlp",
            "deep learning technologies",
            "python",
            "data visualization tools",
            "imbalanced data",
            "git version control",
            "automl tools",
            "nosql",
            "graph databases",
            "json",
            "protocol buffers",
            "windows operating systems",
            "cloud environment",
            "aws"
        ]
    },
    "b313953a37e916a3": {
        "terms": [
            "data science",
            "data engineer"
        ],
        "salary_min": 99481.61,
        "salary_max": 125965.89,
        "title": "TSS Data Engineer",
        "company": "General Dynamics Information Technology",
        "desc": "Clearance Level None Category Data Science Location Remote, Working from District of Columbia \n \n \n Public Trust:  BI Full 6C (T4)   \n Requisition Type:  Pipeline   \n Your Impact  \n Own your opportunity to manage the network that makes mission success possible. Make an impact by using your skills to deliver \u201cOne GDIT Network\u201d for our clients. \n  Job Description \n \n  Deliver simple solutions to complex problems as a Data Engineer at GDIT. Here, you\u2019ll tailor cutting-edge solutions to the unique requirements of our clients. With a career in application development, you\u2019ll make the end user\u2019s experience your priority and we\u2019ll make your career growth ours. \n \n  At GDIT, people are our differentiator. As a Data Engineer you will help ensure today is safe and tomorrow is smarter. Our work depends on a Data Engineer joining our team to help improve Aviation safety in collaboration with our stakeholders. Our customers deal with real-world safety problems and this candidate can help develop the required software designs and code to scale our system and improve user insights into safety issues.  Applies fundamental concepts, processes, practices, and procedures on technical assignments. Performs work that requires practical experience and training. Work is performed under supervision. \n  HOW A SOFTWARE ENGINEER WILL MAKE AN IMPACT: \n \n  Codes, tests, debugs, implements, and documents low to highly complex programs. \n  Creates appropriate documentation in work assignments such as program code, and technical documentation. \n  Designs systems and programs to meet complex business needs. \n  Prepares detailed specifications from which programs are developed and coded. \n  Ensures programs meet standards and technical specifications; performs technical analysis and component delivery. \n  Gathers information from existing systems, analyzes program and time requirements. \n  Assists project manager in preparing time estimates and justification for assigned tasks. \n  Designs programs for projects or enhancements to existing programs. Writes specifications for programs of low to advanced complexity. \n  Assists support and/or project personnel in resolving varying levels of complex program problems. \n  Works with client and management to resolve issues and validate programming requirements within their areas of responsibility. \n  Provides technical advice on complex programming. \n  Develops test plans to verify logic of new or modified programs. \n  Conducts quality assurance activities such as peer reviews. \n  Creates appropriate documentation in work assignments such as program code, and technical documentation. \n  Remains abreast of industry technical trends and new development to maintain current skills and remain current with industry standards. \n \n \n \n Education and Required Experience: Bachelor\u2019s Degree 2+ years experience \n Required Technical Skills: AWS Cloud Engineer \n Security Clearance Level: Public Trust \n Required Skills and Abilities : AWS Glue, AWS Lambda, Python, JIRA \n Location: Remote \n US Citizenship Required \n \n \n  Preferred Skills   \n \n Preferred Skills: AWS Athena, AWS CI/CD pipeline \n  Secret Clearance or ability to obtain a clearance \n  Home location of DMV \n \n \n  GDIT IS YOUR PLACE: \n \n \n Full-flex work week to own your priorities at work and at home \n 401K with company match \n Comprehensive health and wellness packages \n Internal mobility team dedicated to helping you own your career \n Professional growth opportunities including paid education and certifications \n Cutting-edge technology you can learn from \n Rest and recharge with paid vacation and holidays \n \n \n \n  GDIT IS YOUR PLACE: \n \n \n Full-flex work week to own your priorities at work and at home \n 401K with company match \n Comprehensive health and wellness packages \n Internal mobility team dedicated to helping you own your career \n Professional growth opportunities including paid education and certifications \n Cutting-edge technology you can learn from \n Rest and recharge with paid vacation and holidays",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "902875be8fce6ca2": {
        "terms": [
            "data science",
            "data engineer"
        ],
        "salary_min": 117281.51,
        "salary_max": 148504.52,
        "title": "TSS Data Engineer Advisor",
        "company": "General Dynamics Information Technology",
        "desc": "Clearance Level None Category Data Science Location Remote, Working from District of Columbia \n \n \n Public Trust:  BI Full 6C (T4)   \n Requisition Type:  Pipeline   \n Your Impact  \n Own your opportunity to manage the network that makes mission success possible. Make an impact by using your skills to deliver \u201cOne GDIT Network\u201d for our clients. \n  Job Description \n \n  Deliver simple solutions to complex problems as a Data Engineer Advisor at GDIT. Here, you\u2019ll tailor cutting-edge solutions to the unique requirements of our clients. With a career in application development, you\u2019ll make the end user\u2019s experience your priority and we\u2019ll make your career growth ours.    At GDIT, people are our differentiator. As a Data Engineer Advisor, you will help ensure today is safe and tomorrow is smarter. Our work depends on Data Engineer Advisor joining our team to to help improve Aviation safety in collaboration with our stakeholders. Our customers deal with real-world safety problems and this candidate can help develop the required software designs and code to scale our system and improve user insights into safety issues.     HOW A DATA ENGINEER ADVISOR WILL MAKE AN IMPACT: \n \n  Possesses and applies expertise on multiple complex work assignments. Assignments may be broad in nature, requiring originality and innovation in determining how to accomplish tasks. Contributes to deliverables and performance metrics where applicable. \n  Codes, tests, debugs, implements, and documents low to highly complex programs. \n  Creates appropriate documentation in work assignments such as program code, and technical documentation. \n  Designs systems and programs to meet complex business needs. \n  Prepares detailed specifications from which programs are developed and coded. \n  Ensures programs meet standards and technical specifications; performs technical analysis and component delivery. \n  Gathers information from existing systems, analyzes program and time requirements. \n  Assists project manager in preparing time estimates and justification for assigned tasks. Designs programs for projects or enhancements to existing programs. \n  Writes specifications for programs of low to advanced complexity. \n  Assists support and/or project personnel in resolving varying levels of complex program problems. \n  Works with client and management to resolve issues and validate programming requirements within their areas of responsibility. \n  Provides technical advice on complex programming. \n  Develops test plans to verify logic of new or modified programs. \n  Conducts quality assurance activities such as peer reviews. \n  Creates appropriate documentation in work assignments such as program code, and technical documentation. \n  Remains abreast of industry technical trends and new development to maintain current skills and remain current with industry standards. \n  Designs, develops, evaluates, plans and tests engineering specifications for software programs and applications \n \n  WHAT YOU\u2019LL NEED TO SUCCEED: \n \n \n Education and Required Experience: Bachelor\u2019s Degree + 8 years experience \n Required Technical Skills: AWS Cloud Engineer \n Security Clearance Level: Public Trust \n Required Skills and Abilities : AWS Glue, AWS Lambda, Python, JIRA \n Location: Remote \n US Citizenship Required \n \n \n  Preferred Skills   \n \n Preferred Skills: AWS Athena, AWS CI/CD pipeline \n  Secret Clearance or ability to obtain a clearance \n  Home location of DMV \n \n \n  GDIT IS YOUR PLACE: \n \n \n Full-flex work week to own your priorities at work and at home \n 401K with company match \n Comprehensive health and wellness packages \n Internal mobility team dedicated to helping you own your career \n Professional growth opportunities including paid education and certifications \n Cutting-edge technology you can learn from \n Rest and recharge with paid vacation and holidays",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "22869e19c59b9a1e": {
        "terms": [
            "data science"
        ],
        "salary_min": 111876.664,
        "salary_max": 141660.78,
        "title": "Sr Data Scientist",
        "company": "Citizens",
        "desc": "Description \n  Citizens Financial Group, Inc. (CFG) seeks a Senior Data Scientist for its Johnston, RI location. \n  Duties: Develops and implements highly-performant models using a variety of tools. Build relationships with the business line to access and interpret data sources, and identify process improvement opportunities. Applies quantitative techniques, including exploratory data analysis, regression analysis, supervised and unsupervised learning, feature engineering, statistical sampling, and data visualization, to convert business issues into machine learning analytic problems and create solutions. Documents and communicates analytic insights for both technical and non- technical audiences, often within an Agile framework. Designs prototypes and performs ad hoc analysis for new analytical outputs like graphs and model scores. Prepares model documentation and responds to effective challenges from model validation, internal audit, and regulatory reviews. Oversees the tracking and execution of all external and internal validation findings related to vendor and internally developed models. Supports ad-hoc requests from the line of business by understanding, communicating, and managing model risks. \n  Requirements: Master\u2019s degree in Mathematics, Physics, Statistics, or related quantitative field and one (1) year of experience in the position offered or closely related position. Full term of experience must include: Utilizing relational database and statistical analysis programming languages and software, including SQL and Python, to extract high-volume and complex data to perform related analyses; Developing predictive models by implementing advanced data analysis and research techniques, including data quality checks, remedy techniques, statistical analysis, and inferences, data exploration analysis, data imputation, time series analysis, regression analysis, classification analysis, supervised and unsupervised learning techniques, formulating assumptions, and distilling business problems into analytical problems; Utilizing mathematical principles behind statistical methods to evaluate and interpret the results of predictive models reasonably; Utilizing advanced statistical programming using tools in Python to create complex reports and graphs to illustrate analysis; Conducting optimization tests, including feature selections, cross-validation, threshold selection, and hyperparameters tuning, using Python and data science tools; Leveraging big data and machine learning algorithms like ensemble trees and regression techniques to build models and measure model outcomes using key machine learning metrics, including AUC, average precision, precision and recall; and Writing, maintaining, and improving production code for models and automation; and Utilizing Python IDE and code version control tools like Github. \n  May telecommute from any U.S. location. \n  Direct applicants only. \n Some job boards have started using jobseeker-reported data to estimate salary ranges for roles. If you apply and qualify for this role, a recruiter will discuss accurate pay guidance. \n  Equal Employment Opportunity \n  At Citizens we value diversity, equity and inclusion, and treat everyone with respect and professionalism. Employment decisions are based solely on experience, performance, and ability. Citizens, its parent, subsidiaries, and related companies (Citizens) provide equal employment and advancement opportunities to all colleagues and applicants for employment without regard to age, ancestry, color, citizenship, physical or mental disability, perceived disability or history or record of a disability, ethnicity, gender, gender identity or expression (including transgender individuals who are transitioning, have transitioned, or are perceived to be transitioning to the gender with which they identify), genetic information, genetic characteristic, marital or domestic partner status, victim of domestic violence, family status/parenthood, medical condition, military or veteran status, national origin, pregnancy/childbirth/lactation, colleague\u2019s or a dependent\u2019s reproductive health decision making, race, religion, sex, sexual orientation, or any other category protected by federal, state and/or local laws. \n  Equal Employment and Opportunity Employer \n  Citizens is a brand name of Citizens Bank, N.A. and each of its respective affiliates. \n \n  Why Work for Us \n  At Citizens, you'll find a customer-centric culture built around helping our customers and giving back to our local communities. When you join our team, you are part of a supportive and collaborative workforce, with access to training and tools to accelerate your potential and maximize your career growth",
        "cleaned_desc": "  Duties: Develops and implements highly-performant models using a variety of tools. Build relationships with the business line to access and interpret data sources, and identify process improvement opportunities. Applies quantitative techniques, including exploratory data analysis, regression analysis, supervised and unsupervised learning, feature engineering, statistical sampling, and data visualization, to convert business issues into machine learning analytic problems and create solutions. Documents and communicates analytic insights for both technical and non- technical audiences, often within an Agile framework. Designs prototypes and performs ad hoc analysis for new analytical outputs like graphs and model scores. Prepares model documentation and responds to effective challenges from model validation, internal audit, and regulatory reviews. Oversees the tracking and execution of all external and internal validation findings related to vendor and internally developed models. Supports ad-hoc requests from the line of business by understanding, communicating, and managing model risks. \n  Requirements: Master\u2019s degree in Mathematics, Physics, Statistics, or related quantitative field and one (1) year of experience in the position offered or closely related position. Full term of experience must include: Utilizing relational database and statistical analysis programming languages and software, including SQL and Python, to extract high-volume and complex data to perform related analyses; Developing predictive models by implementing advanced data analysis and research techniques, including data quality checks, remedy techniques, statistical analysis, and inferences, data exploration analysis, data imputation, time series analysis, regression analysis, classification analysis, supervised and unsupervised learning techniques, formulating assumptions, and distilling business problems into analytical problems; Utilizing mathematical principles behind statistical methods to evaluate and interpret the results of predictive models reasonably; Utilizing advanced statistical programming using tools in Python to create complex reports and graphs to illustrate analysis; Conducting optimization tests, including feature selections, cross-validation, threshold selection, and hyperparameters tuning, using Python and data science tools; Leveraging big data and machine learning algorithms like ensemble trees and regression techniques to build models and measure model outcomes using key machine learning metrics, including AUC, average precision, precision and recall; and Writing, maintaining, and improving production code for models and automation; and Utilizing Python IDE and code version control tools like Github. ",
        "techs": [
            "sql",
            "python",
            "python ide",
            "github"
        ],
        "cleaned_techs": [
            "sql",
            "python",
            "github"
        ]
    },
    "1fcbce8d8b3782e6": {
        "terms": [
            "data science"
        ],
        "salary_min": 109355.266,
        "salary_max": 138468.12,
        "title": "Software Engineer",
        "company": "Infor",
        "desc": "General information \n       \n \n \n \n \n \n \n        Country \n        \n United States  \n \n \n \n        City \n        \n Remote Location  \n \n \n \n        Department \n        \n Development  \n \n \n \n        Job ID \n        \n 37182  \n \n \n \n \n \n \n \n \n \n       Description & Requirements \n       \n \n \n \n \n \n \n \n This Infor OS DevOps team develops automation and cloud infrastructure for key services in the product and provides solutions to improve the release cycle process and deliver our services with zero downtime as it is the backbone for Infor's products.  \n \n A Day in The Life Typically Includes: \n \n \n  Research and document tooling and process improvements. \n  Troubleshoot issues in appropriate timeframes and providing a reliable resolution report for future prevention or resolutions. \n  Communicate with development and operations teams to collaborate on delivering software to the cloud via infrastructure as code via Cloudformation and scripting via Python. \n  Research services and solutions and provide proof of concept designs for new architectures and service requirements. \n  Identify gaps in current solutions and processes and drive the initiative to close them. \n  Expand development pipelines to automate the delivery of new builds to development environments and ease that process for release cycles and deployment to higher environments. \n  Provide off-hour/weekend support as needed. \n \n \n  Required skills: \n \n \n  Experience with Python. Understand object-oriented programming and be able to develop classes, write scripts that take in arguments and flags. \n  Understand infrastructure as code and other git-first / noun as code principles.  \n Proficiency in git/source code management. Understanding of basic branching patterns, code reviews, and best practices. \n  Familiarity with Linux / Unix-like systems and their tooling. \n  Understanding of virtualization and container (Docker) technologies. \n  Legal authorization to work permanently in the United States for any employer without requiring a visa transfer or visa sponsorship. \n \n \n  Preferred Qualifications: \n \n  Experience with deployment procedures to deliver services to cloud environments. \n  Experience with developing CICD pipelines. \n  Familiarity with developing and maintaining usage monitoring/reporting, and other reporting that provide proactive insights into the health of our cloud resources. \n  Bonus points for working with microservices. \n \n \n  Location: US Remote (Dallas, TX; St. Paul, MN; Alpharetta, GA) \n \n \n \n \n \n \n \n \n \n         About Infor\n         \n \n \n  Infor is a global leader in business cloud software products for companies in industry specific markets. Infor builds complete industry suites in the cloud and efficiently deploys technology that puts the user experience first, leverages data science, and integrates easily into existing systems. Over 60,000 organizations worldwide rely on Infor to help overcome market disruptions and achieve business-wide digital transformation.\n         \n \n          For more information visit www.infor.com\n         \n \n \n \n \n \n \n \n         Our Values\n         \n \n \n  At Infor, we strive for an environment that is founded on a business philosophy called Principle Based Management\u2122 (PBM\u2122) and eight Guiding Principles: integrity, stewardship & compliance, transformation, principled entrepreneurship, knowledge, humility, respect, self-actualization. Increasing diversity is important to reflect our markets, customers, partners, and communities we serve in now and in the future.\n         \n \n \n  We have a relentless commitment to a culture based on PBM. Informed by the principles that allow a free and open society to flourish, PBM\u2122 prepares individuals to innovate, improve, and transform while fostering a healthy, growing organization that creates long-term value for its clients and supporters and fulfillment for its employees.\n         \n \n \n  Infor is an Equal Opportunity Employer. We are committed to creating a diverse and inclusive work environment. Infor does not discriminate against candidates or employees because of their sex, race, gender identity, disability, age, sexual orientation, religion, national origin, veteran status, or any other protected status under the law.\n          \n \n \n \n \n         At Infor we value your privacy that\u2019s why we created a policy that you can read here.\n         \n \n \n \n \n \n \n \n \n         This employer uses E-Verify. Please visit the following website for additional information: www.kochcareers.com/doc/Everify.pdf",
        "cleaned_desc": "  Research and document tooling and process improvements. \n  Troubleshoot issues in appropriate timeframes and providing a reliable resolution report for future prevention or resolutions. \n  Communicate with development and operations teams to collaborate on delivering software to the cloud via infrastructure as code via Cloudformation and scripting via Python. \n  Research services and solutions and provide proof of concept designs for new architectures and service requirements. \n  Identify gaps in current solutions and processes and drive the initiative to close them. \n  Expand development pipelines to automate the delivery of new builds to development environments and ease that process for release cycles and deployment to higher environments. \n  Provide off-hour/weekend support as needed. \n \n \n  Required skills: \n \n \n  Experience with Python. Understand object-oriented programming and be able to develop classes, write scripts that take in arguments and flags. \n  Understand infrastructure as code and other git-first / noun as code principles.  \n Proficiency in git/source code management. Understanding of basic branching patterns, code reviews, and best practices. \n  Familiarity with Linux / Unix-like systems and their tooling. \n  Understanding of virtualization and container (Docker) technologies. \n  Legal authorization to work permanently in the United States for any employer without requiring a visa transfer or visa sponsorship. \n \n \n  Preferred Qualifications: \n \n  Experience with deployment procedures to deliver services to cloud environments. \n  Experience with developing CICD pipelines. \n  Familiarity with developing and maintaining usage monitoring/reporting, and other reporting that provide proactive insights into the health of our cloud resources. \n  Bonus points for working with microservices. ",
        "techs": [
            "cloudformation",
            "python",
            "git",
            "linux",
            "docker",
            "cicd"
        ],
        "cleaned_techs": [
            "cloudformation",
            "python",
            "git",
            "linux",
            "docker",
            "cicd"
        ]
    },
    "eb932c63bd20b7df": {
        "terms": [
            "data science"
        ],
        "salary_min": 150000.0,
        "salary_max": -1.0,
        "title": "Chief Data Scientist",
        "company": "Parts iD LLC",
        "desc": "About Us: \n PARTS iD, LLC is a dynamic and innovative organization at the forefront of [industry/sector], dedicated to delivering cutting-edge solutions that drive data-driven decision-making. We are seeking a visionary Chief Data Scientist to lead our data science team and play a pivotal role in shaping the future of our company. \n Job Description: \n As the Chief Data Scientist, you will be responsible for providing strategic leadership in data science, transforming data into actionable insights, and driving innovation across the organization. You will be instrumental in developing and implementing data-driven strategies that have a profound impact on our business. \n Key Responsibilities: \n Team Leadership:  Lead, mentor, and inspire a team of data scientists and analysts. Foster a culture of collaboration and continuous learning. \n Data Strategy:  Develop and execute a comprehensive data strategy, aligning it with the company's overall business objectives. Identify opportunities to leverage data for competitive advantage. \n Innovation:  Drive innovation by staying abreast of the latest developments in data science, machine learning, and artificial intelligence. Experiment with new technologies and methodologies. \n Data Modeling:  Create predictive and prescriptive models using advanced statistical and machine learning techniques to solve complex business problems. \n Data Analytics:  Lead the analysis of large datasets to extract actionable insights. Develop and implement data-driven solutions that enhance decision-making processes. \n Data Governance:  Ensure data quality, security, and compliance with data protection regulations. Establish and enforce best practices for data management. \n Collaboration:  Collaborate with cross-functional teams, including software developers, product managers, and domain experts, to develop and deploy data-driven solutions. \n Reporting:  Create and deliver clear and concise reports and presentations to convey complex data findings to non-technical stakeholders. \n Resource Management:  Oversee budget allocation, resource planning, and project management for the data science team. \n Continuous Improvement:  Promote a culture of continuous improvement and optimization in data science methodologies and practices. \n Qualifications: \n \n A Ph.D. or master's degree in a quantitative field (e.g., Computer Science, Statistics, Mathematics) or equivalent work experience. \n Experience with Machine Learning (ML) and Artificial Intelligence (AI) \n Proven experience in a leadership role in data science or a related field. \n Strong proficiency in programming languages such as Python or R. \n Extensive experience with machine learning, data modeling, and statistical analysis. \n Knowledge of big data technologies and tools. \n Excellent communication skills to convey complex concepts to non-technical stakeholders. \n Strong problem-solving and critical thinking abilities. \n Ability to manage and inspire a diverse team of data professionals. \n \n Benefits: \n \n Competitive salary and bonus structure \n Health, dental, and vision insurance \n Retirement savings plan \n Professional development opportunities \n Collaborative and innovative work environment \n \n If you are passionate about harnessing the power of data to drive business success and possess the leadership skills to inspire a team, we encourage you to apply for the Chief Data Scientist role at PARTS iD, LLC. \n Job Type: Full-time \n Pay: From $150,000.00 per year \n Benefits: \n \n Paid time off \n Vision insurance \n \n Compensation package: \n \n Bonus opportunities \n \n Experience level: \n \n 10 years \n \n Schedule: \n \n 8 hour shift \n \n Work Location: Remote",
        "cleaned_desc": "About Us: \n PARTS iD, LLC is a dynamic and innovative organization at the forefront of [industry/sector], dedicated to delivering cutting-edge solutions that drive data-driven decision-making. We are seeking a visionary Chief Data Scientist to lead our data science team and play a pivotal role in shaping the future of our company. \n Job Description: \n As the Chief Data Scientist, you will be responsible for providing strategic leadership in data science, transforming data into actionable insights, and driving innovation across the organization. You will be instrumental in developing and implementing data-driven strategies that have a profound impact on our business. \n Key Responsibilities: \n Team Leadership:  Lead, mentor, and inspire a team of data scientists and analysts. Foster a culture of collaboration and continuous learning. \n Data Strategy:  Develop and execute a comprehensive data strategy, aligning it with the company's overall business objectives. Identify opportunities to leverage data for competitive advantage. \n Innovation:  Drive innovation by staying abreast of the latest developments in data science, machine learning, and artificial intelligence. Experiment with new technologies and methodologies. \n Data Modeling:  Create predictive and prescriptive models using advanced statistical and machine learning techniques to solve complex business problems. \n Data Analytics:  Lead the analysis of large datasets to extract actionable insights. Develop and implement data-driven solutions that enhance decision-making processes. \n Data Governance:  Ensure data quality, security, and compliance with data protection regulations. Establish and enforce best practices for data management.   Collaboration:  Collaborate with cross-functional teams, including software developers, product managers, and domain experts, to develop and deploy data-driven solutions. \n Reporting:  Create and deliver clear and concise reports and presentations to convey complex data findings to non-technical stakeholders. \n Resource Management:  Oversee budget allocation, resource planning, and project management for the data science team. \n Continuous Improvement:  Promote a culture of continuous improvement and optimization in data science methodologies and practices. \n Qualifications: \n \n A Ph.D. or master's degree in a quantitative field (e.g., Computer Science, Statistics, Mathematics) or equivalent work experience. \n Experience with Machine Learning (ML) and Artificial Intelligence (AI) \n Proven experience in a leadership role in data science or a related field. \n Strong proficiency in programming languages such as Python or R. \n Extensive experience with machine learning, data modeling, and statistical analysis. ",
        "techs": [
            "machine learning",
            "artificial intelligence",
            "python",
            "r"
        ],
        "cleaned_techs": [
            "ai",
            "python",
            "r"
        ]
    },
    "bd3d3b81ebfe258f": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": 82800.0,
        "salary_max": 106425.0,
        "title": "Machine Learning Engineer (Remote)",
        "company": "Vail Resorts",
        "desc": "As a leading mountain resort operator with over 40 resorts in sixteen states and four countries. We exist to create an  Experience of a Lifetime  for our employees, so they can, in turn, provide and  Experience of a Lifetime  for our guests. We are looking for leaders, innovators, creators, and ambitious professionals to join our talented team. If you\u2019re ready to pursue your fullest potential, we want to get to know you! \n \n  Many of our Corporate function teams can now live and work in any of the states in which Vail Resorts currently operates* \u2013 enabling flexible remote work alongside a commitment to building and maintaining strong culture both in person and virtually. If you\u2019re ready to pursue your fullest potential, we want to get to know you. Find your purpose with us at www.vailresortscareers.com. \n \n  Job Summary: \n  The Data Science & Data Engineering team is responsible for using our internal data assets to deliver performant, scalable, and impactful data-focused solutions across the business! The team supports modeling initiatives in collaboration with our technology and business partners to enable both strategic opportunities as well as guest facing initiatives. As we continue to grow, our objectives are to deliver increased performance of our existing portfolio of in- market models, granularity of algorithmic decision making, and scalability of solutions across our core lines of business. \n \n  As a Machine Learning Engineer, you will work on the end-to-end architecture and deployment of data processing, model execution, and delivery of our team\u2019s machine learning solutions to activation and decision endpoints across the organization. Additionally, you will help assist in the integration and democratization of new technologies that bring scale, agility, and efficiency to the way we work with data. \n \n  Job Specifications:  \n \n Shift & Schedule Availability: Full Time / Year Round \n  Outlet: Corporate \n  Expected Pay Range: $82,800 - $106,425 \n  Other Specifics: Remote \n \n \n  Job Responsibilities: \n \n  Work on the architecture and development of machine learning engineering projects that advance our enterprise goals of realizing data-driven impact through delivering intelligent solutions across our organization \n  Support scalable machine learning solutions that positively impact marketing campaigns, operational excellence, and the guest experience \n  Partner with senior teammates in the model development life cycle \n  Contribute to core infrastructure, libraries, and documentation \n  Other duties as assigned \n \n \n  Job Requirements: \n \n  Awareness of how to develop and automate scalable machine learning pipelines in cloud ecosystems (Azure/AWS/GCP) \n  Knowledge of data warehousing systems, data systems integration, and cloud data solutions including data lakes or managed services like Snowflake or Databricks \n  Some experience with Data Science and Machine Learning development tools and processes (TensorFlow, PyTorch, scikit-learn, MLFlow, etc.) \n  Competent in Python and SQL across a variety of platforms \n  Familiarity with Linux environments and maintaining packages and software versions \n  Working knowledge of common development tools like git and docker \n  Ability to develop clear documentation and business partner management across audiences with multiple  levels of technical skills \n  Eagerness to learn new skills \n \n \n  The expected Total Compensation for this role is $82,800 - $106,425. Individual compensation decisions are based on a variety of factors. \n  The perks include a free ski pass, and a set of benefits including... \n \n  Medical, Dental, Vision insurance, and a 401(k) retirement plan \n  Hourly employees are generally eligible for accrued Paid Time Off (PTO) and Sick Time. Salaried employees are generally eligible for Flexible Time Off (FTO) \n  Paid Parental Leave for eligible mothers and fathers \n  Healthcare & Dependent Care Flexible Spending Accounts \n  Life, AD&D, and disability insurance \n \n \n  Reach Your Peak at Vail Resorts.  At Vail Resorts, our team is made whole by the brave, passionate individuals who ambitiously push boundaries and challenge the status quo. Whether you\u2019re looking for seasonal work or the career of a lifetime, join us today to reach your peak. \n \n   Remote work is currently permitted from British Columbia and the 16 U.S. states in which we currently operate. This includes: California, Colorado, Indiana, Michigan, Minnesota, Missouri, New Hampshire, New York, Nevada, Ohio, Pennsylvania, Utah, Vermont, Washington State, Wisconsin, and Wyoming. Please note that the ability to work remotely, and the particulars related to such work, are subject to change at any time; and, accordingly, the Company reserves the right to change its policies and/or require in-person/in-office work at any time in its sole discretion. \n \n \n  Vail Resorts is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veteran status or any other status protected by applicable law. \n \n  Requisition ID 497801   Reference Date: 08/31/2023   Job Code Function: Marketing",
        "cleaned_desc": "  Contribute to core infrastructure, libraries, and documentation \n  Other duties as assigned \n \n \n  Job Requirements: \n \n  Awareness of how to develop and automate scalable machine learning pipelines in cloud ecosystems (Azure/AWS/GCP) \n  Knowledge of data warehousing systems, data systems integration, and cloud data solutions including data lakes or managed services like Snowflake or Databricks \n  Some experience with Data Science and Machine Learning development tools and processes (TensorFlow, PyTorch, scikit-learn, MLFlow, etc.) \n  Competent in Python and SQL across a variety of platforms \n  Familiarity with Linux environments and maintaining packages and software versions ",
        "techs": [
            "azure",
            "aws",
            "gcp",
            "snowflake",
            "databricks",
            "tensorflow",
            "pytorch",
            "scikit-learn",
            "mlflow",
            "python",
            "sql",
            "linux"
        ],
        "cleaned_techs": [
            "azure",
            "aws",
            "gcp",
            "snowflake",
            "databricks",
            "tensorflow",
            "pytorch",
            "scikit-learn",
            "mlflow",
            "python",
            "sql",
            "linux"
        ]
    },
    "680d271d91230548": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": 139925.48,
        "salary_max": 177176.86,
        "title": "Product Lead, Support Agent AI Assistant",
        "company": "Cash App",
        "desc": "Company Description\n   It all started with an idea at Block in 2013. Initially built to take the pain out of peer-to-peer payments, Cash App has gone from a simple product with a single purpose to a dynamic ecosystem, developing unique financial products, including Afterpay/Clearpay, to provide a better way to send, spend, invest, borrow and save to our 47 million monthly active customers. We want to redefine the world\u2019s relationship with money to make it more relatable, instantly available, and universally accessible.    Today, Cash App has thousands of employees working globally across office and remote locations, with a culture geared toward innovation, collaboration and impact. We\u2019ve been a distributed team since day one, and many of our roles can be done remotely from the countries where Cash App operates. No matter the location, we tailor our experience to ensure our employees are creative, productive, and happy.    Check out our locations, benefits, and more at cash.app/careers. \n \n \n \n Job Description\n   Cash App\u2019s top strategic business priority is trust. Building deep trust with our customers allows us to deepen our financial relationship with them, introduce them to new products, increase our share of wallet for each service, and expand our customer base. \n  We want customer support to be a key reason why people trust Cash App with their money. We want to build lasting trust with customers by making it easier to get help that resolves their issues. As a Product Manager for the Support Agent AI Assistant team, you will be responsible for resolving more customer issues by acting as the AI sidekick to agents: transforming workflows, information, and responses based on the customer issue at the time of the issue. \n  You Will: \n \n  Define the strategy, roadmap, and prioritization for AI assistance for agents, across the entire journey: determining what a customer needs help with, what to do, how to respond, and how to improve \n  Coordinate actions with a dedicated product team of a Designer, Client Engineers, Server Engineers, Machine Learning Modelers, Machine Learning Engineers, and a Data Scientist \n  Deeply understand the needs of agents and customers and act as their voice through planning, design and execution \n  Set the pace for the team by effectively driving clarity and alignment through excellent communication and product documentation \n  See the future of AI-assisted agent support in vivid color and keep the team\u2019s ambition high along the journey \n \n \n \n \n Qualifications\n   You Have: \n \n  8+ years of product management or equivalent experience \n  Possess both expertise and passion in AI - you\u2019re following all the latest improvements and testing it yourself \n  Shown high agency and attention to detail driving a product team forward daily \n  Defined and owned a roadmap without much guidance \n  Displayed strong customer empathy and experience shaping product direction and execution based on customer needs \n  Resilience and experience driving cross-functional product initiatives in a successful way \n  A hunger to tackle complicated and impactful product & business problems \n  Appreciation for product design and the customer experience \n  Strong strategic thinking, problem solving, and logical structuring abilities \n  Comfort with ambiguity; the ability to independently lay out and test clear hypotheses, and solve problems without well-defined direction \n  Bonus: You\u2019ve integrated AI within customer support \n \n  Additional Information\n   Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate\u2019s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.    Zone A: USD $202,500 - USD $247,500  Zone B: USD $192,400 - USD $235,200  Zone C: USD $182,300 - USD $222,800  Zone D: USD $172,200 - USD $210,400 \n \n  To find a location\u2019s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information. \n  Full-time employee benefits include the following: \n \n  Healthcare coverage (Medical, Vision and Dental insurance) \n  Health Savings Account and Flexible Spending Account \n  Retirement Plans including company match \n  Employee Stock Purchase Program \n  Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance \n  Paid parental and caregiving leave \n  Paid time off (including 12 paid holidays) \n  Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees) \n  Learning and Development resources \n  Paid Life insurance, AD&D, and disability benefits \n  Additional Perks such as WFH reimbursements and free access to caregiving, legal, and discounted resources \n \n  These benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans. \n  We\u2019re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, veteran status, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. \n  We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible.  Want to learn more about what we\u2019re doing to build a workplace that is fair and square? Check out our  I+D page . \n  Additionally, we consider qualified applicants with criminal histories for employment on our team, assessing candidates in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance. \n \n \n  Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.",
        "cleaned_desc": "  8+ years of product management or equivalent experience \n  Possess both expertise and passion in AI - you\u2019re following all the latest improvements and testing it yourself \n  Shown high agency and attention to detail driving a product team forward daily \n  Defined and owned a roadmap without much guidance \n  Displayed strong customer empathy and experience shaping product direction and execution based on customer needs \n  Resilience and experience driving cross-functional product initiatives in a successful way \n  A hunger to tackle complicated and impactful product & business problems \n  Appreciation for product design and the customer experience \n  Strong strategic thinking, problem solving, and logical structuring abilities \n  Comfort with ambiguity; the ability to independently lay out and test clear hypotheses, and solve problems without well-defined direction \n  Bonus: You\u2019ve integrated AI within customer support ",
        "techs": [
            "product management",
            "ai",
            "product team",
            "roadmap",
            "customer empathy",
            "customer needs",
            "cross-functional product initiatives",
            "strategic thinking",
            "problem solving",
            "logical structuring",
            "ambiguity",
            "hypotheses",
            "ai integration in customer support"
        ],
        "cleaned_techs": [
            "product management",
            "ai",
            "product team",
            "roadmap",
            "customer empathy",
            "customer needs",
            "cross-functional product initiatives",
            "strategic thinking",
            "problem solving",
            "logical structuring",
            "ambiguity",
            "hypotheses"
        ]
    },
    "ce907e6c01f96b77": {
        "terms": [
            "data science"
        ],
        "salary_min": 66641.164,
        "salary_max": 84382.555,
        "title": "Business Systems Analyst 1 - Remote",
        "company": "Q\u00b2 Solutions",
        "desc": "JOB OVERVIEW \n  Under direct supervision, sets up studies and data integrations within sample tracking software systems, interfacing with internal and external stakeholders. Trains and assists end users with system usage. Performs system configuration and/or testing of configuration as directed.     RESPONSIBILITIES \n \n  Create sample plans and related data for setting up studies for study subject sample and consent tracking within software systems, working with study documents, study teams, and other internal and external stakeholders to gather and synthesize the necessary information. \n  Lead development of Data Transfer Specifications (DTSs), ensuring compliance to system and customer requirements. \n  Configuration and testing of custom data loaders needed to enable data ingestion per DTSs. \n  Perform data integration Quality Control checks to ensure data transfer compliance to DTSs. \n  Assist internal support and operational teams and external data providers in trouble shooting data ingestion errors. \n  Support study teams and other end users with project updates and system training presentation. \n  Assist in formulating and defining systems scope and objectives through research and fact-finding combined with a basic understanding of business systems and industry requirements. \n  Includes analysis of business and user needs, documenting requirements, and revising existing system logic difficulties as necessary under direction of experienced Business System Analysts. \n  Competent to consider most business implications of the application of technology to the current business environment. \n \n \n  MINIMUM REQUIRED EDUCATION AND EXPERIENCE \n \n  Bachelor's Degree in Computer Science, Data Science, or related domain; or equivalent experience required. \n  One (1) year experience in data or systems analysis required. \n  Or equivalent combination of education, training, and experience. \n \n  REQUIRED KNOWLEDGE, SKILLS, AND ABILITIES \n \n  Knowledge of business-wide applications (e.g., third party software and internal operational applications etc.) or IQVIA client facing applications and products. \n  Demonstrated ability to understand client requirements as well as underlying infrastructure applications, systems, and processes to enable execution of those skills. \n  Familiarity or experience with laboratory work, clinical trials, or Software Development Lifecycle is preferred. \n \n \n  Q\u00b2 Solutions, IQVIA\u2019s laboratory business, creates connected intelligence by combining our expertise, technology and analytics - this fuels unparalleled research & development solutions. We uphold a deep commitment to patients, sites, customers, and each other. https://www.q2labsolutions.com/careers \n \n  We are committed to providing equal employment opportunities for all, including veterans and candidates with disabilities. https://jobs.iqvia.com/q2-solutions-eoe \n \n  Q\u00b2 Solutions\u2019 ability to operate and provide certain services to customers and partners necessitates Q\u00b2 Solutions and its employees meet specific requirements regarding COVID-19 vaccination status. https://jobs.iqvia.com/q2-solutions-covid-19-vaccine-status",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "2d69a6c49793b5d3": {
        "terms": [
            "data science"
        ],
        "salary_min": 60000.0,
        "salary_max": 85000.0,
        "title": "Benefits Specialist (Remote Eligible)",
        "company": "Mathematica Policy Research",
        "desc": "Position Description :    About Mathematica:      Mathematica applies expertise at the intersection of data, methods, policy, and practice to improve well-being around the world. We collaborate closely with public- and private-sector partners to translate big questions into deep insights that improve programs, refine strategies, and enhance understanding using data science and analytics. Our work yields actionable information to guide decisions in wide-ranging policy areas, from health, education, early childhood, and family support to nutrition, employment, disability, and international development. Mathematica offers our employees competitive salaries, and a comprehensive benefits package, as well as the advantages of being 100 percent employee owned. As an employee stock owner, you will experience financial benefits of ESOP holdings that have increased in tandem with the company\u2019s growth and financial strength. You will also be part of an independent, employee-owned firm that is able to define and further our mission, enhance our quality and accountability, and steadily grow our financial strength. Read more about our benefits here: https://www.mathematica.org/career-opportunities/benefits-at-a-glance.      About the opportunity:      Mathematica's Human Resources department is seeking a Benefits Specialist to support and administer leave requests under the Family and Medical Leave Act (FMLA), parental leave, personal leave, workers-compensation, short-term or long-term disability plans in conjunction with state and local leave laws.      Responsibilities:     \n \n Manages HR Benefits Mailbox and responds to benefits inquiries from managers and employees on leave and disability \n  Provides staff guidance regarding coordination of benefits, eligibility notices, timesheets, PTO, benefit deductions, and parental leave \n  Maintains leave related internal guidance documents \n  Completes data entry for all leave related absences \n  Provides leave reporting to the payroll team on a semi-monthly cadence and ad-hoc leave reporting \n  Grants permissions to leave restricted timesheet codes \n  Plans, executes, and coordinates communication for leave law changes \n  Administers Parental Leave and Adoption benefit programs \n  Conducts bi-weekly claim review calls with The Hartford to manage the leave management partnership and act as an employee advocate for any claim issues \n  Identify and track claim related issues \n  Actively support the advancement of organizational diversity, equity, and inclusion efforts, and apply diversity, equity, and inclusion lens across job responsibilities. \n  Additional duties may be assigned as needed \n \n \n \n \n Position Requirements :     \n \n Bachelor\u2019s degree in Human Resources, Business Administration, or related degree (or equivalent experience) \n  Three plus years\u2019 experience in leave management administration \n  Proficiency with Microsoft Office, Excel, Word, PowerPoint, and Outlook \n  Working understanding of human resource and employee benefit principles, practices, and procedures \n  Ability to effectively communicate complex information in simple terms with all levels of employees and vendors \n  Ability to handle highly confidential information with discretion \n  High level of accuracy and attention to detail \n  Capable of multi-tasking, highly organized, and able to meet deadlines \n  The ability to manage work in a high-volume, fast-paced environment, and juggle multiple priorities \n  Excellent written and verbal communication skills \n  This position offers an anticipated annual base salary of $60,000 - $85,000. \n      \n  Available locations: Washington, DC; Princeton, NJ; Remote \n     \n \n \n We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "46ee18514f69cbca": {
        "terms": [
            "data science"
        ],
        "salary_min": 69500.0,
        "salary_max": 92500.0,
        "title": "Client Solutions Statistician",
        "company": "Wiland Inc",
        "desc": "Summary \n  The Client Solutions Statistician will work within a supportive team structure to provide client and market driven predictive solutions for their assigned Wiland clients. This individual will be the key point of contact and own all of the analytical decisions for assigned clients. The Statistician will consult with internal and external clients from original strategy to results review and will work closely with Client Services to communicate solutions and rationale to both internal and external audiences. Special projects and research will be necessary and assigned as appropriate.   \n   Job Responsibilities \n \n  Develop highly customized statistical models and product solutions to be leveraged by clients for prospecting and customer segmentation. \n  Create customized multi-model solutions to deliver larger ranked universes to clients. \n  Consult and deliver recommendations with Client Services regarding appropriate product usage, testing protocols and best practices. \n  Complete thorough results analysis including client-supplied results interrogation, performance simulations and promotion file analysis to estimate and validate outcomes. \n  Manage a book of medium to large size Wiland clients with a focus on delivering consistent or improved results for each marketing campaign.  \n Facilitate client meetings meant to organize, strategize and deliver impactful analysis in a proactive manner for Client Services. \n  Present analytical results both internally and to clients as needed, exhibiting knowledge of Wiland products and services as well as appropriate business and industry knowledge. \n \n  Education, Experience, & Qualifications \n \n  Master\u2019s Degree in a quantitative discipline (Statistics, Economics, Mathematics, or Research) or similar Bachelor\u2019s Degree with strong work experiences. Must be able to produce college transcripts showing minimum 3.0 GPA. \n  1+ year experience in an analytics role applying predictive modeling \n  1+ year experience in a consultative role or environment \n  Experience developing successful predictive models using regression analysis, decision trees, neural networks and other machine learning techniques in an academic setting or prior work \n  Excellent written and verbal communication skills \n  Ability to clearly discuss and present recommendations to internal and external clients \n  Exceptional problem-solving skills and ability to take initiative and work independently. \n  Proficient in Excel \n \n  Preferred: \n \n  2+ years experience with R, Python, or other related statistical software package including programming skills \n  Experience in database marketing, data mining, analysis and modeling \n  Experience with relational databases using SQL \n  Strong skills in data visualization \n \n  Additional Details \n \n  Location: Remote \n  Department: Predictive Client Solutions \n  Compensation Range: $69,500 - $92,500. Total compensation for this role also includes a variable component. \n  Our benefits include: Medical, Dental, and Vision Coverage, 401(k) Retirement Savings Plan with Company Matching, Flexible Time Off Program, Paid Company Holidays, Flexible Spending Accounts with Health and Dependent Care Options, Paid Parental Leave, Short- and Long-Term Disability Plans, Basic and Voluntary Life Insurance, and more.  \n Must be a US citizen or authorized to work in the United States. \n  Wiland is an equal opportunity employer. \n \n  Apply Now!  We want to learn more about you! We are seeking bright, exceptional professionals who are looking to join a growing, talented team of data-driven marketing innovators. Please submit your application using the button below.",
        "cleaned_desc": "  Master\u2019s Degree in a quantitative discipline (Statistics, Economics, Mathematics, or Research) or similar Bachelor\u2019s Degree with strong work experiences. Must be able to produce college transcripts showing minimum 3.0 GPA. \n  1+ year experience in an analytics role applying predictive modeling \n  1+ year experience in a consultative role or environment \n  Experience developing successful predictive models using regression analysis, decision trees, neural networks and other machine learning techniques in an academic setting or prior work \n  Excellent written and verbal communication skills \n  Ability to clearly discuss and present recommendations to internal and external clients \n  Exceptional problem-solving skills and ability to take initiative and work independently.    Proficient in Excel \n \n  Preferred: \n \n  2+ years experience with R, Python, or other related statistical software package including programming skills \n  Experience in database marketing, data mining, analysis and modeling \n  Experience with relational databases using SQL ",
        "techs": [
            "r",
            "python",
            "excel",
            "sql"
        ],
        "cleaned_techs": [
            "r",
            "python",
            "excel",
            "sql"
        ]
    },
    "a4df20cc4511292e": {
        "terms": [
            "data science"
        ],
        "salary_min": 59288.43,
        "salary_max": 75072.37,
        "title": "Market Research Analyst",
        "company": "CivicScience",
        "desc": "Do you dream about data? \n  Every day, CivicScience gathers opinions from millions of consumers on thousands of topics that provide high-velocity insights to leading brands such as Apple, Microsoft, Bank of America, McDonald's, and T-Mobile. If there's a question critical to the marketing strategy of the Global 2000, CivicScience has the data. \n  Position Overview \n  We\u2019re looking for a skilled market research analyst who is comfortable designing, executing, and analyzing quantitative consumer research. This position will report to a Research Supervisor and will work cooperatively with many internal teams as part of day-to-day duties. \n  Responsibilities \n \n  Collaborate with account managers, clients, and publisher partners to manage project execution from research objectives through project specifications, including design, data collection, analysis, report preparation, and presentation \n  Support client and publisher partners with onboarding, training, demonstration of new capabilities and best practices, along with day-to-day service \n  Assist with new business efforts, as needed \n  Contribute to ongoing internal process improvement and platform design \n \n  Qualifications \n \n  Bachelor\u2019s degree or equivalent in business, market research, data science, statistics, or social sciences \n  Familiarity with a broad range of market research methodologies \n  High comfort level with questionnaire design and question writing, self-directed analysis, project development, and management \n  Ability to write and clearly communicate research findings and recommendations, with a demonstrated record of successful market research report delivery \n  Expertise in Microsoft Office products and Google Workspace tools \n  Experience using statistical languages such as R, SQL, SPSS, and/or Q \n \n  Additional qualifications include a service-based mindset, careful attention to detail, intellectual curiosity, an energetic approach, and motivation to solve complex problems. \n  Benefits and Perks \n \n  Medical, dental, and vision insurance with premiums fully paid by the company \n  Flexible spending accounts for medical and dependent care \n  Life insurance \n  Short- and long-term disability \n  Employee assistance program \n  Unlimited PTO \n  401(k) \n  Charitable donation matching program \n  Educational opportunities \n  Travel opportunities \n \n  This is a remote (work-from-home) position. Compensation will vary, based on knowledge, skills, abilities, certifications, experience, bonus, and commissions, as guided by fair and equitable employment decisions for qualified candidates. \n  Working at CivicScience \n  CivicScience has a vibrant, high-energy work culture with ambitious, innovative, and forward-thinking team members. We consist of three business units - Media Partnerships, CivicScience Intelligence, and Advertising Solutions - that work together to fuel CivicScience's goals and objectives. \n  We've been recognized for our work in the Inc. 5000, Pittsburgh Top Workplaces,  Pittsburgh Business Times  Fast 50, GRIT Top 50 Innovative Suppliers, and the Pittsburgh Technology Council's Tech 50. \n  More about CivicScience \n  CivicScience provides a foundational source of truth to drive positive impact for humanity, brands, and media. \n  CivicScience is a privacy centric opinion analytics platform that translates real-time, consumer insights into market-changing business intelligence. In simple terms: we power the world's opinions and quickly deliver the data to the decision-makers who care. \n  We poll millions of Americans each week with questions related to thousands of topics, enabling people from all walks of life to be heard and informed. We then use our powerful technology - the InsightStore\u2122 - to analyze those opinions, discover market and cultural trends before they grow, and accurately predict future consumer behavior and market outcomes. It's consumer intelligence like you've never seen before - which is why thousands of marketers and market research professionals across all verticals trust us to answer their most pressing questions. \n  CivicScience is an Equal Opportunity Employer. \n   \n ifa7I7kg2E",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "7d1a9c8b0171bd03": {
        "terms": [
            "data science"
        ],
        "salary_min": 82291.516,
        "salary_max": 104199.39,
        "title": "Fraud Behavioral Insights, Card and Disputes Analyst",
        "company": "Cash App",
        "desc": "Company Description\n   It all started with an idea at Block in 2013. Initially built to take the pain out of peer-to-peer payments, Cash App has gone from a simple product with a single purpose to a dynamic ecosystem, developing unique financial products, including Afterpay/Clearpay, to provide a better way to send, spend, invest, borrow and save to our 47 million monthly active customers. We want to redefine the world\u2019s relationship with money to make it more relatable, instantly available, and universally accessible.    Today, Cash App has thousands of employees working globally across office and remote locations, with a culture geared toward innovation, collaboration and impact. We\u2019ve been a distributed team since day one, and many of our roles can be done remotely from the countries where Cash App operates. No matter the location, we tailor our experience to ensure our employees are creative, productive, and happy.    Check out our locations, benefits, and more at cash.app/careers. \n \n \n \n Job Description\n   Cash App is seeking a Fraud Behavioral Insights, Card and Disputes Analyst to join our dynamic Behavioral Insights Team. A Fraud Behavioral Insights, Card and Disputes Analyst leverages natural analytical and problem-solving skills to analyze large structured and unstructured data sets to identify trends, anomalies and inauthentic behaviors across Cash App accounts and products. This position relies on decisions made independently, with high precision and attention to detail. \n  This position will collaborate often with teammates, as well as work with many organizations within the business, such as Data Science, Machine Learning Modeling, Engineering, Product, Business Operations, Risk Operations, Compliance, and more. You will help drive innovation and influence Cash App\u2019s fraud roadmap, programs, and processes. \n  The ideal candidate possesses exceptional analytical and communication skills. They must also enjoy ambiguous problem solving and have strong self-motivation skills. This role requires a bias toward taking action in a fast-paced and dynamic environment. \n  You Will \n \n  Become an expert on customer behaviors and authentic and inauthentic activity within Cash App\u2019s ecosystem \n  Master the Card and Disputes domain of risk-adjacent behaviors and how your domain affects Cash App \n  Learn to investigate and analyze complex data sets to resolve business challenges, identify opportunities for improvement, and provide insights and solutions \n  Leverage industry experience and expertise to analyze existing risks within product offerings that require a high level of attention to detail \n  Partner with internal business teams to identify and confirm emerging fraud trends and potential customer impact due to risk controls, both ahead of product launches and in an ongoing capacity \n  Respond promptly to internal business partners and exercise exceptional communication skills to optimize each contact \n  Must be able to take initiative, plan, organize and prioritize projects with overlapping deadlines competently; work independently and help others; must be highly motivated and detail-oriented \n  Foster a culture of professionalism, accountability, collaboration, speed, innovation, excellence, and a fun work environment while continuously elevating the quality and caliber of our risk controls \n  Other responsibilities as assigned \n \n \n \n \n Qualifications\n  \n \n  3+ years experience in risk and/or fraud detection in financial services or technology with an emphasis on debit card, disputes, and Reg E processes \n  Experience working with machine learning teams \n  Superior writing and editing skills, with the ability to produce copy requiring minimal rework; technical or procedural writing experience a plus \n  Experience working with teams across countries and time zones \n  Ability to synthesize information and make clear, concise recommendations on a course of action \n  Flexibility to adapt and able to manage multiple assignments while working independently \n \n  Even Better \n \n  Bachelor's Degree in Finance, Accounting, Mathematics, Economics, Computer Science, Information Management or Statistics \n  CFE, ACAMS, or similar accreditation \n  SQL experience \n \n  Additional Information\n   Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate\u2019s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.    Zone A: USD $103,900 - USD $126,900  Zone B: USD $96,600 - USD $118,000  Zone C: USD $88,300 - USD $107,900  Zone D: USD $77,900 - USD $95,300 \n \n  To find a location\u2019s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information. \n  Full-time employee benefits include the following: \n \n  Healthcare coverage (Medical, Vision and Dental insurance) \n  Health Savings Account and Flexible Spending Account \n  Retirement Plans including company match \n  Employee Stock Purchase Program \n  Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance \n  Paid parental and caregiving leave \n  Paid time off (including 12 paid holidays) \n  Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees) \n  Learning and Development resources \n  Paid Life insurance, AD&D, and disability benefits \n  Additional Perks such as WFH reimbursements and free access to caregiving, legal, and discounted resources \n \n  These benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans. \n  We\u2019re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, veteran status, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. \n  We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible.  Want to learn more about what we\u2019re doing to build a workplace that is fair and square? Check out our  I+D page . \n  Additionally, we consider qualified applicants with criminal histories for employment on our team, assessing candidates in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance. \n \n \n  Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.",
        "cleaned_desc": " Qualifications\n  \n \n  3+ years experience in risk and/or fraud detection in financial services or technology with an emphasis on debit card, disputes, and Reg E processes \n  Experience working with machine learning teams \n  Superior writing and editing skills, with the ability to produce copy requiring minimal rework; technical or procedural writing experience a plus \n  Experience working with teams across countries and time zones \n  Ability to synthesize information and make clear, concise recommendations on a course of action \n  Flexibility to adapt and able to manage multiple assignments while working independently \n \n  Even Better \n ",
        "techs": [
            "machine learning",
            "writing and editing skills",
            "procedural writing",
            "working across countries and time zones",
            "synthesizing information"
        ],
        "cleaned_techs": [
            "procedural writing",
            "working across countries and time zones",
            "synthesizing information"
        ]
    },
    "75390a85cc3b0596": {
        "terms": [
            "data science"
        ],
        "salary_min": 122091.36,
        "salary_max": 154594.86,
        "title": "Director of Data Science",
        "company": "Love in Faith",
        "desc": "This is a fully remote position within the United States. \n  About Love in Faith \n  Love in Faith is an energetic, fast-growing e-commerce company that focuses on Christian apparel. We pride ourselves on delivering top-quality products and a unique shopping experience for our customers. \n  Here's why you should consider us: \n \n Rapid Growth: We're not your typical startup. Our exponential growth speaks volumes about the opportunities that await you. Be part of something big! \n Remote Flexibility: Enjoy the freedom to work from your preferred location, without sacrificing your career goals. \n Culture: Our team knows how to work hard and play harder. We value creativity, innovation, and a positive work environment. Join us in celebrating successes, big and small. \n \n \n About the Role \n  As the Director of Data Science, you'll drive decision-making and strategy across the company through advanced analytics, predictive modeling, and a deep understanding of data. \n  Responsibilities: \n  Strategy Development: \n \n Develop and execute a comprehensive data strategy that aligns with company objectives, driving innovation, and growth through data-driven insights. \n Identify key performance indicators (KPIs) and success metrics for various business functions. \n \n Data Management and Governance: \n \n Oversee the collection, storage, management, quality, and protection of data across the company, ensuring data integrity and compliance with relevant regulations. \n Implement and manage data governance practices and policies. \n \n Advanced Analytics and Predictive Modeling: \n \n Lead the development of predictive models and advanced analytics to uncover trends, forecast outcomes, and provide actionable insights that inform business strategies. \n Communicate complex data in a clear, understandable manner to various stakeholders. \n \n Personalized Customer Experiences: \n \n Utilize data and analytics to segment customers and personalize shopping experiences, enhancing customer satisfaction, retention, and lifetime value. \n Collaborate with marketing and product teams to implement data-driven personalization strategies. \n \n Team Leadership and Talent Development: \n \n Recruit, mentor, and lead a high-performing data science team, fostering a culture of excellence, continuous learning, and innovation. \n Develop career paths and provide professional development opportunities for team members. \n \n Cross-Functional Collaboration: \n \n Work closely with various departments, including marketing, sales, IT, and operations, integrating data science into company-wide initiatives. \n Support decision-making across the company with data-driven insights and recommendations. \n \n Research and Innovation: \n \n Stay updated on industry trends, emerging technologies, and best practices in data science, ensuring the company maintains a competitive edge through innovation. \n Lead research initiatives that drive innovation in products, services, and internal processes. \n \n Performance Analysis and Reporting: \n \n Regularly analyze and report on the impact of data science initiatives, demonstrating their value and identifying opportunities for improvement. \n Ensure strategies and projects are grounded in data and deliver a measurable ROI. \n \n \n Requirements \n \n Proven experience leading data science initiatives, with a focus on delivering real-world impact in an e-commerce environment. \n Expertise in data modeling, machine learning algorithms, and advanced analytics. \n Strong leadership skills with experience building and managing high-performing teams. \n Excellent communication and collaboration skills, with the ability to translate complex data into actionable business insights. \n Experience with data visualization tools and data management platforms. \n A passion for leveraging data to drive business decisions and improve customer experiences. \n \n Join our team at Love in Faith and lead the charge in harnessing the power of data to drive our business forward. If you are a seasoned data professional with a strategic mindset and a passion for leadership, we would love to hear from you. \n \n  Physical Demands: \n  While performing the duties of this job, the employee routinely is required to sit; walk; talk and hear; use hands to keyboard, finger, handle, and feel; stoop, kneel, crouch, twist, reach, and stretch. Speaking and hearing ability sufficient to communicate in person, over the telephone, and/or via video conference. \n \n The ability to stand, walk, and sit in a computer chair for long periods of time. \n The ability to see and respond to dangerous situations. \n Speaking and hearing ability sufficient to communicate in person, over the telephone and/or via video conferences. \n Sufficient hand, arm, and finger dexterity to operate a computer keyboard and other office equipment. \n \n Emotional Demands: \n \n Data analysis & interpretation. You will have to process and review a lot of information from a variety of sources, understand how data is collected, assess data quality, and how to use the information. \n Communication skills. You need to have advanced communication skills in both oral and written form. Emails and written communication with colleagues and external partners, written reports for senior executives. Able to communicate complex financial information to people outside of the finance department. \n Comfortable with technology. Able to navigate data through computers, mobile, software, databases. Stay up to date with technology advances. \n Oriented to detail. Financial forecasts rely on projections which can be impacted by even minor changes in sales patterns, consumer sentiment, economic shifts, competitor changes, etc. You will need to be attuned to small changes in all streams of data. \n Confident decision-making skills. You will need to review data and make sound decisions on what actions to take and make confident recommendations to senior management. You may need to make decisions quickly with limited amounts of information in urgent situations. \n \n For Applicants with Disabilities: \n  Reasonable accommodation will be made for those that qualified during application process. If you need accommodations during the hiring process, please let us know when you submit your application, and we\u2019ll do our very best to adjust as needed. \n  Benefits \n \n Medical, Dental, Vision \n Health Savings (HSA) & Flexible Spending Account (FSA) \n Company Paid Life Insurance \n Supplemental Benefits Available:\n    \n Accident Insurance \n Critical Illness Insurance \n Unlimited PTO Policy \n \n Other Company Perks:\n    \n Monthly Utility Stipend \n Team Meetings Coffee/Food Stipend \n Health & Wellness Stipend \n Education & Professional Development Stipend \n Charitable Gift Matching",
        "cleaned_desc": " Oversee the collection, storage, management, quality, and protection of data across the company, ensuring data integrity and compliance with relevant regulations. \n Implement and manage data governance practices and policies. \n \n Advanced Analytics and Predictive Modeling: \n \n Lead the development of predictive models and advanced analytics to uncover trends, forecast outcomes, and provide actionable insights that inform business strategies. \n Communicate complex data in a clear, understandable manner to various stakeholders. \n \n Personalized Customer Experiences: \n \n Utilize data and analytics to segment customers and personalize shopping experiences, enhancing customer satisfaction, retention, and lifetime value. \n Collaborate with marketing and product teams to implement data-driven personalization strategies. \n \n Team Leadership and Talent Development: \n \n Recruit, mentor, and lead a high-performing data science team, fostering a culture of excellence, continuous learning, and innovation. \n Develop career paths and provide professional development opportunities for team members. \n \n Cross-Functional Collaboration: \n   Work closely with various departments, including marketing, sales, IT, and operations, integrating data science into company-wide initiatives. \n Support decision-making across the company with data-driven insights and recommendations. \n \n Research and Innovation: \n \n Stay updated on industry trends, emerging technologies, and best practices in data science, ensuring the company maintains a competitive edge through innovation. \n Lead research initiatives that drive innovation in products, services, and internal processes. \n \n Performance Analysis and Reporting: \n \n Regularly analyze and report on the impact of data science initiatives, demonstrating their value and identifying opportunities for improvement. \n Ensure strategies and projects are grounded in data and deliver a measurable ROI. \n \n \n Requirements \n \n Proven experience leading data science initiatives, with a focus on delivering real-world impact in an e-commerce environment. \n Expertise in data modeling, machine learning algorithms, and advanced analytics. \n Strong leadership skills with experience building and managing high-performing teams. \n Excellent communication and collaboration skills, with the ability to translate complex data into actionable business insights.   Experience with data visualization tools and data management platforms. \n A passion for leveraging data to drive business decisions and improve customer experiences. \n \n Join our team at Love in Faith and lead the charge in harnessing the power of data to drive our business forward. If you are a seasoned data professional with a strategic mindset and a passion for leadership, we would love to hear from you. \n \n  Physical Demands: \n  While performing the duties of this job, the employee routinely is required to sit; walk; talk and hear; use hands to keyboard, finger, handle, and feel; stoop, kneel, crouch, twist, reach, and stretch. Speaking and hearing ability sufficient to communicate in person, over the telephone, and/or via video conference. \n \n The ability to stand, walk, and sit in a computer chair for long periods of time. \n The ability to see and respond to dangerous situations. \n Speaking and hearing ability sufficient to communicate in person, over the telephone and/or via video conferences. \n Sufficient hand, arm, and finger dexterity to operate a computer keyboard and other office equipment. \n \n Emotional Demands: \n \n Data analysis & interpretation. You will have to process and review a lot of information from a variety of sources, understand how data is collected, assess data quality, and how to use the information. \n Communication skills. You need to have advanced communication skills in both oral and written form. Emails and written communication with colleagues and external partners, written reports for senior executives. Able to communicate complex financial information to people outside of the finance department. \n Comfortable with technology. Able to navigate data through computers, mobile, software, databases. Stay up to date with technology advances. \n Oriented to detail. Financial forecasts rely on projections which can be impacted by even minor changes in sales patterns, consumer sentiment, economic shifts, competitor changes, etc. You will need to be attuned to small changes in all streams of data. \n Confident decision-making skills. You will need to review data and make sound decisions on what actions to take and make confident recommendations to senior management. You may need to make decisions quickly with limited amounts of information in urgent situations. ",
        "techs": [
            "data modeling",
            "machine learning algorithms",
            "advanced analytics",
            "data visualization tools",
            "data management platforms"
        ],
        "cleaned_techs": [
            "machine learning algorithms",
            "advanced analytics",
            "data visualization tools",
            "data management platforms"
        ]
    },
    "5f03dde9bb640d7c": {
        "terms": [
            "data science",
            "mlops"
        ],
        "salary_min": 82174.39,
        "salary_max": 104051.09,
        "title": "Associate DevOps Engineer",
        "company": "Infor",
        "desc": "General information \n       \n \n \n \n \n \n \n        Country \n        \n United States  \n \n \n \n        City \n        \n Remote Location  \n \n \n \n        Department \n        \n Development  \n \n \n \n        Job ID \n        \n 37122  \n \n \n \n \n \n \n \n \n \n       Description & Requirements \n       \n \n \n \n \n \n \n The Associate DevOps Engineer is a key individual part of Enterprise Performance Management (EPM), a platform to create state-of-the-art analytical and performance management applications. The Associate DevOps Engineer for this group will be responsible for the design and development of cloud deployment automation for EPM for both AWS and Azure. \n  A Day in The Life Typically Includes: \n \n Maintain and develop new functionality for the automated deployment of EPM \n Work in a small group of highly skilled individuals to develop and enhance automation, adapt new technologies, and integrate with other Infor products \n Support operations and management on an enterprise level production of cloud deployment \n Follow best practices as part of the development process \n Collaborate with the product architect on the design of new functionality along with the non-functional requirements to support future demands \n \n  Basic Qualifications: \n \n  Legal authorization to work permanently in the United States for any employer without requiring a visa transfer or visa sponsorship \n  Experience using PowerShell and Python scripting languages in respect to automation of deployment processes \n  Experience with Amazon Web Services (EC2, Cloud Formation, CodeDeploy, Lambda Functions or Salt) \n \n  Preferred Qualifications: \n \n Technical degree, certification, diploma, or boot camp \n Experience with .NET Framework and programming in C#  \n Familiarity with Azure and automation services used there, ideally Pipelines and DevOps Services \n  Experience with SQL Server, Oracle, Postgres or multidimensional databases \n  Knowledge of concepts like network protocols, network security, SSL, PKIX, routing, and load balancing \n \n  US Remote (Dallas, TX; St. Paul, MN; Alpharetta, GA) \n \n \n \n \n \n \n \n \n         About Infor\n         \n \n \n  Infor is a global leader in business cloud software products for companies in industry specific markets. Infor builds complete industry suites in the cloud and efficiently deploys technology that puts the user experience first, leverages data science, and integrates easily into existing systems. Over 60,000 organizations worldwide rely on Infor to help overcome market disruptions and achieve business-wide digital transformation.\n         \n \n          For more information visit www.infor.com\n         \n \n \n \n \n \n \n \n         Our Values\n         \n \n \n  At Infor, we strive for an environment that is founded on a business philosophy called Principle Based Management\u2122 (PBM\u2122) and eight Guiding Principles: integrity, stewardship & compliance, transformation, principled entrepreneurship, knowledge, humility, respect, self-actualization. Increasing diversity is important to reflect our markets, customers, partners, and communities we serve in now and in the future.\n         \n \n \n  We have a relentless commitment to a culture based on PBM. Informed by the principles that allow a free and open society to flourish, PBM\u2122 prepares individuals to innovate, improve, and transform while fostering a healthy, growing organization that creates long-term value for its clients and supporters and fulfillment for its employees.\n         \n \n \n  Infor is an Equal Opportunity Employer. We are committed to creating a diverse and inclusive work environment. Infor does not discriminate against candidates or employees because of their sex, race, gender identity, disability, age, sexual orientation, religion, national origin, veteran status, or any other protected status under the law.\n          \n \n \n \n \n         At Infor we value your privacy that\u2019s why we created a policy that you can read here.\n         \n \n \n \n \n \n \n \n \n         This employer uses E-Verify. Please visit the following website for additional information: www.kochcareers.com/doc/Everify.pdf",
        "cleaned_desc": " \n Maintain and develop new functionality for the automated deployment of EPM \n Work in a small group of highly skilled individuals to develop and enhance automation, adapt new technologies, and integrate with other Infor products \n Support operations and management on an enterprise level production of cloud deployment \n Follow best practices as part of the development process \n Collaborate with the product architect on the design of new functionality along with the non-functional requirements to support future demands \n \n  Basic Qualifications: \n \n  Legal authorization to work permanently in the United States for any employer without requiring a visa transfer or visa sponsorship \n  Experience using PowerShell and Python scripting languages in respect to automation of deployment processes \n  Experience with Amazon Web Services (EC2, Cloud Formation, CodeDeploy, Lambda Functions or Salt) \n \n  Preferred Qualifications: \n \n Technical degree, certification, diploma, or boot camp \n Experience with .NET Framework and programming in C#  \n Familiarity with Azure and automation services used there, ideally Pipelines and DevOps Services \n  Experience with SQL Server, Oracle, Postgres or multidimensional databases \n  Knowledge of concepts like network protocols, network security, SSL, PKIX, routing, and load balancing \n \n  US Remote (Dallas, TX; St. Paul, MN; Alpharetta, GA) \n \n ",
        "techs": [
            "powershell",
            "python",
            "amazon web services (ec2",
            "cloud formation",
            "codedeploy",
            "lambda functions",
            "salt)",
            ".net framework",
            "c#",
            "azure",
            "pipelines",
            "devops services",
            "sql server",
            "oracle",
            "postgres",
            "multidimensional databases",
            "network protocols",
            "network security",
            "ssl",
            "pkix",
            "routing",
            "load balancing"
        ],
        "cleaned_techs": [
            "powershell",
            "python",
            "aws",
            "cloud formation",
            "codedeploy",
            "lambda functions",
            "salt)",
            ".net framework",
            "c#",
            "azure",
            "pipelines",
            "devops services",
            "sql",
            "oracle",
            "postgres",
            "multidimensional databases",
            "network protocols",
            "ssl",
            "pkix",
            "routing",
            "load balancing"
        ]
    },
    "90ca40677b46e6ea": {
        "terms": [
            "data science",
            "data engineer",
            "machine learning engineer"
        ],
        "salary_min": 138236.8,
        "salary_max": 200408.0,
        "title": "IT Lead Data Engineer - Remote",
        "company": "Mayo Clinic",
        "desc": "Why Mayo Clinic \n \n \n  Mayo Clinic is top-ranked in more specialties than any other care provider according to U.S. News & World Report. As we work together to put the needs of the patient first, we are also dedicated to our employees, investing in competitive compensation and comprehensive benefit plans \u2013 to take care of you and your family, now and in the future. And with continuing education and advancement opportunities at every turn, you can build a long, successful career with Mayo Clinic. You\u2019ll thrive in an environment that supports innovation, is committed to ending racism and supporting diversity, equity and inclusion, and provides the resources you need to succeed.\n  \n \n \n Responsibilities \n \n  As a member of the Data and Analytics organization, you will be responsible for building and delivering best-in-class clinical data initiatives aimed at driving best-in-class solutions. You will collaborate with analytic partners and business partners from product strategy, program management, IT, data strategy, and predictive analytics teams to develop effective solutions for our partners. \n  Lead data design, prototype, and development of data pipeline architecture pipelines. Lead implementation of internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. Lead cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions. Excellent analytic skills associated with working on unstructured datasets. Understand the architecture, be a team player, lead technical discussions and communicate the technical discussion. Be a senior Individual contributor of the Data or Software Engineering teams. Be part of Technical Review Board along with Manager and Principal Engineer. Be a technical liaison between Manager, Software Engineers and Principal Engineers. Collaborate with software engineers to analyze, develop and test functional requirements. Write clean, maintainable code 30% of the time and performing peer code-reviews. Mentor and Coach Engineers. Work with team members to investigate design approaches, prototype new technology and evaluate technical feasibility. Work in an Agile/Safe/Scrum environment to deliver high quality software. Establish architectural principles, select design patterns, and then mentor team members on their appropriate application. Facilitate and drive communication between front-end, back-end, data and platform engineers. Play a formal Engineering lead role in the area of expertise. Keep up-to-date with industry trends and developments. \n  Job Responsibilities: \n \n Act as Product Owner for Data platform\u2019s and Lead the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. \n Evaluate the full technology stack of services required including PaaS, IaaS, SaaS, DataOps, operations, availability, and automation. \n Research, design, and develop Public & Private Data Solutions, including impacts to enterprise architecture \n Build high-performing clinical data processing frameworks leveraging Google Cloud Platform, GCP Shared Services like Google Healthcare API, Big Query, and HL7 FHIR store. \n Participate in evaluation of supporting technologies and industry best practices with our cloud partners and peer teams. \n Lead Modern Data Warehouse Solutions and Sizing efforts to create defined plans and work estimates for customer proposals and Statements of work. \n Conduct full technical discovery, identifying pain points, business, and technical requirements, \u201cas is\u201d and \u201cto be\u201d scenarios! \n Design and Develop clinical data pipelines integrating ingestion, harmonization, and consumption frameworks for onboarding clinical data from various data sources formatted in various industry standards (FHIR, C-CDA, HL7 V2, JSON, XML, etc.). \n Build state-of-the-art data pipelines supporting both batch and real-time streams to enable Clinical data collection, storage, processing, transformation, aggregation, and dissemination through heterogeneous channels. \n Build design specifications for health care data objects and surrounding data processing logic. \n Lead innovation and research building proof of concepts for complex transformations, notification engines, analytical engines, and self-service analytics \n Bring a DevOps mindset to enable big data and batch/real-time analytical solutions that leverage emerging technologies. \n \n \n \n Qualifications \n \n  Bachelor\u2019s Degree in Computer Science/Engineering or related field with 6 years of experience OR an Associate\u2019s degree in Computer Science/Engineering or related field with 8 years of experience. \n  Knowledge of professional software engineering practices and best practices for the full software development life cycle (SDLC), including coding standards, code reviews, source control management, build processes, testing, and operations. Have in-depth knowledge of data engineering and building data pipelines with a minimum of 5 years of experience in data engineering, data science or analytical modeling and basic knowledge of related disciplines. Worked and lead Data Engineering teams in Continuous Integration / Continuous Delivery model. Build/Lead Data products highly resilient in nature. Build/Lead Test Automation suites, Unit Testing coverage, Data Quality, Monitoring & Observability. A minimum experience of 5 years using relational databases and NoSQL Databases. Experience with cloud platforms such as GCP, Azure, AWS. \n  Continuous Integration using Jenkins, Git Hub Actions or Azure Pipelines. Experience with cloud technologies, development and deployment. Experience with tools like Jira, GitHub, SharePoint, Azure Boards. Experience using advanced data processing solutions/capabilities such as Apache Spark, Hive, Airflow and Kafka, GCP Dataflow. Experience using big data, statistics and knowledge of data related aspects of machine learning. Experience with Google BigQuery, FHIR APIs, and Vertex AI. Knowledge of how workflow scheduling solutions such as Apache Airflow and Google Composer related to data systems. Knowledge of using Infrastructure as code (Kubernetes, Docker) in a cloud environment. \n \n  Hands-on experience in architecture, design, and development of enterprise data applications and analytics solutions within the health care domain \n  Experience in Google Cloud Platform/Shared Services such as Cloud Dataflow, Cloud Storage, Pub/sub, Cloud Composer, Big Query, and Health care API (FHIR store) \n  They should be able to deliver an ingestion framework for relational data sources, understand layers and rules of a data lake and carry out all the tasks to operationalize data pipelines. \n  Experience in Python, Java, Spark, Airflow, and Kafka development \n  Hands-on experience working with \u201cBig Data\u201d technologies and experience with traditional RDBMS, Python, Unix Shell scripting, JSON, and XML \n  Experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT) \n  Must have great articulation and communication skills. \n  Working in a fluid environment, defining, and owning priorities that adapt to our larger goals. You can bring clarity to ambiguity while remaining open-minded to new information that might change your mind. \n  Should have a strong understanding of healthcare data, including clinical data in proprietary and industry-standard formats. \n  Participate in architectural discussions, perform system analysis which involves a review of the existing systems and operating methodologies. Participate in the analysis of newest technologies and suggest the optimal solutions which will be best suited for satisfying the current requirements and will simplify the future modifications \n  Design appropriate data models for the use in transactional and big data environments as an input into Machine Learning processing. \n  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability \n  Design and Build the necessary infrastructure for optimal ETL from a variety of data sources to be used on GCP services \n  Collaborate with multiple stakeholders including Product Teams, Data Domain Owners, Infrastructure, Security and Global IT \n  Identify, Implement, and continuously enhance the data automation process \n  Develop proper Data Governance and Data Security \n  Demonstrate strategic thinking and strong planning skills to establish long term roadmap and business plan \n  Work with stakeholders to establish and meet data quality requirements, SLAs and SLOs for data ingestion \n  Experience in Self-service Analytics/Visualization tools like PowerBI, Looker, Tableau \n  Proven knowledge in implementing security & IAM requirements \n  Experience building and maintaining a Data-Lake with DeltaLake \n  Experience with ETL/ELT/DataMesh frameworks \n  Experience with GCP Dataplex (Data Catalog, Clean Rooms) \n \n  Authorization to work and remain in the United States, without necessity for Mayo Clinic sponsorship now, or in the future (for example, be a U.S. Citizen, national, or permanent resident, refugee, or asylee). Also, Mayo Clinic does not participate in the F-1 STEM OPT extension program. \n  Exemption Status \n \n  Exempt\n  \n \n Compensation Detail \n \n  $138,236.80 - $200,408.00 / year\n  \n \n Benefits Eligible \n \n  Yes\n  \n \n Schedule \n \n  Full Time\n  \n \n Hours/Pay Period \n \n  80\n  \n \n Schedule Details \n \n  Monday - Friday, 8:00 am - 5:00 pm\n  \n \n Weekend Schedule \n \n  As needed\n  \n \n International Assignment \n \n  No\n  \n \n Site Description \n \n \n  Just as our reputation has spread beyond our Minnesota roots, so have our locations. Today, our employees are located at our three major campuses in Phoenix/Scottsdale, Arizona, Jacksonville, Florida, Rochester, Minnesota, and at Mayo Clinic Health System campuses throughout Midwestern communities, and at our international locations. Each Mayo Clinic location is a special place where our employees thrive in both their work and personal lives. Learn more about what each unique Mayo Clinic campus has to offer, and where your best fit is.\n   \n \n \n \n \n Affirmative Action and Equal Opportunity Employer \n \n \n  As an Affirmative Action and Equal Opportunity Employer Mayo Clinic is committed to creating an inclusive environment that values the diversity of its employees and does not discriminate against any employee or candidate. Women, minorities, veterans, people from the LGBTQ communities and people with disabilities are strongly encouraged to apply to join our teams. Reasonable accommodations to access job openings or to apply for a job are available.\n    \n \n \n \n \n Recruiter \n \n  Miranda Grabner",
        "cleaned_desc": "Why Mayo Clinic \n \n \n  Mayo Clinic is top-ranked in more specialties than any other care provider according to U.S. News & World Report. As we work together to put the needs of the patient first, we are also dedicated to our employees, investing in competitive compensation and comprehensive benefit plans \u2013 to take care of you and your family, now and in the future. And with continuing education and advancement opportunities at every turn, you can build a long, successful career with Mayo Clinic. You\u2019ll thrive in an environment that supports innovation, is committed to ending racism and supporting diversity, equity and inclusion, and provides the resources you need to succeed.\n  \n \n \n Responsibilities \n \n  As a member of the Data and Analytics organization, you will be responsible for building and delivering best-in-class clinical data initiatives aimed at driving best-in-class solutions. You will collaborate with analytic partners and business partners from product strategy, program management, IT, data strategy, and predictive analytics teams to develop effective solutions for our partners. \n  Lead data design, prototype, and development of data pipeline architecture pipelines. Lead implementation of internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. Lead cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions. Excellent analytic skills associated with working on unstructured datasets. Understand the architecture, be a team player, lead technical discussions and communicate the technical discussion. Be a senior Individual contributor of the Data or Software Engineering teams. Be part of Technical Review Board along with Manager and Principal Engineer. Be a technical liaison between Manager, Software Engineers and Principal Engineers. Collaborate with software engineers to analyze, develop and test functional requirements. Write clean, maintainable code 30% of the time and performing peer code-reviews. Mentor and Coach Engineers. Work with team members to investigate design approaches, prototype new technology and evaluate technical feasibility. Work in an Agile/Safe/Scrum environment to deliver high quality software. Establish architectural principles, select design patterns, and then mentor team members on their appropriate application. Facilitate and drive communication between front-end, back-end, data and platform engineers. Play a formal Engineering lead role in the area of expertise. Keep up-to-date with industry trends and developments. \n  Job Responsibilities: \n \n Act as Product Owner for Data platform\u2019s and Lead the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. \n Evaluate the full technology stack of services required including PaaS, IaaS, SaaS, DataOps, operations, availability, and automation. \n Research, design, and develop Public & Private Data Solutions, including impacts to enterprise architecture \n Build high-performing clinical data processing frameworks leveraging Google Cloud Platform, GCP Shared Services like Google Healthcare API, Big Query, and HL7 FHIR store. \n Participate in evaluation of supporting technologies and industry best practices with our cloud partners and peer teams. \n Lead Modern Data Warehouse Solutions and Sizing efforts to create defined plans and work estimates for customer proposals and Statements of work. \n Conduct full technical discovery, identifying pain points, business, and technical requirements, \u201cas is\u201d and \u201cto be\u201d scenarios! \n Design and Develop clinical data pipelines integrating ingestion, harmonization, and consumption frameworks for onboarding clinical data from various data sources formatted in various industry standards (FHIR, C-CDA, HL7 V2, JSON, XML, etc.). \n Build state-of-the-art data pipelines supporting both batch and real-time streams to enable Clinical data collection, storage, processing, transformation, aggregation, and dissemination through heterogeneous channels. \n Build design specifications for health care data objects and surrounding data processing logic.   Lead innovation and research building proof of concepts for complex transformations, notification engines, analytical engines, and self-service analytics \n Bring a DevOps mindset to enable big data and batch/real-time analytical solutions that leverage emerging technologies. \n \n \n \n Qualifications \n \n  Bachelor\u2019s Degree in Computer Science/Engineering or related field with 6 years of experience OR an Associate\u2019s degree in Computer Science/Engineering or related field with 8 years of experience. \n  Knowledge of professional software engineering practices and best practices for the full software development life cycle (SDLC), including coding standards, code reviews, source control management, build processes, testing, and operations. Have in-depth knowledge of data engineering and building data pipelines with a minimum of 5 years of experience in data engineering, data science or analytical modeling and basic knowledge of related disciplines. Worked and lead Data Engineering teams in Continuous Integration / Continuous Delivery model. Build/Lead Data products highly resilient in nature. Build/Lead Test Automation suites, Unit Testing coverage, Data Quality, Monitoring & Observability. A minimum experience of 5 years using relational databases and NoSQL Databases. Experience with cloud platforms such as GCP, Azure, AWS. \n  Continuous Integration using Jenkins, Git Hub Actions or Azure Pipelines. Experience with cloud technologies, development and deployment. Experience with tools like Jira, GitHub, SharePoint, Azure Boards. Experience using advanced data processing solutions/capabilities such as Apache Spark, Hive, Airflow and Kafka, GCP Dataflow. Experience using big data, statistics and knowledge of data related aspects of machine learning. Experience with Google BigQuery, FHIR APIs, and Vertex AI. Knowledge of how workflow scheduling solutions such as Apache Airflow and Google Composer related to data systems. Knowledge of using Infrastructure as code (Kubernetes, Docker) in a cloud environment. \n \n  Hands-on experience in architecture, design, and development of enterprise data applications and analytics solutions within the health care domain \n  Experience in Google Cloud Platform/Shared Services such as Cloud Dataflow, Cloud Storage, Pub/sub, Cloud Composer, Big Query, and Health care API (FHIR store) \n  They should be able to deliver an ingestion framework for relational data sources, understand layers and rules of a data lake and carry out all the tasks to operationalize data pipelines. \n  Experience in Python, Java, Spark, Airflow, and Kafka development \n  Hands-on experience working with \u201cBig Data\u201d technologies and experience with traditional RDBMS, Python, Unix Shell scripting, JSON, and XML \n  Experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT) \n  Must have great articulation and communication skills. \n  Working in a fluid environment, defining, and owning priorities that adapt to our larger goals. You can bring clarity to ambiguity while remaining open-minded to new information that might change your mind. \n  Should have a strong understanding of healthcare data, including clinical data in proprietary and industry-standard formats. \n  Participate in architectural discussions, perform system analysis which involves a review of the existing systems and operating methodologies. Participate in the analysis of newest technologies and suggest the optimal solutions which will be best suited for satisfying the current requirements and will simplify the future modifications \n  Design appropriate data models for the use in transactional and big data environments as an input into Machine Learning processing. \n  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability    Design and Build the necessary infrastructure for optimal ETL from a variety of data sources to be used on GCP services \n  Collaborate with multiple stakeholders including Product Teams, Data Domain Owners, Infrastructure, Security and Global IT \n  Identify, Implement, and continuously enhance the data automation process \n  Develop proper Data Governance and Data Security \n  Demonstrate strategic thinking and strong planning skills to establish long term roadmap and business plan \n  Work with stakeholders to establish and meet data quality requirements, SLAs and SLOs for data ingestion \n  Experience in Self-service Analytics/Visualization tools like PowerBI, Looker, Tableau \n  Proven knowledge in implementing security & IAM requirements \n  Experience building and maintaining a Data-Lake with DeltaLake \n  Experience with ETL/ELT/DataMesh frameworks \n  Experience with GCP Dataplex (Data Catalog, Clean Rooms) \n \n  Authorization to work and remain in the United States, without necessity for Mayo Clinic sponsorship now, or in the future (for example, be a U.S. Citizen, national, or permanent resident, refugee, or asylee). Also, Mayo Clinic does not participate in the F-1 STEM OPT extension program. \n  Exemption Status \n \n  Exempt\n  \n \n Compensation Detail \n \n  $138,236.80 - $200,408.00 / year\n  \n ",
        "techs": [
            "mayo clinic",
            "u.s. news & world report",
            "google cloud platform",
            "gcp shared services",
            "google healthcare api",
            "big query",
            "hl7 fhir store",
            "jenkins",
            "github actions",
            "azure pipelines",
            "jira",
            "github",
            "sharepoint",
            "azure boards",
            "apache spark",
            "hive",
            "airflow",
            "kafka",
            "gcp dataflow",
            "google bigquery",
            "fhir apis",
            "vertex ai",
            "kubernetes",
            "docker",
            "cloud dataflow",
            "cloud storage",
            "pub/sub",
            "cloud composer",
            "python",
            "java",
            "spark",
            "airflow",
            "kafka",
            "rdbms",
            "unix shell scripting",
            "json",
            "xml",
            "jenkins",
            "git",
            "powerbi",
            "looker",
            "tableau",
            "deltalake",
            "gcp dataplex"
        ],
        "cleaned_techs": [
            "mayo clinic",
            "u.s. news & world report",
            "gcp",
            "google healthcare api",
            "big query",
            "hl7 fhir store",
            "jenkins",
            "github actions",
            "azure",
            "jira",
            "github",
            "sharepoint",
            "apache spark",
            "hive",
            "airflow",
            "kafka",
            "google bigquery",
            "fhir apis",
            "vertex ai",
            "kubernetes",
            "docker",
            "cloud dataflow",
            "cloud storage",
            "pub/sub",
            "cloud composer",
            "python",
            "java",
            "spark",
            "rdbms",
            "unix shell scripting",
            "json",
            "xml",
            "git",
            "powerbi",
            "looker",
            "tableau",
            "deltalake"
        ]
    },
    "32dd969adaa2f18f": {
        "terms": [
            "data science"
        ],
        "salary_min": 104276.79,
        "salary_max": 132037.66,
        "title": "Data Scientist",
        "company": "Vibronyx",
        "desc": "Job Title:  Data Scientist \n \n Department:  Information Technology \n \n Reports To:  Vishnu Venkatesh \n \n Work Location:  Flexible for remote and/or onsite. East coast preference. \n \n Who we are:  Vibronyx is a rapidly growing startup focused on building future-ready supply chains using AI technologies and data analytics. We are helping our department of defense and commercial clients solve some of the toughest digital supply chain challenges. \n \n The value that you will bring:  As a Data Scientist, you will help in the development of predictive technologies that will power the next generation of supply chains. Your abilities to apply machine learning and artificial neural network methodologies will underly our customers\u2019 predictive capabilities. You will be using your coding skills to discover features and train models. You will be working on teams on multiple projects concurrently that involve cross-functional disciplines across customer and internal organizations \n \n \n What will you bring to the role: \n  Apply analytical thinking and curiosity to build analytical solutions \n Communication skills that enable you to tell the story of the data \n Willingness to learn, be coached and take feedback \n Be able to embrace a prototype and fail-fast culture \n Consultative approach to problem solving with clients and internal teams \n \n \n Job Description and Duties: \n  Collaborate with various stakeholders to understand requirements, and translate those requirements into technical solutions \n Develop templates, design, develop, and integrate data and models for repeatability \n Stay current on new technologies and methods across data science and analytics, data engineering, and data visualization, share best practices to improve technical capabilities of the team \n Identify appropriate data sources to answer business question \n Explore, identify and evaluate features and models \n Extract, transform, cleanse, and organize and standardize data to make data usable for machine learning and neural networks \n Visualize accuracy and relationships within data \n Engage in best practices discussions with other team members, discuss project activities and results with team \n Build documentation and presentations of project results \n Communicate results of analyses and associated business benefits to business partners and executives including the application of statistical exploratory data analysis to communicate value of data \n \n \n Skills/Qualifications: \n  1 to 3+ years of technical experience with relevant analytics programming languages, frameworks, and visualization tools \n Ability to collect and analyze complex data \n Must be able to organize and prioritize work to meet multiple deadlines \n Must be able to communicate effectively in both oral and written form \n Must have strong time management skills \n Curiosity, learning ambition and collaborative behavior essential to this role. \n Must have demonstrated experience in working with relational databases and advanced SQL techniques \n Must have demonstrated experience in feature discovery and engineering \n Must have demonstrated experience in communicating relationships in data using statistical modeling \n Must have demonstrated experience in advanced Classification and Regression machine learning algorithms \n Must have demonstrated experience in Natural Language Processing technologies \n Must have demonstrated experience in Deep Learning technologies \n Must have demonstrated experience in Python data analysis frameworks and visualization including Pandas, Seaborn/Matplotlib, Jupyter and Keras/Tensorflow \n Must have demonstrated experience in dealing with imbalanced data \n Must have demonstrated experience in Git version control \n Showcasing your data science skills and experience through public repos is a plus \n Familiarity with AutoML tools such as PyCaret is a plus \n Knowledge of NoSQL and Graph databases is a plus \n Knowledge and familiarity with JSON, Protocol Buffers is a plus \n Knowledge and familiarity with Windows operating systems. Familiarity with Linux is a plus. \n Working knowledge of a virtual machines in a cloud environment like AWS \n Regular, predictable, full attendance is an essential function of the job \n Must be a U.S. Citizen or authorized to work in the U.S. without sponsorship and able to pass government security process to work within government systems \n \n \n Preferred Experience: \n  Experience with supply chain management data and technology is desirable (e.g. ERP, Transportation Management and Warehouse Management systems) \n Priority for current or recent DoD Common Access Card holders \n \n \n Physical Demands: \n  Willingness to travel as necessary, work the required schedule, work at the specific location required \n \n \n Education: \n  Bachelor\u2019s degree required, concentration in Data Analytics, Engineering, Operations Research, Statistics, Applied Math, Computer Science, or related quantitative certifications \n At Vibronyx \u2013 the ability to help our clients, colleagues and communities flourish is driven by the diversity of thought, talent, interests, and experiences of our valued team members. We don\u2019t just accept these differences, we cherish them. Vibronyx is proud to be an equal opportunity workplace and is an affirmative action employer. We invite you to join us and feel the VIBES: https://vibronyx.com/about/",
        "cleaned_desc": "  Apply analytical thinking and curiosity to build analytical solutions \n Communication skills that enable you to tell the story of the data \n Willingness to learn, be coached and take feedback \n Be able to embrace a prototype and fail-fast culture \n Consultative approach to problem solving with clients and internal teams \n \n \n Job Description and Duties: \n  Collaborate with various stakeholders to understand requirements, and translate those requirements into technical solutions \n Develop templates, design, develop, and integrate data and models for repeatability \n Stay current on new technologies and methods across data science and analytics, data engineering, and data visualization, share best practices to improve technical capabilities of the team \n Identify appropriate data sources to answer business question \n Explore, identify and evaluate features and models \n Extract, transform, cleanse, and organize and standardize data to make data usable for machine learning and neural networks   Visualize accuracy and relationships within data \n Engage in best practices discussions with other team members, discuss project activities and results with team \n Build documentation and presentations of project results \n Communicate results of analyses and associated business benefits to business partners and executives including the application of statistical exploratory data analysis to communicate value of data \n \n \n Skills/Qualifications: \n  1 to 3+ years of technical experience with relevant analytics programming languages, frameworks, and visualization tools \n Ability to collect and analyze complex data \n Must be able to organize and prioritize work to meet multiple deadlines \n Must be able to communicate effectively in both oral and written form \n Must have strong time management skills \n Curiosity, learning ambition and collaborative behavior essential to this role. \n Must have demonstrated experience in working with relational databases and advanced SQL techniques   Must have demonstrated experience in feature discovery and engineering \n Must have demonstrated experience in communicating relationships in data using statistical modeling \n Must have demonstrated experience in advanced Classification and Regression machine learning algorithms \n Must have demonstrated experience in Natural Language Processing technologies \n Must have demonstrated experience in Deep Learning technologies \n Must have demonstrated experience in Python data analysis frameworks and visualization including Pandas, Seaborn/Matplotlib, Jupyter and Keras/Tensorflow \n Must have demonstrated experience in dealing with imbalanced data \n Must have demonstrated experience in Git version control \n Showcasing your data science skills and experience through public repos is a plus \n Familiarity with AutoML tools such as PyCaret is a plus \n Knowledge of NoSQL and Graph databases is a plus \n Knowledge and familiarity with JSON, Protocol Buffers is a plus \n Knowledge and familiarity with Windows operating systems. Familiarity with Linux is a plus. \n Working knowledge of a virtual machines in a cloud environment like AWS ",
        "techs": [
            "analytics programming languages",
            "frameworks",
            "visualization tools",
            "relational databases",
            "advanced sql techniques",
            "feature discovery",
            "engineering",
            "statistical modeling",
            "classification and regression machine learning algorithms",
            "natural language processing technologies",
            "deep learning technologies",
            "python data analysis frameworks",
            "pandas",
            "seaborn/matplotlib",
            "jupyter",
            "keras/tensorflow",
            "imbalanced data",
            "git version control",
            "automl tools",
            "pycaret",
            "nosql databases",
            "graph databases",
            "json",
            "protocol buffers",
            "windows operating systems",
            "linux",
            "virtual machines",
            "cloud environment",
            "aws"
        ],
        "cleaned_techs": [
            "analytics programming languages",
            "frameworks",
            "visualization tools",
            "relational databases",
            "advanced sql techniques",
            "feature discovery",
            "engineering",
            "statistical modeling",
            "classification and regression machine learning algorithms",
            "nlp",
            "deep learning technologies",
            "python",
            "pandas",
            "seaborn/matplotlib",
            "jupyter",
            "keras/tensorflow",
            "imbalanced data",
            "git version control",
            "automl tools",
            "pycaret",
            "nosql",
            "graph databases",
            "json",
            "protocol buffers",
            "windows operating systems",
            "linux",
            "virtual machines",
            "cloud environment",
            "aws"
        ]
    },
    "41d68c99e5711dd6": {
        "terms": [
            "data science"
        ],
        "salary_min": 66317.01,
        "salary_max": 83972.12,
        "title": "Senior Analyst, Compliance (remote based in the US)",
        "company": "Texas Staff HQ",
        "desc": "Strategically responsible for providing a holistic view of risks across key enterprise functions and identifying high compliance risks across the enterprise; transforming data into actionable information and plans; improving timeliness of identifying higher risk actions within the enterprise; providing uniform methodology for visualizing risks; engaging key functional areas within the entities to prioritize risk areas of focus; and, providing increased monitoring capabilities regarding issue recurrence and issue mitigation and remediation effectiveness.  \n Tactically responsible for the following processes: compliance dashboards and other reporting mechanisms throughout the enterprise; compliance data mining, including identifying trends and potential risks using various data modeling and machine learning methodologies; as well as compliance audit standardization. \n  Required: \n \n Bachelor\u2019s degree and equivalent work experience (preferred areas: Business, Computer Science, Economics, Healthcare Administration, Health Information Management, Data Analytics/Science, Mathematics or Statistics). \n 3-5 years of experience in data science/management, validation, and analysis. \n Intermediate to advanced programming/querying proficiency in SQL and Python. \n Proficiency in Excel and data visualization and analytics tools such as Tableau, Power BI, and MicroStrategy. \n Skilled in effective report/dashboard development. \n Ability to communicate technical information to a non-technical audience. \n \n Preferred: \n \n 3-5 years of experience in healthcare industry. \n Intermediate knowledge of Healthcare Compliance oversight and monitoring concepts. \n Intermediate to advanced Programming/querying skills in SQL, R, SAS, Python, etc. \n Familiarity with cloud and enterprise data warehouse tools such as Big Query, Azure SQL, SSMS, \n Familiarity with enterprise ETL processes. \n Knowledge of any Governance, Risk, & Compliance (GRC) program. \n \n Healthcare/USPI complies with federal, state, and/or local laws regarding mandatory vaccination of its workforce. If you are offered this position and must be vaccinated under any applicable law, you will be required to show proof of full vaccination or obtain an approval of a religious or medical exemption prior to your start date. If you receive an exemption from the vaccination requirement, you will be required to submit to regular testing in accordance with the law.  2305045185\n   Employment practices will not be influenced or affected by an applicant\u2019s or employee\u2019s race, color, religion, sex (including pregnancy), national origin, age, disability, genetic information, sexual orientation, gender identity or expression, veteran status or any other legally protected status. Tenet will make reasonable accommodations for qualified individuals with disabilities unless doing so would result in an undue hardship.",
        "cleaned_desc": " Bachelor\u2019s degree and equivalent work experience (preferred areas: Business, Computer Science, Economics, Healthcare Administration, Health Information Management, Data Analytics/Science, Mathematics or Statistics). \n 3-5 years of experience in data science/management, validation, and analysis. \n Intermediate to advanced programming/querying proficiency in SQL and Python. \n Proficiency in Excel and data visualization and analytics tools such as Tableau, Power BI, and MicroStrategy.   Familiarity with cloud and enterprise data warehouse tools such as Big Query, Azure SQL, SSMS, \n Familiarity with enterprise ETL processes. \n Knowledge of any Governance, Risk, & Compliance (GRC) program. \n ",
        "techs": [
            "sql",
            "python",
            "excel",
            "tableau",
            "power bi",
            "microstrategy",
            "big query",
            "azure sql",
            "ssms"
        ],
        "cleaned_techs": [
            "sql",
            "python",
            "excel",
            "tableau",
            "powerbi",
            "microstrategy",
            "big query",
            "azure",
            "ssms"
        ]
    },
    "8a2de8bffa0e147b": {
        "terms": [
            "data science"
        ],
        "salary_min": 65475.152,
        "salary_max": 82906.13,
        "title": "Supply Chain Analyst",
        "company": "Elemental Enzymes",
        "desc": "Supply Chain Analyst \n \n  Elemental Enzymes is seeking a process and data-oriented, highly-collaborative team member to join our organization as a full-time Supply Chain Analyst. This role will be instrumental in establishing critical process and communication methodologies for a growing organization. The Supply Chain Analyst will sit within in our Global Manufacturing Organization and will be responsible for recommending and establishing communication standards, dashboards, and data analysis for best practices/industry standards, including documentation. The successful candidate will be self-motivated, thrive in a changing environment, and leverage both talent and experience in cross-functional communication to drive results through a matrix organization. \n \n  Job Description \n \n Data Analysis \n Evaluate current data and KPIs for proactive recommendations and leadership decisions \n Validate and audit data capture to ensure accuracy in reporting \n Communications and Dashboards \n Recommend and/or establish tracking processes to support operational standards, targets, and long-range planning \n Able to develop communications roadmap for growing matrix organization needs with a focus on sustainable, clear, and consistent insights that drive actions/decisions within Leadership \n Cross-functional (Matrix) Collaboration \n Collaborate with and drive results through matrix teams across a majority of the organization to ensure appropriate data capture and communication from Product Pipeline through Sales Targets \n Confidence in raising questions, communicating recommendations, and driving results across teams and platforms to ensure data capture and analysis are meeting EE needs \n Continuous Improvement and Strategy \n Focus on establishing appropriate platform, stabilizing, and identifying continuous improvement opportunities for a successful Manufacturing and Supply Chain organization \n Documentation and tracking of processes and protocols to ensure practices are being followed and align with strategic objectives \n Lead value stream mapping sessions to ensure product and information flow is optimized \n Project Management \n Proven ability to lead cross-functional projects and drive results supporting key strategic initiatives \n \n \n \n  Qualifications: \n \n Bachelor's Degree in Business Analytics, Supply Chain, Logistics, Data Science or related field \n MBA preferred \n Supply Chain certification preferred \n 3+ years experience in similar role, including Global Operations \n 3+ years working experience and proficiency in digital data analysis platforms \n Demonstrated ability to provide descriptive, predictive, and prescriptive data analysis as well as create models and forecasting tools to evaluate scenarios \n Demonstrated ability to create data visual tools such as dashboards in Excel or other programs (ex: Tableau, Power BI, etc.) \n Highly-developed attention to detail and problem-solving skills \n Process improvement and Project Management experience preferred (Six Sigma belt certification, and PMP certification preferred) \n Experience with Lean, Six Sigma, similar methodologies preferred \n \n \n \n  Physical Requirements: \n \n Alternate Sitting or Standing at will \n Lifting or Carrying - 10 lbs or less, as needed for PC, office materials, etc \n Keyboarding - entering data by use of a traditional keyboard \n Gross Manipulation - seizing, holding, grasping, turning, or otherwise working with your hands \n Fine Manipulation - touching, picking, pinching, or otherwise working with your fingers \n Speaking - expressing or exchanging ideas by means of the spoken word \n Hearing Requirements - the ability to hear, understand, and distinguish speech and/or other sounds \n Near Visual Acuity - clarity of vision at approximately 20 inches or less (including use of computers) \n \n \n \n  Job Location:  St. Louis, MO, United States or Remote \n  Position Type:  Full-Time/Regular/Salary/Exempt \n  Salary:  Competitive base \n  Benefits : Employee Stock Options, 401K, Healthcare, Dental, Vision, Life, HSA/FSA \n \n  About Elemental Enzymes: \n  Elemental Enzymes was founded upon the simple belief that we must do everything we can to improve agricultural performance in a way that not only enables plants to flourish but enriches and renews the planet with eco-friendly and sustainable agricultural solutions. From products that enable effective enzymes, peptide and protein inputs to foliar treatments that hold the promise to renew entire industries, our focus is to make a positive difference in the world through people \u2013 like you and me. Success is a byproduct of responsible production. That is why Elemental Enzymes works to bring cross-disciplinary scientists and processes together to create novel solutions to practical problems with a shared goal of helping both the plant and the planet. \n \n  Apply: \n  Please complete the application and submit your resume and cover letter to the open job posting on Bamboo HR. References may be required upon request",
        "cleaned_desc": "  Qualifications: \n \n Bachelor's Degree in Business Analytics, Supply Chain, Logistics, Data Science or related field \n MBA preferred \n Supply Chain certification preferred \n 3+ years experience in similar role, including Global Operations \n 3+ years working experience and proficiency in digital data analysis platforms \n Demonstrated ability to provide descriptive, predictive, and prescriptive data analysis as well as create models and forecasting tools to evaluate scenarios \n Demonstrated ability to create data visual tools such as dashboards in Excel or other programs (ex: Tableau, Power BI, etc.) \n Highly-developed attention to detail and problem-solving skills \n Process improvement and Project Management experience preferred (Six Sigma belt certification, and PMP certification preferred) \n Experience with Lean, Six Sigma, similar methodologies preferred ",
        "techs": [
            "tableau",
            "power bi",
            "six sigma belt certification",
            "pmp certification",
            "lean",
            "six sigma"
        ],
        "cleaned_techs": [
            "tableau",
            "powerbi",
            "six sigma belt certification",
            "pmp certification",
            "lean",
            "six sigma"
        ]
    },
    "f6a2f4a5b893251b": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Sr. Director, Technical Program Management AI/ML (Remote- Eligible)",
        "company": "Capital One",
        "desc": "Locations: VA - McLean, United States of America, McLean, Virginia\n   Sr. Director, Technical Program Management AI/ML (Remote- Eligible)\n  \n  Sr. Director, Technical Program Management \n \n  Are you interested in leading programs that deliver on critical business goals and build large scale products & platforms? \n \n  About Capital One: At Capital One, we\u2019re changing banking for good. We were founded on the belief that no one should be locked out of the financial system. We\u2019re dedicated to helping foster a world where everyone has an equal opportunity to prosper. \n \n  We\u2019re a bank, but we don\u2019t think like one. We\u2019re always thinking about what\u2019s next, about how we can innovate and inspire, and about how we can develop the tools our customers need to improve their financial lives - by leveraging best in class technology. That\u2019s where you come in. \n \n  About the team: As a Sr. Director of Technical Program Management (TPM) on Capital One\u2019s Enterprise product & platform organization, we\u2019re looking for someone that can help us build solid platforms on mobile and web surfaces that will help Capital One customers have incredible experiences. The platforms are the foundational blocks on which Capital One\u2019s various lines of businesses (like credit card, retail banking, auto finance) will build delightful experiences for our customers. \n \n  In addition to the technical program, you will also work to pave the way for an expanding TPM discipline within the team, by leveraging your industry knowledge and experience to teach the organization what a great TPM can achieve. \n \n  Our TPM Sr. Directors have: \n \n  Strong technical backgrounds (ideally building highly scalable platforms, products, or services) with the ability to proactively identify and mitigate technical risks throughout delivery life-cycle \n  Experience building and leading a world-class team of technical program managers with a mission to power real-time, intelligent experiences for our customers and associates \n  Lead technology focused discussions with senior leadership by providing valuable insights and recommendations that guide technology-based decisions \n  Exceptional communication and collaboration skills \n  Excellent problem solving and influencing skills \n  A quantitative approach to problem solving and a collaborative implementer to holistic solutions; a systems thinker \n  Experienced TPM leader to grow and develop a team of TPMs, while also building the foundations for the TPM practice \n  Ability to simplify the technically complex and drive well-educated decisions across product, engineering, design, and data science representatives \n  Deep focus on execution, follow-through, accountability, and results \n  Exceptional cross-team collaboration; able to work across different functions, organizations, and reporting boundaries to get the job done. \n  Highly tuned emotional intelligence, good listener, and deep seated empathy for teams and partners \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications \n \n  Bachelor's degree \n  At least 9 years of experience managing technical programs \n \n \n  Preferred Qualifications \n \n  10 + years of experience designing and building data-intensive solutions using distributed computing \n  2+ years of experience with the full ML development lifecycle using modern technology in a business critical setting. Previous experience with machine learning (building models, deploying models, setting up cloud infrastructure and/or data pipelines) and familiarity with major ML frameworks such as XGBoost, PyTorch, AWS SageMaker, etc. \n  5+ years of experience in building distributed systems & highly available services using cloud computing services / architecture - preferably using AWS \n  5+ years of building & leading experienced TPM team \n  5+ years of experience in leading software engineering teams \n  8+ years of experience within a large/data-intensive multi-line business environment \n  Expertise designing, implementing, and scaling complex production-ready data pipelines for ML models \n  Experience partnering with technology peers responsible for data architecture and distributed computing infrastructure/platforms \n  Ability to communicate complex technical concepts clearly to a variety of audiences \n  ML industry impact through conference presentations, papers, blog posts, open source contributions, or patents \n  Bachelor's degree in a related technical field (Computer Science, Software Engineering) \n  MBA or Master\u2019s Degree in a related technical field (Computer Science, Software Engineering) or equivalent experience \n  Ability to attract and develop high-performing software engineers with an inspiring leadership style \n \n \n  At this time, Capital One will not sponsor a new applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $267,300 - $305,100 for Sr. Director, Technical Program Management\n   San Francisco, California (Hybrid On-Site): $283,100 - $323,100 for Sr. Director, Technical Program Management\n   Remote (Regardless of Location): $226,500 - $258,500 for Sr. Director, Technical Program Management\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": " \n  Strong technical backgrounds (ideally building highly scalable platforms, products, or services) with the ability to proactively identify and mitigate technical risks throughout delivery life-cycle \n  Experience building and leading a world-class team of technical program managers with a mission to power real-time, intelligent experiences for our customers and associates \n  Lead technology focused discussions with senior leadership by providing valuable insights and recommendations that guide technology-based decisions \n  Exceptional communication and collaboration skills \n  Excellent problem solving and influencing skills \n  A quantitative approach to problem solving and a collaborative implementer to holistic solutions; a systems thinker \n  Experienced TPM leader to grow and develop a team of TPMs, while also building the foundations for the TPM practice \n  Ability to simplify the technically complex and drive well-educated decisions across product, engineering, design, and data science representatives \n  Deep focus on execution, follow-through, accountability, and results \n  Exceptional cross-team collaboration; able to work across different functions, organizations, and reporting boundaries to get the job done. \n  Highly tuned emotional intelligence, good listener, and deep seated empathy for teams and partners \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n    Basic Qualifications \n \n  Bachelor's degree \n  At least 9 years of experience managing technical programs \n \n \n  Preferred Qualifications \n \n  10 + years of experience designing and building data-intensive solutions using distributed computing \n  2+ years of experience with the full ML development lifecycle using modern technology in a business critical setting. Previous experience with machine learning (building models, deploying models, setting up cloud infrastructure and/or data pipelines) and familiarity with major ML frameworks such as XGBoost, PyTorch, AWS SageMaker, etc. \n  5+ years of experience in building distributed systems & highly available services using cloud computing services / architecture - preferably using AWS \n  5+ years of building & leading experienced TPM team \n  5+ years of experience in leading software engineering teams \n  8+ years of experience within a large/data-intensive multi-line business environment \n  Expertise designing, implementing, and scaling complex production-ready data pipelines for ML models \n  Experience partnering with technology peers responsible for data architecture and distributed computing infrastructure/platforms ",
        "techs": [
            "distributed computing",
            "ml development lifecycle",
            "machine learning",
            "models",
            "cloud infrastructure",
            "data pipelines",
            "ml frameworks",
            "xgboost",
            "pytorch",
            "aws sagemaker",
            "cloud computing services",
            "aws",
            "software engineering teams",
            "data-intensive multi-line business environment",
            "data pipelines",
            "data architecture",
            "distributed computing infrastructure/platforms"
        ],
        "cleaned_techs": [
            "distributed computing",
            "models",
            "cloud infrastructure",
            "data pipelines",
            "ml frameworks",
            "xgboost",
            "pytorch",
            "aws",
            "cloud computing services",
            "software engineering teams",
            "data-intensive multi-line business environment",
            "data architecture",
            "distributed computing infrastructure/platforms"
        ]
    },
    "ca873819ac09663a": {
        "terms": [
            "data science",
            "data analyst"
        ],
        "salary_min": 97319.734,
        "salary_max": 123228.47,
        "title": "Senior R Programmer/Analyst \u2013 RWD & Analytics",
        "company": "SimulStat",
        "desc": "To Apply for this Job Click Here \n \n  Remote, 12-month contract, potential to extend long-term. \n \n  As a Senior Programmer/Analyst, Real-World Data and Analytics, you will conduct hands-on programming (expert level for R, proficiency with SAS is also desired) in supporting our real-world data & analytics needs under the supervision of our Director HEOR analytics scientists. \n \n \n Responsibilities: \n  Respond to HEOR Director\u2019s request timely. \n Understand query, analytic requests and study specifications \n Proactively clarify requests and ensure accurate implementation of protocols and analytical specifications. \n Conduct queries to claims, EHR, and/or other real-world data sources \n Ensure high-quality work products \n Keep detailed documentation and generate results with a clear and professional presentation \n Detail-oriented with excellent communication skills. At ease with an abundance of detail and complexity, yet mindful of the big picture \n Can work independently and efficiently to meet aggressive timelines where needed \n Adapt to rapidly changing priorities \n \n \n Qualifications: \n  A Master\u2019s degree in health services research, epidemiology, biostatistics, public health, or a subject area with a strong focus on the application of data science to healthcare data with 5+ years of hands-on programming experience; or a Bachelor\u2019s degree in a quantitative field with 10+ years of hands-on programming and analytic experience using healthcare data. \n Must have hands-on RWD/RWE experience with data sources such as large-scale administrative claims data (e.g., MarketScan, Optum, Medicare LDS SAF, Komodo claims), and EHR \n Advanced programming capacity in R (R Shiny) with proficiency in SAS \n Proficiency with common statistical methods, such as survival analysis, generalized linear models \n Prior experience working on oncology projects is preferred \n Ability to effectively communicate methods and findings \n \n  To Apply for this Job Click Here",
        "cleaned_desc": " Proactively clarify requests and ensure accurate implementation of protocols and analytical specifications. \n Conduct queries to claims, EHR, and/or other real-world data sources \n Ensure high-quality work products \n Keep detailed documentation and generate results with a clear and professional presentation \n Detail-oriented with excellent communication skills. At ease with an abundance of detail and complexity, yet mindful of the big picture    A Master\u2019s degree in health services research, epidemiology, biostatistics, public health, or a subject area with a strong focus on the application of data science to healthcare data with 5+ years of hands-on programming experience; or a Bachelor\u2019s degree in a quantitative field with 10+ years of hands-on programming and analytic experience using healthcare data. \n Must have hands-on RWD/RWE experience with data sources such as large-scale administrative claims data (e.g., MarketScan, Optum, Medicare LDS SAF, Komodo claims), and EHR \n Advanced programming capacity in R (R Shiny) with proficiency in SAS \n Proficiency with common statistical methods, such as survival analysis, generalized linear models \n Prior experience working on oncology projects is preferred ",
        "techs": [
            "r",
            "r shiny",
            "sas",
            "survival analysis",
            "generalized linear models"
        ],
        "cleaned_techs": [
            "r",
            "r shiny",
            "sas",
            "survival analysis",
            "generalized linear models"
        ]
    },
    "743e7da975150baa": {
        "terms": [
            "data science"
        ],
        "salary_min": 108306.0,
        "salary_max": 157040.0,
        "title": "Senior Technical Analyst - Remote",
        "company": "Mayo Clinic",
        "desc": "Why Mayo Clinic \n \n \n  Mayo Clinic is top-ranked in more specialties than any other care provider according to U.S. News & World Report. As we work together to put the needs of the patient first, we are also dedicated to our employees, investing in competitive compensation and comprehensive benefit plans \u2013 to take care of you and your family, now and in the future. And with continuing education and advancement opportunities at every turn, you can build a long, successful career with Mayo Clinic. You\u2019ll thrive in an environment that supports innovation, is committed to ending racism and supporting diversity, equity and inclusion, and provides the resources you need to succeed.\n  \n \n \n Responsibilities \n \n  This position will enable the success of Mayo Clinic\u2019s strategic investment in Oracle\u2019s reporting and analytics (R&A) solutions through building and maintaining key data, reporting and analytics via OTBI, BI Publisher and the Oracle Analytics / Fusion Analytics Warehouse (FAW). Success in this role will require the ability to lead, direct and execute data, reporting and analytical needs through all analytics project stages: Business Understanding, Data Understanding, Data Acquisition and Preparation, Testing, Validation, Visualization/Model Development, Deployment, Maintenance, and Retirement. Key responsibilities include: \n \n Design, build and test data augmentation and reporting solutions in the Oracle Analytics / Fusion Analytics Warehouse (FAW) including processing, cleansing, and verifying the integrity of data used for analysis, in partnership with Information Technology \n Build data models in Oracle BI Publisher for report development consumption. \n Design, develop, maintain, and deliver key Oracle Analytics / Fusion Analytics Warehouse (FAW) and ERP/HCM (OTBI and BI Publisher) reporting dashboards, analytics, and extracts. \n Schedule and deploy ad hoc and recurring analytics. \n Collaborate with Finance, HR, Supply Chain, and other domains to create reporting functional and technical design documentation (FDDs/TDDs) that accurately document required functionality. \n Engage with Harwick Project implementation partner for onboarding and knowledge transfer. \n Regression test solutions as part of regular upgrade cycle for both Oracle Fusion and Oracle Analytics / Fusion Analytics Warehouse (FAW) products. \n \n Must have a strong working knowledge of Finance, Human Resources, Workforce Management and/or Supply Chain processes while providing reporting and analytics (R&A) analysis supporting the software programs or modules across multiple clinical and/or business functions. Understands the end-to-end business processes and the impact IT functions can have on the business environment. Leads reporting and analytics (R&A) analysis Leads R&A analysis and support for installed systems as well as R&A implementation and projects that require systems data, reporting and analytics analysis, design, testing and implementation for specific R&A modules or solutions. Leads transformation efforts to optimize end to end R&A processes to achieve efficiency and enhanced end user experience. Informs and advises Product Owners on strategic product direction relevant to Mayo to develop product roadmaps. Proficient knowledge of existing systems and core domain operations and is certified in the vended application, as required. Understands the business strategy and configures, designs, develops, or modifies R&A solutions via vendor tools or augmented tools developed within Mayo Clinic, and does so within complex, integrated systems. Is able to validate data integrity of changes introduced and impact on end-to-end R&A solutions R&A solutions for end users. Validates R&A solutions R&A solutions align with stakeholders\u2019 needs / requirements to ensure completeness, correctness, and clarity. Validates solutions to ensure it satisfies the stated requirements. Provides R&A quality assurance services for projects and systems and ensures regulatory compliance as it applies to R&A support for the business areas served. Leads and resolves customer problems with R&A and responds to requested improvements and enhancements. Provides or reviews content for updating FAQs, job aids, and training materials. Research requests to determine scope, size, and impact. Works with customers to elicit requirements through a variety of techniques. Functions as a liaison between clinical, business, and technical areas throughout the lifecycle of the system, including transformation efforts. Supports and participates in data governance, reference data management, and/or data standards. Builds credibility and rapport with customers to understand their needs. Manages medium- to large-sized projects and manages complexities within an integrated environment. Contributes to end to end R&A process testing activities. May be required to provide 24/7 on-call support. \n  This vacancy is not eligible for sponsorship/ we will not sponsor or transfer visas for this position. \n  This position is 100% remote; can work from anywhere in the U.S. Possible infrequent travel to a Mayo Clinic campus may be required. \n  Qualifications \n \n  Bachelor\u2019s degree with minimum of 8 years of relevant experience or Master\u2019s degree with 6 years relevant experience. Relevant experience is considered in administrative areas such as HR, Finance, Supply Chain, Healthcare, or Information Technology. Vendor application certification may be required. \n  Preferred Experience: \n \n Deep understanding of Oracle data lineage \n Strong SQL experience for data retrieval and data modeling. \n Knowledge of Oracle Cloud Financial (FIN), HR (HCM) System and / or Supply Chain (SCM) modules \n Strong grasp of core data warehouse concept and design paradigms \n Desire & ability to work in a team environment and train on new Oracle technologies, including options for Oracle University certifications \n Experience as a data scientist or developer with Fusion Analytics Warehouse (FAW), Oracle Autonomous Data Warehouse (ADW), EPM, Oracle Analytics Cloud (OAC) and / or Oracle Data Integrator (ODI) \n \n \n \n Exemption Status \n \n  Exempt\n  \n \n Compensation Detail \n \n  $108,306 - $157,040 / year\n  \n \n Benefits Eligible \n \n  Yes\n  \n \n Schedule \n \n  Full Time\n  \n \n Hours/Pay Period \n \n  80\n  \n \n Schedule Details \n \n  Typically business hours\n  \n \n Weekend Schedule \n \n  as needed\n  \n \n International Assignment \n \n  No\n  \n \n Site Description \n \n \n  Just as our reputation has spread beyond our Minnesota roots, so have our locations. Today, our employees are located at our three major campuses in Phoenix/Scottsdale, Arizona, Jacksonville, Florida, Rochester, Minnesota, and at Mayo Clinic Health System campuses throughout Midwestern communities, and at our international locations. Each Mayo Clinic location is a special place where our employees thrive in both their work and personal lives. Learn more about what each unique Mayo Clinic campus has to offer, and where your best fit is.\n   \n \n \n \n \n Affirmative Action and Equal Opportunity Employer \n \n \n  As an Affirmative Action and Equal Opportunity Employer Mayo Clinic is committed to creating an inclusive environment that values the diversity of its employees and does not discriminate against any employee or candidate. Women, minorities, veterans, people from the LGBTQ communities and people with disabilities are strongly encouraged to apply to join our teams. Reasonable accommodations to access job openings or to apply for a job are available.\n    \n \n \n \n \n Recruiter \n \n  Maggie Kramer",
        "cleaned_desc": "Why Mayo Clinic \n \n \n  Mayo Clinic is top-ranked in more specialties than any other care provider according to U.S. News & World Report. As we work together to put the needs of the patient first, we are also dedicated to our employees, investing in competitive compensation and comprehensive benefit plans \u2013 to take care of you and your family, now and in the future. And with continuing education and advancement opportunities at every turn, you can build a long, successful career with Mayo Clinic. You\u2019ll thrive in an environment that supports innovation, is committed to ending racism and supporting diversity, equity and inclusion, and provides the resources you need to succeed.\n  \n \n \n Responsibilities \n \n  This position will enable the success of Mayo Clinic\u2019s strategic investment in Oracle\u2019s reporting and analytics (R&A) solutions through building and maintaining key data, reporting and analytics via OTBI, BI Publisher and the Oracle Analytics / Fusion Analytics Warehouse (FAW). Success in this role will require the ability to lead, direct and execute data, reporting and analytical needs through all analytics project stages: Business Understanding, Data Understanding, Data Acquisition and Preparation, Testing, Validation, Visualization/Model Development, Deployment, Maintenance, and Retirement. Key responsibilities include: \n \n Design, build and test data augmentation and reporting solutions in the Oracle Analytics / Fusion Analytics Warehouse (FAW) including processing, cleansing, and verifying the integrity of data used for analysis, in partnership with Information Technology \n Build data models in Oracle BI Publisher for report development consumption. \n Design, develop, maintain, and deliver key Oracle Analytics / Fusion Analytics Warehouse (FAW) and ERP/HCM (OTBI and BI Publisher) reporting dashboards, analytics, and extracts. \n Schedule and deploy ad hoc and recurring analytics. \n Collaborate with Finance, HR, Supply Chain, and other domains to create reporting functional and technical design documentation (FDDs/TDDs) that accurately document required functionality. \n Engage with Harwick Project implementation partner for onboarding and knowledge transfer. \n Regression test solutions as part of regular upgrade cycle for both Oracle Fusion and Oracle Analytics / Fusion Analytics Warehouse (FAW) products. \n   Must have a strong working knowledge of Finance, Human Resources, Workforce Management and/or Supply Chain processes while providing reporting and analytics (R&A) analysis supporting the software programs or modules across multiple clinical and/or business functions. Understands the end-to-end business processes and the impact IT functions can have on the business environment. Leads reporting and analytics (R&A) analysis Leads R&A analysis and support for installed systems as well as R&A implementation and projects that require systems data, reporting and analytics analysis, design, testing and implementation for specific R&A modules or solutions. Leads transformation efforts to optimize end to end R&A processes to achieve efficiency and enhanced end user experience. Informs and advises Product Owners on strategic product direction relevant to Mayo to develop product roadmaps. Proficient knowledge of existing systems and core domain operations and is certified in the vended application, as required. Understands the business strategy and configures, designs, develops, or modifies R&A solutions via vendor tools or augmented tools developed within Mayo Clinic, and does so within complex, integrated systems. Is able to validate data integrity of changes introduced and impact on end-to-end R&A solutions R&A solutions for end users. Validates R&A solutions R&A solutions align with stakeholders\u2019 needs / requirements to ensure completeness, correctness, and clarity. Validates solutions to ensure it satisfies the stated requirements. Provides R&A quality assurance services for projects and systems and ensures regulatory compliance as it applies to R&A support for the business areas served. Leads and resolves customer problems with R&A and responds to requested improvements and enhancements. Provides or reviews content for updating FAQs, job aids, and training materials. Research requests to determine scope, size, and impact. Works with customers to elicit requirements through a variety of techniques. Functions as a liaison between clinical, business, and technical areas throughout the lifecycle of the system, including transformation efforts. Supports and participates in data governance, reference data management, and/or data standards. Builds credibility and rapport with customers to understand their needs. Manages medium- to large-sized projects and manages complexities within an integrated environment. Contributes to end to end R&A process testing activities. May be required to provide 24/7 on-call support. \n  This vacancy is not eligible for sponsorship/ we will not sponsor or transfer visas for this position. \n  This position is 100% remote; can work from anywhere in the U.S. Possible infrequent travel to a Mayo Clinic campus may be required. \n  Qualifications \n \n  Bachelor\u2019s degree with minimum of 8 years of relevant experience or Master\u2019s degree with 6 years relevant experience. Relevant experience is considered in administrative areas such as HR, Finance, Supply Chain, Healthcare, or Information Technology. Vendor application certification may be required. \n  Preferred Experience: \n \n Deep understanding of Oracle data lineage \n Strong SQL experience for data retrieval and data modeling. \n Knowledge of Oracle Cloud Financial (FIN), HR (HCM) System and / or Supply Chain (SCM) modules \n Strong grasp of core data warehouse concept and design paradigms \n Desire & ability to work in a team environment and train on new Oracle technologies, including options for Oracle University certifications \n Experience as a data scientist or developer with Fusion Analytics Warehouse (FAW), Oracle Autonomous Data Warehouse (ADW), EPM, Oracle Analytics Cloud (OAC) and / or Oracle Data Integrator (ODI) \n \n \n \n Exemption Status \n ",
        "techs": [
            "mayo clinic",
            "u.s. news & world report",
            "oracle reporting and analytics (r&a) solutions",
            "otbi",
            "bi publisher",
            "oracle analytics",
            "fusion analytics warehouse (faw)",
            "information technology",
            "finance",
            "hr",
            "supply chain",
            "harwick project implementation partner",
            "oracle fusion",
            "r&a modules",
            "vendor tools",
            "end-to-end r&a solutions",
            "regulatory compliance",
            "faqs",
            "data governance",
            "reference data management",
            "data standards",
            "sql",
            "oracle cloud financial (fin)",
            "hr (hcm) system",
            "supply chain (scm) modules",
            "oracle data lineage",
            "fusion analytics warehouse (faw)",
            "oracle autonomous data warehouse (adw)",
            "epm",
            "oracle analytics cloud (oac)",
            "oracle data integrator (odi)"
        ],
        "cleaned_techs": [
            "mayo clinic",
            "u.s. news & world report",
            "oracle",
            "otbi",
            "bi publisher",
            "fusion analytics warehouse (faw)",
            "information technology",
            "finance",
            "hr",
            "supply chain",
            "harwick project implementation partner",
            "r&a modules",
            "vendor tools",
            "end-to-end r&a solutions",
            "faqs",
            "data governance",
            "reference data management",
            "data standards",
            "sql",
            "hr (hcm) system",
            "supply chain (scm) modules",
            "epm"
        ]
    },
    "0caa00e2fbc8aa53": {
        "terms": [
            "data science"
        ],
        "salary_min": 156300.0,
        "salary_max": -1.0,
        "title": "Senior Principal, Digital Product Design (Remote Eligible)",
        "company": "Nike",
        "desc": "Become a Part of the NIKE, Inc. Team\n  \n  \\r\n  \n  \\r\n  \n  NIKE, Inc. does more than outfit the world's best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it's about each person bringing skills and passion to a challenging and constantly evolving game.\n  \n  Open to remote work except in South Dakota, Vermont and West Virginia.\n  \n  The annual base salary for this position ranges from $156,300.00 in our lowest geographic market to $370,100.00 in our highest geographic market. Actual salary will vary based on a candidate's location, qualifications, skills and experience.\n  \n  Information about benefits can be found here .\n  \n  WHO WE ARE LOOKING FOR\n  \n  We are looking for a Sr. Principal, Digital Product Design leader who is an architect of entire product ecosystems and has created and launched more industry-leading products than you can count! We seek a strong generative and transformative designer with exceptional interaction design skills. We will empower you to lead the largest and most complex projects and push the organization to seek out new opportunities that will have a positive impact on the business!\n  \n  WHAT YOU WILL WORK ON\n  \n \n  Rapidly visualizing, prototyping and exploring multiple product design directions in response to hypotheses - with sharp consideration of the business and technical impacts of the solutions \n  Owning large end-to-end initiatives that span multiple teams and outcomes at the forefront of a new product area \n  Defining an entire category of experiences, proposing solutions that drive business objectives and using design methods to identify new opportunities \n  Elevating the role of digital design at Nike and facilitating a dialog/awareness between our org and the external design community \n  Driving industry-wide thought leadership for highly-visible projects, policies or high-impact initiatives in either scale or level of innovation \n  Ensuring cross-functional support for executions by prioritizing and guiding product goals and influencing the strategic approach to the market/industry \n  Guiding multiple teams to solve problems previously thought to be insurmountable while mentoring, inspiring and developing senior talent \n  Influencing team culture to be more inclusive, welcoming and candid - identifying and leading Nike Digital Design organization, cross pillar and company-wide initiatives \n \n  WHO YOU WILL WORK WITH\n  \n  In this role, you will work collaboratively with a cross-functional team of product and program managers, engineers, researchers, producers, and fellow designers - representing the work, and defending your solutions in front of executives. You will report in to the Director, Digital Product Design. We enable you to partner with senior leaders and VPs across the company to create a vision for what a product should be, and then guide and shape the design from beginning to end as it becomes a reality.\n  \n  WHAT YOU BRING\n  \n \n  10+ years of relevant experience in the design industry \n  Bachelor's degree or equivalent combination of education, experience, or training \n  An entrepreneurial spirit with refined business insight - executives across the company seek your advice and expertise when understanding the future of digital product design \n  Strategic problem solver who knows how to connect business goals with consumer needs to build the best experience \n  Recognized as a subject matter expert by peers both internally and externally due to your track record and understanding of data science, experimentation, product, brand, business and strategy \n  Hands-on creative who's an expert in all facets of information architecture, interaction design and UI design \n  Works enthusiastically and collaboratively with cross-functional partners, building trusting relationships \n  Experienced working with both abstract information and complex, detailed consumer research and data \n  Expert in design and prototyping tools such as Figma, Adobe CC, Principle and Keynote \n  Ownership mentality and accountability with experience leading complex projects and driving innovation efforts that uncover new value with new kinds of user experiences \n  Excels in all forms of communication, including using storytelling and narrative to influence and persuade partners \n  Experienced presenting designs clearly to facilitate useful feedback while shifting your approach based on the audience and desired outcome \n  Outstanding attention to visual detail, high sense of craft/quality, delivering consistent, polished, and balanced layouts through typography, color and visual hierarchy \n \n  NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.\n  \n  NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.\n  \n \n How We Hire \n \n  At NIKE, Inc. we promise to provide a premium, inclusive, compelling and authentic candidate experience. Delivering on this promise means we allow you to be at your best - and to do that, you need to understand how the hiring process works. Transparency is key.\n  \n \n \n This overview explains our hiring process for corporate roles. Note there may be different hiring steps involved for non-corporate roles. \n \n \n  Benefits \n \n  Whether it's transportation or financial health, we continually invest in our employees to help them achieve greatness - inside and outside of work. All who work here should be able to realize their full potential.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "8d2a284213d3a17b": {
        "terms": [
            "data science"
        ],
        "salary_min": 60000.0,
        "salary_max": 85000.0,
        "title": "Benefits Specialist (Remote Eligible)",
        "company": "Mathematica Policy Research",
        "desc": "Position Description :    About Mathematica:      Mathematica applies expertise at the intersection of data, methods, policy, and practice to improve well-being around the world. We collaborate closely with public- and private-sector partners to translate big questions into deep insights that improve programs, refine strategies, and enhance understanding using data science and analytics. Our work yields actionable information to guide decisions in wide-ranging policy areas, from health, education, early childhood, and family support to nutrition, employment, disability, and international development. Mathematica offers our employees competitive salaries, and a comprehensive benefits package, as well as the advantages of being 100 percent employee owned. As an employee stock owner, you will experience financial benefits of ESOP holdings that have increased in tandem with the company\u2019s growth and financial strength. You will also be part of an independent, employee-owned firm that is able to define and further our mission, enhance our quality and accountability, and steadily grow our financial strength. Read more about our benefits here: https://www.mathematica.org/career-opportunities/benefits-at-a-glance.      About the opportunity:      Mathematica's Human Resources department is seeking a Benefits Specialist to support and administer leave requests under the Family and Medical Leave Act (FMLA), parental leave, personal leave, workers-compensation, short-term or long-term disability plans in conjunction with state and local leave laws.      Responsibilities:     \n \n Manages HR Benefits Mailbox and responds to benefits inquiries from managers and employees on leave and disability \n  Provides staff guidance regarding coordination of benefits, eligibility notices, timesheets, PTO, benefit deductions, and parental leave \n  Maintains leave related internal guidance documents \n  Completes data entry for all leave related absences \n  Provides leave reporting to the payroll team on a semi-monthly cadence and ad-hoc leave reporting \n  Grants permissions to leave restricted timesheet codes \n  Plans, executes, and coordinates communication for leave law changes \n  Administers Parental Leave and Adoption benefit programs \n  Conducts bi-weekly claim review calls with The Hartford to manage the leave management partnership and act as an employee advocate for any claim issues \n  Identify and track claim related issues \n  Actively support the advancement of organizational diversity, equity, and inclusion efforts, and apply diversity, equity, and inclusion lens across job responsibilities. \n  Additional duties may be assigned as needed \n \n \n \n \n Position Requirements :     \n \n Bachelor\u2019s degree in Human Resources, Business Administration, or related degree (or equivalent experience) \n  Three plus years\u2019 experience in leave management administration \n  Proficiency with Microsoft Office, Excel, Word, PowerPoint, and Outlook \n  Working understanding of human resource and employee benefit principles, practices, and procedures \n  Ability to effectively communicate complex information in simple terms with all levels of employees and vendors \n  Ability to handle highly confidential information with discretion \n  High level of accuracy and attention to detail \n  Capable of multi-tasking, highly organized, and able to meet deadlines \n  The ability to manage work in a high-volume, fast-paced environment, and juggle multiple priorities \n  Excellent written and verbal communication skills \n  This position offers an anticipated annual base salary of $60,000 - $85,000. \n      \n  Available locations: Washington, DC; Princeton, NJ; Remote \n     \n \n \n We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "ec892e6be7c18279": {
        "terms": [
            "data science",
            "data engineer"
        ],
        "salary_min": 114196.33,
        "salary_max": 144598.0,
        "title": "TSS Data Engineer Senior",
        "company": "General Dynamics Information Technology",
        "desc": "Clearance Level None Category Data Science Location Remote, Working from District of Columbia \n \n \n Public Trust:  BI Full 6C (T4)   \n Requisition Type:  Pipeline   \n Your Impact  \n Own your opportunity to manage the network that makes mission success possible. Make an impact by using your skills to deliver \u201cOne GDIT Network\u201d for our clients. \n  Job Description \n \n  Deliver simple solutions to complex problems as a Data Engineer Senior at GDIT. Here, you\u2019ll tailor cutting-edge solutions to the unique requirements of our clients. With a career in application development, you\u2019ll make the end user\u2019s experience your priority and we\u2019ll make your career growth ours.    At GDIT, people are our differentiator. As a Data Engineer Senior you will help ensure today is safe and tomorrow is smarter. Our work depends on Data Engineer Senior joining our team to help improve Aviation safety in collaboration with our stakeholders. Our customers deal with real-world safety problems and this candidate can help develop the required software designs and code to scale our system and improve user insights into safety issues.     HOW A DATA ENGINEER SENIOR WILL MAKE AN IMPACT: \n \n  Possesses and applies a comprehensive knowledge across key tasks and high impact assignments.  \n Plans and leads major technology assignments.  \n Evaluates performance results and functions as a technical expert across multiple project assignments.  \n May supervise others. \n  Codes, tests, debugs, implements, and documents low to highly complex programs. \n  Creates appropriate documentation in work assignments such as program code, and technical documentation. \n  Designs systems and programs to meet complex business needs. \n  Prepares detailed specifications from which programs are developed and coded. \n  Ensures programs meet standards and technical specifications; performs technical analysis and component delivery. \n  Gathers information from existing systems, analyzes program and time requirements. \n  Assists project manager in preparing time estimates and justification for assigned tasks. \n  Designs programs for projects or enhancements to existing programs. \n  Writes specifications for programs of low to advanced complexity. \n  Assists support and/or project personnel in resolving varying levels of complex program problems. \n  Works with client and management to resolve issues and validate programming requirements within their areas of responsibility. \n  Provides technical advice on complex programming. \n  Develops test plans to verify logic of new or modified programs. \n  Conducts quality assurance activities such as peer reviews. \n  Creates appropriate documentation in work assignments such as program code, and technical documentation. \n  Remains abreast of industry technical trends and new development to maintain current skills and remain current with industry standards. \n  Designs, develops, evaluates, plans and tests engineering specifications for software programs and applications \n \n \n Education and Required Experience: Bachelor\u2019s Degree + 5 years experience \n Required Technical Skills: AWS Cloud Engineer \n Security Clearance Level: Public Trust \n Required Skills and Abilities : AWS Glue, AWS Lambda, Python, JIRA \n Location: Remote \n US Citizenship Required \n \n \n  Preferred Skills   \n \n Preferred Skills: AWS Athena, AWS CI/CD pipeline \n  Secret Clearance or ability to obtain a clearance \n  Home location of DMV \n \n \n  GDIT IS YOUR PLACE: \n \n \n Full-flex work week to own your priorities at work and at home \n 401K with company match \n Comprehensive health and wellness packages \n Internal mobility team dedicated to helping you own your career \n Professional growth opportunities including paid education and certifications \n Cutting-edge technology you can learn from \n Rest and recharge with paid vacation and holidays \n \n \n \n \n  GDIT IS YOUR PLACE: \n \n \n Full-flex work week to own your priorities at work and at home \n 401K with company match \n Comprehensive health and wellness packages \n Internal mobility team dedicated to helping you own your career \n Professional growth opportunities including paid education and certifications \n Cutting-edge technology you can learn from \n Rest and recharge with paid vacation and holidays",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "b1ce00cae72d8184": {
        "terms": [
            "data science"
        ],
        "salary_min": 60000.0,
        "salary_max": 60001.0,
        "title": "Postdoctoral Fellow Quantitative Ecology Deep Learning",
        "company": "UF/IFAS Florida Medical Entomology Laboratory",
        "desc": "Posting snapshot:  Postdoctoral associate opportunity - spatiotemporal deep learning models of mosquito borne viruses and the environment \n Remote applicants welcome. \n Position Summary.  Applicants are invited to apply for a postdoctoral associate position focused on spatiotemporal deep learning models of arbovirus surveillance data. This 1-year position is part of a multidisciplinary project aimed at predicting zoonotic arbovirus transmission hazard across different environments in Florida and is supported through funding from the University of Florida Research Opportunity Seed Fund. The project is a cross-college collaborative effort between Dr. Rob Guralnick (PI) Florida Museum, Dr. Lindsay Campbell (coPI) FMEL and Department of Entomology and Nematology, and Dr. Whit Schroder (coPI) Florida Institute of the Built Environment Resilience and Department of Anthropology. \n The incumbent has the option of working remotely or at the Florida Medical Entomology Laboratory (FMEL) in Vero Beach, Florida. Research responsibilities will include applying, troubleshooting, and evaluating deep learning models using time series of arbovirus surveillance and environmental data to predict West Nile virus and eastern equine encephalitis virus transmission in sentinel chickens for the purpose of improved and targeted surveillance. \n Expectations \n The initial appointment is for 12 months, with renewal based on additional funding availability. \n Expected salary is ~ $60,000/year, fringe benefits. The position start date will begin in fall of 2023. \n Required Degree \n Doctorate - ecology, biology, computer science, biostatistics, geography, or related fields. \n Minimum Requirements \n \n The candidate must have a Ph.D. in ecology, biology, computer science, biostatistics, geography, or related fields. \n Experience in deep learning approaches with environmental spatial, temporal, and/or spatiotemporal modeling is required. \n Demonstrated experience in geographic information systems (GIS). \n Demonstrated experience in Python and/or R programming languages \n The candidate must have demonstrated effective written and oral communication skills in science. \n Demonstrated success working as part of a team \n \n Preferred Qualifications \n \n Experience with remotely sensed environmental data \n \n Additional desired qualifications include: \n \n Strong organizational and communication skills \n Interest and experience with mentoring graduate students \n \n Professional Development Opportunities \n Opportunities to engage in professional development activities that will enhance their experience and better prepare them for careers in academia, government institutions, or industry. These opportunities include but are not limited to grant proposal writing, mentorship of graduate students, and science communication. \n Required Application Materials \n Candidates should submit a cover letter describing research experience and career goals and how they fit this position, a curriculum vitae, and names and contact information for three references to Lindsay Campbell (lcampbell2@ufl.edu). \n Special Instructions \n Review of applications will begin on November 1, 2023 and continue until the position is filled. \n We are striving continuously to build and sustain an equitable and inclusive environment. Applicants from historically excluded groups are encouraged to apply. \n The final candidate will be required to provide an official transcript to the hiring department upon hire. Degrees earned from an education institution outside of the United States are required to be evaluated by a professional credentialing service provider approved by National Association of Credential Evaluation Services (NACES), which can be found athttp://www.naces.org/. \n The University of Florida is An Equal Employment Opportunity Institution dedicated to building broadly diverse and inclusive employees. Hiring is contingent upon eligibility to work in the US. Searches are conducted in accordance with Florida's Sunshine Law. \n Search Committee \n Lindsay P. Campbell (chair), PhD ,  FMEL, UF; lcampbell2@ufl.edu \n Rob Guralnick, PhD, Florida Museum, UF; rguralnick@flmnh.ufl.edu \n Job Type: Full-time \n Pay: $60,000.00 - $60,001.00 per year \n Benefits: \n \n Health insurance \n \n Schedule: \n \n Monday to Friday \n \n Experience: \n \n Python: 1 year (Preferred) \n SQL: 1 year (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": " \n The candidate must have a Ph.D. in ecology, biology, computer science, biostatistics, geography, or related fields. \n Experience in deep learning approaches with environmental spatial, temporal, and/or spatiotemporal modeling is required. \n Demonstrated experience in geographic information systems (GIS). \n Demonstrated experience in Python and/or R programming languages \n The candidate must have demonstrated effective written and oral communication skills in science. \n Demonstrated success working as part of a team \n \n Preferred Qualifications \n ",
        "techs": [
            "ph.d. in ecology",
            "biology",
            "computer science",
            "biostatistics",
            "geography\ndeep learning approaches\nenvironmental spatial modeling\ntemporal modeling\nspatiotemporal modeling\ngeographic information systems (gis)\npython programming language\nr programming language\nwritten and oral communication skills in science"
        ],
        "cleaned_techs": [
            "biology",
            "computer science",
            "biostatistics"
        ]
    },
    "b6322345d39699fd": {
        "terms": [
            "data science"
        ],
        "salary_min": 51000.0,
        "salary_max": 82000.0,
        "title": "Data Scientist",
        "company": "Peraton",
        "desc": "Peraton Overview \n  Peraton drives missions of consequence spanning the globe and extending to the farthest reaches of the galaxy. As the world's leading mission capability integrator and transformative enterprise IT provider, we deliver trusted and highly differentiated national security solutions and technologies that keep people safe and secure. Peraton serves as a valued partner to essential government agencies across the intelligence, space, cyber, defense, civilian, health, and state and local markets. Every day, our employees do the can't be done, solving the most daunting challenges facing our customers.\n  \n Responsibilities \n \n  The Defense Mission and Health Solutions team is seeking a \n  Data Scientist  to join the FPS2 program to fulfill a high-visibility and critical role to support maintenance and operations for models, which includes development of requirements for new analytic tools and system enhancements and assisting lead Data Scientist with management of their execution. This is a complex project with many stakeholders, and it requires intimate knowledge and experience of Databricks E2 and MLflow. \n  \n \n What you'll do:  \n \n  This role will be responsible for the following but not limited to: \n  \n \n Full life cycle product development on a variety of architectures and target platforms including applications data acquisition and analysis, networks and communications/security, health care data cross correlation, artificial intelligence, database management, GUI development.  \n Designs, develops, documents, tests and debugs applications software and systems that contain logical and mathematical solutions.  \n Perform testing and quality control on analytic models to ensure a smooth production release using Python, R, Scala, etc.  \n Conducting research, requirements design and validation of new Analytic products.  \n Properly document code, artifact description, implementation, and use cases  \n Provides data analytic support to client and leadership teams with reports, analytics, and with other ad hoc requests.  \n Understand the current reporting process and work out ways to optimize and enhance regular reports.  \n Coordinate with different functional teams to help implement models and monitor outcomes  \n Assists modelers with data-related questions to ensure models run efficiently and use the correct data  \n Triages, investigates, and helps resolve data issues  \n Customer and end user support and communication  \n Creation of training videos for the Analytic platform  \n \n \n Qualifications \n \n \n Required Qualifications:  \n \n \n Bachelor's degree with 0-2 years of experience OR HS diploma with 6-8 years of experience  \n Demonstrated experience with Programming in Python, R, SnowFlake, DataBricks  \n Experience with SQL  \n Database experience  \n Analytics experience  \n Requirements design and validation experience  \n Must be a US citizen and able to obtain a Public Trust security clearance  \n \n \n Preferred Qualifications:  \n \n \n Demonstrated experience with Programming Databricks E2, MLFlow, Neo4j, geopspatial model development, Airflow  \n Demonstrated experience with NLP  \n Demonstrates ability to communicate with individuals at many different levels from within and from outside of the company  \n Experience analyzing complex situations and delivery advance and direction to technical staff in times of uncertainty  \n Excellent communication and team-building expertise  \n \n \n Benefits:  \n \n  At Peraton, our benefits are designed to help keep you at your best beyond the work you do with us daily. We're fully committed to the growth of our employees. From fully comprehensive medical plans to tuition reimbursement, tuition assistance, and fertility treatment, we are there to support you all the way. \n  \n \n Target Salary Range \n \n  $51,000 - $82,000. This represents the typical salary range for this position based on experience and other factors.\n  \n \n SCA / Union / Intern Rate or Range \n \n \n EEO \n  An Equal Opportunity Employer including Disability/Veteran.\n  \n \n Our Values \n \n \n Benefits \n  At Peraton, our benefits are designed to help keep you at your best beyond the work you do with us daily. We're fully committed to the growth of our employees. From fully comprehensive medical plans to tuition reimbursement, tuition assistance, and fertility treatment, we are there to support you all the way.\n  \n \n  Paid Time-Off and Holidays \n  Retirement \n  Life & Disability Insurance \n  Career Development \n  Tuition Assistance and Student Loan Financing \n  Paid Parental Leave \n  Additional Benefits \n  Medical, Dental, & Vision Care",
        "cleaned_desc": " Perform testing and quality control on analytic models to ensure a smooth production release using Python, R, Scala, etc.  \n Conducting research, requirements design and validation of new Analytic products.  \n Properly document code, artifact description, implementation, and use cases  \n Provides data analytic support to client and leadership teams with reports, analytics, and with other ad hoc requests.  \n Understand the current reporting process and work out ways to optimize and enhance regular reports.  \n Coordinate with different functional teams to help implement models and monitor outcomes  \n Assists modelers with data-related questions to ensure models run efficiently and use the correct data  \n Triages, investigates, and helps resolve data issues  \n Customer and end user support and communication  \n Creation of training videos for the Analytic platform  \n \n \n Qualifications \n \n \n Required Qualifications:    \n \n Bachelor's degree with 0-2 years of experience OR HS diploma with 6-8 years of experience  \n Demonstrated experience with Programming in Python, R, SnowFlake, DataBricks  \n Experience with SQL  \n Database experience  \n Analytics experience  \n Requirements design and validation experience  \n Must be a US citizen and able to obtain a Public Trust security clearance  \n \n \n Preferred Qualifications:  \n \n \n Demonstrated experience with Programming Databricks E2, MLFlow, Neo4j, geopspatial model development, Airflow  \n Demonstrated experience with NLP  ",
        "techs": [
            "python",
            "r",
            "scala",
            "sql",
            "snowflake",
            "databricks",
            "mlflow",
            "neo4j",
            "airflow"
        ],
        "cleaned_techs": [
            "python",
            "r",
            "scala",
            "sql",
            "snowflake",
            "databricks",
            "mlflow",
            "neo4j",
            "airflow"
        ]
    },
    "33ea0afe08e72de9": {
        "terms": [
            "data science"
        ],
        "salary_min": 51000.0,
        "salary_max": 82000.0,
        "title": "Data Scientist",
        "company": "Peraton",
        "desc": "Responsibilities: \n   The Defense Mission and Health Solutions team is seeking a  Data Scientist  to join the FPS2 program to fulfill a high-visibility and critical role to support maintenance and operations for models, which includes development of requirements for new analytic tools and system enhancements and assisting lead Data Scientist with management of their execution. This is a complex project with many stakeholders, and it requires intimate knowledge and experience of Databricks E2 and MLflow. \n \n  What you\u2019ll do: \n \n  This role will be responsible for the following but not limited to:  \n \n Full life cycle product development on a variety of architectures and target platforms including applications data acquisition and analysis, networks and communications/security, health care data cross correlation, artificial intelligence, database management, GUI development. \n  Designs, develops, documents, tests and debugs applications software and systems that contain logical and mathematical solutions. \n  Perform testing and quality control on analytic models to ensure a smooth production release using Python, R, Scala, etc. \n  Conducting research, requirements design and validation of new Analytic products.  \n Properly document code, artifact description, implementation, and use cases \n  Provides data analytic support to client and leadership teams with reports, analytics, and with other ad hoc requests. \n  Understand the current reporting process and work out ways to optimize and enhance regular reports. \n  Coordinate with different functional teams to help implement models and monitor outcomes \n  Assists modelers with data-related questions to ensure models run efficiently and use the correct data \n  Triages, investigates, and helps resolve data issues \n  Customer and end user support and communication  \n Creation of training videos for the Analytic platform  \n Qualifications: \n   Required Qualifications: \n \n  Bachelor's degree with 0-2 years of experience OR HS diploma with 6-8 years of experience \n  Demonstrated experience with Programming in Python, R, SnowFlake, DataBricks \n  Experience with SQL \n  Database experience \n  Analytics experience \n  Requirements design and validation experience \n  Must be a US citizen and able to obtain a Public Trust security clearance \n \n \n Preferred Qualifications: \n \n \n  Demonstrated experience with Programming Databricks E2, MLFlow, Neo4j, geopspatial model development, Airflow \n  Demonstrated experience with NLP  \n Demonstrates ability to communicate with individuals at many different levels from within and from outside of the company \n  Experience analyzing complex situations and delivery advance and direction to technical staff in times of uncertainty \n  Excellent communication and team-building expertise \n \n \n  Benefits:  \n \n \n At Peraton, our benefits are designed to help keep you at your best beyond the work you do with us daily. We\u2019re fully committed to the growth of our employees. From fully comprehensive medical plans to tuition reimbursement, tuition assistance, and fertility treatment, we are there to support you all the way. \n  Peraton Overview: \n  \n   Peraton drives missions of consequence spanning the globe and extending to the farthest reaches of the galaxy. As the world\u2019s leading mission capability integrator and transformative enterprise IT provider, we deliver trusted and highly differentiated national security solutions and technologies that keep people safe and secure. Peraton serves as a valued partner to essential government agencies across the intelligence, space, cyber, defense, civilian, health, and state and local markets. Every day, our employees do the can\u2019t be done, solving the most daunting challenges facing our customers.\n   Target Salary Range: $51,000 - $82,000. This represents the typical salary range for this position based on experience and other factors. EEO: An Equal Opportunity Employer including Disability/Veteran.",
        "cleaned_desc": "Responsibilities: \n   The Defense Mission and Health Solutions team is seeking a  Data Scientist  to join the FPS2 program to fulfill a high-visibility and critical role to support maintenance and operations for models, which includes development of requirements for new analytic tools and system enhancements and assisting lead Data Scientist with management of their execution. This is a complex project with many stakeholders, and it requires intimate knowledge and experience of Databricks E2 and MLflow. \n \n  What you\u2019ll do: \n \n  This role will be responsible for the following but not limited to:  \n \n Full life cycle product development on a variety of architectures and target platforms including applications data acquisition and analysis, networks and communications/security, health care data cross correlation, artificial intelligence, database management, GUI development. \n  Designs, develops, documents, tests and debugs applications software and systems that contain logical and mathematical solutions.    Perform testing and quality control on analytic models to ensure a smooth production release using Python, R, Scala, etc. \n  Conducting research, requirements design and validation of new Analytic products.  \n Properly document code, artifact description, implementation, and use cases \n  Provides data analytic support to client and leadership teams with reports, analytics, and with other ad hoc requests. \n  Understand the current reporting process and work out ways to optimize and enhance regular reports. \n  Coordinate with different functional teams to help implement models and monitor outcomes \n  Assists modelers with data-related questions to ensure models run efficiently and use the correct data \n  Triages, investigates, and helps resolve data issues \n  Customer and end user support and communication    Creation of training videos for the Analytic platform  \n Qualifications: \n   Required Qualifications: \n \n  Bachelor's degree with 0-2 years of experience OR HS diploma with 6-8 years of experience \n  Demonstrated experience with Programming in Python, R, SnowFlake, DataBricks \n  Experience with SQL \n  Database experience \n  Analytics experience    Requirements design and validation experience \n  Must be a US citizen and able to obtain a Public Trust security clearance \n \n \n Preferred Qualifications: \n \n \n  Demonstrated experience with Programming Databricks E2, MLFlow, Neo4j, geopspatial model development, Airflow \n  Demonstrated experience with NLP  ",
        "techs": [
            "databricks e2",
            "mlflow",
            "python",
            "r",
            "scala",
            "snowflake",
            "sql",
            "snowflake",
            "neo4j",
            "airflow",
            "nlp"
        ],
        "cleaned_techs": [
            "databricks e2",
            "mlflow",
            "python",
            "r",
            "scala",
            "snowflake",
            "sql",
            "neo4j",
            "airflow",
            "nlp"
        ]
    },
    "8a38be14367b739b": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 82000.0,
        "salary_max": 82000.0,
        "title": "Healthcare Data Analyst - Hybrid Office or Remote",
        "company": "Qlarant",
        "desc": "Qlarant is a not-for-profit corporation that partners with public and private sectors to create high quality, safe, and efficient delivery of health care and human services programs. We have multiple lines of business including utilization review, managed care organization quality review, and quality assurance for programs serving individuals with developmental disabilities. Qlarant is also a national leader in fighting fraud, waste and abuse for large organizations across the country. In addition, our Foundation provides grant opportunities to those with programs for under-served communities. \n  Looking to start or grow your career in healthcare data analytics? As a Data Analyst III working on our Unified Program Integrity Contractors (UPIC) team for the Western Jurisdiction, you can contribute to our efforts to make a positive difference in the future of our nation's healthcare programs. Our UPIC West team identifies and investigates fraud, waste and abuse in the Medicare and Medicaid programs covering 13 states and 3 territories. You will utilize your statistical analysis programming skills to detect patterns of potential fraud in large healthcare data sets. Experience in at least one of the following is required: Python, R and SAS in addition to SQL. This position is perfect for a recent Master's grad with academic project/internship experience using Python, R or SAS. \n  This position could be based in our Los Alamitos, CA office or home based in most states of the continental US. \n  This position is an entry level professional performing study design, data analysis, and report preparation. Studies originate from preliminary data analysis (trends), literature review, experience and expertise of the team, and mandated projects. Data analysis, including data preparation and presentation of findings is performed in conjunction with other analysts. Reports are drafted by teams with leadership of Data Scientists. \n  Essential Duties and Responsibilities  include the following. Other duties may be assigned. \n \n Trend data to identify potential opportunities (e.g., variances, significant outliers, percentile ranked groups) for quality improvement or focused investigations. \n Aid in design data analysis strategies to identify potential areas for quality improvement or focused investigation. \n Analyze data, draw conclusions, and summarize into quality indicator values \n Develop tabular and graphical presentations of data, which clearly and concisely illustrate current levels of care. \n Populate tabular and graphical presentation of data. \n Contribute to the development of interventions (i.e., develop educational materials for doctors and nurses) which will improve healthcare processes and outcomes. \n Facilitate design re-measurement strategies (after intervention) of healthcare processes and outcomes to effectively quantify impact of interventions for improvement. \n Analyze re-measurement data and summarize into quality indicator values. \n Support development of reports concerning all of the above. \n May mentor junior Data Analysts in technical aspects of their work. \n Assist in preparing findings for publishing in peer reviewed journals. \n Familiar with commonly used concepts, practices and procedures, relying on instructions and pre-established guidelines to perform the functions of the job. \n \n Supervisory Responsibilities \n  This job has no supervisory responsibilities. \n  Salary Range: Offers are based on skill level, experience and geographic location. Internal equity is a major factor in our salary offers. For example, the hiring range for Chico, CA, Atlanta, Dallas and Salt Lake City would be $82,000/yr and Los Angeles would be up to $90,000/yr. We offer a complete compensation package that includes health, dental, vision, and long and short term disability insurance, generous leave accruals, tuition reimbursement and a retirement plan that contributes an amount equal to 10% of your earnings with no required employee contribution. We also offer promotional opportunities and a collaborative and inclusive work environment. \n \n \n  Required Skills\n  \n To perform the job successfully, an individual should demonstrate the following competencies: \n \n Analytical - Synthesizes complex or diverse information; Collects and researches data; Uses intuition and experience to complement data. \n Problem Solving - Identifies and resolves problems in a timely manner; Gathers and analyzes information skillfully; Develops alternative solutions. \n Judgment - Exhibits sound and accurate judgment; Supports and explains reasoning for decisions. \n \n   Other Skills and Abilities \n \n To perform this job successfully, an individual should have fluency in Python, R or SAS, SQL and MS Office. \n Ability to work independently and in teams. \n Must possess familiarity with statistical methodologies, and automation techniques for analytic tasks. \n Ability to work with highly sensitive information while preserving the confidentiality of the information. \n Working knowledge of healthcare systems, Medicare/Medicaid preferred, healthcare databases and coding systems. \n \n \n \n  Required Experience\n  \n \n Bachelor's degree (BA or BS) in Statistics, Biostatistics, Applied Mathematics, Mathematics, Public Health or related majors is required. \n Minimum of 2 years of hands-on work exprerience using Python, R or SAS. A Masters degree and academic project experience can be substitued. \n 6 months experience with health related analytic research and quality improvement methodology (ISO, CQI, TQM, Six Sigma, Lean, Etc.) preferred. \n \n Qlarant is an Equal Opportunity Employer of Minorities, Females, Protected Veterans, and Individuals with Disabilities.",
        "cleaned_desc": " \n Analytical - Synthesizes complex or diverse information; Collects and researches data; Uses intuition and experience to complement data. \n Problem Solving - Identifies and resolves problems in a timely manner; Gathers and analyzes information skillfully; Develops alternative solutions. \n Judgment - Exhibits sound and accurate judgment; Supports and explains reasoning for decisions. \n \n   Other Skills and Abilities \n \n To perform this job successfully, an individual should have fluency in Python, R or SAS, SQL and MS Office. \n Ability to work independently and in teams. ",
        "techs": [
            "python",
            "r",
            "sas",
            "sql",
            "ms office"
        ],
        "cleaned_techs": [
            "python",
            "r",
            "sas",
            "sql",
            "microsoft"
        ]
    },
    "77fdd56440089809": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 79836.13,
        "salary_max": 120000.0,
        "title": "Sr. Business Analyst",
        "company": "iCUBE Inc",
        "desc": "iCUBE, Inc. is seeking a Sr. Business Analyst on a federal healthcare project to accelerate the modernization of a legacy healthcare system with a need to move from manual processes to automated processes. The initial phase of the project involves 1) conduct a current state analysis of the existing system, 2) identify the future state system, and 3) develop a modernization roadmap. \n Responsibilities : \n 1. Perform Current State Assessment of the existing system \n 2. Conduct discovery sessions with the stakeholders \n 3. Design survey instruments to elicit information from stakeholders \n 4. Review any existing service blueprints and usability study of the system to identify the human interactions with the system \n 5. Identify all manual processes, and understand the supporting processes \n 6. Generate use cases based on potential use of system by all stakeholders and interaction with other systems \n 7. Identify the customer / consumer interaction that directly and indirectly contributes to records being entered into the system \n 8. Perform SWOT analysis \n 9. Translate user needs and business goals into innovative and effective service designs \n 10. Develop prototypes and testing designs with users, and iterating based on feedback \n 11. Develop roadmaps and provide recommendations for system modernization \n Qualifications: \n 1. Extensive experience (7+ years) in business analysis \n 2. Extensive experience performing the Current State Assessment and Future State Analysis, interacting with stakeholders and federal clients \n 3. Extensive experience and knowledge in using customer feedback survey tools: Google Forms, Microsoft Forms, Qualtrics for data analysis to inform design decisions. \n 4. Proficient in building workflow diagrams: MS Visio, Lucidchart, OmniGraffle, Miro, etc. \n 5. Proficiency in reading and writing business requirements, features, epics, user stores, functional and technical tasks, implementing process documentation, findings, reports. \n 6. Excellent business and technical writing skills. \n 7. Experience with Agile Methodologies, Agile Release Trains, backlog grooming etc. \n 8. Experience in using and building appealing presentations such in PowerPoint, Google Slides, create graphics, infographics, etc. \n 9. Ability to translate user needs and business goals into innovative and effective solution requirements. \n 10. Deep healthcare domain knowledge and experience working with healthcare systems (5+ years) is a plus. Experience working with Federal government projects is a huge plus. \n 11. Expertise in designing seamless, omnichannel customer experiences. \n 12. Excellent communication skills, both written and verbal, and experience with communication at an executive level, small to midsize groups. \n 13. Experience working with SharePoint for design collaboration and document management. \n Additional Qualifications: \n \n Bachelor's degree in Business Analysis or related field \n Proven experience as a Business Analyst \n High degree of proficiency MS Office Suite (Excel, Word, PPT), Outlook & internet browser applications \n Familiarity with CMMI framework, ISO 9001 Standard is a plus \n Able to successfully pass background check prior to employment \n \n Job Type: Full-time \n Pay: $79,836.13 - $120,000.00 per year \n Benefits: \n \n 401(k) matching \n AD&D insurance \n Dental insurance \n Health insurance \n Life insurance \n Paid holidays \n Paid sick time \n Vision insurance \n \n Compensation package: \n \n Yearly pay \n \n Experience level: \n \n 7 years \n \n Schedule: \n \n 8 hour shift \n \n Application Question(s): \n \n Please describe your experience, approach, and specific projects conducting current state assessments of existing systems, identifying future state, and creating a roadmap for the same. \n \n Work Location: Remote",
        "cleaned_desc": " 11. Develop roadmaps and provide recommendations for system modernization \n Qualifications: \n 1. Extensive experience (7+ years) in business analysis \n 2. Extensive experience performing the Current State Assessment and Future State Analysis, interacting with stakeholders and federal clients \n 3. Extensive experience and knowledge in using customer feedback survey tools: Google Forms, Microsoft Forms, Qualtrics for data analysis to inform design decisions. \n 4. Proficient in building workflow diagrams: MS Visio, Lucidchart, OmniGraffle, Miro, etc. \n 5. Proficiency in reading and writing business requirements, features, epics, user stores, functional and technical tasks, implementing process documentation, findings, reports. \n 6. Excellent business and technical writing skills. \n 7. Experience with Agile Methodologies, Agile Release Trains, backlog grooming etc. \n 8. Experience in using and building appealing presentations such in PowerPoint, Google Slides, create graphics, infographics, etc. \n 9. Ability to translate user needs and business goals into innovative and effective solution requirements. \n 10. Deep healthcare domain knowledge and experience working with healthcare systems (5+ years) is a plus. Experience working with Federal government projects is a huge plus.   11. Expertise in designing seamless, omnichannel customer experiences. \n 12. Excellent communication skills, both written and verbal, and experience with communication at an executive level, small to midsize groups. \n 13. Experience working with SharePoint for design collaboration and document management. \n Additional Qualifications: \n \n Bachelor's degree in Business Analysis or related field \n Proven experience as a Business Analyst \n High degree of proficiency MS Office Suite (Excel, Word, PPT), Outlook & internet browser applications \n Familiarity with CMMI framework, ISO 9001 Standard is a plus \n Able to successfully pass background check prior to employment \n \n Job Type: Full-time ",
        "techs": [
            "google forms",
            "microsoft forms",
            "qualtrics",
            "ms visio",
            "lucidchart",
            "omnigraffle",
            "miro",
            "powerpoint",
            "google slides",
            "sharepoint"
        ],
        "cleaned_techs": [
            "google forms",
            "microsoft forms",
            "qualtrics",
            "ms visio",
            "lucidchart",
            "omnigraffle",
            "miro",
            "powerpoint",
            "google slides",
            "sharepoint"
        ]
    },
    "406383540b54932b": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 77996.77,
        "salary_max": 98761.29,
        "title": "Workforce Analyst/Business Analyst",
        "company": "Akima",
        "desc": "Work Where it Matters \n Akima Systems Engineering (ASE), an Akima company, is not just another federal systems support contractor. As an Alaska Native Corporation (ANC), our mission and purpose extend beyond our exciting federal projects as we support our shareholder communities in Alaska. \n At ASE, the work you do every day makes a difference in the lives of our 15,000 I\u00f1upiat shareholders, a group of Alaska natives from one of the most remote and harshest environments in the United States. \n For our shareholders,  ASE provides support and employment opportunities and contributes to the survival of a culture that has thrived above the Arctic Circle for more than 10,000 years. \n For our government customers,  ASE delivers solutions in maritime IT, systems engineering, and integration across the Department of Defense and stands ready to help improve operational performance at a reasonable and sustainable cost. \n As an ASE employee,  you will be surrounded by a challenging, yet supportive work environment that is committed to innovation and diversity, two of our most important values. You will also have access to our comprehensive benefits and competitive pay in addition to growth opportunities and excellent retirement options. \n Job Summary: \n The U.S. Geological Survey (USGS) is a premier government science organization with approximately 8,500 employees. USGS monitors, analyzes, and predicts current and evolving Earth-system interactions and delivers actionable intelligence at scales and time frames relevant to decision makers. USGS provides science about natural hazards, natural resources, ecosystems and environmental health, and the effects of climate and land-use change. USGS employs approximately 1,500 Hydrologic Technician\u2019s that work in all 50 states and territories. They are located at 28 water science centers in all seven regions, with a handful in the Water Mission Area, and at the Volcano Science Center. The USGS regional science centers have a need for consulting support services to conduct a workforce analysis and development of a workforce plan in support of the Water Science Center\u2019s Hydrologic Technician (GS-1316 occupational series) as described below. \n Akima Systems Engineering is looking for a Workforce Analyst/Business Analyst to conduct activities necessary to collect pertinent information for agency stakeholders (including but no limited to USGS Managers, Supervisors, Hydrologic Technicians, and human resources staff) and analyze the information and data collected. \n Job Responsibilities: \n \n Carries out professional judgment in planning and accomplishing assigned tasks. \n Ability to identify when data on one or more parameters are not available. \n Carries out research/development activities to evaluate workforce metrics, attrition, and productivity to improve future workforce capabilities. \n Collects data from outsides sources, makes detailed observations, analyzes data, and interprets results. \n Compiles results and prepares technical reports and documentation of outcomes. \n Familiar with a variety of workforce analysis concepts, practices, and procedures. \n Analyzes data collected through qualitative and quantitative research methods, to identify gaps that exist based on data collected and determine staffing needs for current and future workforces. \n \n Minimum Qualifications: \n \n Master\u2019s degree and 3-5 years of experience in workforce analytics and business. \n Ability to interact with senior leadership. \n Ability to work collaboratively and independently. \n Experienced in workforce analysis. \n \n We are an equal opportunity employer. All applicants will receive consideration for employment, without regard to race, color, religion, creed, national origin, gender or gender-identity, age, marital status, sexual orientation, veteran status, disability, pregnancy or parental status, or any other basis prohibited by law. If you are an individual with a disability, and would like to request a reasonable accommodation for any part of the employment process, please contact us at job-assist@akima.com or 571-353-7053 (information about job applications status is not available at this contact information). \n Job:  Life, Physical & Social Science \n Travel:  No \n Organization:  ASE \n Clearance:  Not Applicable \n Shift:  Day Job \n Work Type:  Remote \n Req ID:  ASE00659",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "9d6170b683b059ca": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 70000.0,
        "salary_max": 85000.0,
        "title": "Business Data Analyst",
        "company": "Stand Up Wireless",
        "desc": "Business Data Analyst \n Join a Growing Wireless Provider and Unlock the Power of Data-Driven Success as a Business Data Analyst for  StandUp Wireless ! \n \n Do you see yourself thriving as part of a fast-paced team where the decisions made can have a direct impact on the overall business? \n Are you looking for an opportunity where you touch many aspects of the business, one where you would be responsible for driving new ideas and new revenue streams? \n Can you take raw data and generate meaningful consumer insight from it? Present your analysis and influence business, financial, and product decisions? \n \n If so,  StandUp Wireless  is the company for you! \n Who We Are \n StandUp Wireless  is a dynamic wireless service provider focused on leveraging technical insights and innovative solutions to keep our communities connected. The company offers an energetic, entrepreneurial atmosphere where employees work in teams to help achieve goals.  StandUp is a company built on trust that values individuals with personal initiative and creative solutions. Best of all\u2026 We are a small but mighty company that makes a huge impact! \n What You Will Be Doing \n The Business Data Analyst will report directly to the VP of Finance and partner internally with the operational, sales, and product teams along with cross-entity stakeholders to deliver analytical business solutions. The BDA will create ad-hoc reporting and KPIs to drive business decisions along with recommending actions to management by analyzing results and identifying trends. These reports and KPIs will transform into dashboards in partnering with the BI Team. The ultimate goal of the BDA is to fill the ever-open gap of information from two aspects, ad-hoc and recurring. To do this, this position requires the ability to work in a fast-paced and entrepreneurial environment, where flexibility and the eagerness to learn are a must. \n Responsibilities include: \n \n Performing Ad-hoc analysis, primarily in Excel, in a timely and accurate manner to drive impactful decisions while also working with internal teams and external partners to solve outstanding issues \n Working with large data sets to deliver analytics that informs decision-making, strategic planning, and results tracking thoroughly and thoughtfully \n Transforming commonly used ad-hoc analysis created in Excel to dashboards and reports while working closely with the BI team \n Working across multiple departments and with the companies\u2019 leadership to ensure reporting and KPI needs are met on a daily, weekly, and monthly basis \n \n How You Qualify \n \n BS/BA degree in related field \n Minimum 3+ years business data analysis experience \n Advanced Excel proficiency \n Amazon Quicksight and telecom industry experience preferred \n \n What We Offer \n The  StandUp Wireless  family engages in a highly collaborative and fast paced environment that values your opinions in every step of the creative process! As we work together to serve low-income communities, we are committed to work-life balance, an inclusive culture, and shared respect for each other across the organization. And we have a great benefits package for our full-time employees that includes: \n \n Competitive Compensation \n Bonus Incentive Program \n Medical, Dental and Vision Options \n 401K Plan With 3% Employer Contribution \n Company-Paid Life and Long-term Disability Benefits \n Voluntary Benefits and Wellness Programs \n Paid Vacation, Sick Leave, and Holidays \n Remote Telecommuting Stipend \n \n How to Apply \n PLEASE SUBMIT RESUME AND DESIRED SALARY TO BE CONSIDERED FOR THIS ROLE \n Candidates must be legally eligible to work in the US for any employer. \n We are an Equal Opportunity Employer that values diversity in the workplace. \n Job Type: Full-time \n Pay: $70,000.00 - $85,000.00 per year \n Benefits: \n \n 401(k) \n AD&D insurance \n Dental insurance \n Disability insurance \n Health insurance \n Health savings account \n Life insurance \n Paid holidays \n Paid time off \n Vision insurance \n \n Compensation package: \n \n Bonus opportunities \n \n Experience level: \n \n 3 years \n \n Application Question(s): \n \n PLEASE NOTE THAT IF YOU DO NOT RESPOND TO EMPLOYER-WRITTEN QUESTIONS, YOUR APPLICATION IS NOT COMPLETE AND WILL NOT BE CONSIDERED. \n Describe 3 business data analyses projects that you've recently completed. \n What is your annualized pay requirement? Please be specific and do not state 'negotiable'. \n \n Work Location: Remote",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "4d896a6c4fdbf51a": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 105000.0,
        "salary_max": 120000.0,
        "title": "Business Analyst",
        "company": "Ascendion",
        "desc": "Description \n \n About Ascendion \n  : \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Ascendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. \n  Ascendion | Engineering to elevate life \n  We have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us: \n \n Build the coolest tech for world\u2019s leading brands \n Solve complex problems \u2013 and learn new skills \n Experience the power of transforming digital engineering for Fortune 500 clients \n Master your craft with leading training programs and hands-on experience \n \n Experience a community of change makers! \n  Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion. \n  About the Role: \n  Job  Title ::  HR Business Analyst \n  Job  Description: \n \n \n We are seeking a Business System Analyst to join our team to support managers in using forecasting and analytical tools to evaluate existing and potential programs or product lines. 2-3 years' experience. \n \n \n \n \n \n Minimum Qualifications: \n \n \n Experience with OpenText. \n Experience of SuccessFactors and Employee Central. \n Skilled in Process Documentation and Process Mapping. \n \n \n \n \n \n \n \n Location : Remote (All over US) \n  Salary Range:  The salary for this position is between 105,000- 120,000 Annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate. \n  Benefits : The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [09-10 days/hours of paid time off] \n   Want to change the world? Let us know. \n  Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let\u2019s talk! \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Preferred Skills: \n \n Opentext \n  Successfactor \n  Process mapping \n \n Job details \n \n \n Job ID \n \n \n   328922\n   \n \n \n \n Job Requirements \n \n \n   Business Analyst\n   \n \n \n \n \n Location \n \n \n   Seattle, Washington, US\n   \n \n \n \n \n Recruiter \n \n \n   Sankirna\n   \n \n \n \n Email \n \n \n   sankirna.nighot@ascendion.com",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "df4f5af6ac1fbbb0": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 87765.53,
        "salary_max": 111130.72,
        "title": "Sr Business Analyst",
        "company": "Criterion Systems, Inc.",
        "desc": "Overview: \n  \n   At Criterion Systems, we developed a different kind of business\u2014a company whose real value is a reputation for excellence built upon the collective skills, talents, perspectives, and backgrounds of its people. By accepting a position with Criterion Systems, you will join a group of professionals with a collaborative mindset where we share ideas and foster professional development to accomplish our goals. In addition to our great culture, we also offer competitive compensation and benefit packages, company-sponsored team building events, and advancement opportunities. To find out more about how Criterion can help you take your career to the next level please visit our website: www.criterion-sys.com. \n   Criterion Systems is a Military/Veteran Friendly Company therefore we encourage Veterans to apply. \n  Responsibilities: \n  \n   We are seeking a mission-focused \n   Sr Business Analyst  for a \n   100% remote  position supporting a USDA \u2013 Forest Service team. The customer is headquartered in \n   Boise, \n Idaho .\n  \n \n \n  Duties, Tasks & Responsibilities \n  The successful candidate will support USDA Fire Aviation and Management (FAM) Applications for Operations and Maintenance work and enhancements and NextGen application modernization efforts. The ideal candidate should possess the following skills:\n  \n \n  Use the tools directed by the customer for the entry of software defects and track them through implementation and testing to ensure the defect has been addressed, passes testing, and meets the customers\u2019 requirements. \n  Manage communications by providing timely updates to management, customer, and user community. \n  Compose, read, interpret, and edit complex documents and correspondence and relate information to stakeholders. \n  Develop and enhance requirements documentation, processes, and systems in full detail.  \n Gather requirements and prepare requirements documentation and other similar technical documentation.  \n Work with the customer to ensure that all requirements are clearly and unambiguously documented and work with the developers to ensure requirements are clearly understood. \n  Ability to quickly learn and support a new application and related business processes in detail as well as the intricacies and challenges of the functional business area. \n  Ensure all project management deliverables are accurate, of a high quality and completed on time. \n  Ensure product deliverables' quality, such as functional specifications, logical and physical data design, solution design and detailed design, test plans, and completed solution. \n  Communicate enhancements and prioritization of stories, features and epics. \n  Liaise with the customer to prioritize work to be completed.  \n Experience working in an Agile software development environment. \n  Experienced as a software tester and are capable of training people to become users of the system.  \n Test the software applications to verify and validate the requirements have been implemented correctly by the developers. \n  Test the web-based software applications for section 508 compliance using 508 compliance testing tools as directed by the customer. \n  Experience with tools such as Jira, GitHub and ZenHub to monitor and track work in progress, and you are comfortable providing status updates to the customer.  \n Enter stories into GitHub/Zenhub or tools as directed for use by the customer. \n  Proficient at producing burndown charts, velocity charts and other agile reporting tools and techniques  \n Qualifications: \n  \n  Required Experience, Education, Skills & Technologies \n \n \n  Bachelor\u2019s degree or demonstrated 4 years\u2019 recent equivalent experience, with a major in a field that provides substantial knowledge useful for managing IT requirements, four years of experience can be used in lieu of degree. \n  Four (4) + years of experience gathering requirements and documenting requirements for software / web applications. \n  Self-motivated and professional in demeanor with proven success as a business analyst and tester supporting new releases and emergency code fixes. \n  Proven ability to interface successfully with customer/client to drive high customer satisfaction. \n  Excellent written and verbal communications skills required for engagement with customer and team members in daily meetings and in documenting requirements, user stories, testing results and status. \n \n \n  Preferred Experience, Education, Skills & Technologies \n \n \n  Government customer experience \n  Experience with automated testing tools \n  Experience with 508-compliance assessment and testing \n \n \n  Security Clearance Level \n \n \n  Public Trust \n \n \n  Certification  \n \n \n None \n \n \n  Work Schedule \n \n \n  Full-time, remote \n \n \n   \n Benefits Offered \n \n \n  Medical, Dental, Vision, Life Insurance, Short-Term Disability, Long-Term Disability, 401(k) match, Tuition/Training Assistance, Parental Leave, Paid Time Off, and Holidays. \n \n \n  Criterion Systems, LLC and its subsidiaries are committed to equal employment opportunity and non-discrimination at all levels of our organization. We believe in treating all applicants and employees fairly and make employment decisions without regard to any individual\u2019s protected status: race, ethnicity, color, national origin, ancestry, religion, creed, sex/gender, gender identity/gender expression, sexual orientation, physical and mental disability, marital/parental status, pregnancy (including childbirth, lactation, and related medical conditions), age, genetic information (including characteristics and testing), military and veteran status, or any other characteristic protected by law.  \n For our complete EEO/AA and Pay Transparency statement, please visit  \n https://careers-criterion-sys.icims.com/.",
        "cleaned_desc": "  Test the web-based software applications for section 508 compliance using 508 compliance testing tools as directed by the customer. \n  Experience with tools such as Jira, GitHub and ZenHub to monitor and track work in progress, and you are comfortable providing status updates to the customer.  \n Enter stories into GitHub/Zenhub or tools as directed for use by the customer. \n  Proficient at producing burndown charts, velocity charts and other agile reporting tools and techniques  \n Qualifications: \n  \n  Required Experience, Education, Skills & Technologies \n \n \n  Bachelor\u2019s degree or demonstrated 4 years\u2019 recent equivalent experience, with a major in a field that provides substantial knowledge useful for managing IT requirements, four years of experience can be used in lieu of degree. \n  Four (4) + years of experience gathering requirements and documenting requirements for software / web applications. \n  Self-motivated and professional in demeanor with proven success as a business analyst and tester supporting new releases and emergency code fixes. \n  Proven ability to interface successfully with customer/client to drive high customer satisfaction. \n  Excellent written and verbal communications skills required for engagement with customer and team members in daily meetings and in documenting requirements, user stories, testing results and status. \n \n ",
        "techs": [
            "jira",
            "github",
            "zenhub"
        ],
        "cleaned_techs": [
            "jira",
            "github",
            "zenhub"
        ]
    },
    "64af06a9f4109f56": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 80000.0,
        "salary_max": 100000.0,
        "title": "Lead Business Analyst",
        "company": "Jazz Solutions Inc",
        "desc": "Overview: \n Jazz Solutions, Inc. (JSL), seeks a Lead Business Analyst to join our Team. This position is fully remote within the United States. \n \n \n What you will do in this role: \n \n Lead requirement refinement discussions with stakeholders, analyze and write user stories in Jira using Given-When-Then pattern \n Lead the sprint planning session to discuss requirements with developers, stakeholders, and other program members. \n Analyze and document detailed business process flows, E.g., creating and maintaining process flow diagrams, etc. \n Document user\u2019s guides, Business/system requirements and other program related documentation as needed. \n Plan and coordinate end user training, conduct end user training if requested by end users. \n Collaborate with team members like project manager, users, developers, and QA personnel to achieve project objectives and document the required information. \n Lead, Perform and execute team meetings, systems demos, and business user reviews. \n \n \n   \n What you must have: \n \n Bachelors or related experience in IT \n Clearance: Ability to obtain a Public Trust Clearance \n 6 plus years of experience as a Business Analyst or similar role \n Experience working in and delivering work in JIRA. \n Experience working in Agile and Hybrid methodologies. \n Experience working in Client Vendor model \n Knowledge of JIRA, Agile concepts, Project Life Cycle Knowledge and experience \n Ability to produce BA Artifacts like BRD, SRD, User Guides, Microsoft Tools \u2013 Word, Excel, PDF, Process flow chart development \n \n What we would like you to have: \n \n Certifications such as CBAP or AGILE \n Experience working in Outsystems, or low code platforms like Appain, Pega etc. \n \n \n   \n About JSL: \n At Jazz Solutions, Inc., we believe that technology is only as powerful as the dedicated and talented people behind it. We're not just about the latest tech trends; we're about the passionate individuals who drive innovation. As a woman-owned and SBA-certified 8(a) small business, we're committed to serving government sectors while also fostering the growth and well-being of our exceptional team. \n At JSL, we're more than coworkers; we're a close-knit community built on collaboration, innovation, and a shared mission to make a real impact. Our employees are our top priority, and we offer them not just jobs, but meaningful careers filled with growth opportunities, comprehensive benefits, and the flexibility of remote work. We've created a work environment that's inclusive, supportive, and encourages personal growth and creativity. \n We invite you to explore the opportunities at Jazz Solutions, Inc. Together, we'll move forward with technology, talent, and purpose. \n What we offer: \n \n The likely salary range for this position is $80,000 - $100,000. This is not, however, a guarantee of compensation or salary. Rather, salary will be set based on experience, geographic location and possibly contractual requirements and could fall outside of this range. \n Enjoy work life balance with 100% remote work option \n Flexible paid leave \n All federal holidays \n Medical/dental/vision insurance \n 401k with employer match \n Opportunities to obtain federal security clearances \n Opportunities for training and professional certifications",
        "cleaned_desc": " Bachelors or related experience in IT \n Clearance: Ability to obtain a Public Trust Clearance \n 6 plus years of experience as a Business Analyst or similar role \n Experience working in and delivering work in JIRA. \n Experience working in Agile and Hybrid methodologies. \n Experience working in Client Vendor model \n Knowledge of JIRA, Agile concepts, Project Life Cycle Knowledge and experience \n Ability to produce BA Artifacts like BRD, SRD, User Guides, Microsoft Tools \u2013 Word, Excel, PDF, Process flow chart development \n ",
        "techs": [
            "jira",
            "microsoft word",
            "microsoft excel",
            "pdf",
            "process flow chart development"
        ],
        "cleaned_techs": [
            "jira",
            "microsoft",
            "excel",
            "pdf",
            "process flow chart development"
        ]
    },
    "973ba5800f26b081": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 92722.14,
        "salary_max": 117406.89,
        "title": "Senior Analyst, Experience Surveys and Analytics (Remote)",
        "company": "FRESENIUS",
        "desc": "POSITION FEATURES: \n  This is a Remote position \n  At Fresenius Medical Care, the well-being of our patients is our top priority. Patient experience and employee engagement are closely intertwined. While healthcare organizations that improve  either  patient experience or employee engagement see improvements in how patients rate their care, healthcare organizations that enhance  both  factors see compounding effects. The Experience Team at Fresenius Medical Care is focused on measuring and analyzing the experiences of both our patients and our employees, and developing and implementing the programs and processes that will enhance patient care and employee engagement. \n \n  Our Experience Team is a part of our broader Global People Analytics Team within Human Resources, a strategic organizational design decision that was made to align our people-centric strategies and initiatives. We are currently in the process of building out our Global People Analytics and Experience Team \u2013 which makes it an exciting time to join the organization. The Senior Analyst, Experience Surveys and Analytics will execute work in the employee and patient care experience space. This individual will have a background in I/O Psychology and be skilled in psychological theory, scientific research methods, survey design, advanced statistical analysis, and data visualization. The Senior Analyst, Experience Surveys and Analytics will leverage these skills to design measurement tools to collect employee and patient experience data, analyze the data to identify actionable insights, and develop and communicate critical business stories to stakeholders via data visualization. Ultimately, this work supports programs that will drive measurable changes in attitudes and behaviors of employees and patients and improve the experience of both groups.  Please note that experience in healthcare is not required for this role. \n \n  Responsibilities \n \n  Consult with stakeholders to understand business challenges involving our patients and employees and determine what data is needed to further explore and identify the root causes of those challenges (note that this is not always self-report survey data). \n  Support the execution of employee and patient experience work globally \n  Design tools such as surveys, focus group interviews, observational checklists, etc. to reliably measure attitudes, perceptions, behaviors, or other constructs related to employee and patient experience \n  Manage and ensure the quality/accuracy of datasets by cleaning, joining, updating, refining and conducting quality assurance activities as needed \n  Conduct robust analysis of data (e.g., survey results), linking experience data to other internal HR and business data sources (e.g., turnover, patient safety data, financial data, etc.) to identify key business insights that inform effective action plans \n  Build presentations reflecting key findings and present to stakeholders outlining results, recommendations, and action items \n  Design new and/or review effectiveness of existing survey items (e.g., engagement, patient satisfaction, onboarding); ensure items meet survey design and measurement best practices (e.g., items not double-barreled or leading, high reliability, content/construct valid) \n  Develop sampling methods to ensure representativeness of data collected for employee and patient experience programs \n  Analyze data to understand trends and relationships using descriptive and predictive statistical techniques (e.g., correlation, regression) \n  Develop data stories/narratives from analyses and build visualizations to communicate key findings to business stakeholders \n  Drives adoption of analytics tool and dashboard by providing end-users with training on the use of available reporting/dashboards as well as how to extract value and take action on dashboard data \n  Support the design and execution of employee experience survey programs such as the annual employee engagement surveys, lifecycle surveys (e.g., onboarding & exit surveys, stay interviews), pulse surveys, recognition programs, etc. \n  Support the design and execution of patient experience survey programs such as the annual patient experience survey, ICH-CAHPS survey, annual home therapies survey, etc. \n  Partner with key stakeholders to support ad hoc requests by conducting intake meetings to understand business challenges/questions, advise on the appropriate method of gathering employee and patient feedback, lead or support the execution of the work, and collaborate with stakeholders on action planning \n  Provide guidance on best practices in developing survey items to our partners in HR and the business and support the development of best practice documentation to enable self-service support where appropriate \n  Configure surveys in Qualtrics, build distribution lists, support the development of communication campaigns/plans, launch surveys, troubleshoot issues, build dashboards, and train HRBPs and business leaders on leveraging those dashboards \n  Partner with Global Communications, HR Leadership and business stakeholders to support the development and implementation of survey communications prior to survey/program launch that includes pre-survey, survey live, and post-survey periods \n \n \n  Requirements \n \n  Masters degree in Industrial/Organizational Psychology required, PhD preferred \n  3+ years of work experience in an I/O Consulting or People Analytics role, with a focus on data analytics and visualization. \n  2+ years of work experience in Employee Listening/Experience \n  Strong presentation skills, including experience presenting to small/medium audiences, key stakeholders, team members, and supporting or leading training sessions and/or focus groups \n  Strong interpersonal skills; ability to develop professional relationships at all levels of the business \n  Successful track record of service delivery \n  Proven ability to use statistical analysis, analytics, and business knowledge to help solve complex business problems \n  Ability to draw inferences from disparate pieces of data, quickly identify key findings, and recognize implications and provide recommendations \n  Ability to frame analytic findings in a digestible and business friendly format that identifies key business implications and inspires action; must be able to create visually appealing summaries using Microsoft Office (e.g., Word, PowerPoint) \n  Experience designing tools measuring attitudes, behaviors, and other constructs (e.g., engagement surveys, exit surveys, focus group interview) \n  Experience managing small projects or components of medium/large projects including project initiation, planning, execution, monitoring, and closing \n  Strong attention to detail \n  Excellent verbal/written communication skills \n  Advanced Microsoft Excel skills \n  Proficient in at least one statistical/programming language (e.g., R, Python) \n  Proficiency with one or more business intelligence/visualization tools including Power BI, Tableau (or other similar program) is required; advanced skills preferred \n  Qualtrics proficiency required, advanced skills preferred \n  Natural Language Processing proficiency preferred \n  Experience with Enterprise Data Platforms (EDP) and relational data bases preferred \n  Experience with SQL preferred",
        "cleaned_desc": "  Support the execution of employee and patient experience work globally \n  Design tools such as surveys, focus group interviews, observational checklists, etc. to reliably measure attitudes, perceptions, behaviors, or other constructs related to employee and patient experience \n  Manage and ensure the quality/accuracy of datasets by cleaning, joining, updating, refining and conducting quality assurance activities as needed \n  Conduct robust analysis of data (e.g., survey results), linking experience data to other internal HR and business data sources (e.g., turnover, patient safety data, financial data, etc.) to identify key business insights that inform effective action plans \n  Build presentations reflecting key findings and present to stakeholders outlining results, recommendations, and action items \n  Design new and/or review effectiveness of existing survey items (e.g., engagement, patient satisfaction, onboarding); ensure items meet survey design and measurement best practices (e.g., items not double-barreled or leading, high reliability, content/construct valid) \n  Develop sampling methods to ensure representativeness of data collected for employee and patient experience programs \n  Analyze data to understand trends and relationships using descriptive and predictive statistical techniques (e.g., correlation, regression) \n  Develop data stories/narratives from analyses and build visualizations to communicate key findings to business stakeholders    Ability to draw inferences from disparate pieces of data, quickly identify key findings, and recognize implications and provide recommendations \n  Ability to frame analytic findings in a digestible and business friendly format that identifies key business implications and inspires action; must be able to create visually appealing summaries using Microsoft Office (e.g., Word, PowerPoint) \n  Experience designing tools measuring attitudes, behaviors, and other constructs (e.g., engagement surveys, exit surveys, focus group interview) \n  Experience managing small projects or components of medium/large projects including project initiation, planning, execution, monitoring, and closing \n  Strong attention to detail \n  Excellent verbal/written communication skills \n  Advanced Microsoft Excel skills \n  Proficient in at least one statistical/programming language (e.g., R, Python) \n  Proficiency with one or more business intelligence/visualization tools including Power BI, Tableau (or other similar program) is required; advanced skills preferred ",
        "techs": [
            "surveys",
            "focus group interviews",
            "observational checklists",
            "cleaning",
            "joining",
            "updating",
            "refining",
            "quality assurance activities",
            "analysis of data",
            "linking experience data",
            "key business insights",
            "presentations",
            "survey design",
            "measurement best practices",
            "sampling methods",
            "descriptive statistical techniques",
            "predictive statistical techniques",
            "data stories/narratives",
            "visualizations",
            "drawing inferences",
            "key findings",
            "recommendations",
            "microsoft office",
            "attitudes",
            "behaviors",
            "engagement surveys",
            "exit surveys",
            "project management",
            "attention to detail",
            "verbal communication skills",
            "written communication skills",
            "microsoft excel",
            "statistical programming languages (r",
            "python)",
            "business intelligence/visualization tools (power bi",
            "tableau)"
        ],
        "cleaned_techs": [
            "surveys",
            "focus group interviews",
            "observational checklists",
            "cleaning",
            "joining",
            "updating",
            "refining",
            "quality assurance activities",
            "analysis of data",
            "linking experience data",
            "key business insights",
            "presentations",
            "survey design",
            "sampling methods",
            "descriptive statistical techniques",
            "predictive statistical techniques",
            "data stories/narratives",
            "visualizations",
            "drawing inferences",
            "key findings",
            "recommendations",
            "microsoft",
            "attitudes",
            "behaviors",
            "engagement surveys",
            "exit surveys",
            "project management",
            "attention to detail",
            "excel",
            "statistical programming languages (r",
            "python",
            "business intelligence/visualization tools (power bi",
            "tableau)"
        ]
    },
    "9f88fc03766cb72e": {
        "terms": [
            "data analyst"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Actuarial Analyst Associate, Geisinger Health Plan (work from home Pennsylvania resident)",
        "company": "Geisinger",
        "desc": "Job Summary  Performs data analytics to provide leadership with strategic business intelligence around financial and operational performance. Compiles claims, enrollment, revenue, and other statistical data to use in assigned projects. With supervision and guidance from senior team members, the analyst conducts data analysis, performs financial modeling and designs reporting to support the Health Plan requirements and initiatives. Participates in the communication of summarized results to stakeholders via written narratives, graphical representations, and presentations. This position may support initiatives for internal or external/government stakeholders and/or strategic business partners.\n  \n  Job Duties \n \n  Provides analytical and technical support for assigned projects using data analysis tools and software. \n  Performs analyses by utilizing a wide-range of quantitative and qualitative data sets, models, and publications (regulations, guidelines, business rules, reporting requirements and standards), as directed. \n  Analyzes data output to identify and interpret trends or patterns in complex data sets. \n  Applies business, statistical, and actuarial concepts (Credibility, Simulation, Frequency, Severity) to help support and enhance decision making. \n  With guidance, synthesizes information to create and deliver clear, precise, and actionable information to stakeholders. \n  This information may be delivered in writing or through verbal summaries and dashboards and reports and presentations. \n  Participates in validation and exchange of data provided to and received from other departments, strategic business partners, or other external parties. \n  Executes, validates, and maintains routine reporting assignments, including policy and procedure and Model Audit Rule documentation. \n \n  Work is typically performed in an office environment. Accountable for satisfying all job specific obligations and complying with all organization policies and procedures. The specific statements in this profile are not intended to be all-inclusive. They represent typical elements considered necessary to successfully perform the job. \n  Position Details \n  Education  Bachelor's Degree-Related Field of Study (Required)\n  \n  Experience  Minimum of 1 year-Healthcare (Preferred)\n  \n  Certification(s) and License(s) \n  OUR PURPOSE & VALUES: Everything we do is about caring for our patients, our members, our students, our Geisinger family and our communities. KINDNESS: We strive to treat everyone as we would hope to be treated ourselves. EXCELLENCE: We treasure colleagues who humbly strive for excellence. LEARNING: We share our knowledge with the best and brightest to better prepare the caregivers for tomorrow. INNOVATION: We constantly seek new and better ways to care for our patients, our members, our community, and the nation. SAFETY: We provide a safe environment for our patients and members and the Geisinger family We offer healthcare benefits for full time and part time positions from day one, including vision, dental and domestic partners. Perhaps just as important, from senior management on down, we encourage an atmosphere of collaboration, cooperation and collegiality. We know that a diverse workforce with unique experiences and backgrounds makes our team stronger. Our patients, members and community come from a wide variety of backgrounds, and it takes a diverse workforce to make better health easier for all. We are proud to be an affirmative action, equal opportunity employer and all qualified applicants will receive consideration for employment regardless to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or status as a protected veteran.",
        "cleaned_desc": "  Provides analytical and technical support for assigned projects using data analysis tools and software. \n  Performs analyses by utilizing a wide-range of quantitative and qualitative data sets, models, and publications (regulations, guidelines, business rules, reporting requirements and standards), as directed. \n  Analyzes data output to identify and interpret trends or patterns in complex data sets. \n  Applies business, statistical, and actuarial concepts (Credibility, Simulation, Frequency, Severity) to help support and enhance decision making. ",
        "techs": [
            "data analysis tools",
            "software",
            "quantitative data sets",
            "qualitative data sets",
            "models",
            "publications",
            "regulations",
            "guidelines",
            "business rules",
            "reporting requirements",
            "standards",
            "data output",
            "trends",
            "patterns",
            "business concepts",
            "statistical concepts",
            "actuarial concepts",
            "credibility",
            "simulation",
            "frequency",
            "severity"
        ],
        "cleaned_techs": [
            "data analysis tools",
            "software",
            "quantitative data sets",
            "qualitative data sets",
            "models",
            "publications",
            "regulations",
            "guidelines",
            "business rules",
            "reporting requirements",
            "standards",
            "data output",
            "trends",
            "patterns",
            "business concepts",
            "statistical concepts",
            "actuarial concepts",
            "credibility",
            "simulation",
            "frequency",
            "severity"
        ]
    },
    "0aa33d3c84db9848": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 95408.68,
        "salary_max": 120808.65,
        "title": "Senior Business Intelligence Analyst,",
        "company": "FRESENIUS",
        "desc": "Senior Business Intelligence Analyst, National Medical Care, Inc., a Fresenius Medical Care N.A. company, Waltham, Massachusetts (Remote) \n \n \n \n  Purpose and Scope: \n \n \n \n  Responsible for providing support with ongoing systems development and business intelligence activities with respect to diverse financial system projects. \n \n \n \n \n  Principal Duties and Responsibilities: \n \n \n \n \n  Utilizing knowledge and expertise of solutions design and implementation to support divisional efforts to enhance Business Intelligence (BI) systems and processes by monitoring performance and migrating data from outside systems. \n  Interfacing with business leaders and end-users from corporate departments and business areas to scope, design, implement and manage the project to deliver BI solutions to the business. \n \n \n \n \n \n \n  Building the business intelligence layer to deliver reporting and analytical tools to finance and business management. \n  Detailing the business requirements gathering process, requirements definition, and approval. \n  Formulating highly detailed BI solutions which can be practically implemented. \n  Creating, managing, and documenting detailed project plans used to drive successful project completion. \n  Identifying and implementing solutions to enhance the existing BI infrastructure processes and technology. \n \n \n \n \n \n \n  Creating documentation to assist SAP BW developers and business users in designing and effectively/efficiently utilizing the solutions developed. \n  Creating and delivering training to corporate and division users on all BI applications, including Business Objects, BW, IP and BEx tools. \n  Maintaining appropriate documentation of verification of resolved queries as regulated per Sarbanes Oxley requirements to be transported into the BI production environment. \n  Serving as a liaison between the Financial and Corporate Information Systems divisions to manage the coordination of the configuration, the development of business processes, enhancements and the stabilization of the SAP BI environment. \n  Executing on all phases of implementation, including planning, development, training, deployment, post-implementation support, and enhancements as it relates to SAP BI and business users. \n \n \n \n \n \n \n  Researching and determining best business practices to meet the goals and objectives of the business utilizing available operational and financial systems and company resources. \n  Coordinating and verifying the updating of Navigation Attributes as they relate to Profit Centers in SAP BI environment using financial center personnel and available company resources. \n  Coordinating with the SAP Basis team and Finance & Operations personnel regarding the assignment of data authorizations and SAP BI roles. \n  Creating and maintaining global variables, variants and structures for use in query design for end users. \n \n \n \n \n \n \n  Education Experience and Required Skills: \n \n \n \n  Position requires either (i) a Bachelor\u2019s degree (or an equivalent foreign degree) in Information Technology, Information Systems or Finance and 5 years of experience as a systems analyst for enterprise-level systems or (ii) a Master\u2019s degree (or an equivalent foreign degree) in Information Technology, Information Systems or Finance and 2 years of experience as a systems analyst for enterprise-level systems. Must also have 2 years of experience (which can have been gained concurrently with either primary experience requirement above) working with the following: \n \n \n \n  Requirements definition, data acquisition processes, data modeling, process automation, construction and deployment; \n  Business Objects suite and SAP BEx and the Eclipse modeling tool; \n \n \n \n \n \n \n  SAP HANA; \n  SAP Analytics Cloud; and \n  Anaplan Planning Tool. \n \n \n \n \n  This is a telecommuting position working from home. May reside anywhere in the United States.",
        "cleaned_desc": "  Researching and determining best business practices to meet the goals and objectives of the business utilizing available operational and financial systems and company resources. \n  Coordinating and verifying the updating of Navigation Attributes as they relate to Profit Centers in SAP BI environment using financial center personnel and available company resources. \n  Coordinating with the SAP Basis team and Finance & Operations personnel regarding the assignment of data authorizations and SAP BI roles. \n  Creating and maintaining global variables, variants and structures for use in query design for end users. \n \n \n \n \n \n \n  Education Experience and Required Skills: \n \n \n \n  Position requires either (i) a Bachelor\u2019s degree (or an equivalent foreign degree) in Information Technology, Information Systems or Finance and 5 years of experience as a systems analyst for enterprise-level systems or (ii) a Master\u2019s degree (or an equivalent foreign degree) in Information Technology, Information Systems or Finance and 2 years of experience as a systems analyst for enterprise-level systems. Must also have 2 years of experience (which can have been gained concurrently with either primary experience requirement above) working with the following: \n ",
        "techs": [
            "sap bi",
            "sap basis",
            "sap bi roles"
        ],
        "cleaned_techs": [
            "sap bi",
            "sap basis",
            "sap bi roles"
        ]
    },
    "2ba6006821dec6ec": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 84562.5,
        "salary_max": 107074.97,
        "title": "Big Data Engineer Job Ref #: 981496",
        "company": "Concentrix",
        "desc": "Job Title:  Big Data Engineer Job Ref #: 981496\n  \n  Job Description  Concentrix CVG Customer Management Group Inc., Cincinnati OH, has multiple openings for the position of Big Data Engineer. Work will be performed in various unanticipated locations throughout the U.S. Travel and/or relocation is required. Telecommuting may be permitted.\n   The Big Data Engineer will write, update, and maintain software applications; perform production maintenance of code; gather solutions requirements. Own technical commitments to clients and work with the team to successful delivery of solutions. Analyze, design, and code for complex requirements as well as write programs of complexity. Responsible for defining problems, collecting data, establishing facts, drawing valid conclusions, and preparing appropriate reports. \n \n  The position requires a Master\u2019s degree in Computer Science, Engineering (any), or any technical/analytical field that is closely related to the specialty, plus knowledge of: HTML5, CSS3, MySQL, and jQuery. \n  To apply, send resume to ctlyst_postings@concentrix.com with Job Ref# 981496 in the subject line of the email. \n \n  Location:  USA, OH, Work-at-Home\n  \n  Language Requirements: \n \n  Time Type: \n  If you are a California resident, by submitting your information, you acknowledge that you have read and have access to the  Job Applicant Privacy Notice for California Residents \n  Concentrix is an Equal Opportunity/Affirmative Action Employer including Disabled/Vets. \n \n  For more information regarding your EEO rights as an applicant, please visit the following websites:  \n \n English \n Spanish \n \n \n  To request a reasonable accommodation please click here. \n \n  If you wish to review the Affirmative Action Plan, please click here.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "19d5fafc9fb74494": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 60.0,
        "salary_max": 70.0,
        "title": "Data Governance Engineer",
        "company": "InfoMagnus",
        "desc": "Data Governance Engineer \n This is a contract remote role must reside in US . \n We are people-oriented technologists, analysts and designers helping companies solve complex business problems with technology. \n InfoMagnus was built to provide an environment where passion is celebrated, personal time is respected and hard work is rewarded. \n Our Beliefs: \n Integrity, dignity and respect in every interaction. Commitment to our employees. Trust and responsibility in all relationships. Giving back to our community. \n Job Description: \n We are looking for a skilled Data Governance Engineer who is well-versed in data governance principles and practices, with expertise in utilizing data quality platforms like  Anomalo  or data catalog platforms like  Alation . In this role, you will play a crucial part in ensuring the integrity, security, and quality of our data assets. \n Key Responsibilities: \n \u00b7 Collaborate with cross-functional teams to establish and maintain data governance policies, standards, and procedures. \n \u00b7 Implement and manage data quality checks and data lineage using  Anomalo  or  Alation . \n \u00b7 Design and execute data quality audits and assessments to identify and resolve data issues. \n \u00b7 Develop and maintain data cataloging and metadata management processes. \n Qualifications: \n \u00b7 Bachelor\u2019s degree in Computer Science, Information Systems, or a related field. Master's degree preferred. \n \u00b7 Proven experience in data governance and data quality management. \n \u00b7 Proficiency in utilizing data quality platforms like  Anomalo  or data catalog platform like  Alation. \n \u00b7 Strong understanding of data governance frameworks, policies and procedures. \n \u00b7 Excellent problem-solving skills and attention to detail. \n \u00b7 Strong clear communication skills and interpersonal skills to collaborate with various teams. \n Equal Opportunity Employer \n Job Type: Contract \n Pay: $60.00 - $70.00 per hour \n Experience level: \n \n 3 years \n \n Schedule: \n \n Monday to Friday \n \n Application Question(s): \n \n Do you have an employer that sponsors \n \n your Visa? \n \n Do you have experience using the data catalog platform Alation? \n Do you have experience using the data quality platform Anomalo? \n \n Work Location: Remote",
        "cleaned_desc": " Key Responsibilities: \n \u00b7 Collaborate with cross-functional teams to establish and maintain data governance policies, standards, and procedures. \n \u00b7 Implement and manage data quality checks and data lineage using  Anomalo  or  Alation . \n \u00b7 Design and execute data quality audits and assessments to identify and resolve data issues. \n \u00b7 Develop and maintain data cataloging and metadata management processes. \n Qualifications: \n \u00b7 Bachelor\u2019s degree in Computer Science, Information Systems, or a related field. Master's degree preferred. \n \u00b7 Proven experience in data governance and data quality management. ",
        "techs": [
            "anomalo",
            "alation"
        ],
        "cleaned_techs": [
            "anomalo",
            "alation"
        ]
    },
    "6efe3e4de9ab2fef": {
        "terms": [
            "data engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Sr. Software Engineer (Data) [Multiple Openings]-9199",
        "company": "Comcast",
        "desc": "Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast.\n    Job Summary \n  Job Description \n  DUTIES: Provide technical leadership to a team responsible for developing software components for FreeWheel\u2019s advertising platform products; build data platforms, Video Integration products, Linear Integration products, and new web frontend frameworks; develop software using Java, Python, Scala, and Go programming languages, which are run on big data platforms including Apache Hadoop, Spark, and Snowflake on AWS cloud platform; design, develop, test, and maintain software that extracts, transforms, and loads large volumes of data; create dashboards and monitors on Datadog to ensure 24x7 availability of critical software deployments; manage data held in relational database management systems (RDBMS) using SQL; develop and deploy complex SQL queries to validate impressions from set top boxes on a massive scale; write scripts for CI/CD to enable software artifacts to be built and deployed on AWS and Databricks, using Jenkins or similar tools; debug functional and performance issues on software modules running on Databricks and Spark; analyze product specifications, write technical specs, create monitoring dashboards, develop test suites, design workflows, and setup database schemas and tables; interface with global engineering, operations, services, and business operations teams to execute proof of concepts and incorporate new requirements; improve system performance and ensure availability and scalability of services; and guide and mentor junior-level engineers. Position is eligible for 100% remote work. \n \n  REQUIREMENTS: Bachelor\u2019s degree, or foreign equivalent, in Computer Science, Engineering, or related technical field, and five (5) years of experience developing software using Python; managing data held in relational database management systems (RDBMS) using SQL; and developing data architecture; of which one (1) year includes developing software using Spark; and working with cloud technologies, including AWS. \n \n  Disclaimer: \n \n \n  This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications. \n \n \n  Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law. \n  Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That\u2019s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality \u2013 to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.",
        "cleaned_desc": "  Job Description \n  DUTIES: Provide technical leadership to a team responsible for developing software components for FreeWheel\u2019s advertising platform products; build data platforms, Video Integration products, Linear Integration products, and new web frontend frameworks; develop software using Java, Python, Scala, and Go programming languages, which are run on big data platforms including Apache Hadoop, Spark, and Snowflake on AWS cloud platform; design, develop, test, and maintain software that extracts, transforms, and loads large volumes of data; create dashboards and monitors on Datadog to ensure 24x7 availability of critical software deployments; manage data held in relational database management systems (RDBMS) using SQL; develop and deploy complex SQL queries to validate impressions from set top boxes on a massive scale; write scripts for CI/CD to enable software artifacts to be built and deployed on AWS and Databricks, using Jenkins or similar tools; debug functional and performance issues on software modules running on Databricks and Spark; analyze product specifications, write technical specs, create monitoring dashboards, develop test suites, design workflows, and setup database schemas and tables; interface with global engineering, operations, services, and business operations teams to execute proof of concepts and incorporate new requirements; improve system performance and ensure availability and scalability of services; and guide and mentor junior-level engineers. Position is eligible for 100% remote work.   \n  REQUIREMENTS: Bachelor\u2019s degree, or foreign equivalent, in Computer Science, Engineering, or related technical field, and five (5) years of experience developing software using Python; managing data held in relational database management systems (RDBMS) using SQL; and developing data architecture; of which one (1) year includes developing software using Spark; and working with cloud technologies, including AWS. ",
        "techs": [
            "java",
            "python",
            "scala",
            "go",
            "apache hadoop",
            "spark",
            "snowflake",
            "aws",
            "datadog",
            "sql",
            "jenkins",
            "databricks"
        ],
        "cleaned_techs": [
            "java",
            "python",
            "scala",
            "go",
            "apache hadoop",
            "spark",
            "snowflake",
            "aws",
            "datadog",
            "sql",
            "jenkins",
            "databricks"
        ]
    },
    "63e3d7373482182d": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 171000.0,
        "salary_max": 232000.0,
        "title": "Product Manager",
        "company": "Meta",
        "desc": "Meta Product Managers work with cross-functional teams of engineers, designers, data scientists and researchers to build products. We are looking for extremely entrepreneurial Product Managers who value moving quickly.\n  \n \n \n Product Manager Responsibilities:    \n \n Is the primary driver for identifying significant opportunities, and driving product vision, strategies and roadmaps in the context of broader organizational strategies and goals. \n  Understand Meta\u2019s strategic and competitive position and deliver products that are aligned with our mission and recognized best in the industry. \n  Maximize efficiency in a constantly evolving environment where the process is fluid and creative solutions are the norm. \n  Incorporate data, research, and market analysis to inform product strategies and roadmaps. \n  Plan, initiate, and manage information technology projects for web-based products and platforms. \n  Lead the ideation, technical development, and launch of innovative tools, platforms, and/or products. \n  Drive product development with teams of world-class engineers and designers, while maintaining team health. \n  Work closely with cross-functional teams to drive product vision, define product requirements, coordinate resources from other groups (design, legal, etc.), and guide the team through key milestones. \n  Integrate usability studies, research, and market analysis into product requirements to improve engineer productivity and enhance user satisfaction. \n  Define and analyze metrics that inform the success of products. Identify and track key performance metrics. \n \n \n \n \n Minimum Qualifications:   \n \n  8+ years product management or related industry experience \n  Requires a Bachelor's degree (or foreign degree equivalent) in Computer Science, Engineering, Information Systems, Analytics, Mathematics, Physics, Applied Sciences, or a related field and 2+ years of experience in the following: \n  Experience in product management or product design \n  Experience working in a technical environment with a broad, cross functional team to drive product vision, define product requirements, coordinate resources from other groups (design, legal, etc.), and guide the team through key milestones \n  Experience delivering technical presentations \n  Experience in analyzing complex, large-scale data sets and making decisions based on data \n  Experience in gathering requirements across diverse areas and users, and converting and developing them into a product solution \n  Experience in technical experience with analytical tools, methodologies, and design \n  Displaying leadership, organizational and execution skills \n  Proven communication skills \n \n \n \n \n Preferred Qualifications:   \n \n  Experience defining vision and strategy for a product. \n  Experience going through a full product lifecycle, integrating customer feedback into product requirements, driving prioritization and pre/post-launch execution. \n  Experience recruiting and leading a cross-functional team of world-class individuals. \n  Enthusiastic and resilient in a constantly evolving environment where the process is fluid and creative solutions are the norm. \n \n \n \n \n About Meta:    Meta builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps like Messenger, Instagram and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. People who choose to build their careers by building with us at Meta help shape a future that will take us beyond what digital connection makes possible today\u2014beyond the constraints of screens, the limits of distance, and even the rules of physics.\n  \n \n \n \n  Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment. \n   \n  Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",
        "cleaned_desc": " Minimum Qualifications:   \n \n  8+ years product management or related industry experience \n  Requires a Bachelor's degree (or foreign degree equivalent) in Computer Science, Engineering, Information Systems, Analytics, Mathematics, Physics, Applied Sciences, or a related field and 2+ years of experience in the following: \n  Experience in product management or product design \n  Experience working in a technical environment with a broad, cross functional team to drive product vision, define product requirements, coordinate resources from other groups (design, legal, etc.), and guide the team through key milestones \n  Experience delivering technical presentations \n  Experience in analyzing complex, large-scale data sets and making decisions based on data \n  Experience in gathering requirements across diverse areas and users, and converting and developing them into a product solution \n  Experience in technical experience with analytical tools, methodologies, and design ",
        "techs": [
            "none"
        ],
        "cleaned_techs": []
    },
    "ce672ce8edd2c985": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 135100.0,
        "salary_max": 225100.0,
        "title": "Staff SW Development Engineer",
        "company": "Dexcom",
        "desc": "About Dexcom  \n \n Founded in 1999, Dexcom, Inc. (NASDAQ: DXCM), develops and markets Continuous Glucose Monitoring (CGM) systems for ambulatory use by people with diabetes and by healthcare providers for the treatment of people with diabetes. The company is the leader in transforming diabetes care and management by providing CGM technology to help patients and healthcare professionals better manage diabetes. Since the company\u2019s inception, Dexcom has focused on better outcomes for patients, caregivers, and clinicians by delivering solutions that are best in class - while empowering the community to take control of diabetes. Dexcom reported full-year 2022 revenues of $2.9B, a growth of 18% over 2021. Headquartered in San Diego, California, with additional offices in the Americas, Europe, and Asia Pacific, the company employs over 8,000 people worldwide.  \n \n \n Meet the team  :  \n \n \n You will be a key contributor to the Data Platforms team and the larger Dexcom Software and Data Organization. You will work with Data Platform Engineers to build the infrastructure to support our data pipelines, storage and API through which reliable, clean, meaningful data is provided to internal and external stakeholders.  \n \n \n \n At Dexcom, you can use your data engineering skills for the greater good. We develop software for medical devices and systems. Our software quality standards conform to the rigorous requirements of regulatory agencies such as ISO and FDA. If you enjoy connecting the dots, using data-driven engineering decisions, innovating through the server side of our entire platform stack, you\u2019ll enjoy this job.   \n \n \n \n \n Where you come in  :  \n \n \n \n Build cloud-based , event driven software systems and with high availability , scalability, and resilience , as part of a multi-disciplinary team that works with in an Agile methodology environment  \n Work with data scientist team to build m achine l earning p ipeline  \n Contribute to software across the development lifecycle including software design and architecture, code development, functional and performance testing, and documentation and design verification  \n \n \n \n \n \n \n Identify , champion and work to implement improvements in internal processes including automating manual processes, optimizing data delivery, optimizing software infrastructure for better scalability, maintainability, cost efficiency, etc.  \n Contribute to code reviews and champion best practices for quality control, software testing and code development/deployment in an agile environment  \n Mentor junior members to develop a good understanding the design, architecture and coding of existing internally developed applications to be able to maintain and update them  \n Participate in on-call rotation for production systems when required  \n Protect the confidentiality and security of client data   \n \n \n \n \n \n \n What makes you successful  :  \n \n \n \n You must have Strong programming skills \u2013 Kotlin, Java, or Scala  \n You have e xperience with message brokers, queues, buses like Kafka, Azure service bus, ZeroMQ etc  \n You have e xperience with relational databases like Mysql , NoSql databases like Cassandra  \n \n \n \n \n \n \n You are f amiliarity using relevant, modern, software test tools and equipment  \n You have p roven ability to learn new tools and technology  \n You have d emonstrated ability to work in a fast paced and changing environment with short deadlines, interruptions, and several simultaneous tasks and projects   \n \n \n \n \n What  you\u2019ll  get  :  \n \n \n \n \n \n A front row seat to life changing CGM technology. Learn about our brave #dexcomwarriors community .  \n A full and c omprehensive benefit s progra m .  \n Growth opportunities on a global scale.  \n Access to career development through in-house learning programs and/or qualified tuition reimbursemen t.  \n A n exciting and innovative , industry - leading organization committed to our employees , customers , and the communities we serve .  \n \n \n Education & Experience:  \n \n Typically requires a Bachelor\u2019s degree in a technical discipline, and a minimum of 8-12 years related experience or Master\u2019s degree and 5- 7 years equivalent industry experience or a PhD and 2-4 years of experience.   \n \n \n \n \n \n \n Travel Required:  \n \n \n \n 0-5%  \n \n \n \n \n Please note: The information contained herein is not intended to be an all-inclusive list of the duties and responsibilities of the job, nor are they intended to be an all-inclusive list of the skills and abilities required to do the job. Management may, at its discretion, assign or reassign duties and responsibilities to this job at any time. The duties and responsibilities in this job description may be subject to change at any time due to reasonable accommodation or other reasons. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions.  \n \n An Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability. Dexcom\u2019s AAP may be viewed upon request by contacting Talent Acquisition at talentacquisition@dexcom.com.  \n \n If you are an individual with a disability and would like to request a reasonable accommodation as part of the employment selection process, please contact Dexcom Talent Acquisition at talentacquisition@dexcom.com.  \n \n View the OFCCP's Pay Transparency Non Discrimination Provision at this link .  \n \n UnitedHealthcare creates and publishes the Machine-Readable Files on behalf of Dexcom. To link to the Machine-Readable Files, please click on the URL provided: https://transparency-in-coverage.uhc.com/.  \n \n To all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at Dexcom. Only authorized staffing and recruiting agencies may use this site or to submit profiles, applications or resumes on specific requisitions. Dexcom does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to the Talent Acquisition team, Dexcom employees or any other company location. Dexcom is not responsible for any fees related to unsolicited resumes/applications.     \n Salary:  $135,100.00 - $225,100.00",
        "cleaned_desc": " \n Build cloud-based , event driven software systems and with high availability , scalability, and resilience , as part of a multi-disciplinary team that works with in an Agile methodology environment  \n Work with data scientist team to build m achine l earning p ipeline  \n Contribute to software across the development lifecycle including software design and architecture, code development, functional and performance testing, and documentation and design verification  \n \n \n \n \n \n \n Identify , champion and work to implement improvements in internal processes including automating manual processes, optimizing data delivery, optimizing software infrastructure for better scalability, maintainability, cost efficiency, etc.  \n Contribute to code reviews and champion best practices for quality control, software testing and code development/deployment in an agile environment  \n Mentor junior members to develop a good understanding the design, architecture and coding of existing internally developed applications to be able to maintain and update them  \n Participate in on-call rotation for production systems when required  \n Protect the confidentiality and security of client data   \n \n \n \n \n   \n What makes you successful  :  \n \n \n \n You must have Strong programming skills \u2013 Kotlin, Java, or Scala  \n You have e xperience with message brokers, queues, buses like Kafka, Azure service bus, ZeroMQ etc  \n You have e xperience with relational databases like Mysql , NoSql databases like Cassandra  \n \n \n \n \n \n \n You are f amiliarity using relevant, modern, software test tools and equipment  \n You have p roven ability to learn new tools and technology  \n You have d emonstrated ability to work in a fast paced and changing environment with short deadlines, interruptions, and several simultaneous tasks and projects   \n \n \n ",
        "techs": [
            "kotlin",
            "java",
            "scala",
            "kafka",
            "azure service bus",
            "zeromq",
            "mysql",
            "cassandra"
        ],
        "cleaned_techs": [
            "kotlin",
            "java",
            "scala",
            "kafka",
            "azure",
            "zeromq",
            "mysql",
            "cassandra"
        ]
    },
    "1ddd50c44d4f551d": {
        "terms": [
            "machine learning engineer",
            "mlops"
        ],
        "salary_min": 130000.0,
        "salary_max": 160000.0,
        "title": "Senior DevOps Engineer",
        "company": "Tortuga AgTech",
        "desc": "Full-time; Denver, Colorado \n  On-Site, Hybrid, or Remote (US) \n \n  Do you want to help build real technology for a meaningful purpose? Do you want to fight to make the world\u2019s fresh produce healthier, tastier, and more sustainable? \n \n  Tortuga is looking for a Senior DevOps Engineer to help build systems that support software development processes along with configuration, deployment, and monitoring for our worldwide fleet of harvesting robots. As a member of the software team, you\u2019ll work on tools to support best practices in software development, testing, release management, and operations with space to grow into a variety of other critical roles. You will be responsible for developing, optimizing, and sustaining all aspects of the software development tooling and infrastructure to support the software and machine learning teams. \n \n  In this important role at Tortuga, you\u2019ll be fundamental to our success, growth, and culture. You\u2019ll bring an excitement for DevOps and share a passion for our mission while also thriving in a fast-paced, flexible, and energized startup environment. You\u2019re driven to advance your role and skills as Tortuga continues to grow. \n \n  What you\u2019ll do \n \n  Create, support, and promote best practices for software development through integration of modern git based platforms and tooling \n  Configure, optimize and sustain software build and continuous integration pipelines \n  Implement automated testing and deployment paradigms throughout the development, testing, and release lifecycle \n  Work directly with the software team and machine learning team to ensure upstream code quality by automating processes and providing reliable infrastructure for testing. \n \n \n  Up to 10% travel, with potential for international travel \n \n \n  What we\u2019re looking for \n \n  Required:  4+ years experience in a DevOps role \n  Required:  B.S. Computer Science or similar degree \n  Required:  Demonstrably strong Python, shell scripting or other programming language skills \n  Required:  Demonstrably strong with Docker and container management \n  Required:  Experience with git based development workflows, DevOps platforms (Gitlab is a plus), and CI infrastructure for C++ and Python projects \n  Strongly Preferred:  Experience with Ansible, container orchestration, task automation, and software observability technologies \n  Strongly Preferred:  Experience with DevOps for software deployed in application areas of industrial automation, autonomous vehicles, or robotics \n  Preferred:  Experience with software development practices and tools (like Jira) \n  Preferred:  Experience with scalability, monitoring, security and performance engineering for production-grade software with deployments at scale \n  Preferred:  Startup experience a major plus \n \n \n  The base salary range represents the low and high end of our salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to experience, education, responsibilities, and regular and/or necessary travel. The range listed is just one component of Tortuga\u2019s total compensation package for employees. \n \n  Denver area base salary range: $130,000 - $160,000 per year. Relocation assistance is available for out-of-state candidates. \n \n  Please note you must be authorized to work in the United States for this position. \n  About Tortuga \n  Tortuga AgTech\u2019s mission is to build a healthier society, and a thriving ecosystem, through smarter farming. We\u2019re pursuing our mission by building robotic harvesting and precision analytics systems for the world\u2019s leading fresh produce growers (delicious things like strawberries, tomatoes, bell peppers, and herbs). We\u2019re building not just because it\u2019s fun, but because we believe thoughtful, well-built advanced technology can help farmers grow healthier, fresher, more environmentally sustainable produce for everyone. We\u2019re backed by some of the most respected early-stage investors in Silicon Valley, as well as ag industry veterans. \n  Tortuga means \u201cturtle\u201d in Spanish. Around the world, turtles are symbols of wisdom, patience, and connection to the Earth. We\u2019re proud to be based in Denver, CO - America\u2019s 2nd best place to live according to U.S. News (but does Austin have any mountains? We still think we\u2019re #1). \n  Tortuga is an equal opportunity employer. All aspects of employment including the decision to hire, promote, discipline, or discharge, will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law. \n  Compensation Range: $130K - $160K",
        "cleaned_desc": " \n  What you\u2019ll do \n \n  Create, support, and promote best practices for software development through integration of modern git based platforms and tooling \n  Configure, optimize and sustain software build and continuous integration pipelines \n  Implement automated testing and deployment paradigms throughout the development, testing, and release lifecycle \n  Work directly with the software team and machine learning team to ensure upstream code quality by automating processes and providing reliable infrastructure for testing. \n    Required:  Demonstrably strong Python, shell scripting or other programming language skills \n  Required:  Demonstrably strong with Docker and container management \n  Required:  Experience with git based development workflows, DevOps platforms (Gitlab is a plus), and CI infrastructure for C++ and Python projects \n  Strongly Preferred:  Experience with Ansible, container orchestration, task automation, and software observability technologies \n  Strongly Preferred:  Experience with DevOps for software deployed in application areas of industrial automation, autonomous vehicles, or robotics \n  Preferred:  Experience with software development practices and tools (like Jira) \n  Preferred:  Experience with scalability, monitoring, security and performance engineering for production-grade software with deployments at scale \n  Preferred:  Startup experience a major plus ",
        "techs": [
            "git",
            "docker",
            "container management",
            "git based development workflows",
            "devops platforms",
            "ci infrastructure",
            "c++",
            "python",
            "ansible",
            "container orchestration",
            "task automation",
            "software observability technologies",
            "jira",
            "scalability",
            "monitoring",
            "security",
            "performance engineering",
            "production-grade software",
            "deployments at scale",
            "startup experience"
        ],
        "cleaned_techs": [
            "git",
            "docker",
            "container management",
            "git based development workflows",
            "devops platforms",
            "ci infrastructure",
            "c++",
            "python",
            "ansible",
            "container orchestration",
            "task automation",
            "software observability technologies",
            "jira",
            "scalability",
            "monitoring",
            "performance engineering",
            "production-grade software",
            "deployments at scale",
            "startup experience"
        ]
    },
    "84a208b329ad324d": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 90000.0,
        "salary_max": 115000.0,
        "title": "Mid Level Quality Assurance Engineer",
        "company": "WarrCloud",
        "desc": "Summary \n  We are a well-funded start-up that has developed and commercialized a first-of-its-kind machine learning-enabled SaaS platform that automates warranty claims processing for franchise automotive dealerships and OEMs. If you have ever wanted to work for an exciting and fast-growing company in the automotive software space then this is the job for you! We are a first to market leader experiencing rapid growth and are seeking an experienced Software Engineer to join our team. \n  As a QA Engineer, you will be joining a small development team who are passionate, friendly, collaborative, and driven. We use an agile methodology in the way that we work together and on our cloud-based application. To be successful in this role, you must be passionate about building an application that is extraordinary while working in an often ambiguous and ever-evolving business. In your role, you will build robust and scalable solutions in collaboration with the development team. We aim to constantly improve our processes, products, and our best asset, our people. \n  Top Reasons to Work with Us \n \n 100% Remote opportunity \n Competitive salary \n Stock Options \n Health care, 401k and other benefits \n Stable / Fast Growing / Exciting company \n Chance to work with first of its kind AI/ML technology \n Awesome company culture! \n \n What You Will Be Doing \n  General duties the position may be asked to perform and is not intended to be all-inclusive \n \n Maintaining and improving the quality of products and services through systematic testing, validation, and verification processes  \n Develop and execute test plans, test cases, and test scripts based on project requirements \n Identify, document, and track defects using jira \n Provide detailed reports on testing results, including defect status and severity levels \n Collaborate with product managers and development team to understand project requirements and specifications \n Provide training and support to team members on QA processes, tools, and best practices \n Work closely with QA team, development teams, product managers, designers, and other stakeholders to ensure alignment with project goals  \n \n Requirements \n  What You Need for this Position \n \n Bachelor's degree in Computer Science or related field is required \n 5+ years of relevant experience in QA testing \n Strong understanding of software development lifecycles and QA methodologies \n Proficiency in testing tools and technologies (e.g., Selenium, Jira) \n Knowledge of programming languages \n Experience working in an Agile environment \n Attention to detail and strong analytical skills \n Excellent communication and teamwork abilities \n \n Nice to Have \n \n 2+ years of automotive industry experience with DMS software a plus \n Selenium development experience \n Script development experience is a plus \n AWS microservices experience a plus \n \n Benefits \n \n  As a valued employee, you will be able to enroll in benefits immediately upon hire that takes effect the first day of the month following your start date \n \n  Applicants must be authorized to work in the U.S",
        "cleaned_desc": " Bachelor's degree in Computer Science or related field is required \n 5+ years of relevant experience in QA testing \n Strong understanding of software development lifecycles and QA methodologies \n Proficiency in testing tools and technologies (e.g., Selenium, Jira) \n Knowledge of programming languages \n Experience working in an Agile environment \n Attention to detail and strong analytical skills \n Excellent communication and teamwork abilities \n ",
        "techs": [
            "selenium",
            "jira"
        ],
        "cleaned_techs": [
            "selenium",
            "jira"
        ]
    },
    "6c7526d0e31f8099": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 138050.88,
        "salary_max": 174803.16,
        "title": "Principal Software Engineer",
        "company": "Citizens",
        "desc": "Description \n  Citizens Financial Group, Inc. (CFG) seeks a Principal Software Engineer for its Phoenix, AZ location. \n  Duties: Lead efforts to develop and support complex mobile and web banking applications. Leverage Agile methodology, collaborating and contributing insight to solution design ideation, ensuring both the success of the product and adherence to enterprise architecture principles. Design, modify, develop, and implement software solutions. Infuse quality of service characteristics, such as scalability, manageability, and maintainability, into distributed service-based framework to create or expand business or technical capabilities. Employ industry best practices to evaluate, correct and prevent vulnerabilities during the software development process. Serving as a peer-leader, encouraging a culture of innovation and accountability. Present designs and ideas to an Architecture Review Board and to Technology and business senior management. \n  Requirements: Bachelor\u2019s degree in Computer Science, Computer Engineering or related field followed by seven years of progressive experience in the role or in a related position. At least 5 Years of Experience must include: Development of IT strategies and solutions for applications in the Banking and/or Financial services areas; Implement high performing and scalable SOA/BPM based architecture solution, de- coupled from legacy applications via events over an Integration Hub; Prepare and present test plans, scan reports, technical presentations and analyst briefings addressing potential solutions and best practices; Utilize Java, Spring Boot, Spring Security, Spring Cloud, AWS, PaaS, laaS, Saas, Node JS, Angular, React JS, WebAssembly, JavaScript, Typescript, jQuery, Hibernate, GIT, UML, Oracle, and MySQL; Work with Domain Driven Design (DDD), Test Driven Development (TDD) and Continuous Integration (Cl) software development approaches; Manage, design and lead the development of multiple digital channels; Undertake initial code spikes and lay down foundations of the architecture; Manage risk identification and risk mitigation strategies associated with the architecture; Work on Cloud Computing, GAIA, Jules, J2EE, Web Services, SOAP, REST and API Gateway Development. Developed POCs using Hadoop, Python, Express JS, Vue JS, Mango DB, Docker, Kubernetes, Artificial Intelligence (Al) and Machine Learning (ML); Develop technologies using Service Now, DataDog, Splunk, Openshift, XLR, Groovy Pipelines, DTC, IBM MQ, Kafka, IBM Data Power, Jenkins, Docker, Springboot, Glassbox, Git, Atlassian JIRA, Sailpoint, AWS services and other cloud open source, Mainframes, Fortify, NexusIQ; and Utilization of Fiserv/FirstData APIs, Transmit (Security Orchestrator), Threat Metrix and other LexisNexis products, understanding of security concepts and tooling including but not limited to PKI, Cryptography, along with how those apply to web concepts (HTTP(S), SSL/TLS, certificates etc.). \n  May telecommute from any U.S. location. \n  Direct applicants only. \n Some job boards have started using jobseeker-reported data to estimate salary ranges for roles. If you apply and qualify for this role, a recruiter will discuss accurate pay guidance. \n  Equal Employment Opportunity \n  At Citizens we value diversity, equity and inclusion, and treat everyone with respect and professionalism. Employment decisions are based solely on experience, performance, and ability. Citizens, its parent, subsidiaries, and related companies (Citizens) provide equal employment and advancement opportunities to all colleagues and applicants for employment without regard to age, ancestry, color, citizenship, physical or mental disability, perceived disability or history or record of a disability, ethnicity, gender, gender identity or expression (including transgender individuals who are transitioning, have transitioned, or are perceived to be transitioning to the gender with which they identify), genetic information, genetic characteristic, marital or domestic partner status, victim of domestic violence, family status/parenthood, medical condition, military or veteran status, national origin, pregnancy/childbirth/lactation, colleague\u2019s or a dependent\u2019s reproductive health decision making, race, religion, sex, sexual orientation, or any other category protected by federal, state and/or local laws. \n  Equal Employment and Opportunity Employer \n  Citizens is a brand name of Citizens Bank, N.A. and each of its respective affiliates. \n \n  Why Work for Us \n  At Citizens, you'll find a customer-centric culture built around helping our customers and giving back to our local communities. When you join our team, you are part of a supportive and collaborative workforce, with access to training and tools to accelerate your potential and maximize your career growth",
        "cleaned_desc": "  Duties: Lead efforts to develop and support complex mobile and web banking applications. Leverage Agile methodology, collaborating and contributing insight to solution design ideation, ensuring both the success of the product and adherence to enterprise architecture principles. Design, modify, develop, and implement software solutions. Infuse quality of service characteristics, such as scalability, manageability, and maintainability, into distributed service-based framework to create or expand business or technical capabilities. Employ industry best practices to evaluate, correct and prevent vulnerabilities during the software development process. Serving as a peer-leader, encouraging a culture of innovation and accountability. Present designs and ideas to an Architecture Review Board and to Technology and business senior management. \n  Requirements: Bachelor\u2019s degree in Computer Science, Computer Engineering or related field followed by seven years of progressive experience in the role or in a related position. At least 5 Years of Experience must include: Development of IT strategies and solutions for applications in the Banking and/or Financial services areas; Implement high performing and scalable SOA/BPM based architecture solution, de- coupled from legacy applications via events over an Integration Hub; Prepare and present test plans, scan reports, technical presentations and analyst briefings addressing potential solutions and best practices; Utilize Java, Spring Boot, Spring Security, Spring Cloud, AWS, PaaS, laaS, Saas, Node JS, Angular, React JS, WebAssembly, JavaScript, Typescript, jQuery, Hibernate, GIT, UML, Oracle, and MySQL; Work with Domain Driven Design (DDD), Test Driven Development (TDD) and Continuous Integration (Cl) software development approaches; Manage, design and lead the development of multiple digital channels; Undertake initial code spikes and lay down foundations of the architecture; Manage risk identification and risk mitigation strategies associated with the architecture; Work on Cloud Computing, GAIA, Jules, J2EE, Web Services, SOAP, REST and API Gateway Development. Developed POCs using Hadoop, Python, Express JS, Vue JS, Mango DB, Docker, Kubernetes, Artificial Intelligence (Al) and Machine Learning (ML); Develop technologies using Service Now, DataDog, Splunk, Openshift, XLR, Groovy Pipelines, DTC, IBM MQ, Kafka, IBM Data Power, Jenkins, Docker, Springboot, Glassbox, Git, Atlassian JIRA, Sailpoint, AWS services and other cloud open source, Mainframes, Fortify, NexusIQ; and Utilization of Fiserv/FirstData APIs, Transmit (Security Orchestrator), Threat Metrix and other LexisNexis products, understanding of security concepts and tooling including but not limited to PKI, Cryptography, along with how those apply to web concepts (HTTP(S), SSL/TLS, certificates etc.). ",
        "techs": [
            "java",
            "spring boot",
            "spring security",
            "spring cloud",
            "aws",
            "paas",
            "laas",
            "saas",
            "node js",
            "angular",
            "react js",
            "webassembly",
            "javascript",
            "typescript",
            "jquery",
            "hibernate",
            "git",
            "uml",
            "oracle",
            "mysql",
            "domain driven design (ddd)",
            "test driven development (tdd)",
            "continuous integration (cl)",
            "cloud computing",
            "gaia",
            "jules",
            "j2ee",
            "web services",
            "soap",
            "rest",
            "api gateway development",
            "hadoop",
            "python",
            "express js",
            "vue js",
            "mango db",
            "docker",
            "kubernetes",
            "artificial intelligence (al)",
            "machine learning (ml)",
            "service now",
            "datadog",
            "splunk",
            "openshift",
            "xlr",
            "groovy pipelines",
            "dtc",
            "ibm mq",
            "kafka",
            "ibm data power",
            "jenkins",
            "glassbox",
            "git",
            "atlassian jira",
            "sailpoint",
            "fiserv/firstdata apis",
            "transmit (security orchestrator)",
            "threat metrix",
            "lexisnexis products",
            "pki",
            "cryptography",
            "http(s)",
            "ssl/tls",
            "certificates"
        ],
        "cleaned_techs": [
            "java",
            "spring boot",
            "spring cloud",
            "aws",
            "paas",
            "laas",
            "saas",
            "node js",
            "angular",
            "react js",
            "webassembly",
            "javascript",
            "typescript",
            "jquery",
            "hibernate",
            "git",
            "uml",
            "oracle",
            "mysql",
            "domain driven design (ddd)",
            "test driven development (tdd)",
            "continuous integration (cl)",
            "cloud computing",
            "gaia",
            "jules",
            "j2ee",
            "web services",
            "soap",
            "rest",
            "api gateway development",
            "hadoop",
            "python",
            "express js",
            "vue js",
            "mango db",
            "docker",
            "kubernetes",
            "ai",
            "machine learning (ml)",
            "service now",
            "datadog",
            "splunk",
            "openshift",
            "xlr",
            "groovy pipelines",
            "dtc",
            "ibm mq",
            "kafka",
            "ibm data power",
            "jenkins",
            "glassbox",
            "atlassian jira",
            "sailpoint",
            "fiserv/firstdata apis",
            "threat metrix",
            "lexisnexis products",
            "pki",
            "cryptography",
            "http(s)",
            "ssl/tls",
            "certificates"
        ]
    },
    "38d6168912e94b9e": {
        "terms": [
            "machine learning engineer",
            "mlops"
        ],
        "salary_min": 42.17,
        "salary_max": 63.26,
        "title": "MLOPS Engineer",
        "company": "LPL Financial",
        "desc": "Are you a team player? Are you curious to learn? Are you interested in working in meaningful projects? Do you want to work with cutting-edge technology? Are you interested in being part of a team that is working to transform and do things differently? If so, LPL Financial is the place for you! \n \n  LPL Financial (Nasdaq: LPLA) was founded on the principle that the firm should work for the advisor, and not the other way around. Today, LPL is a leader* in the markets we serve, supporting more than 18,000 financial advisors, 800 institution-based investment programs and 450 independent RIA firms nationwide. We are steadfast in our commitment to the advisor-centered model and the belief that Americans deserve access to personalized guidance from a financial advisor. At LPL, independence means that advisors have the freedom they deserve to choose the business model, services, and technology resources that allow them to run their perfect practice. And they have the freedom to manage their client relationships, because they know their clients best. Simply put, we take care of our advisors, so they can take care of their clients. \n \n  Job Overview: \n  We are looking for an experienced  MLOPS Engineer  to join the Data & Analytics team, a team of data and technology professionals who build and operationalize machine learning models for LPL Financial. The successful candidate will build MLOP solutions using state of the art technologies to implement production grade for successful model deployment. \n \n  As a  MLOPS Engineer , you will partner with our users and other data science teams to understand their needs to build and deploy data and machine learning pipelines to support applications and data science projects following software engineering best practices. \n \n  Responsibilities: \n \n  Refactor code from data scientists for production. \n  Develop and deploy data and machine learning pipelines to automate data ingestion, model training, data and model monitoring. \n  Build robust machine learning and data pipelines on Cloud using Airflow, Glue, Spark/EMR, Kinesis, Kafka, Lambda, API Gateway or other technologies. \n  Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement. \n \n \n  What are we looking for? \n  We want  strong collaborators   who can deliver a world-class client experience . We are looking for people who thrive in a  fast-paced environment ,  are client-focused ,  team oriented , and are able to execute in a way that encourages  creativity  and  continuous improvement . \n \n  Requirements: \n \n  DevOps experience with Terraform, GitHub, and developing CI/CD pipelines \n  1+ years' experience with Terraform \n  1+ years\u2019 experience with AWS \n  BS in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to software engineering, data sciences or relevant experience. \n \n \n  Preferences: \n \n  3+ years programming experience in Unix, Python, and SQL \n  1+ years\u2019 experience with deployment and monitoring machine learning applications using AWS sagemaker ecosystem. \n  Experience using JIRA and Agile Project Management software. \n \n \n  #LI-Remote \n \n  Pay Range:  $42.17-$63.26/hour\n  \n  Actual base salary varies based on factors, including but not limited to, relevant skill, prior experience, education, base salary of internal peers, demonstrated performance, and geographic location. Additionally, LPL Total Rewards package is highly competitive, designed to support your success at work, at home, and at play \u2013 such as 401K matching, health benefits, employee stock options, paid time off, volunteer time off, and more. Your recruiter will be happy to discuss all that LPL has to offer!\n  \n  Why LPL?  \n \n At LPL, we believe that objective financial guidance is a fundamental need for everyone. As the nation\u2019s leading independent broker-dealer, we offer an integrated platform of proprietary technology, brokerage, and investment advisor services. We provide you with a work environment that encourages your creativity and growth, a leadership team that is supportive and responsive, and the opportunity to create a career that has no limits, only amazing potential. \n \n  We are  one team on one mission.  We take care of our advisors, so they can take care of their clients. \n \n  Because our company is not too big and not too small, you can seize the opportunity to make a real impact. We are committed to supporting workplace equality, and we embrace the different perspectives and backgrounds of our employees. We also care for our communities, and we encourage our employees to do the same. This creates an environment in which you can do your best work. \n \n  Want to hear from our employees on what it\u2019s like to work at LPL? Watch this! \n \n  We take social responsibility seriously. Learn more here \n \n  Want to see info on our benefits? Learn more here \n \n  Join the LPL team and help us make a difference by turning life\u2019s aspirations into financial realities. Please log in or create an account to apply to this position. Principals only. EOE. \n \n  Information on Interviews: \n \n  LPL will only communicate with a job applicant directly from an  @lp lfinancial.com  email address and will never conduct an interview online or in a chatroom forum. During an interview, LPL will not request any form of payment from the applicant, or information regarding an applicant\u2019s bank or credit card. Should you have any questions regarding the application process, please contact LPL\u2019s Human Resources Solutions Center at (800) 877-7210.",
        "cleaned_desc": "  Refactor code from data scientists for production. \n  Develop and deploy data and machine learning pipelines to automate data ingestion, model training, data and model monitoring. \n  Build robust machine learning and data pipelines on Cloud using Airflow, Glue, Spark/EMR, Kinesis, Kafka, Lambda, API Gateway or other technologies. \n  Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement. \n \n \n  What are we looking for? \n  We want  strong collaborators   who can deliver a world-class client experience . We are looking for people who thrive in a  fast-paced environment ,  are client-focused ,  team oriented , and are able to execute in a way that encourages  creativity  and  continuous improvement . \n \n  Requirements: \n    DevOps experience with Terraform, GitHub, and developing CI/CD pipelines \n  1+ years' experience with Terraform \n  1+ years\u2019 experience with AWS \n  BS in Computer Science, Applied Mathematics, Physics, Statistics or area of study related to software engineering, data sciences or relevant experience. \n \n \n  Preferences: \n \n  3+ years programming experience in Unix, Python, and SQL \n  1+ years\u2019 experience with deployment and monitoring machine learning applications using AWS sagemaker ecosystem. \n  Experience using JIRA and Agile Project Management software. ",
        "techs": [
            "airflow",
            "glue",
            "spark/emr",
            "kinesis",
            "kafka",
            "lambda",
            "api gateway",
            "terraform",
            "github",
            "aws",
            "unix",
            "python",
            "sql",
            "aws sagemaker",
            "jira"
        ],
        "cleaned_techs": [
            "airflow",
            "glue",
            "spark/emr",
            "kinesis",
            "kafka",
            "lambda",
            "api gateway",
            "terraform",
            "github",
            "aws",
            "unix",
            "python",
            "sql",
            "jira"
        ]
    },
    "b2ed92266dbaa124": {
        "terms": [
            "mlops"
        ],
        "salary_min": 125000.0,
        "salary_max": 160000.0,
        "title": "Lead DevOps Engineer",
        "company": "Vita Tech Inc",
        "desc": "Required Skills Minimum ~10 years of IT experience with AWS Minimum ~5 years of systems development experience with Python, Perl and/or Shell-Scripting. Strong problem management, troubleshooting and analytical skills. Experience in troubleshooting within enterprise environments. Diagnosing and addressing performance issues using performance monitors, custom scripts and various tuning techniques. Ability to work well as a team and as an individual with minimal supervision. Experience with configuring and managing high-availability systems. Experience with networking, security & application load balancing. Strong communication and interpersonal skills. \n Job Type: Full-time \n Pay: $125,000.00 - $160,000.00 per year \n Experience level: \n \n 11+ years \n \n Work Location: Remote",
        "cleaned_desc": "Required Skills Minimum ~10 years of IT experience with AWS Minimum ~5 years of systems development experience with Python, Perl and/or Shell-Scripting. Strong problem management, troubleshooting and analytical skills. Experience in troubleshooting within enterprise environments. Diagnosing and addressing performance issues using performance monitors, custom scripts and various tuning techniques. Ability to work well as a team and as an individual with minimal supervision. Experience with configuring and managing high-availability systems. Experience with networking, security & application load balancing. Strong communication and interpersonal skills. ",
        "techs": [
            "aws",
            "python",
            "perl",
            "shell-scripting",
            "troubleshooting",
            "analytical skills",
            "performance monitors",
            "custom scripts",
            "tuning techniques",
            "team collaboration",
            "high-availability systems",
            "networking",
            "security",
            "application load balancing",
            "communication skills",
            "interpersonal skills."
        ],
        "cleaned_techs": [
            "aws",
            "python",
            "perl",
            "shell-scripting",
            "troubleshooting",
            "performance monitors",
            "custom scripts",
            "tuning techniques",
            "team collaboration",
            "high-availability systems",
            "networking",
            "application load balancing"
        ]
    },
    "69f4c84cd9749821": {
        "terms": [
            "mlops"
        ],
        "salary_min": 124166.24,
        "salary_max": 157222.12,
        "title": "DevOps Engineer",
        "company": "Cyberjin",
        "desc": "Hybrid/Remote role \n  Multiple locations \n  Looking for a DevOps Engineer with prior experience with Big Data Solutions, Cloud technology, and strong working knowledge of Linux. Passionate about the concept of infrastructure as code and leverages modern tools to define, build and manage virtual infrastructure in the cloud. Work is performed in a hybrid environment with a great team. \n \n  Essential Job Responsibilities \n  The ideal candidate believes in exploring alternatives and quickly prototyping to validate hypothetical architectures or solutions.  \n Will significantly contribute to the development of custom software components and integration of open source code to address complex time series analysis problems through the use of cutting edge Big Data/ Cloud technology. Design, implement, and maintain core architecture and capabilities for software from prototype to operational applications.  \n Must understand software engineering fundamentals, OO programming, relational and time series databases, scripting knowledge and a basic level of development operations (DevOps) skill set.  \n Minimum Qualifications \n  Security Clearance - A current Secret is required and therefore all candidates must be a U.S. citizen.  \n 5+ years of experience in DevOps Engineering or Software Development (Java preferred) and Bachelors in related field; or 3 years relevant experience with Masters in related field; or High School Diploma or equivalent and 9 years relevant experience. \n  Have a strong working knowledge of Linux systems, hosts, storage, networks, security, applications and proficiency in shell scripting (Shell/Bash, JavaScript, Python). \n  Excellent oral and written communication skills. \n  Must have a Security+ certification.  \n Must be able to work in a hybrid environment. \n  Preferred Requirements \n  Must have experience with big data technologies such as Hadoop and NoSQL Databases. Experience with AWS is highly desired. \n  Prior experience or familiarity with Unified Platform (UP) Big Data Platform (formerly owned by DISA) is a plus. \n  Data parsing/transforming techniques to include JSON, XML, CSV formats. \n  Understanding of AGILE software development methodologies and use of standard software development tool suites. (e.g., JIRA, Confluence, Github Enterprise, etc.) \n  Willing to do on-call/pager duty is a big plus. Possible rotating shift in the future for this role as the current team is full. \n   \n dmNZbkC4Pl",
        "cleaned_desc": " Minimum Qualifications \n  Security Clearance - A current Secret is required and therefore all candidates must be a U.S. citizen.  \n 5+ years of experience in DevOps Engineering or Software Development (Java preferred) and Bachelors in related field; or 3 years relevant experience with Masters in related field; or High School Diploma or equivalent and 9 years relevant experience. \n  Have a strong working knowledge of Linux systems, hosts, storage, networks, security, applications and proficiency in shell scripting (Shell/Bash, JavaScript, Python).    Must have experience with big data technologies such as Hadoop and NoSQL Databases. Experience with AWS is highly desired. \n  Prior experience or familiarity with Unified Platform (UP) Big Data Platform (formerly owned by DISA) is a plus. \n  Data parsing/transforming techniques to include JSON, XML, CSV formats. \n  Understanding of AGILE software development methodologies and use of standard software development tool suites. (e.g., JIRA, Confluence, Github Enterprise, etc.) ",
        "techs": [
            "java",
            "linux systems",
            "shell scripting",
            "javascript",
            "python",
            "hadoop",
            "nosql databases",
            "aws",
            "unified platform (up) big data platform",
            "json",
            "xml",
            "csv formats",
            "agile software development methodologies",
            "jira",
            "confluence",
            "github enterprise"
        ],
        "cleaned_techs": [
            "java",
            "linux systems",
            "shell scripting",
            "javascript",
            "python",
            "hadoop",
            "nosql",
            "aws",
            "unified platform (up) big data platform",
            "json",
            "xml",
            "csv formats",
            "agile software development methodologies",
            "jira",
            "confluence",
            "github enterprise"
        ]
    },
    "d75c8f1633b38759": {
        "terms": [
            "mlops"
        ],
        "salary_min": 138160.0,
        "salary_max": 215875.0,
        "title": "(Remote) Principal DevOps Engineer",
        "company": "First American Financial Corporation",
        "desc": "Who We Are \n \n  Join a team that puts its People First! Since 1889, First American (NYSE: FAF) has held an unwavering belief in its people. They are passionate about what they do, and we are equally passionate about fostering an environment where all feel welcome, supported, and empowered to be innovative and reach their full potential. Our inclusive, people-first culture has earned our company numerous accolades, including being named to the Fortune 100 Best Companies to Work For\u00ae list for eight consecutive years. We have also earned awards as a best place to work for women, diversity and LGBTQ+ employees, and have been included on more than 50 regional best places to work lists. First American will always strive to be a great place to work, for all. For more information, please visit www.careers.firstam.com. \n \n  What We Do \n \n  ** Remote Work Welcome** Be part of a transformative team that is shaping the way First American builds and delivers world-class technology products that fuel the real estate industry. We are looking for the best-of-the-best technology experts that will envision, design, build, and deliver innovative solutions that provides exceptional experiences and lasting value to our customers. First American is seeking candidates for an engineering role in our Platform Engineering team. An ideal candidate has strong technical expertise, a product mindset, and takes a hands-on approach for driving engineering best practices. As a Principal DevOps Engineer, you will help design and implement our technical architecture, software, and tooling used to deliver and operate our core, mission critical, software platforms. The ideal candidate will have a firm grasp of emerging technologies, platforms, and applications and an ability to customize them to help our business remain secure and efficient. \n  What You\u2019ll Do \n \n  Technical Leadership:  Provide vision and technical leadership to guide us in designing, delivering, and supporting \u201cgolden paths\u201d including on demand delivery of infrastructure and CI/CD for our software teams to consume with low cognitive load and high security. Define and document best practices and strategies regarding application deployment and infrastructure maintenance. \n  Collaborate/Partner:  Work in tandem with our engineering teams and leaders to identify and implement optimal cloud-based solutions for the company. \n  People Leadership & Development:  Our people are our greatest asset, and you provide guidance, thought leadership, and mentorship to help our teams develop their technical skillsets. You will educate teams on the implementation of new cloud-based initiatives, providing associated training when necessary. \n  Quality & Security Focused:  Drive implementation of functionally appropriate and technically sound solutions meeting all quality & security standards. \n  Continuous Improvement:  Lead the team in achieving ambitious goals, providing regular feedback, and driving continuous improvement. You will participate in all aspects of our software development lifecycle for cloud based solutions, including planning, defining requirements, developing, and testing those solutions with the team. \n \n \n  What You\u2019ll Bring \n \n  6+ years in the cloud and DevOps field \n  Proven track record for delivering fit for purpose technical solutions balancing complexity, performance, and maintainability \n  A balanced approach between Software Engineering principles and modern Infrastructure Operations \n  Automation orchestration tooling such Azure DevOps (ADO), Github Enterprise, or Gitlab \n  Infrastructure automation tooling such as Terraform and Ansible \n  CI/CD best practices and implementation \n  AWS infrastructure and cloud native services such as Lambda and EKS \n  Windows Server automation \n  Various git patterns such as gitflow and trunk based development \n  Working within a highly regulated industry such as Financials Services or Healthcare \n \n \n  Technical Skills \n \n  Powershell, Bash, Python, and/or Golang development \n  Supporting .NET (Core & Framework), Node.js, and Golang applications \n  Containers (Windows and Linux) \n  Kubernetes \n  Database automation & administration \n  Cloud cost management & automation \n  Network/Cloud security core concepts and tooling \n  AppSec/DevSecOps tooling such as Veracode SAST and DAST \n  Automated software testing (unit, component, API, functional) \n  GitOps methodologies \n  Effective communication skills, both verbal and written, with strong relationship, collaborative, and organization skills \n \n \n  Pay Range: $138,160 - $215,875 Annually \n \n  This hiring range is a reasonable estimate of the base pay range for this position at the time of posting. Pay is based on a number of factors which may include job-related knowledge, skills, experience, business requirements and geographic location . \n \n  What We Offer \n \n  By choice, we don\u2019t simply accept individuality \u2013 we embrace it, we support it, and we thrive on it! Our People First Culture celebrates diversity, equity and inclusion not simply because it\u2019s the right thing to do, but also because it\u2019s the key to our success. We are proud to foster an authentic and inclusive workplace For All. You are free and encouraged to bring your entire, unique self to work. First American is an equal opportunity employer in every sense of the term. \n \n  Based on eligibility, First American offers a comprehensive benefits package including medical, dental, vision, 401k, PTO/paid sick leave and other great benefits like an employee stock purchase plan.",
        "cleaned_desc": "  A balanced approach between Software Engineering principles and modern Infrastructure Operations \n  Automation orchestration tooling such Azure DevOps (ADO), Github Enterprise, or Gitlab \n  Infrastructure automation tooling such as Terraform and Ansible \n  CI/CD best practices and implementation \n  AWS infrastructure and cloud native services such as Lambda and EKS \n  Windows Server automation \n  Various git patterns such as gitflow and trunk based development \n  Working within a highly regulated industry such as Financials Services or Healthcare \n \n ",
        "techs": [
            "azure devops (ado)",
            "github enterprise",
            "gitlab",
            "terraform",
            "ansible",
            "lambda",
            "eks",
            "windows server",
            "gitflow",
            "trunk based development"
        ],
        "cleaned_techs": [
            "azure",
            "github enterprise",
            "gitlab",
            "terraform",
            "ansible",
            "lambda",
            "eks",
            "windows server",
            "gitflow",
            "trunk based development"
        ]
    },
    "74bd6cf020943d6e": {
        "terms": [
            "mlops"
        ],
        "salary_min": 106520.12,
        "salary_max": 134878.2,
        "title": "Senior Devops Engineer-Ansible",
        "company": "Cognizant Technology Solutions",
        "desc": "Job Title-   Sr Devops Engineer - Ansible \n \n \n  As a  Sr Devops Engineer- Ansible  a you can contribute your skills as we harness the power of technology to help our clients improve the health and well-being of the members they serve \u2014 a community\u2019s most vulnerable. Connect your passion with purpose, teaming with people who thrive on finding innovative solutions to some of healthcare\u2019s biggest challenges. Here are the details on this position. \n  Roles and Responsibility \n \n \n \n Collaborate with development, operations, and quality assurance teams to streamline software delivery and infrastructure management processes.  \n Implement and maintain CI/CD pipelines for efficient and automated build, test, and deployment of applications.  \n Configure and manage infrastructure resources, including cloud-based environments and on-premises systems.  \n Experience creating and maintaining job templates, projects, workflows in Ansible Automation Platform (Ansible Tower) \n Understanding how to plan and implement Ansible Automation Platform (Ansible Tower) \n Firm understanding of best practices for the use of Ansible Automation Platform (Ansible Tower) \n Ability to mentor other engineers on the use of Ansible Automation Platform (Ansible Tower) \n Automate the creation, updating, and deleting of Azure, GCP, and AWS cloud resource with Ansible Automation Platform \n Experience creating and maintaining Ansible playbooks and roles, including testing with tools \n Firm understanding on the use of gitops deployment model. \n Identify and resolve infrastructure issues, security vulnerabilities, and performance bottlenecks. \n Collaborate with development teams to improve application performance and scalability through code optimization and infrastructure enhancements. \n \n What we're looking for \n \n \n \n Bachelor's degree in computer science or 5+ years of relevant education/experience \n 5+ years hands-on experience and expert level understanding of Ansible  \n 5+ years developing complex integrations in Ansible, Puppet, Chef, or other IaC language \n 1+ years of Knowledge of container management and orchestration tools  \n 2+ years of AWS cloud or Azure experience \n \n \n \n   \n Location -Remote \n  Employee Status :  Full Time Employee \n  Shift :  Day Job \n  Travel :  No \n  Job Posting :  Oct 20 2023 \n \n \n  About Cognizant \n  Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.\n  \n  Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview. \n \n  Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law. \n  If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.",
        "cleaned_desc": " Implement and maintain CI/CD pipelines for efficient and automated build, test, and deployment of applications.  \n Configure and manage infrastructure resources, including cloud-based environments and on-premises systems.  \n Experience creating and maintaining job templates, projects, workflows in Ansible Automation Platform (Ansible Tower) \n Understanding how to plan and implement Ansible Automation Platform (Ansible Tower) \n Firm understanding of best practices for the use of Ansible Automation Platform (Ansible Tower) \n Ability to mentor other engineers on the use of Ansible Automation Platform (Ansible Tower) \n Automate the creation, updating, and deleting of Azure, GCP, and AWS cloud resource with Ansible Automation Platform \n Experience creating and maintaining Ansible playbooks and roles, including testing with tools \n Firm understanding on the use of gitops deployment model. ",
        "techs": [
            "ansible automation platform (ansible tower)",
            "azure",
            "gcp",
            "aws",
            "gitops deployment model"
        ],
        "cleaned_techs": [
            "ansible automation platform (ansible tower)",
            "azure",
            "gcp",
            "aws",
            "gitops deployment model"
        ]
    },
    "241a80b3883d2e12": {
        "terms": [
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Data Scientist / Machine Learning Engineer",
        "company": "Paramount+",
        "desc": "OVERVIEW & RESPONSIBILITIES \n We are looking for someone who is thrilled to build Data Science and ML products that formulate our business strategy, optimize our content, and guide Marketing investment decisions. To build these products, you will leverage data collected from tens of millions of Paramount+ content users, including video consumption data from Adobe Analytics\u2122 ClickStream, purchase / subscription data, and Ad revenue data from Doubleclick\u2122. Efficiently building Data Science and ML products at Paramount+ requires experience in conceiving and deploying statistical and/or machine learning models, deep Python and SQL experience, perspicacity writing code in a collaborative setting, and excellent communication skills. \n \n  RESPONSIBILITIES \n Translate complex business problems into coherent, actionable quantitative solutions \n Implement, automate, and maintain reliable, performant, and end-to-end ML systems using software engineering and MLOps standard methodologies \n Deliver clear and actionable insights to collaborators \n Collaborate with partners across the org to implement data science solutions that inform the business strategy, optimize our content, and guide marketing investment decisions \n Build models to guide our content strategy such as content affinity and valuation models \n Build MLOps products used to monitor, persist, and expose ML models \n Build models to guide our customer lifecycle strategy, such as subscriber churn models \n Build models to guide our marketing strategy, such as audience segmentation models \n \n  BASIC QUALIFICATIONS \n STEM undergraduate degree in Statistics, Engineering, Informatics, Computer Science or similar \n 4+ years experience in Data Science or ML Engineering \n Broad experience with both supervised and unsupervised machine learning techniques \n Ability to break complex problems into simple, coherent solutions \n Deep knowledge of the range and breadth of Data Science tools best suited for a given business problem \n Deep experience writing robust Python (Pandas, Airflow) and complex SQL code in a collaborative team setting, using Git\u2122 distributed version control system \n Full stack Data Science experience, ranging from data pipeline to model deployment and maintenance \n Strong detail orientation with a penchant for data accuracy \n An ability to communicate concisely and persuasively with engineers, product managers, partners and collaborators \n \n  ADDITIONAL QUALIFICATIONS \n STEM graduate or post-graduate degree in Statistics, Engineering, Informatics, Computer Science or similar \n Experience with media or subscription businesses \n Experience using Google Cloud Platform (BigQuery, ML Engine, and APIs) \n Experience using project management tools (JIRA, Confluence) \n #LI-FV 37877 \n #LI-REMOTE \n \n  Paramount+, a direct-to-consumer digital subscription video on-demand and live streaming service from Paramount Global, combines live sports, breaking news, and a mountain of entertainment. The premium streaming service features an expansive library of original series, hit shows and popular movies across every genre from world-renowned brands and production studios, including BET, CBS, Comedy Central, MTV, Nickelodeon, Paramount Pictures and the Smithsonian Channel. The service is also the streaming home to unmatched sports programming, including every CBS Sports event, from golf to football to basketball and more, plus exclusive streaming rights for major sports properties, including some of the world\u2019s biggest and most popular soccer leagues. Paramount+ also enables subscribers to stream local CBS stations live across the U.S. in addition to the ability to stream Paramount Streaming\u2019s other live channels: CBSN for 24/7 news, CBS Sports HQ for sports news and analysis, and ET Live for entertainment coverage. \n \n \n Paramount Global (NASDAQ:  PARA, PARAA) is a leading global media and entertainment company that creates premium content and experiences for audiences worldwide. Driven by iconic studios, networks and streaming services, Paramount's portfolio of consumer brands includes CBS, Showtime Networks, Paramount Pictures, Nickelodeon, MTV, Comedy Central, BET, Paramount+, Pluto TV and Simon & Schuster, among others. Paramount delivers the largest share of the U.S. television audience and boasts one of the industry's most important and extensive libraries of TV and film titles. In addition to offering innovative streaming services and digital video products, the company provides powerful capabilities in production, distribution and advertising solutions. \n \n  ADDITIONAL INFORMATION \n \n \n Hiring Salary Range:  $124,000.00 - 165,000.00. \n \n  The hiring salary range for this position applies to New York City, California, Colorado, Washington state, and most other geographies. Starting pay for the successful applicant depends on a variety of job-related factors, including but not limited to geographic location, market demands, experience, training, and education. The benefits available for this position include medical, dental, vision, 401(k) plan, life insurance coverage, disability benefits, tuition assistance program and PTO or, if applicable, as otherwise dictated by the appropriate Collective Bargaining Agreement. This position is bonus eligible. \n \n  https://www.paramount.com/careers/benefits \n \n  Paramount is an equal opportunity employer (EOE) including disability/vet. \n \n  At Paramount, the spirit of inclusion feeds into everything that we do, on-screen and off. From the programming and movies we create to employee benefits/programs and social impact outreach initiatives, we believe that opportunity, access, resources and rewards should be available to and for the benefit of all. Paramount is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, and Veteran status. \n \n  If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to use or access. https://www.paramount.com/careers as a result of your disability. You can request reasonable accommodations by calling 212.846.5500 or by sending an email to paramountaccommodations@paramount.com. Only messages left for this purpose will be returned.",
        "cleaned_desc": "OVERVIEW & RESPONSIBILITIES \n We are looking for someone who is thrilled to build Data Science and ML products that formulate our business strategy, optimize our content, and guide Marketing investment decisions. To build these products, you will leverage data collected from tens of millions of Paramount+ content users, including video consumption data from Adobe Analytics\u2122 ClickStream, purchase / subscription data, and Ad revenue data from Doubleclick\u2122. Efficiently building Data Science and ML products at Paramount+ requires experience in conceiving and deploying statistical and/or machine learning models, deep Python and SQL experience, perspicacity writing code in a collaborative setting, and excellent communication skills. \n \n  RESPONSIBILITIES \n Translate complex business problems into coherent, actionable quantitative solutions \n Implement, automate, and maintain reliable, performant, and end-to-end ML systems using software engineering and MLOps standard methodologies \n Deliver clear and actionable insights to collaborators \n Collaborate with partners across the org to implement data science solutions that inform the business strategy, optimize our content, and guide marketing investment decisions \n Build models to guide our content strategy such as content affinity and valuation models \n Build MLOps products used to monitor, persist, and expose ML models   Build models to guide our customer lifecycle strategy, such as subscriber churn models \n Build models to guide our marketing strategy, such as audience segmentation models \n \n  BASIC QUALIFICATIONS \n STEM undergraduate degree in Statistics, Engineering, Informatics, Computer Science or similar \n 4+ years experience in Data Science or ML Engineering \n Broad experience with both supervised and unsupervised machine learning techniques \n Ability to break complex problems into simple, coherent solutions \n Deep knowledge of the range and breadth of Data Science tools best suited for a given business problem \n Deep experience writing robust Python (Pandas, Airflow) and complex SQL code in a collaborative team setting, using Git\u2122 distributed version control system   Full stack Data Science experience, ranging from data pipeline to model deployment and maintenance \n Strong detail orientation with a penchant for data accuracy \n An ability to communicate concisely and persuasively with engineers, product managers, partners and collaborators \n \n  ADDITIONAL QUALIFICATIONS \n STEM graduate or post-graduate degree in Statistics, Engineering, Informatics, Computer Science or similar \n Experience with media or subscription businesses \n Experience using Google Cloud Platform (BigQuery, ML Engine, and APIs) \n Experience using project management tools (JIRA, Confluence) \n #LI-FV 37877 ",
        "techs": [
            "adobe analytics\u2122 clickstream",
            "doubleclick\u2122",
            "python",
            "sql",
            "mlops",
            "pandas",
            "airflow",
            "git\u2122",
            "bigquery",
            "ml engine",
            "jira",
            "confluence"
        ],
        "cleaned_techs": [
            "adobe",
            "doubleclick\u2122",
            "python",
            "sql",
            "mlops",
            "pandas",
            "airflow",
            "git\u2122",
            "bigquery",
            "ml engine",
            "jira",
            "confluence"
        ]
    },
    "metadata": {
        "keywords": [
            "data science",
            "data analyst",
            "data engineer",
            "machine learning engineer",
            "mlops"
        ],
        "locations": [
            "remote"
        ],
        "time_ran": "12:28:14-22-10-23",
        "num_jobs": 55,
        "timings": {
            "start_drivers": 45.721019983291626,
            "find_job_ids": 402.7326397895813,
            "get_job_descs": 23.452367782592773
        },
        "models": {
            "classifier": {
                "clf": "data/classifier_models/job_desc_classifier_v1.0.pkl",
                "tfidf": "data/classifier_models/job_desc_tfidf_vectorizer_v1.0.pkl"
            },
            "NER": "gpt-3.5-turbo"
        }
    }
}