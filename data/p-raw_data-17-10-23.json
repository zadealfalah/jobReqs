{
    "44d756c604d503df": {
        "terms": [
            "data science"
        ],
        "salary_min": 75.0,
        "salary_max": 81.0,
        "title": "AI ML Developer",
        "company": "Real Soft, Inc.",
        "desc": "Job Description:  \n \n Experience in Python Development. Integration with APIs and Databases. \n AI/ML coding expertise, developing applications and APIs leverage data & AI models. \n LLM and GenAI is a plus \n \n #IND123 \n Job Types: Contract, Full-time \n Pay: $75.00 - $81.00 per hour \n Schedule: \n \n 8 hour shift \n Day shift \n Monday to Friday \n \n Experience: \n \n Python: 3 years (Required) \n Machine learning: 5 years (Required) \n artificial intelligence: 1 year (Required) \n \n Work Location: Remote",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "63a5cc1aa55b3e1d": {
        "terms": [
            "data science"
        ],
        "salary_min": 105000.0,
        "salary_max": 155000.0,
        "title": "Data Scientist 2",
        "company": "Zynga",
        "desc": "Careers Category:  Other \n \n \n Careers location:  Remote, US, Other \n \n \n Connected Worker Type:  Connected \n \n \n Requisition Id:  R_110264 \n \n \n Zynga is a leading developer of the world\u2019s most popular social games that are played by millions of people around the world every single day. To-date, more than 1 billion people have played our games across Web and mobile, including Words With Friends, FarmVille, Zynga Poker, Merge Dragons, Empires & Puzzles, Hit it Rich! Slots and CSR. \n \n Zynga\u2019s Applied AI group is searching for a Data Scientist 2. Our team focuses on developing innovative approaches to problems within the game development process, demonstrating directly the ways that AI can add value. Applicants should be passionate about the intersection of Games, Creativity and Artificial Intelligence, and interested in supporting cutting edge innovation. Ideally you will have experience with techniques such as Procedural Content Generation, Player Modeling and Machine Learning approaches, as well as having a \u201cget stuff done\u201d approach to implementing these kinds of systems. During your tenure, you will gain hands-on experience working to implement exciting systems to support game development, and learn what life looks like in a fast-moving unit operating on the frontier of Computational Creativity. \n \n Responsibilities \n \n Work with the Applied AI group to ideate and design systems to support successful integration of AI techniques in a player-facing context   \n Develop systems and data pipelines to provide backend infrastructure capable of supporting AI-driven workflows and systems   \n Create hypotheses about player behavior, build experimentation plans and new player experiences in order to validate these   \n Design and evaluate novel approaches to using Machine Learning to support creative efforts   \n \n Requirements \n \n Experience developing software with at least one of Unity (C#), Python or React   \n Functional understanding of SQL for querying large quantities of data   \n Able to quickly learn new technologies and systems as needed.   \n In particular we expect our team members to already be familiar with, or quickly ramp up on: AWS fundamentals (e.g. EC2, S3, ECS), distributed data analysis (e.g. Spark) and containerized paradigms (e.g. Docker)   \n Strong written and oral communication skills   \n Affinity for working effectively in a fast-paced environment with changing priorities   \n Ability to work operate in a cross-functional capacity, taking personal ownership of a project from inception, through planning, execution and delivery   \n \n What we offer you: \n \n Zynga Stock RSUs and Bonus Plan   \n Full medical, dental, vision benefits as well as life insurance   \n Generous Paid Parental leave   \n Open vacation policy for all full time employees   \n Flexible working hours on many teams   \n Work alongside driven individuals towards a common goal   \n \n Notes \n Zynga is an equal opportunity employer. We are proud of our diverse and inclusive community; we do not discriminate on the basis of race, sex, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, medical condition, disability, or any other class or characteristic protected by applicable law. We welcome candidates, players, employees, and partners from all backgrounds. Join us! \n Zynga will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law. \n We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation. \n The pay range for this position in California at the start of employment is expected to be between $105,000 and $155,000 per Year. However, base pay offered is based on market location, and may vary further depending on individualized factors for job candidates, such as job-related knowledge, skills, experience, and other objective business considerations. Subject to those same considerations, the total compensation package for this position may also include other elements, including a bonus and/or equity awards, in addition to a full range of medical, financial, and/or other benefits. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. If hired, employee will be in an 'at-will position' and the company reserves the right to modify base salary (as well as any other discretionary payment or compensation or benefit program) at any time, including for reasons related to individual performance, company or individual department/team performance, and market factors. \n The pay range for this position in California at the start of employment is expected to be between $105,000 and $155,000 per Year. However, base pay offered is based on market location, and may vary further depending on individualized factors for job candidates, such as job-related knowledge, skills, experience, and other objective business considerations. Subject to those same considerations, the total compensation package for this position may also include other elements, including a bonus and/or equity awards, in addition to a full range of medical, financial, and/or other benefits. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. If hired, employee will be in an 'at-will position' and the company reserves the right to modify base salary (as well as any other discretionary payment or compensation or benefit program) at any time, including for reasons related to individual performance, company or individual department/team performance, and market factors. \n \n Zynga does not engage in financial exchanges during the recruitment or onboarding process. We do not conduct job interviews over third-party messaging apps such as Telegram, WhatsApp or others. We will never ask you for your personal or financial information over unofficial chat channels. Our in-house recruitment team only contacts individuals via official company email addresses (i.e., via a zynga.com or naturalmotion.com email domain). \n  If you believe you have been the victim of a scam, you may wish to contact the authorities. In the United States, you may file a complaint with the FBI. More information is available here: https://www.ic3.gov.",
        "cleaned_desc": " Create hypotheses about player behavior, build experimentation plans and new player experiences in order to validate these   \n Design and evaluate novel approaches to using Machine Learning to support creative efforts   \n \n Requirements \n \n Experience developing software with at least one of Unity (C#), Python or React   \n Functional understanding of SQL for querying large quantities of data   \n Able to quickly learn new technologies and systems as needed.   \n In particular we expect our team members to already be familiar with, or quickly ramp up on: AWS fundamentals (e.g. EC2, S3, ECS), distributed data analysis (e.g. Spark) and containerized paradigms (e.g. Docker)   \n Strong written and oral communication skills   ",
        "techs": [
            "unity (c#)",
            "python",
            "react",
            "sql",
            "aws (e.g. ec2",
            "s3",
            "ecs)",
            "spark",
            "docker"
        ],
        "cleaned_techs": [
            "unity (c#)",
            "python",
            "react",
            "sql",
            "aws",
            "s3",
            "ecs)",
            "spark",
            "docker"
        ]
    },
    "5ed6b69686ca5aa7": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 1 (19052), United States of America, McLean, Virginia\n   Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Lead Generative AI Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services that enable real-time customer-facing applications.  \n \n Examples of projects you will work on include: \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience. \n  Enable our users to build new GenAI capabilities. \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field \n  At least 6 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 2 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  At this time, Capital One will not sponsor a new applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $197,400 - $225,300 for Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $209,200 - $238,700 for Lead Machine Learning Engineer\n   Remote (Regardless of Location): $167,400 - $191,000 for Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 1 (19052), United States of America, McLean, Virginia\n   Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Lead Generative AI Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services that enable real-time customer-facing applications.  \n \n Examples of projects you will work on include: \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.    Enable our users to build new GenAI capabilities. \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field \n  At least 6 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 2 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n   \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  At this time, Capital One will not sponsor a new applicant for employment authorization for this position. \n ",
        "techs": [
            "python",
            "go",
            "scala",
            "java"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java"
        ]
    },
    "5fe5dfd4869c4077": {
        "terms": [
            "data science"
        ],
        "salary_min": 95562.61,
        "salary_max": 121003.56,
        "title": "Data Scientist",
        "company": "Frankenmuth Insurance Company",
        "desc": "Summary:  Under direct supervision and following standard procedures, applies advanced analytical techniques to complex data sets to develop distinctive analytical and risk insights that deliver improvements in business results; structures business challenges/opportunities for data sourcing, insight generation, and testing; and generates distinctive analytical and risk insights while working with one or more business units such as Personal Lines, Commercial Lines, Claims, IT and Marketing, by performing the following duties. \n \n \n Essential Duties and Responsibilities: \n  1. Understands ratemaking and reserving actuarial principles and researches and applies those principles to business problems. \n 2. Acts as a strong contributor to distinctive analytical and risk insights within the Actuarial, Risk, & Compliance Services team by: \n a. Understanding and structuring business challenges and opportunities for insights \n b. Generating insights, drawing from a broad range of sources and functions \n c. Scanning a broad spectrum of internal/external, structured/unstructured data sources to define the optimal data set for testing the hypotheses, and partnering across the organization to connect these data sources to build the data set \n d. Presenting insights and recommendations to business units \n e. Establishing and maintaining collaborative relationships throughout the organization \n \n  3. Tests and validates hypotheses to generate risk insights that can deliver improved business results. \n 4. Assists with synthesizing risk insights into compelling ideas and messages. \n 5. Assists with the development of improvements to analytical techniques for generating insight. \n 6. Assists with the preparation of internal/external, structured/unstructured data sets to build and refresh predictive models. \n 7. Maintains a working knowledge of loss reserving software and rate modeling software. \n 8. Performs other duties as assigned. \n \n \n Qualifications: \n  To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. \n \n \n Education/Experience: \n  Bachelor\u2019s degree (B.A.) from four-year college or university; or one to two years of internship experience and/or training; or equivalent combination of education and experience. \n \n  A minimum requirement for this position is the ability to work legally in the United States. No visa sponsorship/support is available for this position, including for any type of U.S. permanent residency (green card) process.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "de8ddac8db718684": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": 130842.63,
        "salary_max": 140909.28,
        "title": "Data Scientist",
        "company": "US Heathcare Product Company",
        "desc": "Designation:- ML Engineer/ Data Scientist Role. Role:- Full Time & C2H Working Status: Green Card/US Citizenship only Salary: 130-140K \n What you'll do Deploy and manage on-premise and cloud-based Machine Learning tools Build ML Ops processes to support the development of ML pipelines and the production deployment of models Benchmark and optimize algorithms to provide users with top performance. Develop ML Ops processes to support training and deployment of models based on DevOps and CI/CD principles Partner with Data Scientists to test, validate and deploy Machine Learning and Optimization models Extend existing ML Frameworks, API connectivity and libraries to meet user needs Establish processes by which models can be trained and retrained Partner closely with stakeholders like Data Scientists, Data Engineers, Quality Assurance analysts \n Required qualifications: 3+ years of experience in an ML Engineer role Experience deploying ML platforms and tools both on-premise and in cloud Bachelor's degree in Computer Science, Information Systems or another applicable field is preferred Experience building and optimizing ML pipelines using DevOps and CI/CD principles Experience training and validating ML pipelines based on model output 3+ years of experience using the following platforms/technologies: Azure or AWS cloud environments Code repository and notebook solutions (Git, Jupyter, etc.) Knowledge of C-family and Python languages, SQL. \n Job Type: Full-time \n Salary: $130,842.63 - $140,909.28 per year \n Education: \n \n Bachelor's (Preferred) \n \n Experience: \n \n Python: 4 years (Preferred) \n SQL: 4 years (Preferred) \n Machine learning: 4 years (Preferred) \n AWS: 3 years (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": "Designation:- ML Engineer/ Data Scientist Role. Role:- Full Time & C2H Working Status: Green Card/US Citizenship only Salary: 130-140K \n What you'll do Deploy and manage on-premise and cloud-based Machine Learning tools Build ML Ops processes to support the development of ML pipelines and the production deployment of models Benchmark and optimize algorithms to provide users with top performance. Develop ML Ops processes to support training and deployment of models based on DevOps and CI/CD principles Partner with Data Scientists to test, validate and deploy Machine Learning and Optimization models Extend existing ML Frameworks, API connectivity and libraries to meet user needs Establish processes by which models can be trained and retrained Partner closely with stakeholders like Data Scientists, Data Engineers, Quality Assurance analysts \n Required qualifications: 3+ years of experience in an ML Engineer role Experience deploying ML platforms and tools both on-premise and in cloud Bachelor's degree in Computer Science, Information Systems or another applicable field is preferred Experience building and optimizing ML pipelines using DevOps and CI/CD principles Experience training and validating ML pipelines based on model output 3+ years of experience using the following platforms/technologies: Azure or AWS cloud environments Code repository and notebook solutions (Git, Jupyter, etc.) Knowledge of C-family and Python languages, SQL. ",
        "techs": [
            "ml engineer/ data scientist",
            "ml ops",
            "devops",
            "ci/cd",
            "machine learning tools",
            "ml pipelines",
            "ml ops processes",
            "algorithms",
            "ml frameworks",
            "api connectivity",
            "data scientists",
            "data engineers",
            "quality assurance analysts",
            "azure",
            "aws",
            "git",
            "jupyter",
            "c-family languages",
            "python",
            "sql"
        ],
        "cleaned_techs": [
            "ml engineer/ data scientist",
            "ml ops",
            "devops",
            "ci/cd",
            "machine learning tools",
            "ml pipelines",
            "ml ops processes",
            "algorithms",
            "ml frameworks",
            "api connectivity",
            "data scientists",
            "data engineers",
            "quality assurance analysts",
            "azure",
            "aws",
            "git",
            "jupyter",
            "c-family languages",
            "python",
            "sql"
        ]
    },
    "41f3119f8077f6ce": {
        "terms": [
            "data science"
        ],
        "salary_min": 115015.0,
        "salary_max": 145617.0,
        "title": "Data Scientist",
        "company": "US Board of Veterans' Appeals",
        "desc": "Duties \n The incumbent serves as a technical program expert for Technical Infrastructure Branch performing a wide range of analyses, reporting, project management, and training activities. Develops, integrates, and maintains data and extract, transform and load (ETL) procedures to ensure the integrity and efficiency of data interchanges. Develops forensically sound, auditable, flexible, repeatable, and scalable ETL capabilities through data engineering best practices on a variety of structure, unstructured, and metadata. Performs testing as necessary for workflows for routing and data transformation. Performs data mapping and validation between existing, modified, and future applications to ensure efficient and effective electronic translation of data between systems, ensuring that the Board's components are provided with the data necessary to carry out the Board's mission of providing timely and accurate services to Veterans.    Major Duties:  \n \n Mastery of and skill in applying a broad range of data analyses, mathematics, statistical analysis, modeling/simulation, and/or other scientific concepts, principles, standards, methods, techniques, practices, and procedures. \n Mastery of and skill in applying, project management and organizational principles and practices, and data administration and data related policies and standards, and process engineering concepts. \n Develops operating procedures for the Board stakeholders to use data systems to format and submit data for inclusion in the Board's data warehouse. \n Collaborates with all developers and business users to gather required data and execute all ETL programs and scripts on systems. Implements all data warehouse activities and prepares reports for same. \n Works with database, data warehouse, and data lake developers/contractors to ensure analytical utility of data in the data warehouse and applications. Creates applications allowing Board users access to the data. \n Conveys results of analyses, insights, and work products to decision makers, managers, and other personnel. \n Develops, coordinates, implements, and documents all ETL procedures for all new projects and maintains effective awareness of all production activities according to required standards. Performs tests and validations for all ETL processes and provides support to all existing ETL processes and applications. \n \n \n Work Schedule:  Monday - Friday, 8:00am - 4:30pm\n   \n Position Description Title/PD#: Data Scientist/197460 \n \n Physical Requirements:  The work is primarily sedentary. The incumbent may carry light items such as books, instruments, and other similar materials. The incumbent may travel to various VA facilities, training sites, or conferences. The work does not require any special physical effort. \n  \n \n \n Requirements \n Conditions of Employment \n \n You must be a U.S. citizen to apply for this job \n Subject to a background/suitability investigation \n May serve a probationary period \n Selective Service Registration is required for males born after 12/31/1959 \n A complete application package; Resume, Transcripts, etc. \n Selected applicants will be required to complete an online onboarding process \n \n \n Qualifications \n \n Basic Qualifications Standard for Data Science Series, 1560:  To be eligible for this position you must meet one of the below basic qualifications. \n     \n \n Have earned a degree in: Mathematics, statistics, computer science, data science or field directly related to the position. The degree must be in a major field of study (at least at the baccalaureate level) that is appropriate for the position.   or   \n \n \n Posses a combination of education and experience: Courses equivalent to a major field of study (30 semester hours) as shown in the paragraph above, plus additional education or appropriate experience.   \n \n \n ** Note: You MUST submit transcripts as verification of educational requirement. \n  In addition to meeting the basic requirement above, to qualify for this position you must also meet the qualification requirements listed below. \n \n \n Specialized Experience:  You must have one year of specialized experience equivalent to at least the next lower grade GS-12 in the normal line of progression for the occupation in the organization. \n     Your resume must clearly support at least 3 of the listed specialized experience items to be minimally qualified. Examples of specialized experience would typically include, but are not limited to: \n     \n Developing, integrating, and maintaining data and extract transform and load (ETL) procedures to ensure the integrity and efficiency of data interchanges. \n Collaborating with all developers and business users to gather required data and execute all ETL programs and scripts on systems. \n Working with databases, data warehouse, and data lake developers/contractors to ensure analytical utility of data in various applications. \n Creating applications allowing users access to data. \n You will be rated on the following Competencies for this position: \n     \n Data Systems \n Customer Service \n Mathematical Reasoning \n Project Management \n Data Management \n Reasoning \n Experience refers to paid and unpaid experience, including volunteer work done through National Service programs (e.g., Peace Corps, AmeriCorps) and other organizations (e.g., professional; philanthropic; religions; spiritual; community; student; social). Volunteer work helps build critical competencies, knowledge, and skills and can provide valuable training and experience that translates directly to paid employment. You will receive credit for all qualifying experience, including volunteer experience. \n    \n \n \n Education \n Note:  Only education or degrees recognized by the U.S. Department of Education from accredited colleges, universities, schools, or institutions may be used to qualify for Federal employment. You can verify your education here: http://ope.ed.gov/accreditation/.    If you are using foreign education to meet qualification requirements, you must send a Certificate of Foreign Equivalency with your transcript in order to receive credit for that education.  \n \n \n Additional information \n \n \n \n  VA supports the use of telework as a way to help attract and retain talented individuals in public service, increase worker productivity, and better prepare the agency to operate during emergencies. This position may be authorized for telework. Telework eligibility will be discussed during the interview process.    The  Interagency Career Transition Assistance Plan (ICTAP)  and  Career Transition Assistance Plan (CTAP)  provide eligible displaced VA competitive service employees with selection priority over other candidates for competitive service vacancies. To be well-qualified, applicants must possess experience that exceeds the minimum qualifications of the position including all selective factors if applicable, and must be proficient in most of the requirements of the job. Information about ICTAP and CTAP eligibility is on OPM's Career Transition Resources website which can be found at https://www.opm.gov/.     Receiving Service Credit for Earning Annual (Vacation) Leave:  Federal Employees earn annual leave at a rate (4, 6 or 8 hours per pay period) which is based on the number of years they have served as a Federal employee. VA may offer newly-appointed Federal employee's credit for their job-related non-federal experience or active duty uniformed military service. This credited service can be used in determining the rate at which they earn annual leave. Such credit must be requested and approved prior to the appointment date and is not guaranteed.     This job opportunity announcement may be used to fill additional vacancies. \n If you are unable to apply online or need to fax a document you do not have in electronic form, view the following link for information regarding an Alternate Application. \n \n \n \n \n \n \n Benefits \n \n A career with the U.S. government provides employees with a comprehensive benefits package. As a federal employee, you and your family will have access to a range of benefits that are designed to make your federal career very rewarding. Opens in a new windowLearn more about federal benefits.  \n \n Review our benefits  \n \n Eligibility for benefits depends on the type of position you hold and whether your position is full-time, part-time or intermittent. Contact the hiring agency for more information on the specific benefits offered. \n \n \n \n \n \n \n How You Will Be Evaluated \n \n You will be evaluated for this job based on how well you meet the qualifications above. \n IN DESCRIBING YOUR EXPERIENCE, PLEASE BE CLEAR AND SPECIFIC. WE WILL NOT MAKE ASSUMPTIONS REGARDING YOUR EXPERIENCE.  Applicants will be referred in the order in which they were received.  \n \n \n \n \n Benefits \n \n A career with the U.S. government provides employees with a comprehensive benefits package. As a federal employee, you and your family will have access to a range of benefits that are designed to make your federal career very rewarding. Opens in a new windowLearn more about federal benefits.  \n \n Review our benefits  \n \n Eligibility for benefits depends on the type of position you hold and whether your position is full-time, part-time or intermittent. Contact the hiring agency for more information on the specific benefits offered. \n \n \n Required Documents \n \n As a new or existing federal employee, you and your family may have access to a range of benefits. Your benefits depend on the type of position you have - whether you're a permanent, part-time, temporary or an intermittent employee. You may be eligible for the following benefits, however, check with your agency to make sure you're eligible under their policies. \n Documents Accepted:   \n \n  DD-214/ Statement of Service \n  Disability Letter (VA) \n  Resume \n  SF-50/ Notification of Personnel Action \n  Transcript \n \n  Documents Required:\n      \n \n Resume \n  Transcript \n \n  Please review the above list(s) to ensure you have included all necessary documents required for your application.Not every applicant will require the same documents, therefore it is the applicants responsibility to ensure that their application package includes all necessary documents to determine qualifications and eligibility for appointment, such as a copy of your SF-50, transcript, ICTAP/CTAP documentation (for displaced Federal employees).\n      You will not be contacted for additional information.  Applicants will be deemed ineligible if supporting documentation is not submitted.\n      \n \n Veterans' Preference:  Since the Direct-Hire Recruitment Authority is being used, traditional Veterans' Preference rules do not apply. Qualified veterans will, however, be given full consideration for this position.\n      \n \n Applications are accepted online.  Applying online will allow you to review and track the status of your application. \n      \n \n If you are relying on your education to meet qualification requirements:  \n Education must be accredited by an accrediting institution recognized by the U.S. Department of Education in order for it to be credited towards qualifications. Therefore, provide only the attendance and/or degrees from schools accredited by accrediting institutions recognized by the U.S. Department of Education.  \n Failure to provide all of the required information as stated in this vacancy announcement may result in an ineligible rating or may affect the overall rating.  \n \n \n How to Apply \n \n \n All applicants are encouraged to apply online.     To apply for this position,  you must complete the occupational questionnaire and submit the documentation specified in the Required Documents section. The complete application package must be submitted by 11:59 PM (EST) on 10/23/2023 to receive consideration. To preview the questionnaire click https://apply.usastaffing.gov/ViewQuestionnaire/12166093.    1. To begin, click  Apply Online  to create a USAJOBS account or log in to your existing account. Follow the prompts to select your USAJOBS resume and/or other supporting documents and complete the occupational questionnaire.    2. Click  Submit My Answers  to submit your application package.     NOTE:  It is your responsibility to ensure your responses and appropriate documentation is submitted prior to the closing date. To verify your application is complete, log into your USAJOBS account, https://my.usajobs.gov/Account/Login, select the  Application Status  link and then select the M ore Information  link for this position. The Details page will display the status of your application, the documentation received and processed, and any correspondence the agency has sent related to this application. Your uploaded documents may take several hours to clear the virus scan process.     To return to an incomplete application , log into your USAJOBS account and click  Update Application  in the vacancy announcement. You must re-select your resume and/or other documents from your USAJOBS account or your application will be incomplete. \n \n \n Agency contact information \n VHA National Recruitment Center  \n \n \n Phone \n (844)456-5208  \n Email \n VHANationalRecruitmentCenter@va.gov  \n \n \n Address \n \n \n Board of Veterans' Appeals \n \n 425 I St NW \n \n Washington, DC 20001 \n \n US  \n \n \n \n \n Next steps \n \n After the vacancy announcement closes, applicants are evaluated to ensure qualification and eligibility requirements are met. After the review is complete, a referral certificate(s) is issued and applicants will be notified of their status by email.  \n \n \n Fair and Transparent \n \n The Federal hiring process is set up to be fair and transparent. Please read the following guidance.  \n \n Equal Employment Opportunity (EEO) Policy  \n Reasonable accommodation policy  \n Financial suitability  \n Selective Service  \n New employee probationary period  \n Signature and false statements  \n Privacy Act  \n Social security number request  \n \n \n \n \n \n Required Documents \n \n Documents Accepted:   \n \n  DD-214/ Statement of Service \n  Disability Letter (VA) \n  Resume \n  SF-50/ Notification of Personnel Action \n  Transcript \n \n  Documents Required:\n    \n \n Resume \n  Transcript \n \n  Please review the above list(s) to ensure you have included all necessary documents required for your application.Not every applicant will require the same documents, therefore it is the applicants responsibility to ensure that their application package includes all necessary documents to determine qualifications and eligibility for appointment, such as a copy of your SF-50, transcript, ICTAP/CTAP documentation (for displaced Federal employees).\n    You will not be contacted for additional information.  Applicants will be deemed ineligible if supporting documentation is not submitted.\n    \n \n Veterans' Preference:  Since the Direct-Hire Recruitment Authority is being used, traditional Veterans' Preference rules do not apply. Qualified veterans will, however, be given full consideration for this position.\n    \n \n Applications are accepted online.  Applying online will allow you to review and track the status of your application. \n    \n \n If you are relying on your education to meet qualification requirements:  \n Education must be accredited by an accrediting institution recognized by the U.S. Department of Education in order for it to be credited towards qualifications. Therefore, provide only the attendance and/or degrees from schools accredited by accrediting institutions recognized by the U.S. Department of Education.  \n Failure to provide all of the required information as stated in this vacancy announcement may result in an ineligible rating or may affect the overall rating. \n \n \n \n \n \n \n Help \n  This job is open to \n \n \n \n \n Career transition (CTAP, ICTAP, RPL) \n Federal employees who meet the definition of a \"surplus\" or \"displaced\" employee. \n \n \n \n \n The public \n U.S. Citizens, Nationals or those who owe allegiance to the U.S. \n \n \n \n Clarification from the agency \n ALL US CITIZENS DIRECT HIRE AUTHORITY: This position is being filled using Direct-Hire Authority (5 CFR 337.201) for this occupation.",
        "cleaned_desc": "Duties \n The incumbent serves as a technical program expert for Technical Infrastructure Branch performing a wide range of analyses, reporting, project management, and training activities. Develops, integrates, and maintains data and extract, transform and load (ETL) procedures to ensure the integrity and efficiency of data interchanges. Develops forensically sound, auditable, flexible, repeatable, and scalable ETL capabilities through data engineering best practices on a variety of structure, unstructured, and metadata. Performs testing as necessary for workflows for routing and data transformation. Performs data mapping and validation between existing, modified, and future applications to ensure efficient and effective electronic translation of data between systems, ensuring that the Board's components are provided with the data necessary to carry out the Board's mission of providing timely and accurate services to Veterans.    Major Duties:  \n \n Mastery of and skill in applying a broad range of data analyses, mathematics, statistical analysis, modeling/simulation, and/or other scientific concepts, principles, standards, methods, techniques, practices, and procedures. \n Mastery of and skill in applying, project management and organizational principles and practices, and data administration and data related policies and standards, and process engineering concepts. \n Develops operating procedures for the Board stakeholders to use data systems to format and submit data for inclusion in the Board's data warehouse. \n Collaborates with all developers and business users to gather required data and execute all ETL programs and scripts on systems. Implements all data warehouse activities and prepares reports for same. \n Works with database, data warehouse, and data lake developers/contractors to ensure analytical utility of data in the data warehouse and applications. Creates applications allowing Board users access to the data. \n Conveys results of analyses, insights, and work products to decision makers, managers, and other personnel. \n Develops, coordinates, implements, and documents all ETL procedures for all new projects and maintains effective awareness of all production activities according to required standards. Performs tests and validations for all ETL processes and provides support to all existing ETL processes and applications. \n \n \n Work Schedule:  Monday - Friday, 8:00am - 4:30pm\n   \n Position Description Title/PD#: Data Scientist/197460 \n \n Physical Requirements:  The work is primarily sedentary. The incumbent may carry light items such as books, instruments, and other similar materials. The incumbent may travel to various VA facilities, training sites, or conferences. The work does not require any special physical effort. \n  \n \n \n Requirements \n Conditions of Employment \n \n You must be a U.S. citizen to apply for this job \n Subject to a background/suitability investigation \n May serve a probationary period \n Selective Service Registration is required for males born after 12/31/1959 \n A complete application package; Resume, Transcripts, etc. \n Selected applicants will be required to complete an online onboarding process \n \n \n Qualifications \n \n Basic Qualifications Standard for Data Science Series, 1560:  To be eligible for this position you must meet one of the below basic qualifications. \n     \n \n Have earned a degree in: Mathematics, statistics, computer science, data science or field directly related to the position. The degree must be in a major field of study (at least at the baccalaureate level) that is appropriate for the position.   or   \n \n \n Posses a combination of education and experience: Courses equivalent to a major field of study (30 semester hours) as shown in the paragraph above, plus additional education or appropriate experience.   \n \n \n ** Note: You MUST submit transcripts as verification of educational requirement. \n  In addition to meeting the basic requirement above, to qualify for this position you must also meet the qualification requirements listed below. \n \n \n Specialized Experience:  You must have one year of specialized experience equivalent to at least the next lower grade GS-12 in the normal line of progression for the occupation in the organization. \n     Your resume must clearly support at least 3 of the listed specialized experience items to be minimally qualified. Examples of specialized experience would typically include, but are not limited to: \n     ",
        "techs": [
            "etl procedures",
            "data engineering",
            "data mapping",
            "data validation",
            "data systems",
            "data warehouse",
            "data lake",
            "etl programs",
            "data administration",
            "process engineering",
            "data analysis",
            "statistical analysis",
            "modeling/simulation",
            "project management",
            "organizational principles",
            "data-related policies and standards",
            "data scientist",
            "mathematics",
            "statistics",
            "computer science"
        ],
        "cleaned_techs": [
            "etl procedures",
            "data mapping",
            "data validation",
            "data systems",
            "data warehouse",
            "data lake",
            "etl programs",
            "data administration",
            "process engineering",
            "statistical analysis",
            "modeling/simulation",
            "project management",
            "organizational principles",
            "data-related policies and standards",
            "data scientist",
            "mathematics",
            "statistics",
            "computer science"
        ]
    },
    "8f70e54606748d50": {
        "terms": [
            "data science"
        ],
        "salary_min": 124647.85,
        "salary_max": 157831.95,
        "title": "Data Scientist",
        "company": "Pacaso",
        "desc": "About Pacaso: \n  Pacaso exists to enrich lives by making second home ownership possible and enjoyable for more people. Our innovative co-ownership model is the easiest, smartest and most responsible way for people to experience the joy of a second home. We provide all the benefits of true ownership without the hassles through our simplified financial structure, easy and equitable scheduling, and dedicated local property management. \n  Founded by former Zillow executives, Pacaso has secured more than $215 million in growth financing and is valued at $1.5 billion. In March 2021, Pacaso achieved unicorn status (a valuation of $1 billion) faster than any other company in the United States. We have been featured in The New York Times, Wall Street Journal, Fortune, Forbes, CNBC and more. \n  Pacaso is a certified Great Place to Work, is #6 on Glassdoor's 2022 list of Best Places to Work, is one of LinkedIn's top startups of 2022, and was ranked on Fortune's Top 100 Small and Medium Workplaces list 2021- 2023. \n  www.pacaso.com \n \n  About the Role \n  As a data scientist at Pacaso you will play a pivotal role in shaping the future of real estate co-ownership. This is a cross-functional position where you will help develop the advanced analytics products and infrastructure that drive our new initiatives and businesses, and ensure we are staying on the path to success. We are looking for a full-stack data scientist who is comfortable building predictive models, understanding business dynamics, and distilling data into actionable insights across all business functions. \n \n \n  What You Will Do \n \n Build and deploy machine learning models that help scale our business and automate our decision making \n Turn information into knowledge by acquiring, exploring, and transforming data from different systems \n Define the right KPIs across functions to measure and track success \n Identify opportunities to grow the business and improve efficiency \n Data visualization and automated reporting \n Support for experimentation design and A/B testing to validate product improvement \n Communicate effectively with executives and stakeholders, and advocate for how our team can help the business \n \n Skills / Qualifications \n \n 5+ years of experience developing analytical insights across different business functions \n Proficiency in Python and/or R, strong SQL skills, and knowledge of ETL processes \n Experience with cloud data platforms and tools (AWS preferred) \n Expert knowledge of data visualization tools (Tableau, Looker, Power BI, Quicksight) \n Experience developing machine learning algorithms and deploying them in a production environment \n Strong communication skills and the ability to translate results to different audiences \n \n \n  You'll love working at Pacaso because of our ... \n \n Amazing remote-first team and culture. \n Competitive salary and stock options. \n Unlimited, flexible PTO for exempt employees. \n Excellent medical, dental and vision insurance. \n Sponsored memberships to One Medical, Ginger and Carrot. \n 401(k) to help you save for the future. \n Paid maternity and paternity leave. \n Generous home office stipend and monthly cell phone reimbursement. \n Quarterly remote team building events and L&D opportunities. \n \n Pacaso encourages applications from people of all races, religions, national origins, genders, sexual orientations, gender identities, gender expressions and ages, as well as veterans and individuals with disabilities. \n  #LI-Remote",
        "cleaned_desc": " Data visualization and automated reporting \n Support for experimentation design and A/B testing to validate product improvement \n Communicate effectively with executives and stakeholders, and advocate for how our team can help the business \n \n Skills / Qualifications \n \n 5+ years of experience developing analytical insights across different business functions \n Proficiency in Python and/or R, strong SQL skills, and knowledge of ETL processes   Experience with cloud data platforms and tools (AWS preferred) \n Expert knowledge of data visualization tools (Tableau, Looker, Power BI, Quicksight) \n Experience developing machine learning algorithms and deploying them in a production environment \n Strong communication skills and the ability to translate results to different audiences \n \n \n  You'll love working at Pacaso because of our ... \n ",
        "techs": [
            "python",
            "r",
            "sql",
            "aws",
            "tableau",
            "looker",
            "power bi",
            "quicksight"
        ],
        "cleaned_techs": [
            "python",
            "r",
            "sql",
            "aws",
            "tableau",
            "looker",
            "powerbi",
            "quicksight"
        ]
    },
    "cceb4cd0fd2b8a3a": {
        "terms": [
            "data science"
        ],
        "salary_min": 25.0,
        "salary_max": 35.0,
        "title": "Payment Accuracy Data Mining Specialist 2",
        "company": "Cotiviti",
        "desc": "Overview: \n   A Payment Accuracy 2, Data Mining (DM) Specialist, is a member of the greater Data Mining Business Unit (BU). Cotiviti's Data Mining team configures custom claim reviews to investigate untapped billing compliance issues specific to regulations and contracted policies across product, market, and provider types. \n \n  The Specialist 2, Payment Accuracy is responsible for developing new and existing audit concepts, gaining client acceptance, training all Specialist levels to execute audit projects, and evaluating the effectiveness of audit concepts. Audits client data and generates high quality recoverable claims for the benefit of Cotiviti and our clients. Conducts and trains more complex audit projects with some to limited supervision. Considered a mentor, trainer, and developer of less-tenured team members. Displays a high degree of independent judgment and professional skepticism that enhances the work performed in order to achieve success in the position.  Responsibilities: \n  \n This individual will work under moderate supervision and will be monitored for efficiency in production and quality review of assigned work. \n  Has the ability to build and maintain a basic understanding of Centers for Medicare and Medicaid Services (CMS) and National Association of Insurance Commissioners (NAIC) guidelines to establish the correct order of liability. \n  Advanced with Cotiviti audit tools Recovery Management System (RMS), specific client systems) to complete auditing, review simple - medium proprietary reports, has an expert understanding of Microsoft Excel and client applications \n  Utilizes healthcare experience to perform audit procedures that include identifying and defining issues, developing criteria, reviewing, and analyzing evidence with the intent to audit medium and complex reports. Work is advanced in scope and complexity. Knowledge is applied to resolve routine issues, as necessary. The scope may include Data Mining, Claim Adjudication, Contract Compliance, Provider Billing & Duplicate Payment Reviews, Policy & Reimbursement Analysis, and Quality Assurance. \n  Advanced analysis of paid claims and identification of audit findings including documentation for training and knowledge sharing. Works with Engineering to increase the efficiency of tools and reporting. \n  May update current reports, develop and run custom queries and validate the accuracy of current reports used. Makes determinations based on prior knowledge and experience of client contract terms with the likelihood of recovery acceptance. \n  Meets or Exceeds Standards for Productivity in addition to regular and predictable attendance, maintains production goals and standards set by the audit for the auditing concept. Achieves the expected level of quality and quantity for assigned work (i.e. hit rate, claims written, vendor/project volume completion, ID and/or fees per hour) \n  Meets or Exceeds Standards for Quality by Achieving the expected level of quality set by the audit for the auditing concept, for valid claim identification and documentation. \n  Highly proficient, subject matter expert in responding to inquiries and disputes received on all claims written. Provides verification of claims validation and confirmation, in a concise written manner, utilizing facts and details for justification purposes. \n  Demonstrates aptitude in reviewing transaction types, client contracts/vendor agreements, and client data with limited supervision of how to identify potential over or underpayments. Makes recommendations on medical policy applications, state and federal statutes, and other reimbursement methodologies as it applies to the audit concept. \n  Considered a skilled resource in onboarding new hires and/or training existing staff on new concepts and processes. \n  Identifies New Claim Types & Concept Expansion by using proven methodologies to research and substantiate claims outside the audit concept. Enlists others internally or externally to help validate, suggest, develop, and analyze high-quality, high-value concepts and/or process improvements, tool enhancements, etc. Strong driver and voice in the development of audit concepts. \n  Recommends New Concepts & Processes based on experience and in-depth knowledge of client contract terms and complex claim types. Has a proven record of developing and implementing new ideas, approaches, and/or technological improvements that support and enhance audit production. Uses advanced validation methods to test and produce a desired/intended result of the new concept. Regularly collaborates with Engineering in the development of new reports and tool functionality. \n  Demonstrates understanding of Cotiviti policies & procedures, and external regulatory requirements and performs duties in accordance with such regulatory requirements \n  Ensures confidentiality and security of all data, adhering to all HIPAA (Health Insurance Portability and Accountability) laws and requirements. Demonstrates the skills, knowledge, and ability to ensure that our environment is safe, complying with industry standards. \n  Qualifications: \n  \n High School Diploma - Required \n  Bachelor\u2019s degree (Preferred) and/or a minimum of at least (4 - 6) year/s related experience in healthcare. \n  At least 3 - 4 year/s of Cotiviti experience is recommended for individuals seeking their next opportunity internally. \n  Healthcare industry experience, including knowledge of Coordination of Benefits. (Preferred). \n  Computer proficiency including Microsoft Office (Word, Excel, Outlook, Access) \n  Excellent verbal and written communication skills. \n  Strong interest in working with large data sets and various databases. \n  Ability to work well in an individual and team environment demonstrating self\u2013motivation to deliver success. \n  Understands and embodies Cotiviti Core Values, Strategic Pillars, and Operations Disciplines to achieve successful performance in completing assigned responsibilities and interactions with the Organization both internally and externally. \n \n \n \n  Base compensation ranges from $25.00 to $35.00. Specific offers are determined by various factors, such as experience, education, skills, certifications, and other business needs. This role is eligible for discretionary bonus consideration.\n  \n \n \n  Cotiviti offers team members a competitive benefits package to address a wide range of personal and family needs, including medical, dental, vision, disability, and life insurance coverage, 401(k) savings plans, paid family leave, 9 paid holidays per year, and 17-27 days of Paid Time Off (PTO) per year, depending on specific level and length of service with Cotiviti. For information about our benefits package, please refer to our Careers page.\n  \n \n   #LI-Remote\n    #LI-KB1\n  \n \n   #senior",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "dc2bac79791f470e": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "speech",
            "computer vision"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "speech",
            "computer vision"
        ]
    },
    "4797965a9323180a": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "77 West Wacker Dr (35012), United States of America, Chicago, Illinois\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "77 West Wacker Dr (35012), United States of America, Chicago, Illinois\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "llms",
            "fms",
            "apis",
            "sdks",
            "python",
            "go",
            "scala",
            "java"
        ],
        "cleaned_techs": [
            "llm",
            "fms",
            "apis",
            "sdks",
            "python",
            "go",
            "scala",
            "java"
        ]
    },
    "9cfb3721b4c555c1": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": 93000.0,
        "salary_max": 173000.0,
        "title": "Data Scientist",
        "company": "Verizon",
        "desc": "When you join Verizon \n  Verizon is one of the world\u2019s leading providers of technology and communications services, transforming the way we connect around the world. We\u2019re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together\u2014lifting up our communities and striving to make an impact to move the world forward. If you\u2019re fueled by purpose, and powered by persistence, explore a career with us. Here, you\u2019ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife. \n \n  What you\u2019ll be doing... \n \n  Financial Planning & Analytics Service (FPAS) is at the heart of Verizon's Digital Finance vision focusing on solving most critical challenges using Data, Advanced Analytics, AI, and Automation. You will be joining the Analytics & Insights - Data Science team within FPAS that is focused on providing wide variety of ML and optimization solutions in Credit and Marketing Investments including Loan Decisions, Promotions and Pricing by applying advanced analytics techniques. \n \n  This role involves working in a complex, multi-functional, Agile team environment with other data scientists and data engineers to develop and productionize analytics solutions. As a Data Scientist on the team you will work side-by-side with an experienced team to develop cutting edge AI/ML Models and solutions to tackle some of Verizon's biggest challenges. \n  This position requires a deep understanding of customer behaviors, strategic thinking, strong financial acumen, and cross-functional aptitude in order to design analytical solutions to make the best informed decisions possible. The successful candidate will interact with all HQ & Market departments including Marketing, Credit risk, CX (Customer Experience), Corporate Finance, Digital Finance and other Analytical teams in the organization. Some of the high level focus areas for the role include: \n \n  Hands on active participation in analytical projects delivering improvements in decision-making and business strategies via advanced analytics. \n  Translate predictive insights from complex analytical frameworks to marketing investment optimization. \n  Prepare presentation materials and formal business case documents for use with senior management to promote findings and drive science based decision support recommendations. \n  Develop and deploy both traditional statistical models and machine learning techniques and algorithms: Regression, clustering, neural networks, random forest etc. to guide marketing investments and strategy. \n  Partner with in-house data strategy experts to design analytically ready datasets by stitching customer data across multiple platforms and incorporating business rules. \n  Partner with functional groups (Marketing, Bill2Cash, Commercial Finance, Operations, Network, etc.) to embed analytics and science driven approaches in all business decisions. \n \n \n  What We\u2019re Looking For\u2026 \n \n  You\u2019ll need to have: \n \n  Bachelor's degree or four or more years of work experience. \n  Four or more years of relevant work experience. \n  Experience in developing and implementing analytical solutions. \n  Experience in Data management, analysis and visualization. \n  Experience with programming in SQL and Python. \n \n \n  Even better if you have \n \n  A Master's degree in a quantitative discipline such as Mathematics, Statistics, Financial Economics/Econometrics, Engineering, Computer Science, or Operations Research. \n  Experience in developing and implementing analytical solutions to complex business problems/opportunities. \n  Active experience in Data management, analysis and visualization to realize absolute and incremental commercial gains. \n  Experience working in the Consumer/Retail space within the Financial Services, Telecommunications, Technology or other related mass market industries or related work in the Public Policy, Bio-statistics and Scientific Research industries. \n  Experience with any of R/SAS and experience programming in VBA, SPSS, MATLAB, JAVA, Tableau, Qlik or other related tools. \n \n \n  If Verizon and this role sound like a fit for you, we encourage you to apply even if you don\u2019t meet every \u201ceven better\u201d qualification listed above. \n \n \n \n \n \n \n \n \n  Where you\u2019ll be working \n \n \n \n \n \n \n  In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n  \n  Scheduled Weekly Hours  40\n  \n  Equal Employment Opportunity \n  We\u2019re proud to be an equal opportunity employer - and celebrate our employees\u2019 differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more. \n  Our benefits are designed to help you move forward in your career, and in areas of your life outside of Verizon. From health and wellness benefits, short term incentives, 401(k) Savings Plan, stock incentive programs, paid time off, parental leave, adoption assistance and tuition assistance, plus other incentives, we\u2019ve got you covered with our award-winning total rewards package. For part-timers, your coverage will vary as you may be eligible for some of these benefits depending on your individual circumstances.\n   If you are hired into a California, Colorado, Connecticut, Nevada, New York, Rhode Island or Washington work location, the compensation range for this position is between $93,000.00 and $173,000.00 annually based on a full-time schedule. The salary will vary depending on your location and confirmed job-related skills and experience. This is an incentive based position with the potential to earn more. For part time roles, your compensation will be adjusted to reflect your hours.",
        "cleaned_desc": "  Experience in developing and implementing analytical solutions. \n  Experience in Data management, analysis and visualization. \n  Experience with programming in SQL and Python. \n \n \n  Even better if you have \n \n  A Master's degree in a quantitative discipline such as Mathematics, Statistics, Financial Economics/Econometrics, Engineering, Computer Science, or Operations Research. \n  Experience in developing and implementing analytical solutions to complex business problems/opportunities. \n  Active experience in Data management, analysis and visualization to realize absolute and incremental commercial gains. \n  Experience working in the Consumer/Retail space within the Financial Services, Telecommunications, Technology or other related mass market industries or related work in the Public Policy, Bio-statistics and Scientific Research industries. \n  Experience with any of R/SAS and experience programming in VBA, SPSS, MATLAB, JAVA, Tableau, Qlik or other related tools. ",
        "techs": [
            "sql",
            "python",
            "r/sas",
            "vba",
            "spss",
            "matlab",
            "java",
            "tableau",
            "qlik"
        ],
        "cleaned_techs": [
            "sql",
            "python",
            "r/sas",
            "vba",
            "spss",
            "matlab",
            "java",
            "tableau",
            "qlik"
        ]
    },
    "157f1a14937d6dc7": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "201 Third Street (61049), United States of America, San Francisco, California\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "201 Third Street (61049), United States of America, San Francisco, California\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java"
        ]
    },
    "5857294da7f950d7": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java",
            "apis",
            "sdks",
            "large-language models (llms)",
            "foundation models (fms)",
            "deep neural networks",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems",
            "distributed computing",
            "cache optimization techniques",
            "ai safety",
            "ai infrastructure",
            "ai capabilities",
            "api access patterns",
            "api security",
            "observability",
            "cloud access control",
            "privacy best practices"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java",
            "apis",
            "sdks",
            "large-language models (llms)",
            "foundation models (fms)",
            "deep neural networks",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems",
            "distributed computing",
            "cache optimization techniques",
            "ai",
            "api access patterns",
            "observability",
            "cloud access control"
        ]
    },
    "a723908be4de0cef": {
        "terms": [
            "data science",
            "data analyst"
        ],
        "salary_min": 60000.0,
        "salary_max": 85000.0,
        "title": "E-Commerce Data Analyst",
        "company": "BigFly, Inc.",
        "desc": "We are seeking a skilled and analytical E-Commerce Data Analyst to join our team. The ideal candidate will play a critical role in optimizing our presence on the Amazon marketplace and decision-making processes by leveraging data-driven insights. You will be responsible for collecting, analyzing, and interpreting data to help drive improvements in our Amazon sales, customer experience, and overall business performance. \n Key Responsibilities: \n Amazon Data Collection and Integration \n \n Gather and integrate data from various sources within the Amazon marketplace, including Seller Central, Amazon Advertising, and third-party analytics tools \n Ensure data accuracy and consistency for analysis. \n \n Amazon Sales Analysis \n \n Analyze Amazon-specific metrics, such as sales performance, product ranking, Buy Box ownership, and Amazon Advertising campaigns. \n Identify trends, patterns, and anomalies within the data. \n \n Customer Behavior Analysis on Amazon \n \n Study customer behavior on our Amazon product listings, including product views, click-through rates, and conversion rates. \n Utilize Amazon customer data to provide targeted marketing strategies and recommendations. \n \n Amazon Product and Inventory Analysis \n \n Monitor the performance of products on Amazon, track inventory levels, and optimize pricing strategies based on Amazon's marketplace dynamics. \n Make data-driven recommendations to improve product assortment and inventory management on Amazon. \n \n Amazon-Specific Performance Reporting \n \n Develop and maintain dashboards and reports specifically for Amazon marketplace KPIs. \n Share Amazon-related insights with cross-functional teams. \n \n Amazon Market Research \n \n Stay updated with Amazon marketplace trends, competitor activities, and Amazon policy changes. \n Provide insights and recommendations based on Amazon market research. \n \n Recommendations and Amazon Marketplace Strategy \n \n Collaborate with cross-functional teams to make data-driven recommendations to enhance our Amazon marketplace strategies, user experience, and operational processes. \n \n Data Quality Assurance for Amazon \n \n Ensure data integrity and reliability of Amazon-specific data through regular data cleaning and validation processes. \n \n Qualifications: \n \n Bachelor's degree in Data Science, Statistics, Business, or a related field is preferred, but not required. Equivalent experience in this position would be considered as an alternative. \n Proven experience in e-commerce data analysis, with a focus on the Amazon Marketplace. \n Proficiency in data analysis tools, including Excel. \n Strong understanding of Amazon Seller Central, Amazon Advertising, and other Amazon-specific analytics tools. \n Experience with A/B testing and experimentation methodologies specific to the Amazon platform is a plus. \n Knowledge of Amazon's marketplace and advertising concepts. \n Excellent problem-solving skills and the ability to communicate complex data findings to non-technical stakeholders. \n Strong attention to detail and a passion for data accuracy. \n Self-motivated, proactive, and able to work independently or in a team. \n Must be available during East Coast Business Hours (9am-5pm EST). \n \n Benefits: \n \n Competitive salary and benefits package. Range $60k-$85k, commensurate with experience. \n Collaborative and inclusive company culture. \n Continuous learning and development opportunities. \n Potential for career advancement within the organization. \n Flexibility for full remote work. \n \n Job Type: Full-time \n Pay: $60,000.00 - $85,000.00 per year \n Benefits: \n \n AD&D insurance \n Dental insurance \n Dependent health insurance coverage \n Disability insurance \n Health insurance \n Life insurance \n Paid holidays \n Paid time off \n Prescription drug insurance \n Retirement plan \n Vision insurance \n Work from home \n \n Experience level: \n \n 3 years \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote",
        "cleaned_desc": " Collaborate with cross-functional teams to make data-driven recommendations to enhance our Amazon marketplace strategies, user experience, and operational processes. \n \n Data Quality Assurance for Amazon \n \n Ensure data integrity and reliability of Amazon-specific data through regular data cleaning and validation processes. \n \n Qualifications: \n \n Bachelor's degree in Data Science, Statistics, Business, or a related field is preferred, but not required. Equivalent experience in this position would be considered as an alternative. \n Proven experience in e-commerce data analysis, with a focus on the Amazon Marketplace. \n Proficiency in data analysis tools, including Excel. \n Strong understanding of Amazon Seller Central, Amazon Advertising, and other Amazon-specific analytics tools. \n Experience with A/B testing and experimentation methodologies specific to the Amazon platform is a plus. \n Knowledge of Amazon's marketplace and advertising concepts. \n Excellent problem-solving skills and the ability to communicate complex data findings to non-technical stakeholders. \n Strong attention to detail and a passion for data accuracy. \n Self-motivated, proactive, and able to work independently or in a team. ",
        "techs": [
            "excel",
            "amazon seller central",
            "amazon advertising",
            "a/b testing"
        ],
        "cleaned_techs": [
            "excel",
            "aws",
            "a/b testing"
        ]
    },
    "61228aee56752251": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "201 Third Street (61049), United States of America, San Francisco, California\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "201 Third Street (61049), United States of America, San Francisco, California\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems"
        ]
    },
    "4fcb52dbbe41457e": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java"
        ]
    },
    "a84efc431d16e17f": {
        "terms": [
            "data science"
        ],
        "salary_min": 72254.4,
        "salary_max": 91490.164,
        "title": "Manager, Project Training and Quality",
        "company": "Inizio Engage",
        "desc": "Inizio Engage has a long-standing partnership with a leading Biotechnology company, across Commercial, Patient Solutions and Medical Affairs businesses.\n  \n \n \n   Assist with client-specific QA initiatives including training, monitoring quality review including analysis of aggregate quality/performance data, maintenance of SOPs as well as creation/updates to training materials based on quality reviews, etc. Assist with project specific or company-wide QA work such as management of employee training documentation, audit preparation, document review/editing and filing. Responsible for identifying training needs, developing procedures and training Inizio staff and management, including ongoing refresher training and project specific training.\n  \n \n \n   This is your opportunity to join Inizio Engage and represent a top biotechnology company!\n  \n \n \n   What\u2019s in it for you?\n  \n \n \n  Competitive compensation \n  Excellent Benefits \u2013 accrued time off, medical, dental, vision, 401k, disability & life insurance, paid maternity and bonding time benefits, employee discounts/promotions \n  Generous performance-driven Incentive Compensation package \n  Competitive environment with company wide recognition, contests, and coveted awards \n  Exceptional company culture \n  Recognized as a Top Workplace USA 2021 \n  Awarded a \u201cGreat Place to Work\u201d award in 2022 and 2023 \n  Fortune Best Workplaces in Biopharma 2022 \n \n \n \n   What will you be doing?\n  \n \n \n  Work with the Project team, Client Account Management Team and Quality Assurance departments to develop and enhance client trainings resulting in improved customer satisfaction, quality of service and compliance with Inizio and Client standards of excellence. \n  Perform ongoing call quality monitoring and scoring in coordination with the Client Account Management Team \n  Define and document training plans, schedules and requirements. Design, develop and revise training and instructional materials for classroom use, ad-hoc and/or self-administered training. Provide ongoing and refresher training for project team. \n  Make recommendations on the best methodology and implementation methods for training programs. Formulate teaching outline and determine instructional methods such as individual training, group instruction, lectures, demonstrations, conferences, meetings and workshops. \n  Test trainees to measure progress and to evaluate effectiveness of training. \n  Review and interpret documents such as Client program procedure manuals and training modules. Interpret and translate to staff instructions furnished in written, oral, diagram, or schedule form. \n  Perform monitoring of services for assigned project and/or Quality Assurance Department. Provide constructive coaching feedback to staff with goal of improving overall customer experience. \n  Coordinate quality review calibration sessions with Client Account Management Team, Project team and Client, if applicable. \n  Compile QA data and prepare reports as necessary; analyze results and make recommendations including revisions to procedural documents and training documents as applicable. \n  Assist with annual review and ongoing updates for Client Standard Operating Procedure (SOP). \n  Assist with internal and external client audits as dictated in the Inizio Quality Management Plan. \n  Assist with any/all quality activities and reporting as requested by the Client, Business Unit Director, Vice President, Operational Excellence & Quality Assurance and/or the Manager, Quality Monitoring. \n  Perform project training certifications and complete appropriate documentation. \n  Manage QMS training records. \n  File training materials in an organized manner for efficient access by staff. \n  Perform general clerical duties. \n  Must safeguard patient privacy and confidentiality by following the guidelines set forth in the Privacy and Security Rules of the Health Insurance Portability and Accountability Act (HIPAA). \n \n \n \n  What do you need for this position?\n  \n \n \n  Bachelor\u2019s degree is required. \n  Minimum of 1-2 years relevant experience in a healthcare setting is required. \n  Ability to read and understand data and file layouts. \n  Proficient in Microsoft Outlook, Word, Excel and PowerPoint. \n  Ability to deliver feedback to employees in a constructive manner. \n  Strong organizational and analytical skills, as well as critical thinking and problem-solving skills. \n  Ability to multitask. \n  Critical thinking and problem-solving skills. \n  Detail oriented. \n  Ability to develop and proof training materials. \n  Ability to speak effectively in interpersonal situations and before groups of employees. \n  Ability to work as a team in a supportive environment to achieve shared objectives. \n  Ability to function in a leadership capacity and as a positive role model. \n  Excellent listening skills in order to identify and anticipate training needs. \n  Proven presentation and facilitation skills. \n \n \n \n  About Inizio Engage\n  \n \n \n  Inizio Engage is a strategic, commercial, and creative engagement partner that specializes in healthcare. Our passionate, global workforce augments local expertise and diverse mix of skills with data, science, and technology to deliver bespoke engagement solutions that help clients reimagine how they engage with their patients, payers, people and providers to improve treatment outcomes. Our mission is to partner with our clients, improving lives by helping healthcare professionals and patients get the medicines, knowledge and support they need.\n  \n \n \n   To learn more about Inizio Engage, visit us at: \n   \n   https://inizio.health/\n   \n \n \n \n   We believe in our values: We empower everyone/We rise to the challenge/We work as one/We ask what if/We do the right thing, and we will ask you how your personal values align to them.\n  \n \n \n   Inizio Engage is proud to be an equal opportunity employer. Individuals seeking employment at Inizio are considered without regards to age, ancestry, color, gender, gender identity or expression, genetic information, marital status, medical condition (including pregnancy, childbirth, or related medical conditions), mental or physical disability, national origin, protected family care or medical leave status, race, religion (including beliefs and practices or the absence thereof), sexual orientation, military or veteran status, or any other characteristic protected by federal, state, or local laws. Further, pursuant to applicable local ordinances, Inizio will consider for employment qualified applicants with arrest and conviction records.\n  \n \n \n   Inizio Engage is an equal opportunity employer M/F/V/D. We appreciate your interest in our company, however, only qualified candidates will be considered.",
        "cleaned_desc": "  Ability to read and understand data and file layouts. \n  Proficient in Microsoft Outlook, Word, Excel and PowerPoint. \n  Ability to deliver feedback to employees in a constructive manner. \n  Strong organizational and analytical skills, as well as critical thinking and problem-solving skills. \n  Ability to multitask. \n  Critical thinking and problem-solving skills. \n  Detail oriented. \n  Ability to develop and proof training materials. \n  Ability to speak effectively in interpersonal situations and before groups of employees. \n  Ability to work as a team in a supportive environment to achieve shared objectives. \n  Ability to function in a leadership capacity and as a positive role model. \n  Excellent listening skills in order to identify and anticipate training needs. \n  Proven presentation and facilitation skills. \n \n \n \n  About Inizio Engage\n  \n ",
        "techs": [
            "ability to read and understand data and file layouts,\nproficient in microsoft outlook",
            "word",
            "excel and powerpoint,\nstrong organizational and analytical skills,\ncritical thinking and problem-solving skills,\nability to multitask,\ndetail oriented,\nability to develop and proof training materials,\nability to speak effectively in interpersonal situations and before groups of employees,\nability to work as a team in a supportive environment to achieve shared objectives,\nability to function in a leadership capacity and as a positive role model,\nexcellent listening skills in order to identify and anticipate training needs,\nproven presentation and facilitation skills"
        ],
        "cleaned_techs": [
            "word"
        ]
    },
    "12c63f4cfd807863": {
        "terms": [
            "data science"
        ],
        "salary_min": 90000.0,
        "salary_max": 120000.0,
        "title": "Senior Manager - Data Mining - Healthcare",
        "company": "EXL",
        "desc": "EXL Health is seeking an experienced and self-motivated Data Mining Subject Matter Experts \u2013 Data Mining with medical claims experience. This is a Home Office Opportunity. \n The Data Mining SME position is to assist EXL clients in controlling healthcare costs by identifying concepts and performing audit reviews to determine if correct payments were made to providers. The Data Mining SME is a senior position responsible for reviewing the output of queries to identify overpayments as well as develop and validate new overpayment concepts and act as a subject matter expert when called upon. \n Responsibilities: \n \n Owns the development and deployment of audit concepts for his/her client, from idea conception through steady state production. \n Develops deep knowledge of his/her client\u2019s data sets, Adjudication and other systems, policies, geography, provider contracts, provider behavior, workflows, file exchanges, speed analysis, competitive intelligence, stakeholder decision-making behavior etc.,. \n Responsible for ongoing maintenance of deployed content, i.e., query size, auditor efficiency, profitability, competitor behavior etc.,. \n Works closely with other content development SME\u2019s to extract/apply new content from the library \n Acquires and maintains access to client systems. Learns navigation, trains new users (auditors), documents process steps etc.,. \n Maintains feedback loops to put learned info back into the queries, workflows, content library etc.,. \n Accurately reprice complex claims utilizing multiple client claims systems to perform the tasks. \n Identify work files to be reviewed through a query process. \n Review provider contracts/fee schedules, to determine correct payment methodology. \n Documentation of audit concepts and creation of whitepapers to be shared with clients for approval \n Serve as a resource for junior auditors and new employees \n Serve as a client and program specific SME when called upon in both internal and external settings \n Evaluate current procedures for efficiency and accuracy as well as recommend solutions for process improvements. \n Other duties as assigned \n \n Qualifications: \n \n Minimum 10+ years medical claims experience, Particularly in claims adjudication, \n Experience in handling appeals, claims reworks, recovery projects are added advantage. \n Expert knowledge of Client systems / applications ( CAS , MTV , PAAG , CIS , BCOP search , Mentor , CIS Pro , APEX , eHub ,CRM, HOWIE Certificate, Search Humana Image, View Station IPAR Tracking tool , PMDM Provider Search ) \n Good verbal and written communication skills \n \n Knowledge and Skills: \n \n Knowledge of CMS Guidelines for Inpatient hospital, SNF, Home health, LTCH, Outpatient and physician claims. Knowledge of Coding Methodologies. \n Should have extensive knowledge in different types of reimbursement methodologies available in Medicare and in commercial market. \n Should be able to read and understand the provider contracts. Should be able to identify the mistakes in contract system and there by the incorrectly priced claims. Should have already worked in provider contract related areas and claims adjudication. \n Sound knowledge of ICD-9, ICD-10, CPT and HCPCS coding guidelines. \n Experience with processing complex medical health claims. \n Experience with encoder tools and industry systems (FACETS, NASCO, Encoder Pro, TrueCode, 3M, Webtrat, Pricers). \n Experience with MS Office programs such as: Excel, PowerPoint, Outlook, and Word \n Sound analytical mindset, capable of finding solutions to problems encountered. \n \n Team Collaboration & Synergies \n \n Training in the functionality of the new SMEs on board. \n Should possess strong interpersonal skills to be able to work well with other teams \n Ability to drive, to innovate and optimize the use of available resources. \n Work cohesively with Analytics and technology, Operations and Quality teams. \n Completely aligned with program goals and objectives \n \n Compliance & Administrative \n \n Adhering to compliance policies \n 40 hours work week. \n \n What We Offer: \n \n EXL Health offers an exciting, fast paced and innovative environment, which brings together a group of sharp and entrepreneurial professionals who are eager to influence business decisions. \n From your very first day, you get an opportunity to work closely with highly experienced, world class Healthcare consultants. \n You can expect to learn many aspects of businesses that our clients engage in. You will also learn effective teamwork and time-management skills - key aspects for personal and professional growth. \n We provide guidance/ coaching to every employee through our mentoring program wherein every junior level employee is assigned a senior level professional as advisors. \n Sky is the limit for our team members. The unique experiences gathered at EXL Health sets the stage for further growth and development in our company and beyond. \n \n Job Type: Full-time \n Pay: $90,000.00 - $120,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n Paid time off \n Vision insurance \n \n Compensation package: \n \n Bonus opportunities \n Employee stock purchase plan \n \n Experience level: \n \n 10 years \n \n Schedule: \n \n 8 hour shift \n Day shift \n Monday to Friday \n \n Work Location: Remote",
        "cleaned_desc": " Experience with processing complex medical health claims. \n Experience with encoder tools and industry systems (FACETS, NASCO, Encoder Pro, TrueCode, 3M, Webtrat, Pricers). \n Experience with MS Office programs such as: Excel, PowerPoint, Outlook, and Word \n Sound analytical mindset, capable of finding solutions to problems encountered. \n \n Team Collaboration & Synergies \n \n Training in the functionality of the new SMEs on board. \n Should possess strong interpersonal skills to be able to work well with other teams \n Ability to drive, to innovate and optimize the use of available resources. \n Work cohesively with Analytics and technology, Operations and Quality teams. \n Completely aligned with program goals and objectives \n \n Compliance & Administrative \n \n Adhering to compliance policies ",
        "techs": [
            "encoder pro",
            "truecode",
            "3m",
            "webtrat",
            "pricers",
            "ms office programs (excel",
            "powerpoint",
            "outlook",
            "word)"
        ],
        "cleaned_techs": [
            "encoder pro",
            "truecode",
            "3m",
            "webtrat",
            "pricers",
            "microsoft",
            "powerpoint",
            "outlook",
            "word)"
        ]
    },
    "274fb9aed7875fb5": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "201 Third Street (61049), United States of America, San Francisco, California\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "201 Third Street (61049), United States of America, San Francisco, California\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "api"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "api"
        ]
    },
    "f8cbe030db0c4a7c": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": 142102.19,
        "salary_max": 179933.03,
        "title": "Principal Data Scientist - Customer Growth Marketing",
        "company": "Dell Technologies",
        "desc": "Principal Data Scientist \u2013 Customer Growth Marketing \n  Data Science is all about breaking new ground to enable businesses to answer their most urgent questions.Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand-new methodologies, tools, statistical methods, and models. What\u2019s more, we are in collaboration with leading academics, industry experts, and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data. \n  Join us to do the best work of your career and make a profound social impact as a  Principal Data Scientist  on our Customer  Growth Marketing Team  working remotely in  Brazil,  or  Panama . \n  What you\u2019ll achieve  As a Principal Data Scientist on a growing team, you will bring your industry expertise to build machine-learning models.You will collaborate closely with our Field Marketing and Sales stakeholders to solve critical and highly-visible business problems with machine learning. \n \n \n  You will: \n \n \n \n  You will work with other Data Scientists, Data Engineers, ML Engineers, and Business Analyststo lead the end-to-end ML lifecycle, from use-case identification through model productionization, business outcome measurement, and data storytelling \n  Lead the growth and maturation of our marketing capabilities with machine learning at its core \n  Engage with business stakeholders to support customer-centric design of solutions \n  Mentor junior data scientists \n \n \n \n \n  Take the first step toward your dream career \n \n \n  Every Dell Technologies team member brings something unique to the table. Here\u2019s what we are looking for with this role: \n \n \n  Essential Requirements \n \n \n \n  Expertise in machine learning, data science, statistics, or related, including a demonstrated portfolio of productionized models. \n  Expertise in statistical programming with proficiency in Python (including packages such as pandas, scikit-learn, TensorFlow, or PyTorch) and Jupyter Notebooks \n  Ability to quickly gain deep understanding of multiple domain data, business processes, business objectives, and build appropriate stakeholderrelationships. \n  Experience with data storytelling, including providing clear and concise analyses around model performance and business outcomes to nonexpert stakeholders in very good written and verbal English. \n \n \n \n  Desired Requirements \n \n \n \n  Bachelor\u2019s degree in Data Science, Machine Learning, Statistics, Economics, Physics, other quantitative fields, or equivalent professional experience; Master\u2019s degree preferred \n  Demonstrated experience of researching and applying new industry methodologies or techniques to solve business problems in unique ways \n \n \n \n \n  Who we are \n \n \n  We believe that each of us has the power to make an impact. That\u2019s why we put our team members at the center of everything we do. If you\u2019re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we\u2019re looking for you.     Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us. \n  Application close date: September 30th - 2023    Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment.",
        "cleaned_desc": " \n  Expertise in machine learning, data science, statistics, or related, including a demonstrated portfolio of productionized models. \n  Expertise in statistical programming with proficiency in Python (including packages such as pandas, scikit-learn, TensorFlow, or PyTorch) and Jupyter Notebooks \n  Ability to quickly gain deep understanding of multiple domain data, business processes, business objectives, and build appropriate stakeholderrelationships. \n  Experience with data storytelling, including providing clear and concise analyses around model performance and business outcomes to nonexpert stakeholders in very good written and verbal English. \n \n \n \n  Desired Requirements ",
        "techs": [
            "machine learning",
            "data science",
            "statistics",
            "python",
            "pandas",
            "scikit-learn",
            "tensorflow",
            "pytorch",
            "jupyter notebooks",
            "data storytelling"
        ],
        "cleaned_techs": [
            "data science",
            "statistics",
            "python",
            "pandas",
            "scikit-learn",
            "tensorflow",
            "pytorch",
            "jupyter notebooks",
            "data storytelling"
        ]
    },
    "450f2696a75feafd": {
        "terms": [
            "data science",
            "data engineer"
        ],
        "salary_min": 105062.484,
        "salary_max": 133032.52,
        "title": "Data Engineer",
        "company": "Analytica",
        "desc": "Analytica is seeking a remote  Data Engineer  to support one or more dynamic, long-term federal government enterprise big-data programs. The company works as a trusted advisor to U.S. federal government clients in health, civilian, and national security missions. The ideal candidate will be comfortable as a key member of a multi-disciplinary team working in an Agile environment to produce big data analytics solutions.    Analytica has been recognized by  Inc. Magazine  as one of the 250 fastest-growing US small businesses for three consecutive years. We work with U.S. government clients to build data-driven products and cultures that make an impact on our daily lives. Analytica offers competitive compensation with opportunities for bonuses, employer paid health care, unlimited training funds, and a 401k match.     Requirements  (may include, but not limited to) : \n \n  Collaborate with client stakeholders and technical employees to optimize data collection, storage, and usage to maximize the value of information within the organization. \n  Research, design, build, optimize and maintain efficient and reliable data systems, data pipelines, and models. \n  Align closely with operating user requirements on data science, architecture, governance, infrastructure, and security to apply standards and optimize production environments and practices. \n  Translate business needs into data architecture solutions, designing and implementing in production environments within supported data systems. \n  Implement data orchestration pipelines, data sourcing, cleansing, augmentation, and quality control processes within supported data systems. \n  Deploy applications to production in partnership with business units. \n  Develop, test, and integrate new data features and functionality as defined by the product owners and business teams \n  Work with a multi-disciplinary team of analysts, data engineers, data scientists, developers, and data consumers in a fast-paced Agile environment \n \n  Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Engineering, Science, or related field. \n  5+ years of data engineering and data warehousing experience \n  3+ years of experience building cloud-data pipeline solutions for data ingestion, data storage, real-time processing, and analytics \n  Experience working within Agile development environment, DevOps, and using version control platforms, e.g., GitHub) \n  Have domain knowledge of standard data methodologies (DMBOK, NIST, etc.).  \n Team player and the ability to work effectively within a group as well as self-motivated with minimal supervision \n  Excellent problem solving, collaboration, and communication skills \n  AWS verifiable certification is strongly preferred  \n US Citizen with the ability to secure a US Federal Clearance \n \n  About   Analytica : Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD, the company is an established 8(a) small business that has been recognized by  Inc. Magazine  each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The Software Engineering Institute (SEI) appraises the company at CMMI\u00ae Maturity Level 3 and is an ISO 9001:2008 certified provider. \n  As a federal contractor, Analytica is required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation.   \n   \n k1MLhVKFNs",
        "cleaned_desc": "  Translate business needs into data architecture solutions, designing and implementing in production environments within supported data systems. \n  Implement data orchestration pipelines, data sourcing, cleansing, augmentation, and quality control processes within supported data systems. \n  Deploy applications to production in partnership with business units. \n  Develop, test, and integrate new data features and functionality as defined by the product owners and business teams \n  Work with a multi-disciplinary team of analysts, data engineers, data scientists, developers, and data consumers in a fast-paced Agile environment    3+ years of experience building cloud-data pipeline solutions for data ingestion, data storage, real-time processing, and analytics \n  Experience working within Agile development environment, DevOps, and using version control platforms, e.g., GitHub) \n  Have domain knowledge of standard data methodologies (DMBOK, NIST, etc.).  \n Team player and the ability to work effectively within a group as well as self-motivated with minimal supervision \n  Excellent problem solving, collaboration, and communication skills ",
        "techs": [
            "translate business needs into data architecture solutions",
            "designing and implementing in production environments within supported data systems",
            "implement data orchestration pipelines",
            "data sourcing",
            "cleansing",
            "augmentation",
            "and quality control processes within supported data systems",
            "deploy applications to production in partnership with business units",
            "develop",
            "test",
            "and integrate new data features and functionality as defined by the product owners and business teams",
            "work with a multi-disciplinary team of analysts",
            "data engineers",
            "data scientists",
            "developers",
            "and data consumers in a fast-paced agile environment",
            "experience building cloud-data pipeline solutions for data ingestion",
            "data storage",
            "real-time processing",
            "and analytics",
            "experience working within agile development environment",
            "devops",
            "and using version control platforms (e.g. github)",
            "have domain knowledge of standard data methodologies (dmbok",
            "nist",
            "etc.)."
        ],
        "cleaned_techs": [
            "designing and implementing in production environments within supported data systems",
            "implement data orchestration pipelines",
            "data sourcing",
            "cleansing",
            "augmentation",
            "and quality control processes within supported data systems",
            "develop",
            "test",
            "and integrate new data features and functionality as defined by the product owners and business teams",
            "work with a multi-disciplinary team of analysts",
            "data engineers",
            "data scientists",
            "developers",
            "and data consumers in a fast-paced agile environment",
            "experience building cloud-data pipeline solutions for data ingestion",
            "data storage",
            "real-time processing",
            "and analytics",
            "experience working within agile development environment",
            "devops",
            "and using version control platforms (e.g. github)",
            "have domain knowledge of standard data methodologies (dmbok",
            "nist",
            "etc.)."
        ]
    },
    "364d6fdf732403c1": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "77 West Wacker Dr (35012), United States of America, Chicago, Illinois\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "77 West Wacker Dr (35012), United States of America, Chicago, Illinois\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java"
        ]
    },
    "a8098e2ffb266b0c": {
        "terms": [
            "data science"
        ],
        "salary_min": 83721.99,
        "salary_max": 106010.695,
        "title": "GIS Analyst",
        "company": "INCATech LLC",
        "desc": "GIS ANALYST \n \n  This GIS Analyst role will support the development and implementation of geospatial technologies and methodologies to address a wide range of natural resource applications, encompassing image classification techniques, data mining, feature extraction, accuracy assessment, and innovative geospatial analysis and modeling. Will support developing and analyzing remote sensing and geospatial data sets, supporting resource management projects by evaluating and integrating remote sensing technologies and associated\n  \n \n  methods to enhance US Forestry Service business functions.\n  \n \n \n  The worksite for this position is: Remote\n   \n \n RESPONSIBILITIES \n \n \n Develop maps, applications, and other data visualizations using a variety of different software to include, but not limited to, Esri ArcGIS platform services (web/geoprocessing/location), APIs (REST), and applications (ArcGIS Experience Builder, StoryMaps, Operations Dashboard, Insights, etc) \n Document functionalities, processes, and best practices to support the program and wider geospatial stakeholder community to socialize the value of geospatial technology in supporting operational and strategic objectives/workflows \n Build, maintain, and interact with geospatial databases or data stores; support the development of GIS dataset/service/database designs \n Support tasks related to geospatial technologies to address natural resource applications including change detection, monitoring, extraction and classification of information from imagery using image classification techniques, data mining and feature extraction technologies, accuracy assessment procedures, and identifying and applying novel approaches to geospatial analysis and modeling. \n Develop, manage, and analyze remote sensing and geospatial data sets, including unique and disparate data and information, to support resource management projects. \n Build geospatial databases or data stores from imagery and cartographic/digital sources; support the development of GIS database designs \n Review, interpret, and utilize established production processes \n Perform map and web map production tasks \n Coordinate workflow with project/technical managers \n \n \n REQUIREMENTS: \n \n \n Qualifications \n \n Related Experience: \n \n \n Demonstrated proficiency with spatial data science concepts and spatial analysis tools and methodologies \n 2+ years\u2019 experience creating products using ArcGIS Pro, ArcGIS Online (AGOL), and Portal configurable apps to create integrated solutions \n 2+ years\u2019 experience with both hosted and local geodatabase management to include editing, GIS data development and standardization, including performing quality control on geometry, attributes and metadata \n Demonstrated expert level knowledge, experience, and abilities to process, analyze, and model geospatial data in commercial and open-source image processing and GIS software such as ERDAS, Esri ArcGIS, Python, Random Forest, and eCognition. \n Demonstrated expertise in preparing technical reports, technology transfer and geospatial training materials for technology evaluation and development projects. \n Demonstrated experience performing data modeling on corporate and disparate natural resource data \n Certification or degree in non-related discipline along with related experience to geography, geographic information systems, remote sensing, or physical/biological/environment/natural resource discipline with substantive coursework in geospatial analysis and modeling, and three (3) years relevant work experience. OR Bachelor\u2019s degree related to geography, GIS, remote sensing, or physical/biological/environment/natural resource discipline with substantive coursework in geospatial analysis and modeling, and two (2) years of relevant work experience. \n \n \n \n About INCATech LLC \n \n \n   INCATech is an award-winning small business that has over 14 years of experience supporting the US Government. We provide both Professional Services as well as Transformational Software Solutions for our customers. Our deep technical expertise in Geospatial Information Systems, Enterprise Data Management, User-Centered Design, Cloud & Platform Services, and a wide range of Acquisition Support Services will provide your career the boost you have been looking for. Our culture is based on empowering our staff and customers with knowledge and tools to achieve mission success and career advancement.\n    \n \n Benefits \n  INCATech offers a competitive comprehensive benefits package. Our most valuable asset at INCATech is our people. Our benefits package is designed to help and safeguard our employees and their families. We provide a variety of life and family benefits to meet a variety of demands. Benefits include: Health, Dental, 401K, Vision, Paid Time Off, Life Insurance etc.\n    \n  INCATech is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.",
        "cleaned_desc": "   \n \n RESPONSIBILITIES \n \n \n Develop maps, applications, and other data visualizations using a variety of different software to include, but not limited to, Esri ArcGIS platform services (web/geoprocessing/location), APIs (REST), and applications (ArcGIS Experience Builder, StoryMaps, Operations Dashboard, Insights, etc) \n Document functionalities, processes, and best practices to support the program and wider geospatial stakeholder community to socialize the value of geospatial technology in supporting operational and strategic objectives/workflows \n Build, maintain, and interact with geospatial databases or data stores; support the development of GIS dataset/service/database designs \n Support tasks related to geospatial technologies to address natural resource applications including change detection, monitoring, extraction and classification of information from imagery using image classification techniques, data mining and feature extraction technologies, accuracy assessment procedures, and identifying and applying novel approaches to geospatial analysis and modeling. \n Develop, manage, and analyze remote sensing and geospatial data sets, including unique and disparate data and information, to support resource management projects.   \n Related Experience: \n \n \n Demonstrated proficiency with spatial data science concepts and spatial analysis tools and methodologies \n 2+ years\u2019 experience creating products using ArcGIS Pro, ArcGIS Online (AGOL), and Portal configurable apps to create integrated solutions \n 2+ years\u2019 experience with both hosted and local geodatabase management to include editing, GIS data development and standardization, including performing quality control on geometry, attributes and metadata \n Demonstrated expert level knowledge, experience, and abilities to process, analyze, and model geospatial data in commercial and open-source image processing and GIS software such as ERDAS, Esri ArcGIS, Python, Random Forest, and eCognition. \n Demonstrated expertise in preparing technical reports, technology transfer and geospatial training materials for technology evaluation and development projects. \n Demonstrated experience performing data modeling on corporate and disparate natural resource data ",
        "techs": [
            "esri arcgis platform services",
            "esri arcgis experience builder",
            "esri arcgis storymaps",
            "esri arcgis operations dashboard",
            "esri arcgis insights",
            "esri arcgis pro",
            "arcgis online (agol)",
            "portal configurable apps",
            "erdas",
            "python",
            "random forest",
            "ecognition"
        ],
        "cleaned_techs": [
            "esri arcgis platform services",
            "esri arcgis experience builder",
            "esri arcgis storymaps",
            "esri arcgis operations dashboard",
            "esri arcgis insights",
            "esri arcgis pro",
            "arcgis online (agol)",
            "portal configurable apps",
            "erdas",
            "python",
            "random forest",
            "ecognition"
        ]
    },
    "bf9b496ef3af0ace": {
        "terms": [
            "data science"
        ],
        "salary_min": 109600.0,
        "salary_max": 185300.0,
        "title": "Assistant Director, Data Science",
        "company": "Liberty Mutual",
        "desc": "Pay Philosophy \n  The typical starting salary range for this role is determined by a number of factors including skills, experience, education, certifications and location. The full salary range for this role reflects the competitive labor market value for all employees in these positions across the national market and provides an opportunity to progress as employees grow and develop within the role. Some roles at Liberty Mutual have a corresponding compensation plan which may include commission and/or bonus earnings at rates that vary based on multiple factors set forth in the compensation plan for the role. \n  Description  \n \n Collaborates with business partners to develop predictive analytic solutions that enable data-driven strategic decision-making \n  Utilizes data science techniques to manipulate large structured and unstructured data sets, identify patterns in raw data, and develop models to predict the likelihood of a future outcome and/or to optimize business solutions \n  Applies knowledge of sophisticated analytics techniques to manipulate large structured and unstructured data sets in order to generate insights to inform business decisions \n  Identifies and tests hypotheses, ensuring statistical significance, as part of building and developing predictive models for business application \n  Translates quantitative analyses and findings into accessible visuals for non-technical audiences, providing a clear view into interpreting the data \n  Enables the business to make clear trade-offs between and among choices, with a reasonable view into likely outcomes \n  Customizes analytic solutions to specific client needs \n  Responsible for smaller components of projects of moderate-to-high complexity \n  Regularly engages with the data science community and participates in cross-functional working groups \n  Telecommuting permitted up to 100% \n \n  Qualifications \n  The position requires a Master\u2019s degree or foreign equivalent, in Statistics or a related field plus four (4) years\u2019 experience in the job offered or a related occupation. Alternatively, employer will accept a Ph.D., or foreign equivalent, in Statistics or a related field plus two (2) years\u2019 experience in the job offered or a related occupation. Position also requires demonstrable experience with each of the following:  \n \n Ability to research and adapt statistical and machine learning methods for business related modeling use \n  Knowledge of predictive analytic techniques/toolset and statistical diagnostics of models \n  Experience building predictive models using techniques such as GLMs and machine learning models \n  Interrogation of machine learning models using contribution methods such as Shapley values \n  Object Oriented Programing in python \n  Package development for predictive modeling or statistics (python packages and/or R packages) \n  Knowledge of insurance pricing models \n  Telecommuting permitted up to 100% \n \n  To apply, please visit https://jobs.libertymutualgroup.com/, select \u201cSearch Jobs,\u201d enter job requisition #2023-61193 in the \u201cJob ID or Keywords\u201d field, and submit resume. Alternatively, you may apply by submitting a resume via e-mail to RecruitLM@LibertyMutual.com. Reference requisition number in subject of e-mail. \n  About Us \n  **This position may have in-office requirements depending on candidate location.** \n \n  At Liberty Mutual, our purpose is to help people embrace today and confidently pursue tomorrow. That\u2019s why we provide an environment focused on openness, inclusion, trust and respect. Here, you\u2019ll discover our expansive range of roles, and a workplace where we aim to help turn your passion into a rewarding profession. \n \n  Liberty Mutual has proudly been recognized as a \u201cGreat Place to Work\u201d by Great Place to Work\u00ae US for the past several years. We were also selected as one of the \u201c100 Best Places to Work in IT\u201d on IDG\u2019s Insider Pro and Computerworld\u2019s 2020 list. For many years running, we have been named by Forbes as one of America\u2019s Best Employers for Women and one of America\u2019s Best Employers for New Graduates\u2014as well as one of America\u2019s Best Employers for Diversity. To learn more about our commitment to diversity and inclusion please visit: https://jobs.libertymutualgroup.com/diversity-equity-inclusion/ \n \n  We value your hard work, integrity and commitment to make things better, and we put people first by offering you benefits that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits \n \n  Liberty Mutual is an equal opportunity employer. We will not tolerate discrimination on the basis of race, color, national origin, sex, sexual orientation, gender identity, religion, age, disability, veteran\u2019s status, pregnancy, genetic information or on any basis prohibited by federal, state or local law.",
        "cleaned_desc": " \n  Qualifications \n  The position requires a Master\u2019s degree or foreign equivalent, in Statistics or a related field plus four (4) years\u2019 experience in the job offered or a related occupation. Alternatively, employer will accept a Ph.D., or foreign equivalent, in Statistics or a related field plus two (2) years\u2019 experience in the job offered or a related occupation. Position also requires demonstrable experience with each of the following:  \n \n Ability to research and adapt statistical and machine learning methods for business related modeling use \n  Knowledge of predictive analytic techniques/toolset and statistical diagnostics of models \n  Experience building predictive models using techniques such as GLMs and machine learning models ",
        "techs": [
            "statistical and machine learning methods",
            "predictive analytic techniques/toolset",
            "statistical diagnostics of models",
            "glms",
            "machine learning models."
        ],
        "cleaned_techs": [
            "statistical and machine learning methods",
            "predictive analytic techniques/toolset",
            "statistical diagnostics of models",
            "glms",
            "machine learning models."
        ]
    },
    "a6dd9aa552a34e18": {
        "terms": [
            "data science",
            "data engineer"
        ],
        "salary_min": 103692.85,
        "salary_max": 131298.25,
        "title": "Data Engineer",
        "company": "Bus Patrol",
        "desc": "Overview: \n  \n  Location:  Remote (must be based out of US or Canada)\n  \n \n \n  Travel:  Less than 5%\n  \n \n \n  Manages Others:  No\n  \n \n \n  Education:  Bachelor\u2019s Degree in a technical field\n  \n \n \n  Experience:  3+ years of relevant experience in Data engineering, database engineering, business intelligence or business analytics\n  \n \n \n  THE OPPORTUNITY: \n \n \n   The Data Engineer at BusPatrol will help to build and maintain our data infrastructure to support reporting, analytics, and data science. The right candidate will have strong data architecture, ETL, and SQL skills, and a proven track record partnering with both the business and data team colleagues to construct a framework which delivers on all our needs. The candidate will also have strong operational skills to drive efficiency and speed, expertise in building repeatable data engineering processes, strong project management skills, and a vision for how to deliver data products.\n   Responsibilities: \n  \n Create and orchestrate data pipelines in state-of-the-art AWS environment (Redshift, EC2, EMR, S3, Lambda, etc. with AWS Glue) \n  Manage all aspects of the data and analytics system from ingestion to ETL to aggregate tables for analytics and reporting needs \n  Design and implement internal data pipeline jobs / process improvements using various modern techniques: automating manual processes, optimizing data delivery, re-designing infrastructure for scalability, etc. \n  Write scripts to schedule data ingestion and syncing. Evaluate, lead and form backend logic to create data marts from requirements for the purposes of self-serving stakeholders \n  Assist data architects to build the working framework to create metrics as code and optimize data search and retrieval \n  Troubleshoot data jobs / processes in production to fix data quality bugs or pipeline performance issues \n  Build and assemble large, complex data sets with multi-dimensional relationships that meet both functional and non-functional requirements from business stakeholders \n  Build integrations between systems \n  Qualifications: \n  \n Bachelor's degree or higher in a quantitative/technical field (e.g., Computer Science, Statistics, Engineering) or equivalent experience \n  3+ years of relevant experience in Data engineering, database engineering, business intelligence or business analytics \n  1+ years of SQL knowledge for various reporting and transformation needs (Redshift, MySQL, PostgreSQL, Snowflake, Databricks) \n  1+ years of experience in core languages such as Python (experience building classes preferred) \n  1+ years of experience with schema design and dimensional data modeling \n  Extensive experience with AWS (Redshift, EC2, EMR, S3, Lambda, etc. with AWS Glue) \n  An interest in understanding the business and its strategy, not just the data architecture, and how our work contributes to meeting business goals \n  Experience with Data Lake architectures, and with combining structured and unstructured data into unified representations. \n  Experience with API design and development of RESTful web services \n  Analytical mindset with the ability to structure and process qualitative data and draw insightful conclusions. \n  Data science and machine learning experience a plus \n  GIS experience a plus \n  Data visualization experience with tools like Tableau a plus \n  Experience with JIRA preferred \n  Must be an intellectually curious self-starter and motivated to continually learn.",
        "cleaned_desc": " \n \n  THE OPPORTUNITY: \n \n \n   The Data Engineer at BusPatrol will help to build and maintain our data infrastructure to support reporting, analytics, and data science. The right candidate will have strong data architecture, ETL, and SQL skills, and a proven track record partnering with both the business and data team colleagues to construct a framework which delivers on all our needs. The candidate will also have strong operational skills to drive efficiency and speed, expertise in building repeatable data engineering processes, strong project management skills, and a vision for how to deliver data products.\n   Responsibilities: \n  \n Create and orchestrate data pipelines in state-of-the-art AWS environment (Redshift, EC2, EMR, S3, Lambda, etc. with AWS Glue) \n  Manage all aspects of the data and analytics system from ingestion to ETL to aggregate tables for analytics and reporting needs    Design and implement internal data pipeline jobs / process improvements using various modern techniques: automating manual processes, optimizing data delivery, re-designing infrastructure for scalability, etc. \n  Write scripts to schedule data ingestion and syncing. Evaluate, lead and form backend logic to create data marts from requirements for the purposes of self-serving stakeholders \n  Assist data architects to build the working framework to create metrics as code and optimize data search and retrieval \n  Troubleshoot data jobs / processes in production to fix data quality bugs or pipeline performance issues \n  Build and assemble large, complex data sets with multi-dimensional relationships that meet both functional and non-functional requirements from business stakeholders \n  Build integrations between systems \n  Qualifications: \n  \n Bachelor's degree or higher in a quantitative/technical field (e.g., Computer Science, Statistics, Engineering) or equivalent experience \n  3+ years of relevant experience in Data engineering, database engineering, business intelligence or business analytics    1+ years of SQL knowledge for various reporting and transformation needs (Redshift, MySQL, PostgreSQL, Snowflake, Databricks) \n  1+ years of experience in core languages such as Python (experience building classes preferred) \n  1+ years of experience with schema design and dimensional data modeling \n  Extensive experience with AWS (Redshift, EC2, EMR, S3, Lambda, etc. with AWS Glue) \n  An interest in understanding the business and its strategy, not just the data architecture, and how our work contributes to meeting business goals \n  Experience with Data Lake architectures, and with combining structured and unstructured data into unified representations. \n  Experience with API design and development of RESTful web services \n  Analytical mindset with the ability to structure and process qualitative data and draw insightful conclusions. \n  Data science and machine learning experience a plus \n  GIS experience a plus ",
        "techs": [
            "redshift",
            "ec2",
            "emr",
            "s3",
            "lambda",
            "aws glue",
            "mysql",
            "postgresql",
            "snowflake",
            "databricks",
            "python",
            "aws"
        ],
        "cleaned_techs": [
            "redshift",
            "ec2",
            "emr",
            "s3",
            "lambda",
            "aws",
            "mysql",
            "postgresql",
            "snowflake",
            "databricks",
            "python"
        ]
    },
    "9ac6f4b8a9386c0d": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": 111900.91,
        "salary_max": 141691.48,
        "title": "Product Leader - Industrial and Manufacturing AI & Data Science Solutions",
        "company": "Xen.ai",
        "desc": "About the company \n Xen.AI, is an Artificial Intelligence (AI) research and development (R&D) organization with operations in USA and India. We help our customers to develop innovative and customized SaaS solutions using Artificial Intelligence, Machine Learning, Deep Learning, Data Science and Open Source technologies. Please visit https://xen.ai/ to know more about us. \n Compensation for this position \n \n For the 1st year - 100% revenue based compensation based on the client projects that you help to get and deliver. \n After 1st year, based on company's progress we can consider formal corporate positions with market standard salary and equity etc. \n \n About the position \n Xen.AI is seeking a highly skilled and experienced Product Leader and Data Scientist with the Industrial and manufacturing domain knowledge to join our team in the Industrial vertical. The successful candidate will have deep knowledge of the processes, pain points, and solution approaches in areas such as predictive maintenance and Computer Vision based visual inspection for manufacturing defect detection and quality control. \n The successful candidate will be responsible for end to end lifecycle of the identified solutions including product vision, product development, and implementation. \n The role will require working closely with data scientists, engineers, and business stakeholders to understand industry-specific requirements and ensure the successful delivery of solutions. \n Selected candidates would be able to work remotely from any location. This can be a full time or part-time role. For the 1st year compensation would be 100% revenue based on the client projects that you help to deliver. \n Qualifications \n \n Master's or PhD degree degree in Industrial Engineering or related fields \n At least 10 years of relevant work experience in industrial maintenance, engineering, or quality assurance \n Expert knowledge of predictive maintenance strategies, including condition-based monitoring, failure analysis, and reliability engineering \n Expert knowledge of inspection techniques, including visual inspection, \n Experience working with structured, unstructured, streaming and batch data, \n Strong knowledge of machine learning, deep learning techniques, \n Strong programming skills in Python and SQL \n Experience developing solutions on cloud platforms like Azure, AWS, or GCP, \n IoT streaming sensor data analytics, \n Computer Vision, Image and streaming video processing, \n Experience with data visualization tools like PowerBI is a plus, \n Strong experience with Linux, \n Experience with various hardware devices, \n Excellent communication skills and ability to work in a team environment, \n Ability to provide technical leadership and guidance to junior team members. \n \n Responsibilities \n \n Develop product strategy and product vision for identified solutions, \n Develop release strategy and product backlog, \n Work closely with data scientists to identify data sources, \n Provide expert guidance and support during the development, test, and implementation phases, \n Contribute to development of content for marketing and sales activities, \n Stay up to date with the latest developments in industrial engineering, predictive maintenance, and quality inspection \n \n Compensation for this position \n \n For the 1st year - 100% revenue based compensation based on the client projects that you help to get and deliver. \n After 1st year, based on company's progress we can consider formal corporate positions with market standard salary and equity etc. \n \n If you have a passion for solving complex industrial problems using data-driven solutions, this is the role for you. Join our team and make a difference in the world of predictive maintenance and visual inspection. \n Please visit https://xen.ai/ to know more about us. \n !!!! Please note that we do not work with any recruiters !!!! \n Job Types: Contract, Part-time \n Pay: $1.00 per year \n Experience level: \n \n 10 years \n \n Schedule: \n \n Choose your own hours \n \n Experience: \n \n Product Management in Industrial and Manufacturing: 5 years (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": " Qualifications \n \n Master's or PhD degree degree in Industrial Engineering or related fields \n At least 10 years of relevant work experience in industrial maintenance, engineering, or quality assurance \n Expert knowledge of predictive maintenance strategies, including condition-based monitoring, failure analysis, and reliability engineering \n Expert knowledge of inspection techniques, including visual inspection, \n Experience working with structured, unstructured, streaming and batch data, \n Strong knowledge of machine learning, deep learning techniques, \n Strong programming skills in Python and SQL \n Experience developing solutions on cloud platforms like Azure, AWS, or GCP, \n IoT streaming sensor data analytics, \n Computer Vision, Image and streaming video processing,   Experience with data visualization tools like PowerBI is a plus, \n Strong experience with Linux, \n Experience with various hardware devices, \n Excellent communication skills and ability to work in a team environment, \n Ability to provide technical leadership and guidance to junior team members. \n \n Responsibilities \n \n Develop product strategy and product vision for identified solutions, \n Develop release strategy and product backlog, \n Work closely with data scientists to identify data sources, \n Provide expert guidance and support during the development, test, and implementation phases, ",
        "techs": [
            "condition-based monitoring",
            "failure analysis",
            "reliability engineering",
            "visual inspection",
            "structured data",
            "unstructured data",
            "streaming data",
            "batch data",
            "machine learning",
            "deep learning",
            "python",
            "sql",
            "azure",
            "aws",
            "gcp",
            "iot streaming sensor data analytics",
            "computer vision",
            "image processing",
            "streaming video processing",
            "powerbi",
            "linux",
            "hardware devices",
            "data visualization",
            "communication skills"
        ],
        "cleaned_techs": [
            "condition-based monitoring",
            "failure analysis",
            "reliability engineering",
            "visual inspection",
            "structured data",
            "unstructured data",
            "streaming data",
            "batch data",
            "python",
            "sql",
            "azure",
            "aws",
            "gcp",
            "iot streaming sensor data analytics",
            "computer vision",
            "image processing",
            "streaming video processing",
            "powerbi",
            "linux",
            "hardware devices",
            "data visualization"
        ]
    },
    "63df6b3471f3495e": {
        "terms": [
            "data science",
            "data analyst"
        ],
        "salary_min": 32.71,
        "salary_max": 48.51,
        "title": "Data Analysis and Reporting - Agency Policy Specialist",
        "company": "State of Minnesota",
        "desc": "When  all  employees are embraced, respected and heard, we will build a collaborative, equitable, inclusive and anti-racist culture where everyone thrives. \n \n \n \n \n \n \n  Job Details \n \n \n \n \n \n Working Title: Data Analysis and Reporting   Job Class: Agency Policy Specialist   Agency: Human Services Dept \n \n Who May Apply : Open to all qualified job seekers \n Date Posted : 10/17/2023 \n Closing Date : 10/23/2023 \n Hiring Agency/Seniority Unit : Human Services Dept / MAPE DHS Central Office \n Division/Unit : Children and Family Services / Data Analysis and Statistics \n Work Shift : Day Shift \n Work Hours : 8:00am - 4:30pm \n Days of Work : Monday - Friday \n Travel Required : No \n Salary Range:  $32.71 - $48.51 / hourly; $68,298 - $101,288 / annually \n Classified Status : Classified \n Bargaining Unit/Union : 214 - MN Assoc of Professional Empl/MAPE \n FLSA Status : Exempt - Administrative \n Telework Eligible : Yes \n Designated in Connect 700 Program for Applicants with Disabilities: Yes \n \n  Make a difference in the lives of Minnesotans. \n  The work you\u2019ll do is more than just a job. Join the talented, engaged and inclusive workforce dedicated to creating a better Minnesota. \n \n \n \n \n \n \n  Job Summary \n \n \n \n \n \n ***   Telework   (Within Minnesota or Neighboring States) and flexible hours options are available .*** \n \n  The Data Analytics and Data Management Agency Policy Specialist will lead complex data management, extraction, and analytic projects to help decision makers and program administrators understand policy and operational components of Minnesota\u2019s family cash and food assistance programs. This position will have a primary role in the development of data models/data marts to support reporting and performance measurement related to the Minnesota Family Investment Program (MFIP), Diversionary Work Program (DWP) and Supplemental Nutrition Assistance Program (SNAP), respond to ongoing and ad hoc requests for information and lead a continuous improvement effort around metadata development. \n  Importantly, this position serves a consultant role, translating complicated policy questions, performance measures, reporting requirements, legislative proposals, media inquiries and/or operational challenges, into data marts/models and/or data warehouse queries, or devising other data collection methods. A team of agency policy specialists and research scientists in the unit share responsibility for data modeling, data extraction, data analysis and sampling, federal and state reporting, research, evaluation, and measurement work. \n   \n Responsibilities include but may not be limited to: \n \n Develop and maintain data marts/models to support timely ongoing and ad hoc reporting and serve as the foundation for program performance measures. \n Maintain a high level of expertise in data extraction, preparation, and presentation including data visualization to respond to request for information and reporting from within the Department, the legislature, media, and other stakeholders. \n Maintain expert level knowledge of Structured Query Language (SQL) using the Department\u2019s available data management, extraction and analytic tools including Teradata SQL Assistant /Studio Express, SAS, R Statistical software to extract data from complex relational databases, analyze data, synthesize, and present results.  \n Utilize data visualization tools currently available to the department such as Tableau, Microsoft Excel and Infogram, to present results in a variety of formats. \n Communicate results effectively to a variety of audiences. Interpret findings to support policy and action recommendations. Meet with requestors and policy makers to think through their questions and interpret results.  \n Lead the development of data documentation of the MAXIS data system, including supporting the production of the first complete metadata on this source system. Serve as one of the Department experts in MAXIS views relational data model and other data systems available to Division analysts. \n \n \n \n \n \n \n \n Qualifications \n \n \n \n Minimum Qualifications \n  To facilitate proper crediting, please ensure that your resume clearly describes your experience in the areas listed and indicates the beginning and ending month and year for each job held. \n  Three (3) years of professional data management and analysis experience that demonstrates competency in: \n \n Writing complex queries using SQL, SAS, R (or equivalent) to extract data from relational databases. \n Utilizing relational database structure and/or data warehouse environments. \n Creating dashboards, reports, and other methods of synthesizing analytic results. \n Producing metadata products or processes. \n \n \n A Bachelor\u2019s degree in social services, data science or related field may substitute for 1 year of professional experience. \n \n  OR \n \n A Master\u2019s degree in social services, data science or related field may substitute for 2 years of professional experience. \n \n  Preferred Qualifications \n \n A graduate degree in data science, social science, or related field. \n Demonstrated experience developing data models / data marts. \n Demonstrated data visualization expertise. \n Demonstrated expertise in quantitative analysis, performance measurement, sampling etc. \n Knowledge of state and federal assistance programs and resulting data systems especially MAXIS. \n Working effectively with technical and non-technical staff. \n \n  Additional Requirements \n  REFERENCE/BACKGROUND CHECKS \u2013 DHS will conduct reference checks to verify job-related credentials and a criminal background check prior to appointment. \n  EDUCATION VERIFICATION - Applicants will be required to provide a copy of their college transcript or college degree/diploma at time of interview. Copies of the college degree/diploma are acceptable ONLY if it clearly identifies the field in which it was earned. \n \n \n \n \n \n \n  Application Details \n \n \n \n How to Apply   \n Select \u201cApply for Job\u201d at the top of this page. If you have questions about applying for jobs, contact the job information line at 651-259-3637 or email careers@state.mn.us. For additional information about the application process, go to http://www.mn.gov/careers.  \n If you have questions about the position, contact Kristen Boelcke-Stennes at kristen.boelcke-stennes@state.mn.us. \n  To receive consideration as a Connect 700 Program applicant, apply online, email the Job ID#, the Working Title and your valid Proof of Eligibility Certificate by the closing date to Alexander Duren at alexander.duren@state.mn.us.  \n If you are an individual with a disability and need an ADA accommodation for an interview, you may contact the Department of Human Services\u2019 ADA Coordinator at 651-431-4945 or DHS_ADA@state.mn.us for assistance. \n  About Human Services Dept   \n WE MAKE A DIFFERENCE! The Minnesota Department of Human Services impacts the lives of 1.7 million people every year, helping them meet their basic needs so they can achieve their highest potential.  \n Why Work for Us   \n Diverse Workforce   \n We are committed to continually developing a workforce that reflects the diversity of our state and the populations we serve. The varied experiences and perspectives of employees strengthen the work we do together and our ability to best serve the people of Minnesota.  \n A recent engagement survey of State of Minnesota employees found:   \n \n 95% of employees understand how their work helps achieve their agency\u2019s mission  \n 91% of employees feel trusted to do their jobs  \n 88% of employees feel equipped to look at situations from other cultural perspectives when doing their job  \n 87% of employees report flexibility in their work schedule  \n \n Comprehensive Benefits   \n Our benefits aim to balance four key elements that make life and work meaningful: health and wellness, financial well-being, professional development, and work/life harmony. As an employee, your benefits may include:  \n \n Public pension plan  \n Training and professional development  \n Paid vacation and sick leave  \n 12 paid holidays each year  \n Paid parental leave  \n Low-cost medical and dental coverage  \n Prescription drug coverage  \n Vision coverage  \n Wellness programs and resources  \n Employer paid life insurance  \n Short-term and long-term disability  \n Health care spending and savings accounts  \n Dependent care spending account  \n Tax-deferred compensation  \n Employee Assistance Program (EAP)  \n Tuition reimbursement  \n Federal Public Service Student Loan Forgiveness Program  \n \n Programs, resources and benefits eligibility varies  based on type of employment, agency, funding availability, union/collective bargaining agreement, location, and length of service with the State of Minnesota.  \n \n AN EQUAL OPPORTUNITY EMPLOYER   \n Minnesota state agencies are equal opportunity, affirmative action, and veteran-friendly employers. The State of Minnesota recognizes that a diverse workforce is essential and strongly encourages qualified women, minorities, individuals with disabilities, and veterans to apply.  \n We will make reasonable accommodations to all qualified applicants with disabilities. If you are an individual with a disability who needs assistance or cannot access the online job application system, please contact the job information line at 651-259-3637 or email careers@state.mn.us and indicate what assistance is needed.",
        "cleaned_desc": " Designated in Connect 700 Program for Applicants with Disabilities: Yes \n \n  Make a difference in the lives of Minnesotans. \n  The work you\u2019ll do is more than just a job. Join the talented, engaged and inclusive workforce dedicated to creating a better Minnesota. \n \n \n \n \n \n \n  Job Summary \n \n \n \n \n \n ***   Telework   (Within Minnesota or Neighboring States) and flexible hours options are available .*** \n \n  The Data Analytics and Data Management Agency Policy Specialist will lead complex data management, extraction, and analytic projects to help decision makers and program administrators understand policy and operational components of Minnesota\u2019s family cash and food assistance programs. This position will have a primary role in the development of data models/data marts to support reporting and performance measurement related to the Minnesota Family Investment Program (MFIP), Diversionary Work Program (DWP) and Supplemental Nutrition Assistance Program (SNAP), respond to ongoing and ad hoc requests for information and lead a continuous improvement effort around metadata development. \n  Importantly, this position serves a consultant role, translating complicated policy questions, performance measures, reporting requirements, legislative proposals, media inquiries and/or operational challenges, into data marts/models and/or data warehouse queries, or devising other data collection methods. A team of agency policy specialists and research scientists in the unit share responsibility for data modeling, data extraction, data analysis and sampling, federal and state reporting, research, evaluation, and measurement work. \n   \n Responsibilities include but may not be limited to: \n \n Develop and maintain data marts/models to support timely ongoing and ad hoc reporting and serve as the foundation for program performance measures. \n Maintain a high level of expertise in data extraction, preparation, and presentation including data visualization to respond to request for information and reporting from within the Department, the legislature, media, and other stakeholders. \n Maintain expert level knowledge of Structured Query Language (SQL) using the Department\u2019s available data management, extraction and analytic tools including Teradata SQL Assistant /Studio Express, SAS, R Statistical software to extract data from complex relational databases, analyze data, synthesize, and present results.  \n Utilize data visualization tools currently available to the department such as Tableau, Microsoft Excel and Infogram, to present results in a variety of formats. \n Communicate results effectively to a variety of audiences. Interpret findings to support policy and action recommendations. Meet with requestors and policy makers to think through their questions and interpret results.  \n Lead the development of data documentation of the MAXIS data system, including supporting the production of the first complete metadata on this source system. Serve as one of the Department experts in MAXIS views relational data model and other data systems available to Division analysts.   \n \n \n \n \n \n \n Qualifications \n \n \n \n Minimum Qualifications \n  To facilitate proper crediting, please ensure that your resume clearly describes your experience in the areas listed and indicates the beginning and ending month and year for each job held. \n  Three (3) years of professional data management and analysis experience that demonstrates competency in: \n \n Writing complex queries using SQL, SAS, R (or equivalent) to extract data from relational databases. \n Utilizing relational database structure and/or data warehouse environments. \n Creating dashboards, reports, and other methods of synthesizing analytic results. \n Producing metadata products or processes. \n \n \n A Bachelor\u2019s degree in social services, data science or related field may substitute for 1 year of professional experience. \n \n  OR \n \n A Master\u2019s degree in social services, data science or related field may substitute for 2 years of professional experience. \n \n  Preferred Qualifications \n ",
        "techs": [
            "teradata sql assistant/studio express",
            "sas",
            "r statistical software",
            "tableau",
            "microsoft excel",
            "infogram"
        ],
        "cleaned_techs": [
            "teradata sql assistant/studio express",
            "sas",
            "r statistical software",
            "tableau",
            "excel",
            "infogram"
        ]
    },
    "7e1adb1c477e3a9e": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "201 Third Street (61049), United States of America, San Francisco, California\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "201 Third Street (61049), United States of America, San Francisco, California\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java"
        ]
    },
    "fca57103c0e40de1": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java"
        ]
    },
    "814ba5de32777d5d": {
        "terms": [
            "data science",
            "data engineer",
            "machine learning engineer"
        ],
        "salary_min": 73100.0,
        "salary_max": 166000.0,
        "title": "Data Engineer",
        "company": "Booz Allen Hamilton",
        "desc": "Job Description \n \n \n \n \n \n \n \n \n \n \n         Location: \n         \n \n         Washington,DC,US \n         \n \n \n \n         Remote Work: \n         \n \n         Hybrid \n         \n \n \n \n         Job Number: \n         \n \n         R0181926\n         \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n         Data Engineer\n           The Opportunity: \n  Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there\u2019s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it\u2019s gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their big data to impact important missions\u2014from fraud detection to cancer research to national intelligence. \n \n  As a big data engineer at Booz Allen, you\u2019ll implement data engineering activities on some of the most mission-driven projects in the industry. You\u2019ll deploy and develop pipelines and platforms that organize and make disparate data meaningful. Here, you\u2019ll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You\u2019ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients. Work with us to use big data for good. \n \n  Join us. The world can\u2019t wait. \n \n  You Have: \n \n  5+ years of experience in application development \n  5+ years of experience designing, developing, operationalizing and maintaining complex data applications at enterprise scale \n  3+ years of experience creating software for retrieving, parsing and processing structured and unstructured data \n  3+ years of experience building scalable ETL/ELT workflows for reporting and analytics \n  Experience creating solutions within a collaborative, cross-functional team environment \n  Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms \n  Ability to obtain and maintain a Public Trust or Suitability/Fitness determination based on client requirements \n  Bachelor\u2019s degree \n \n \n  Nice If You Have: \n \n  Experience with Python, SQL, Scala, or Java \n  Experience with Unix/Linux, including basic commands and Shell scripting \n  Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud \n  Experience with distributed data, computing tools including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka \n  Experience working on real-time data and streaming applications \n  Experience with NoSQL implementation, including MongoDB or Cassandra \n  Experience with data warehousing using AWS Redshift, MySQL, or Snowflake \n  Experience with Agile engineering practices \n  Experience with HR system data \n \n \n  Vetting: \n  Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client. \n \n  Create Your Career: \n  Grow With Us \n  Your growth matters to us\u2014that\u2019s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. \n \n  A Place Where You Belong \n  Diverse perspectives cultivate collective ingenuity. Booz Allen\u2019s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you\u2019ll build your community in no time. \n \n  Support Your Well-Being \n  Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we\u2019ll support you as you pursue a balanced, fulfilling life\u2014at work and at home. \n \n  Your Candidate Journey \n  At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we\u2019ve compiled a list of resources so you\u2019ll know what to expect as we forge a connection with you during your journey as a candidate with us. \n \n  Compensation \n  At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen\u2019s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. \n  Salary at Booz Allen is determined by various factors, including but not limited to location, the individual\u2019s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $73,100.00 to $166,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen\u2019s total compensation package for employees.\n          \n  Work Model  Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. \n \n  If this position is listed as remote or hybrid, you\u2019ll periodically work from a Booz Allen or client site facility. \n  If this position is listed as onsite, you\u2019ll work with colleagues and clients in person, as needed for the specific role. \n \n \n  EEO Commitment \n  We\u2019re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change \u2013 no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.",
        "cleaned_desc": " \n \n \n \n \n \n         Data Engineer\n           The Opportunity: \n  Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there\u2019s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it\u2019s gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their big data to impact important missions\u2014from fraud detection to cancer research to national intelligence. \n \n  As a big data engineer at Booz Allen, you\u2019ll implement data engineering activities on some of the most mission-driven projects in the industry. You\u2019ll deploy and develop pipelines and platforms that organize and make disparate data meaningful. Here, you\u2019ll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You\u2019ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients. Work with us to use big data for good. \n \n  Join us. The world can\u2019t wait. \n \n  You Have: \n \n  5+ years of experience in application development \n  5+ years of experience designing, developing, operationalizing and maintaining complex data applications at enterprise scale \n  3+ years of experience creating software for retrieving, parsing and processing structured and unstructured data \n  3+ years of experience building scalable ETL/ELT workflows for reporting and analytics \n  Experience creating solutions within a collaborative, cross-functional team environment \n  Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms    Ability to obtain and maintain a Public Trust or Suitability/Fitness determination based on client requirements \n  Bachelor\u2019s degree \n \n \n  Nice If You Have: \n \n  Experience with Python, SQL, Scala, or Java \n  Experience with Unix/Linux, including basic commands and Shell scripting \n  Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud \n  Experience with distributed data, computing tools including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka \n  Experience working on real-time data and streaming applications \n  Experience with NoSQL implementation, including MongoDB or Cassandra \n  Experience with data warehousing using AWS Redshift, MySQL, or Snowflake \n  Experience with Agile engineering practices \n  Experience with HR system data \n \n \n  Vetting: \n  Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client. \n \n  Create Your Career: \n  Grow With Us ",
        "techs": [
            "iot",
            "machine learning",
            "artificial intelligence",
            "big data",
            "pipelines",
            "platforms",
            "analytical exploration",
            "data examination",
            "etl/elt workflows",
            "python",
            "sql",
            "scala",
            "java",
            "unix/linux",
            "aws",
            "microsoft azure",
            "google cloud",
            "spark",
            "databricks",
            "hadoop",
            "hive",
            "aws emr",
            "kafka",
            "nosql",
            "mongodb",
            "cassandra",
            "aws redshift",
            "mysql",
            "snowflake",
            "agile engineering",
            "hr system data."
        ],
        "cleaned_techs": [
            "iot",
            "ai",
            "big data",
            "pipelines",
            "platforms",
            "data examination",
            "etl/elt workflows",
            "python",
            "sql",
            "scala",
            "java",
            "unix/linux",
            "aws",
            "microsoft azure",
            "gcp",
            "spark",
            "databricks",
            "hadoop",
            "hive",
            "kafka",
            "nosql",
            "mongodb",
            "cassandra",
            "mysql",
            "snowflake",
            "agile engineering",
            "hr system data."
        ]
    },
    "81d5ad4b45165e4e": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)",
        "company": "Capital One",
        "desc": "314 Main Street (21020), United States of America, Cambridge, Massachusetts\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing \n  At least 4 years of experience with HPCs, vector embedding, or semantic search technologies \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing systems for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "314 Main Street (21020), United States of America, Cambridge, Massachusetts\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n    Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ],
        "cleaned_techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ]
    },
    "492b23ec9fa4ac36": {
        "terms": [
            "data science"
        ],
        "salary_min": 49800.0,
        "salary_max": 102000.0,
        "title": "Frontend Web Developer, Junior",
        "company": "Booz Allen Hamilton",
        "desc": "Job Description \n \n \n \n \n \n \n \n \n \n \n         Location: \n         \n \n         Sterling,VA,US \n         \n \n \n \n         Remote Work: \n         \n \n         Hybrid \n         \n \n \n \n         Job Number: \n         \n \n         R0182325\n         \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n         Frontend Web Developer, Junior\n           The Opportunity: \n  The right interface can make a tool, system, or site easy to use, encourage early adoption, and save time and resources. We\u2019re looking for you, a web developer who will use equal parts skill and vision to create an experience that delivers functionality and efficiency. \n \n  Bring your passion for creating an amazing user experience to Booz Allen. This is an opportunity to contribute to a project that meets challenges in the mission area and customer industry by collaborating with the development team to build a type of system, tool, or web applications with user-centric design. You\u2019ll work with a team of UI/UX designers and developers to create a seamless user experience using React, Angular, or Ember. \n \n  On our team, you\u2019ll hone your skills and learn how to ensure accessibility for all users by developing a front end that functions across browsers, platforms, and devices while meeting accessibility and security requirements. With mentoring, positive code reviews, and opportunities to learn new tools and skills, we focus on growing as a team to make the best solutions for our customers. \n \n  Work with us as we shape systems to change the mission area and customer industry for the better. \n \n  Join us. The world can\u2019t wait. \n \n  You Have: \n \n  2+ years of experience in working with Web development or design, including HTML or CSS \n  Experience with using Angular, React, or Vue and their ecosystems, including Gulp, Angular CLI, integrating NPM libraries and modularizing code for reuse across multiple projects \n  Experience with responsive design libraries, including Bootstrap and Material Design \n  Experience in working with configuration management and source code control, including Git or SVN \n  TS/SCI clearance \n  Bachelor's degre \n \n \n  Nice If You Have: \n \n  2+ years of experience in working with Web or data analysis languages, including Java or Python \n  Experience with JS visualization libraries, including Cytoscape,D3, Ogma, or AmCharts \n  Experience with CI/CD pipelines., including Jenkins, Kubernetes and Docker \n  Experience in working with RESTful Web services \n  Experience with advanced data structures and algorithms that apply to them, including connected graph analysis \n  Master's degree in Data Science or Operations Research \n \n \n  Clearance: \n  Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; TS/SCI clearance is required. \n \n  Create Your Career: \n \n  Grow With Us \n  Your growth matters to us\u2014that\u2019s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. \n \n  A Place Where You Belong \n  Diverse perspectives cultivate collective ingenuity. Booz Allen\u2019s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you\u2019ll build your community in no time. \n \n  Support Your Well-Being \n  Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we\u2019ll support you as you pursue a balanced, fulfilling life\u2014at work and at home. \n \n  Your Candidate Journey \n  At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we\u2019ve compiled a list of resources so you\u2019ll know what to expect as we forge a connection with you during your journey as a candidate with us. \n \n  Compensation \n  At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen\u2019s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. \n  Salary at Booz Allen is determined by various factors, including but not limited to location, the individual\u2019s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $49,800.00 to $102,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen\u2019s total compensation package for employees.\n          \n  Work Model  Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. \n \n  If this position is listed as remote or hybrid, you\u2019ll periodically work from a Booz Allen or client site facility. \n  If this position is listed as onsite, you\u2019ll work with colleagues and clients in person, as needed for the specific role. \n \n \n  EEO Commitment \n  We\u2019re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change \u2013 no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.",
        "cleaned_desc": "  Experience with responsive design libraries, including Bootstrap and Material Design \n  Experience in working with configuration management and source code control, including Git or SVN \n  TS/SCI clearance \n  Bachelor's degre \n \n \n  Nice If You Have: \n \n  2+ years of experience in working with Web or data analysis languages, including Java or Python \n  Experience with JS visualization libraries, including Cytoscape,D3, Ogma, or AmCharts \n  Experience with CI/CD pipelines., including Jenkins, Kubernetes and Docker \n  Experience in working with RESTful Web services \n  Experience with advanced data structures and algorithms that apply to them, including connected graph analysis \n  Master's degree in Data Science or Operations Research \n \n \n  Clearance: \n  Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; TS/SCI clearance is required. \n \n  Create Your Career: \n \n  Grow With Us ",
        "techs": [
            "bootstrap",
            "material design",
            "git",
            "svn",
            "java",
            "python",
            "cytoscape",
            "d3",
            "ogma",
            "amcharts",
            "jenkins",
            "kubernetes",
            "docker",
            "restful web services"
        ],
        "cleaned_techs": [
            "bootstrap",
            "material design",
            "git",
            "svn",
            "java",
            "python",
            "cytoscape",
            "d3",
            "ogma",
            "amcharts",
            "jenkins",
            "kubernetes",
            "docker",
            "restful web services"
        ]
    },
    "51fbd7a639c21316": {
        "terms": [
            "data science"
        ],
        "salary_min": 122580.95,
        "salary_max": 155214.8,
        "title": "Senior Data Scientist",
        "company": "Analytica",
        "desc": "Analytica is seeking a remote  Senior Data Scientist  to support long term federal client engagement across health or federal financial regulatory environments. The position will apply statistical programming, modeling, visualization techniques, data mining, and forecasting skills to analyze public sector use cases. The ideal candidate will enjoy engaging with stakeholders, learning their business problems, providing technical guidance, and evangelizing the team\u2019s data science capabilities.    Analytica has been recognized by Inc. Magazine as the fastest-growing private US small business. We work with U.S. government customers in health, civilian, and national security missions. As a core member you\u2019ll work with a diverse team of professionals to solution matters, architect nuisances, and come up with alternatives. We offer competitive compensation with opportunities for bonuses, employer paid health care, training and development funds, and 401k match.     Requirements: \n \n  Feature Engineering and Attribute Evaluation - Candidate must demonstrate experience with identifying the key determinants for modeling that exist in the business process and within existing data sets, as well as selecting evaluation protocols (model techniques). \n  Pre-processing - Demonstrate the skills and experience to collect, clean, and prepare data sets for input into a computational model using technologies such as Python or R. \n  Modeling - Candidates will have practiced skills and experience selecting and applying modeling techniques to fit the business problem. Ideal candidates should be able to apply machine learning (ML) supervised and unsupervised learning, regression, neural networks and deep learning, natural language processing, etc. \n  Validation - Candidates will have experience investigating, testing, reporting, and justifying model results. \n  Visualization \u2013 Candidates should be comfortable presenting the results of their modeling activities, depicting the insights realized, and explaining the relevance of their results to the organization\u2019s business challenges \n \n  Qualifications: \n \n  Bachelor's degree in Statistics, Mathematics, Physics, Computer Science, or another quantitative field; Master's degree or PhD a plus \n  Seven (7)+ years in data science, data engineering, as a statistician, or a related quantitative field \n  Strong problem-solving skills, leveraging domain knowledge, information technology, and data science techniques \n  High degree of experience utilizing Python or R to support the development of complex advanced analytics use cases \n  Experience implementing unsupervised (e.g., clustering), semi-supervised (e.g., label propagation) and supervised learning (e.g., ensemble classifiers) algorithms, and when it is appropriate to use each is a strong plus \n  Ability to providing complex data analysis, evaluation and communication to identify and analyze complex financial, securities, tax or health data as required \n  Ability to implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms \n  Experience querying data from relational databases using SQL \n  Experience with development of predictive analytics models \n  Experience working in an Agile project management environment \n  Must be a U.S. Citizen and be able to obtain and maintain a security clearance \n \n  About Analytica:  Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. The company is an award-winning SBA certified 8(a) small business that has been recognized by  Inc. Magazine  each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI\u00ae Maturity Level 3 and is an ISO 9001:2008 certified provider.    As a federal contractor, Analytica is required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation. \n   \n W621OyFLhC",
        "cleaned_desc": "Analytica is seeking a remote  Senior Data Scientist  to support long term federal client engagement across health or federal financial regulatory environments. The position will apply statistical programming, modeling, visualization techniques, data mining, and forecasting skills to analyze public sector use cases. The ideal candidate will enjoy engaging with stakeholders, learning their business problems, providing technical guidance, and evangelizing the team\u2019s data science capabilities.    Analytica has been recognized by Inc. Magazine as the fastest-growing private US small business. We work with U.S. government customers in health, civilian, and national security missions. As a core member you\u2019ll work with a diverse team of professionals to solution matters, architect nuisances, and come up with alternatives. We offer competitive compensation with opportunities for bonuses, employer paid health care, training and development funds, and 401k match.     Requirements: \n \n  Feature Engineering and Attribute Evaluation - Candidate must demonstrate experience with identifying the key determinants for modeling that exist in the business process and within existing data sets, as well as selecting evaluation protocols (model techniques). \n  Pre-processing - Demonstrate the skills and experience to collect, clean, and prepare data sets for input into a computational model using technologies such as Python or R.    Strong problem-solving skills, leveraging domain knowledge, information technology, and data science techniques \n  High degree of experience utilizing Python or R to support the development of complex advanced analytics use cases \n  Experience implementing unsupervised (e.g., clustering), semi-supervised (e.g., label propagation) and supervised learning (e.g., ensemble classifiers) algorithms, and when it is appropriate to use each is a strong plus \n  Ability to providing complex data analysis, evaluation and communication to identify and analyze complex financial, securities, tax or health data as required    Ability to implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms \n  Experience querying data from relational databases using SQL \n  Experience with development of predictive analytics models \n  Experience working in an Agile project management environment ",
        "techs": [
            "analytica",
            "python",
            "r",
            "sql"
        ],
        "cleaned_techs": [
            "analytica",
            "python",
            "r",
            "sql"
        ]
    },
    "4eb3da80458c4c7f": {
        "terms": [
            "data science",
            "data engineer"
        ],
        "salary_min": 100000.0,
        "salary_max": 200000.0,
        "title": "Forward Deployed Data Engineer - Remote",
        "company": "Reality Defender",
        "desc": "** No firms - we cannot work with firms due to regulatory reasons.** \n \n \n \n  Reality Defender seeks a forward deployed data engineer to join the Data Engineering team. You would work on product-oriented data infrastructure development for in-the-wild deepfake media detection, with an emphasis on engaging directly with clients and facilitating communication between technical and non-technical teams.\n  \n \n \n  #LI-Remote\n  \n Responsibilities \n \n  Building scalable robust infrastructure for data ingestion, storage, and sampling. \n  Communicate complex technical concepts effectively to client executives, ensuring alignment between technical implementations and organizational objectives. \n  Develop custom data tools tailored to meet specific client requirements, including adapting internally developed solutions to meet clients' needs. \n  Create and maintain comprehensive technical documentation, including APIs, algorithms, and system architecture. \n  Provide technical support to resolve complex issues escalated from customer support teams. Collaborate with cross-functional teams to diagnose and troubleshoot production incidents, and report results back to the customer in clear non-technical language. \n \n  Requirements \n \n  We encourage candidates who may not meet all the specified requirements to still apply. We value diverse perspectives and skills, and believe that unique experiences can contribute significantly to our team. If you are passionate about the role and confident in your ability to make a meaningful impact, we welcome your application. Your enthusiasm, adaptability, and potential for growth are equally important to us. Please use your cover letter to elaborate on how your background and experience make you an ideal fit for this role! \n \n \n \n  Required \n \n \n  3+ years of professional experience in software/data science and a bachelor's or master's degree in computer science, engineering, math, or STEM discipline. \n  Strong communication skills. \n  Proficiency in Python, NodeJs, Typescript, with a strong emphasis on adapting scalable software solutions to customer needs. \n  Database experience, particularly NoSQL databases (MongoDB, DynamoDB, etc). \n \n \n \n  Nice to have \n \n \n  Interest in data exploration, visualization, cleaning, and analytics for real-world data modeling. \n  Solid understanding of linear algebra, statistics and deep learning concepts. \n  Experience working with audio, visual, and/or text datasets and models. \n  Experience with AWS, Google Cloud, Azure, and On-Premises. \n  Experience working with very large databases and deep learning APIs, including Pandas, PyTorch, PySpark, etc.  \n Highly organized, detail-oriented, and possess a proven ability to thrive under deadline pressure. \n \n \n \n  Additional Requirements \n \n \n  Willing to work extended hours when needed. \n  Willing to occasionally work from or travel to client\u2019s location.",
        "cleaned_desc": "  Database experience, particularly NoSQL databases (MongoDB, DynamoDB, etc). \n \n \n \n  Nice to have \n \n \n  Interest in data exploration, visualization, cleaning, and analytics for real-world data modeling. \n  Solid understanding of linear algebra, statistics and deep learning concepts. \n  Experience working with audio, visual, and/or text datasets and models.    Experience with AWS, Google Cloud, Azure, and On-Premises. \n  Experience working with very large databases and deep learning APIs, including Pandas, PyTorch, PySpark, etc.  \n Highly organized, detail-oriented, and possess a proven ability to thrive under deadline pressure. \n \n \n \n  Additional Requirements \n \n \n  Willing to work extended hours when needed. ",
        "techs": [
            "nosql databases (mongodb",
            "dynamodb)",
            "aws",
            "google cloud",
            "azure",
            "on-premises",
            "pandas",
            "pytorch",
            "pyspark"
        ],
        "cleaned_techs": [
            "nosql",
            "dynamodb)",
            "aws",
            "gcp",
            "azure",
            "on-premises",
            "pandas",
            "pytorch",
            "pyspark"
        ]
    },
    "2a4e3e08f9699899": {
        "terms": [
            "data science"
        ],
        "salary_min": 70000.0,
        "salary_max": 75000.0,
        "title": "Marketing Analyst - NBS",
        "company": "Nelnet",
        "desc": "Nelnet Business Services (NBS), a division of Nelnet, Inc., provides payment technology, education services, and learning management solutions to education and faith-based organizations, serving more than 1,300 higher education institutions, 11,500 K-12 schools, 3,500 churches, and millions of individual students, families, and supporters across the globe. Our culture of service enables us to form long-lasting and trusted partnerships, while our focus on creativity and innovative solutions empowers our customer communities to thrive.\n  \n \n \n  As a Nelnet company, the perks at NBS go beyond our benefits package. You\u2019re part of a community, invested in you as an individual and united by our mission to create opportunities for people where they live, learn and work.\n   The Marketing Analyst\u2019s primary focus is to work with the marketing team and the data science team, along with various other departments within Nelnet to identify actionable marketing opportunities and measure marketing performance through sophisticated analytics, dashboards and reporting. This position supports a wide range of projects across Nelnet Business Solution brands to generate insights for the business which lead to effective multi-channel marketing and sales strategies. In addition, the Marketing Analyst is responsible for the development and distribution of quantitative and qualitative marketing analysis reports.\n  \n   JOB RESPONSIBILITIES:\n  \n \n   Work independently to provide analytical and statistical expertise to help inform the strategy for the marketing team. Identify data driven insights as they relate to channel and audience performance and provide suggestions for improvement. Develop actionable customer, brand and market insights that will be leveraged for strategic planning, program development and creative production. Develop and distribute quantitative and qualitative marketing analysis reports and dashboards in Salesforce CRM, Pardot, Google Data Studio, and other platforms. Ability to tell a story with data and communicate insights effectively. Continually analyze and improve our marketing programs (managing spend, response rates, MQL, SQL, ROI, LTV, and attribution). Produce forecasts of lead initiatives and track program results to help inform and implement efficient, effective strategies. Handle all technical phases of an analytical project, ultimately producing data and other deliverables for presentation to internal stakeholders. Assist in the development and execution of test design and planning of multi-channel marketing programs. Become the subject matter expert on marketing/sales analysis and statistical methodologies for the business.\n  \n \n   Wage for this role is: $70k-$75k\n  \n \n \n   EDUCATION:\n  \n \n   Bachelor\u2019s degree in Marketing, Business, Data Science, Statistics, Mathematics, or related field. MBA preferred.\n  \n \n \n   EXPERIENCE:\n  \n \n   2-5 years of marketing or business analysis. Experience in various statistical techniques and data mining methodologies. Experience with Salesforce, Pardot, and Google Analytics preferred. Strong communication experience with multiple teams. Prior project management experience.\n  \n \n \n   COMPETENCIES \u2013 SKILLS/KNOWLEDGE/ABILITIES:\n  \n \n   Strong proficiency in all Microsoft Office modules. Superior verbal and written communication skills, with the ability to communicate with individuals at all levels in the company in an articulate, professional manner. Effective time management skills with the ability to consistently meet deadlines. Ability to handle multiple projects at one time, prioritize, and identify what needs to be done and take accountability for those tasks. Strong analytics toolset skills: PowerBI, Snowflake, Google Analytics, SAS, or SQL. Effective ability to represent insights in PowerPoint, Excel, and other software packages to produce high quality presentations. Capable of independent problem solving. Attention to detail is a must.\n  \n \n \n   Nelnet is an Equal Opportunity Employer, complies with Executive Order 11246, and takes affirmative action to ensure that qualified applicants are employed, and that employees are treated during employment, without regard to race, color, religion/creed, national origin, gender, or sex, marital status, age, disability, use of a guide dog or service animal, sexual orientation, military/veteran status, or any other status protected by Federal or State law or local ordinance. Qualified individuals with disabilities who require reasonable accommodations in order to apply or compete for positions at Nelnet may request such accommodations by contacting Nelnet Talent Acquisition & Recruiting.\n  \n \n \n   Nelnet is a Drug Free and Tobacco Free Workplace.\n  \n \n \n   Our benefits package includes medical, dental, vision, HSA and FSA, generous earned time off, 401K/student loan repayment, life insurance & AD&D insurance, employee assistance program, employee stock purchase program, tuition reimbursement, performance-based incentive pay, short- and long-term disability, and a robust wellness program. Click here to learn more about our benefits: \n   \n   LINK\n   .\n  \n \n   Nelnet is an Equal Opportunity Employer, complies with Executive Order 11246, and takes affirmative action to ensure that qualified applicants are employed, and that employees are treated during employment, without regard to race, color, religion/creed, national origin, gender, or sex, marital status, age, disability, use of a guide dog or service animal, sexual orientation, military/veteran status, or any other status protected by Federal or State law or local ordinance.\n  \n \n \n   Qualified individuals with disabilities who require reasonable accommodations in order to apply or compete for positions at Nelnet may request such accommodations by contacting Corporate Recruiting at 402-486-5725 or \n   \n   corporaterecruiting@nelnet.net\n   .\n  \n \n \n   Nelnet is a Drug Free and Tobacco Free Workplace.",
        "cleaned_desc": "   EXPERIENCE:\n  \n \n   2-5 years of marketing or business analysis. Experience in various statistical techniques and data mining methodologies. Experience with Salesforce, Pardot, and Google Analytics preferred. Strong communication experience with multiple teams. Prior project management experience.\n  \n \n \n   COMPETENCIES \u2013 SKILLS/KNOWLEDGE/ABILITIES:\n  \n \n   Strong proficiency in all Microsoft Office modules. Superior verbal and written communication skills, with the ability to communicate with individuals at all levels in the company in an articulate, professional manner. Effective time management skills with the ability to consistently meet deadlines. Ability to handle multiple projects at one time, prioritize, and identify what needs to be done and take accountability for those tasks. Strong analytics toolset skills: PowerBI, Snowflake, Google Analytics, SAS, or SQL. Effective ability to represent insights in PowerPoint, Excel, and other software packages to produce high quality presentations. Capable of independent problem solving. Attention to detail is a must.\n  ",
        "techs": [
            "salesforce",
            "pardot",
            "google analytics",
            "microsoft office modules",
            "powerbi",
            "snowflake",
            "sas",
            "sql",
            "powerpoint",
            "excel"
        ],
        "cleaned_techs": [
            "salesforce",
            "pardot",
            "google analytics",
            "microsoft",
            "powerbi",
            "snowflake",
            "sas",
            "sql",
            "powerpoint",
            "excel"
        ]
    },
    "7c39cf927bfd7b93": {
        "terms": [
            "data science"
        ],
        "salary_min": 105004.82,
        "salary_max": 132959.5,
        "title": "Product Leader - Banking, Financial Services, Insurance, AI & Data Science",
        "company": "Xen.ai",
        "desc": "About the company \n Xen.AI, is an Artificial Intelligence (AI) research and development (R&D) organization with operations in USA and India. We help our customers to develop innovative and customized SaaS solutions using Artificial Intelligence, Machine Learning, Deep Learning, Data Science and Open Source technologies. Please visit https://xen.ai/ to know more about us. \n Compensation for this position \n \n For the 1st year - 100% revenue based compensation based on the client projects that you help to get and deliver. \n After 1st year, based on company's progress we can consider formal corporate positions with market standard salary and equity etc. \n \n About the position \n Xen.AI is looking for a Product Leader with experience in Banking, Financial Services, Insurance and AI, Machine Learning and Data Science. Selected candidates would be able to work remotely from any location. This can be a full time or part-time role. For the 1st year compensation would be 100% revenue based on the client projects that you help to deliver. \n Qualifications \n \n You bring deep knowledge of processes within the small to medium businesses in the banking and financial services \n Empathy for users and a drive to discover and resolve their pain points \n Experience conducting user research to educate product design \n Strong analytical skills, and experience with tools for doing data analysis \n Experience designing intuitive user interfaces and executing product plans with a well-defined process \n You bring deep empathy but can effectively prioritize with limited resources \n You have a growth mindset and want to be challenged \n You have a high bar and want to be a difference-maker in the organization \n Experience managing and integrating a diverse set of partners and processes into a cohesive product \n An owner's mindset - you don't shy away from the hard stuff \n Strong collaboration in team environments but able to act quickly as the decision-maker \n Experience at a startup in a cross-functional environment \n 5+ years experience as a product manager \n \n Responsibilities \n \n You will own the end-end product strategy and roadmap for development of user centric AI/ML products in banking and financial services sector serving small to medium businesses \n You will lead the ideation and launch of innovative features \n You will drive decision-making through customer insights, quantitative analysis, and AB testing \n Establish shared vision across the company by building consensus on priorities leading to product execution \n Embrace the testing process: form and validate data-driven hypotheses for accelerating member acquisition, retention and revenue \n Perform quantitative and qualitative research to identify product opportunities and prioritize your roadmap \n Ship minimum-loveable-products that allow us to test our hypotheses while delivering a quality customer experience \n Inform product decisions with quantitative and qualitative data on user behavior and experimentation (eg a/b testing, survey data, usability studies) \n Rapidly iterate on services using customer feedback \n Define and analyze metrics that inform the success of products/experiments \n Capture insights from growth experiments successes and failures, and communicate them to the broader team \n Partner with various teams to build software collaboratively and inclusively \n \n Compensation for this position \n \n For the 1st year - 100% revenue based compensation based on the client projects that you help to get and deliver. \n After 1st year, based on company's progress we can consider formal corporate positions with market standard salary and equity etc. \n \n Please visit https://xen.ai/ to know more about us. \n Job Types: Contract, Part-time \n Pay: $1.00 per year \n Experience level: \n \n 10 years \n \n Schedule: \n \n Choose your own hours \n \n Experience: \n \n Product management in Banking, Financial services: 5 years (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": " Empathy for users and a drive to discover and resolve their pain points \n Experience conducting user research to educate product design \n Strong analytical skills, and experience with tools for doing data analysis \n Experience designing intuitive user interfaces and executing product plans with a well-defined process \n You bring deep empathy but can effectively prioritize with limited resources \n You have a growth mindset and want to be challenged \n You have a high bar and want to be a difference-maker in the organization \n Experience managing and integrating a diverse set of partners and processes into a cohesive product \n An owner's mindset - you don't shy away from the hard stuff \n Strong collaboration in team environments but able to act quickly as the decision-maker \n Experience at a startup in a cross-functional environment \n 5+ years experience as a product manager ",
        "techs": [
            "empathy for users",
            "user research",
            "analytical skills",
            "data analysis tools",
            "intuitive user interfaces",
            "product planning",
            "growth mindset",
            "high bar",
            "managing partners and processes",
            "owner's mindset",
            "collaboration in team environments",
            "decision-making",
            "startup experience",
            "product management experience"
        ],
        "cleaned_techs": [
            "empathy for users",
            "user research",
            "data analysis tools",
            "intuitive user interfaces",
            "product planning",
            "growth mindset",
            "high bar",
            "managing partners and processes",
            "owner's mindset",
            "collaboration in team environments",
            "decision-making",
            "startup experience",
            "product management experience"
        ]
    },
    "f6b0881a1c6d5ee3": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java"
        ]
    },
    "c92b80e246c22a90": {
        "terms": [
            "data science",
            "machine learning engineer",
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Sr Distinguished Engineer, Generative AI Systems - (Remote- Eligible)",
        "company": "Capital One",
        "desc": "Locations: VA - McLean, United States of America, McLean, Virginia\n   Sr Distinguished Engineer, Generative AI Systems - (Remote- Eligible)\n  \n  Sr Distinguished Engineer, Generative AI Systems \n \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Senior Distinguished Engineer, AI Systems, to help us build the foundations of our enterprise AI Capabilities. You will work on a wide range of initiatives, whether that\u2019s designing robust, secure infrastructure, building large-scale distributed training clusters, deploying LLMs on GPU instances for real-time use cases, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work with a team of AI engineers and researchers to envision the target state of our capabilities while helping to design and implement key services. Examples of projects you will work on include: \n \n \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build infrastructure for serving large ML models, in our public cloud. \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 9 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 6 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud. \n \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $315,100 - $359,700 for Sr Distinguished Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $333,900 - $381,000 for Sr Distinguished Machine Learning Engineer\n   Remote (Regardless of Location): $267,100 - $304,800 for Sr Distinguished Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 9 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 6 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud.   \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n ",
        "techs": [
            "python",
            "c/c++",
            "aws",
            "azure",
            "gcp",
            "pytorch",
            "tensorflow",
            "lightning"
        ],
        "cleaned_techs": [
            "python",
            "c/c++",
            "aws",
            "azure",
            "gcp",
            "pytorch",
            "tensorflow",
            "lightning"
        ]
    },
    "5e06d8c3e9608508": {
        "terms": [
            "data science",
            "data analyst"
        ],
        "salary_min": 66691.0,
        "salary_max": 111151.0,
        "title": "Data Analyst - MakeMusic",
        "company": "Peaksware",
        "desc": "We are musicians, athletes, coaches and teachers who truly believe in our mission to help people achieve their best. Our software platforms connect performers, instructors and creators enabling them to publish, teach and train using the principles of deliberate practice. \n  You may know us as TrainingPeaks, MakeMusic, TrainHeroic and Alfred Music. All these brands are under the Peaksware umbrella. TrainingPeaks develops software for coaches and athletes to track, analyze and plan endurance training. TrainHeroic develops software solutions for the strength and conditioning needs of coaches and athletes. MakeMusic develops software to transform how music is composed, taught, learned and performed. Alfred Music creates and publishes educational music to help teachers, students, professionals and hobbyists experience the joy of making music.  \n We would love to have you join our ever-growing team! All applicants will receive equal consideration for employment regardless of gender, race, national origin, age, sexual orientation, gender identity, physical disability, religion, or length of time spent unemployed. \n General Summary \n  As a Data Analyst, you will be required to think strategically and deeply understand our financial, customer, and product engagement data in order to lead ad hoc exploratory investigations into our data, design and maintain parameterized reports, and help guide stakeholders across the organization to ask the right questions and make the best decisions for the business. \n  You are a continuous learner with a hunger for knowledge. You approach challenges as opportunities to improve. You value team members\u2019 input from all levels and you actively seek ways to support your colleagues. \n  You will sit directly with the Data Analytics & Insights Team, work in close collaboration with Music Brands leadership team, Product team, and Customer Team and report to Manager, Data Analytics & Insights.  \n Core Functions \n \n Manage scorecards, dashboards and ad hoc reports for both SaaS and traditional products. \n Work with various individuals and/or teams across the organization to perform exploratory investigations into our data. \n Answer questions and make recommendations on data ranging from financial/SaaS metrics to customer demographics, product engagement, marketing/sales/CS efficiencies and more from stakeholders across the business. \n Work with various stakeholders to define new data points and measures of success as new business questions arise. \n Perform other duties as assigned. \n \n The work characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. \n  Requirements \n \n 2+ years of experience as a Data Analyst or similar role. \n Background in statistics, data analytics, data science or computer engineering.  \n Proficient at working in and leveraging analytical data warehouses (i.e SQL, dimensional data modeling, ETL processes, etc.). \n Strong analytical thinking and problem-solving skills with a high attention to detail. \n Ability to create clear visualization and present complex data to a wide range of audiences. \n Strong verbal and written communications skills. \n Ability to distill problem definitions and constraints from informal business requirements. \n Ability to deal with ambiguity and competing objectives. \n \n Degrees are not required and we value all forms of continued education including traditional four-year degrees, post-graduate degrees, associate degrees, bootcamps, online training, professional certifications, self-teaching, and more. \n Desired Qualifications \n \n Experience with our products. \n Experience with SaaS business metrics. \n Experience with modern BI analytics tools (Sigma, Tableau, Looker). \n Experience with event tracking and analytics tools and working with the inputs and outputs of a CDP (Segment, MixPanel, Insider etc.) \n Experience working with a wide range of transformation models via dbt or comparable tools. \n Experience with Python and Python libraries for analytics (Pandas, NumPy, sklearn, etc.) \n \n Don\u2019t meet every single requirement? Don\u2019t worry. We still want to hear from you and encourage you to apply. \n  Benefits \n  Compensation \n  Peaksware/MakeMusic is committed to fair and equitable compensation practices. The salary range for this role is $66,691 - $111,151. Final compensation for this role will be determined by various factors such as a candidate\u2019s relevant work experience, skills, and certifications. \n  This role is eligible for variable compensation including bonus. \n  Benefits and Perks \n  Health \n \n 100% company-paid Medical for employees with buy-up options \n Dental \n Vision \n Health Savings Account \n Flexible Spending Account \n Dependent Care Flexible Spending Account \n Paid Parental Leave \n Teladoc \n Employee Assistance Program (EAP) \n Additional coverage options such as accident and critical illness insurance and hospital indemnity \n \n Disability and Life \n \n Company-paid Short Term Disability \n Company-paid Long Term Disability \n Company-paid Basic Life Insurance and AD&D \n Employee-paid Supplemental Life Insurance for Employee, Spouse, and/or Child \n \n Additional \n \n 401(K) \n 401(K) Matching \n Pet Insurance \n 9 paid holidays annually and unlimited Flexible Time Off (FTO) \n Free TrainingPeaks, TrainHeroic, MakeMusic accounts, and Alfred Music product \n Access to the Performance and Recovery Center (PARC), our on-site fitness facility \n Employee only access to on-site locker rooms and showers \n Employee only access to secure, indoor bike storage \n Access to our onsite Music Studio \n An assortment of \u201cgrab\u2019n go\u201d fruit and snacks as well as on tap cold brew, kombucha, and beer. \n Beautiful onsite cafe that includes indoor and outdoor seating and lounge areas. \n Access to e-bikes available exclusively to Peaksware employees \n Significant investment in resources for employee growth and development \n Corporate discounts on select gym memberships and top brand gear \n Flexible work schedule in a culture of trust \n \n Please contact  careers@peaksware.com  if you require a reasonable accommodation to review our website or to apply online. \n  Work Environment \n  This job operates in a professional office environment that is well-lighted, heated, and/or air-conditioned with adequate ventilation and a noise level that is usually moderate. This role routinely uses standard office equipment such as computers, phones, photocopiers and filing cabinets. \n  All employees must comply with all safety policies, practices and procedures. Report all unsafe activities to your manager and/or Human Resources. \n  Physical Demands \n  While performing the duties of this job, the employee is regularly required to sit and move about the facility; use hands to handle, or feel; talk by expressing ideas by means of the spoken word; and hear by perceiving the nature of sounds. The employee is occasionally required to stand, walk, and reach with hands and arms. The employee must occasionally lift and/or move up to 10 pounds. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus. \n  To view the Peaksware Privacy Policy, click  here . By submitting an application, you acknowledge and agree to the Peaksware Privacy Policy.",
        "cleaned_desc": " \n 2+ years of experience as a Data Analyst or similar role. \n Background in statistics, data analytics, data science or computer engineering.  \n Proficient at working in and leveraging analytical data warehouses (i.e SQL, dimensional data modeling, ETL processes, etc.). \n Strong analytical thinking and problem-solving skills with a high attention to detail. \n Ability to create clear visualization and present complex data to a wide range of audiences. \n Strong verbal and written communications skills. \n Ability to distill problem definitions and constraints from informal business requirements. \n Ability to deal with ambiguity and competing objectives. \n \n Degrees are not required and we value all forms of continued education including traditional four-year degrees, post-graduate degrees, associate degrees, bootcamps, online training, professional certifications, self-teaching, and more. \n Desired Qualifications \n \n Experience with our products. \n Experience with SaaS business metrics. \n Experience with modern BI analytics tools (Sigma, Tableau, Looker). \n Experience with event tracking and analytics tools and working with the inputs and outputs of a CDP (Segment, MixPanel, Insider etc.) ",
        "techs": [
            "sql",
            "dimensional data modeling",
            "etl processes",
            "sigma",
            "tableau",
            "looker",
            "segment",
            "mixpanel",
            "insider"
        ],
        "cleaned_techs": [
            "sql",
            "dimensional data modeling",
            "etl processes",
            "sigma",
            "tableau",
            "looker",
            "segment",
            "mixpanel",
            "insider"
        ]
    },
    "7346be31dcba4275": {
        "terms": [
            "data science"
        ],
        "salary_min": 150000.0,
        "salary_max": 165000.0,
        "title": "Senior Data Scientist",
        "company": "SRL Totalsource LLC",
        "desc": "SRL Total Source has an outstanding reputation for providing exceptional services for our clients. Our vision is to make long-lasting, reliable solutions focusing on Cyber, Medical Health Professional Staffing, and Training and Support services in the Government contracting space. With our vision in mind, we are always looking for talent to join our team who will take pride and dedication in their work. At SRL Total Source, we take exceptional pride in caring for our employee\u2019s well-being. \n \n  Job Summary \n  Seeking a Senior Data Scientist to provide services in developing new and enhancing existing capabilities for information technology systems and applications for the Department of Veterans Affairs. The Senior Data Scientist will lead efforts to expand the data science capabilities of the customer\u2019s analytics platform. The individual in this position will have a solid background in production-level machine learning systems and broad exposure to various algorithmic techniques. Experience and verifiable proof of impactful work in natural language processing, explaining-driven deep learning, and time-series forecasting are valuable. Our team particularly values experience in building inferential algorithms for high-stakes decisions while dealing with noisy, diverse, distributed datasets. Communicating complex concepts in simple language to diverse stakeholders with an unwavering commitment to empathy and customer obsession is highly desired. The candidate will develop and automate algorithms that integrate with various parts of our customer\u2019s organization, improve customer experience, and drive operational efficiency. The candidate will be able to translate data clearly and dynamically into a coherent and relevant story that will resonate with the customer\u2019s data science initiatives and roadmap. Knowledge and practical work experience with the job functions listed below is highly desirable. This is a full-time remote position. Veterans are encouraged to apply. \n \n  Location:  Virginia \n  Job Type:  Project Development \n  Salary:   150,000-165,000 \n  Responsibilities :  Included by not limited to \n \n \n \n  Build production-grade models on large-scale datasets by utilizing sophisticated statistical modeling, machine learning, or data mining techniques \n  Subject Matter Expertise in using Apache Spark, Predictive Model Markup Language (PMML), Unstructured Information Management Architecture (UIMA), Python, R, and a vast array of AI/ML libraries to analyze data and build statistical models to solve specific customer problems. \n  Provide in-depth analyses and insights for strategic and organizational initiatives while maintaining an analytics roadmap to prioritize initiatives, communicate timelines, and ensure successful, timely completion of projects \n  Use machine learning techniques, visualizations, statistical analysis, etc., to gain insight into various data sets \u2013 some of which are readily available, and some of which you create and curate yourself \n  Develop, document, and refine Mathematical Risk Algorithms Code and Scripts to measure risk \n  Extend conceptual models with business rules to develop logical data models; \n  Develop and document Analytical Models using statistical analysis data mining to create high-level data architecture diagrams that document system and data flows \n  Provide guidance and support in the development of information value chain analysis, data quality metrics, and metadata capture \n  Evaluate alternatives for integrating legacy data sources with enterprise relational models and strategies for transactional and analytical systems to share data effectively. \n  Collaborate cross-functionally to identify impactful organizational problems and translate them into structured analyses, measurable insights, and reports and dashboards \n  Assist with the development and deployment of analytical tools and develop custom models to observe progress, uncover insights in the data, and automate analyses \n  Participate in the full lifecycle of algorithm-based feature development: researching and designing solutions, running tests with clients and performing deep analyses to understand results, implementing scalable solutions in production environments, and monitoring platform-wide impact \n  Collaborate with team members, both to build out specific projects and to continuously teach and learn new technology and techniques \n  Develop/maintain a Data Science Website, analytical models, and new business process automation applications, and support and sustain all developed products deployed into production. \n  Communicate findings and solutions clearly to a variety of audiences. \n  Actively seek out a broad understanding of our customer\u2019s technology platform and products and align design efforts with that context \n \n \n \n \n \n \n  Turn your ideas into actionable designs and deliver solutions to end users. You should be able to persuade stakeholders and champion effective techniques. \n  Work independently with minimal supervision but high accountability \n \n \n \n \n   Requirements & Experience : \n \n  Master\u2019s Degree in Computer Science, Data Science, or other relevant technical discipline \n  Minimum ten years or more of progressive work experience in Data Science. \n  Must have experience with proposing new solutions to problems \n  Must be able to work and analyze independently and as part of a team. \n  Experience with Power BI \n  Experience in Data Modeling, machine learning, and analytics \n  Experience developing Data Science white papers \n  Proven ability to develop algorithm logical approaches and user stories \n  Development and Deployment of machine learning, algorithm development, exploratory analysis, and automated cleaning using Cloud services",
        "cleaned_desc": " \n \n  Build production-grade models on large-scale datasets by utilizing sophisticated statistical modeling, machine learning, or data mining techniques \n  Subject Matter Expertise in using Apache Spark, Predictive Model Markup Language (PMML), Unstructured Information Management Architecture (UIMA), Python, R, and a vast array of AI/ML libraries to analyze data and build statistical models to solve specific customer problems. \n  Provide in-depth analyses and insights for strategic and organizational initiatives while maintaining an analytics roadmap to prioritize initiatives, communicate timelines, and ensure successful, timely completion of projects \n  Use machine learning techniques, visualizations, statistical analysis, etc., to gain insight into various data sets \u2013 some of which are readily available, and some of which you create and curate yourself \n  Develop, document, and refine Mathematical Risk Algorithms Code and Scripts to measure risk \n  Extend conceptual models with business rules to develop logical data models; \n  Develop and document Analytical Models using statistical analysis data mining to create high-level data architecture diagrams that document system and data flows \n  Provide guidance and support in the development of information value chain analysis, data quality metrics, and metadata capture    Evaluate alternatives for integrating legacy data sources with enterprise relational models and strategies for transactional and analytical systems to share data effectively. \n  Collaborate cross-functionally to identify impactful organizational problems and translate them into structured analyses, measurable insights, and reports and dashboards \n  Assist with the development and deployment of analytical tools and develop custom models to observe progress, uncover insights in the data, and automate analyses \n  Participate in the full lifecycle of algorithm-based feature development: researching and designing solutions, running tests with clients and performing deep analyses to understand results, implementing scalable solutions in production environments, and monitoring platform-wide impact \n  Collaborate with team members, both to build out specific projects and to continuously teach and learn new technology and techniques \n  Develop/maintain a Data Science Website, analytical models, and new business process automation applications, and support and sustain all developed products deployed into production. \n  Communicate findings and solutions clearly to a variety of audiences. \n  Actively seek out a broad understanding of our customer\u2019s technology platform and products and align design efforts with that context \n \n     Requirements & Experience : \n \n  Master\u2019s Degree in Computer Science, Data Science, or other relevant technical discipline \n  Minimum ten years or more of progressive work experience in Data Science. \n  Must have experience with proposing new solutions to problems \n  Must be able to work and analyze independently and as part of a team. \n  Experience with Power BI \n  Experience in Data Modeling, machine learning, and analytics \n  Experience developing Data Science white papers \n  Proven ability to develop algorithm logical approaches and user stories ",
        "techs": [
            "apache spark",
            "predictive model markup language (pmml)",
            "unstructured information management architecture (uima)",
            "python",
            "r",
            "ai/ml libraries",
            "machine learning techniques",
            "visualizations",
            "statistical analysis",
            "mathematical risk algorithms code and scripts",
            "business rules",
            "analytical models",
            "data quality metrics",
            "metadata capture",
            "legacy data sources",
            "transactional and analytical systems",
            "reports and dashboards",
            "analytical tools",
            "custom models",
            "data science website",
            "new business process automation applications",
            "power bi",
            "data modeling",
            "machine learning",
            "analytics",
            "data science white papers"
        ],
        "cleaned_techs": [
            "apache spark",
            "predictive model markup language (pmml)",
            "unstructured information management architecture (uima)",
            "python",
            "r",
            "ai",
            "machine learning techniques",
            "visualizations",
            "statistical analysis",
            "mathematical risk algorithms code and scripts",
            "business rules",
            "analytical models",
            "data quality metrics",
            "metadata capture",
            "legacy data sources",
            "transactional and analytical systems",
            "reports and dashboards",
            "custom models",
            "data science website",
            "powerbi",
            "data science white papers"
        ]
    },
    "a72204e73484d77b": {
        "terms": [
            "data science"
        ],
        "salary_min": 93300.0,
        "salary_max": 212000.0,
        "title": "Innovation Lead",
        "company": "Booz Allen Hamilton",
        "desc": "Job Description \n \n \n \n \n \n \n \n \n \n \n         Location: \n         \n \n         Reston,VA,US \n         \n \n \n \n         Remote Work: \n         \n \n         Hybrid \n         \n \n \n \n         Job Number: \n         \n \n         R0182354\n         \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n         Innovation Lead\n           The Opportunity: \n  Are you at the nexus of systems design, architecture, facilitation, and collaboration? Are you passionate about learning and researching industry trends to orchestrate introduction of new technology into a large technical enterprise. You are an innovation lead! You\u2019ve evolved your skills into strategy through a long path of software development or architecture accomplishments and the curiosity to understand how all the pieces of an IT ecosystem fit together. Are you ready to use your combination of knowledge, skill, and experience to take on the present elegant solutions to complex problems in national security? \n \n  As an Innovation Lead on our team, you'll facilitate technology insertion and partnership, working with clients, an immensely talented and driven team, and industry. Through your leadership, we\u2019ll help transform the way client uses technology including cloud migration, integrating advanced technology, and modernizing legacy systems. And, what do you do next in a career where you\u2019ve reached this level? You facilitate and orchestrate partnerships with industry and firm leading capabilities to launch new technology, processes for mission impact. As a technical leader, you\u2019ll shape the digital solutions business and identify opportunities for growth. \n \n  Work with us and build the future of technology national security for the better. \n \n  Join us. The world can\u2019t wait. \n \n  You Have: \n \n  5+ years of experience with solutions architecture, enterprise architecture, or technical lead \n  3+ years of experience with project or product management \n  Experience with evaluating technologies, conducting gap analysis, and performing analysis of alternatives \n  Experience in working and collaborating with clients, technical staff, and engineers, including pitching new technologies, ideas, and concepts \n  Experience with maintaining industry relationships via conferences, meetups, and evaluating and establishing vendor agreements, including SLAs, License Agreements, or User Agreements \n  Experience with writing technology whitepapers or strategy documents surrounding technology insertion \n  Knowledge of industry leading technologies and processes, including Cloud, Container Orchestration, Microservices, Lakehouse and Data Lake, Data Science, and Agile methodologies \n  Ability to author presentations and technology roadmaps, facilitate client discussions surrounding technology insertion, learn and apply new technologies quickly in a fast-paced client environment, maintain a working knowledge of industry and firm technologies, and facilitate discussions with senior program and government staff \n  Top Secret clearance \n  Bachelor's degree \n \n \n  Nice If You Have: \n \n  Experience with software development and integration \n  Experience with data engineering or data science \n  Experience with working on research and development software efforts and initiatives \n  Experience with performing software engineering in support of DoD or IC agencies \n  Experience with Organizational Transformation and Design \n  Experience with the Systems Development Life Cycle (SDLC) \n \n \n  Clearance: \n  Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Top Secret clearance is required. \n  Create Your Career: \n  Grow With Us  Your growth matters to us\u2014that\u2019s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. \n \n  A Place Where You Belong  Diverse perspectives cultivate collective ingenuity. Booz Allen\u2019s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you\u2019ll build your community in no time. \n \n  Support Your Well-Being \n  Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we\u2019ll support you as you pursue a balanced, fulfilling life\u2014at work and at home. \n \n  Your Candidate Journey  At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we\u2019ve compiled a list of resources so you\u2019ll know what to expect as we forge a connection with you during your journey as a candidate with us. \n \n  Compensation \n  At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen\u2019s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. \n  Salary at Booz Allen is determined by various factors, including but not limited to location, the individual\u2019s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $93,300.00 to $212,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen\u2019s total compensation package for employees.\n          \n  Work Model  Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. \n \n  If this position is listed as remote or hybrid, you\u2019ll periodically work from a Booz Allen or client site facility. \n  If this position is listed as onsite, you\u2019ll work with colleagues and clients in person, as needed for the specific role. \n \n \n  EEO Commitment \n  We\u2019re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change \u2013 no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.",
        "cleaned_desc": "  3+ years of experience with project or product management \n  Experience with evaluating technologies, conducting gap analysis, and performing analysis of alternatives \n  Experience in working and collaborating with clients, technical staff, and engineers, including pitching new technologies, ideas, and concepts \n  Experience with maintaining industry relationships via conferences, meetups, and evaluating and establishing vendor agreements, including SLAs, License Agreements, or User Agreements \n  Experience with writing technology whitepapers or strategy documents surrounding technology insertion \n  Knowledge of industry leading technologies and processes, including Cloud, Container Orchestration, Microservices, Lakehouse and Data Lake, Data Science, and Agile methodologies \n  Ability to author presentations and technology roadmaps, facilitate client discussions surrounding technology insertion, learn and apply new technologies quickly in a fast-paced client environment, maintain a working knowledge of industry and firm technologies, and facilitate discussions with senior program and government staff \n  Top Secret clearance \n  Bachelor's degree \n \n \n  Nice If You Have: \n \n  Experience with software development and integration \n  Experience with data engineering or data science \n  Experience with working on research and development software efforts and initiatives \n  Experience with performing software engineering in support of DoD or IC agencies \n  Experience with Organizational Transformation and Design \n  Experience with the Systems Development Life Cycle (SDLC) \n \n ",
        "techs": [
            "software development and integration",
            "data engineering",
            "data science",
            "research and development software efforts",
            "software engineering",
            "organizational transformation and design",
            "systems development life cycle (sdlc)"
        ],
        "cleaned_techs": [
            "software development and integration",
            "data science",
            "research and development software efforts",
            "software engineering",
            "organizational transformation and design",
            "systems development life cycle (sdlc)"
        ]
    },
    "982f7f750db9b4c1": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": 150000.0,
        "salary_max": 170000.0,
        "title": "Applied Scientist - Government (Secret Clearance Preferred)",
        "company": "Descartes Labs",
        "desc": "The Company \n \n \n    Descartes Labs helps organizations whose success depends on scientific analysis of observable, physical world events. We are the leading provider of geospatial analytics that create actionable intelligence by strengthening our customers\u2019 proprietary data with models of the earth. Our solutions create new sources of environmental sustainability and operational advantage and foresight for agriculture, CPG, mining, and government users by combining machine intelligence and domain expertise into a complete operational solution.\n   \n \n \n  Descartes Labs is proud to be a remote-first, distributed organization that recognizes that people have different needs and motivations for building a life and career that matters and works for them. We focus on helping our employees produce positive outcomes and we recognize that the path to getting there will look different for everyone.\n   \n \n \n  The Role \n \n \n    Descartes Labs is building the geospatial data platform of the future to enable world-class analysis at a global scale. In this position, you will work with our internal data science team to build cutting-edge solutions for our public sector partners across a range of applications. You will be an advocate for our technology and help transform the way our customers utilize geospatial data to do their work.\n   \n \n \n  The ability and willingness to obtain and maintain a U.S. government-issued security clearance at the Secret level is required for this role. U.S. citizenship is required, as only U.S. citizens are eligible for a security clearance.\n   \n \n \n \n Your Impact \n \n \n  Use our archive of satellite imagery, weather data, and other geospatial datasets to develop and build change detection, computer vision, and monitoring tools and models.  \n Work with project managers to communicate progress to clients and translate the results of scientific analyses into actionable insights. \n  Develop custom models and algorithms to apply to data sets. \n  Collaborate with other scientists and engineers in developing models to characterize, diagnose and/or predict patterns related to mission problem sets.  \n Actively participate as a member of a cross-disciplinary team, openly communicating your ideas, insights, and feedback and helping to continuously improve processes. \n \n \n \n \n \n \n What You Bring \n \n \n  Ability to innovate and prototype solutions and algorithms given vague project statements using a deep understanding of first principles and creative problem-solving. \n  Strong technical writing and communication skills as evidenced by publications and presentations. \n  Software development experience in data manipulation, data mining or prototype development using Python (agile development, software version control, continuous integration, rapid prototyping) \n  Data analysis and signal processing skills with an ability to independently prototype algorithms, including physics-based, computer vision, statistical, and machine learning (deep learning) pipelines. \n  Experience with areas such as time series analysis, predictive models, developing, training, and evaluating deep learning and machine learning models and their real-world advantages/drawbacks. \n  Proven ability to drive business results with their data-based insights. \n  Ability to communicate and visualize complex data in a simple actionable way. \n \n \n \n \n \n \n Bonus Skill/Experience \n \n \n  MS/PhD in a relevant scientific or quantitative field (e.g., physics, operations research, electrical engineering, geography, or other highly quantitative discipline) \n  Experience with satellite imagery and aerial imagery \n  Experience developing for geospatial applications. \n  Experience working on intelligence and DoD programs. \n  Experience with model deployment using cloud or distributed computing environments. \n  Experience with agile development practice, code repositories (i.e., Git), open-source scientific computation, computer vision, modeling, or machine learning tools. \n  Knowledge of ML Ops processes and software \n  Experience writing technical volumes of proposals for government-funded projects. \n  Secret level clearance or eligibility preferred. \n \n  Our Tech Stack  AWS CDK Linux Python Docker, Kubernetes AWS GitBazel\n      \n \n \n \n \n \n \n Who You Are \n \n \n  Curious. You are always exploring and experimenting, interested in why and how, seeking not only to understand but to make work and the world better. You enthusiastically share your learning with others and actively seek information and knowledge. \n  Conscientious. You are determined, always keep your promises, and are forward thinking. Principled and integrous, you take your commitments seriously. \n  Humble. Unpretentious and self-aware, you cultivate compassion for others and take responsibility for your mistakes. Egos are barriers to doing the best work and always learning. \n  Open and Inclusive. You are receptive and interested in new ideas and perspectives, even when those perspectives don\u2019t agree with your views. You value and respect difference and create ways for all people to contribute to the organization. \n  Collaborative. You know it takes a team to get anything accomplished and you actively and inclusively work across the organization. You listen intently and openly and are always focused first on creating the best results. \n  Adaptable. You can navigate changing circumstances and environments with ease and approach uncertainty with enthusiasm, while inspiring others towards effective goal setting and accomplishment. \n \n \n \n \n \n \n Pay \n \n \n  The base salary range for this position is $150,000-$170,000.  \n Please note the base salary range is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level & competencies as well as security clearance. \n \n \n \n \n \n \n Top Reasons to Work at Descartes Labs Government \n \n \n  We pride collaboration over ownership, iteration over perfection, principles over rules, and discussion over directives. \n  We strongly encourage and enjoy a flexible work environment. \n  We offer a generous compensation package including competitive salary; choice of medical plan; dental, life, and disability insurance; a 401K plan; paid holidays and paid time off. \n \n \n \n \n \n \n You belong here! If your experience and interests match with some of the above, we want you to apply. \n \n \n \n  Accommodations will be provided as requested by candidates taking part in all aspects of the selection process.",
        "cleaned_desc": " Your Impact \n \n \n  Use our archive of satellite imagery, weather data, and other geospatial datasets to develop and build change detection, computer vision, and monitoring tools and models.  \n Work with project managers to communicate progress to clients and translate the results of scientific analyses into actionable insights. \n  Develop custom models and algorithms to apply to data sets. \n  Collaborate with other scientists and engineers in developing models to characterize, diagnose and/or predict patterns related to mission problem sets.  \n Actively participate as a member of a cross-disciplinary team, openly communicating your ideas, insights, and feedback and helping to continuously improve processes. \n \n \n \n \n \n \n What You Bring \n \n \n  Ability to innovate and prototype solutions and algorithms given vague project statements using a deep understanding of first principles and creative problem-solving. \n  Strong technical writing and communication skills as evidenced by publications and presentations. \n  Software development experience in data manipulation, data mining or prototype development using Python (agile development, software version control, continuous integration, rapid prototyping) \n  Data analysis and signal processing skills with an ability to independently prototype algorithms, including physics-based, computer vision, statistical, and machine learning (deep learning) pipelines. \n  Experience with areas such as time series analysis, predictive models, developing, training, and evaluating deep learning and machine learning models and their real-world advantages/drawbacks. \n  Proven ability to drive business results with their data-based insights.    Ability to communicate and visualize complex data in a simple actionable way. \n \n \n \n \n \n \n Bonus Skill/Experience \n \n \n  MS/PhD in a relevant scientific or quantitative field (e.g., physics, operations research, electrical engineering, geography, or other highly quantitative discipline) \n  Experience with satellite imagery and aerial imagery \n  Experience developing for geospatial applications. \n  Experience working on intelligence and DoD programs. \n  Experience with model deployment using cloud or distributed computing environments. \n  Experience with agile development practice, code repositories (i.e., Git), open-source scientific computation, computer vision, modeling, or machine learning tools. \n  Knowledge of ML Ops processes and software \n  Experience writing technical volumes of proposals for government-funded projects. \n  Secret level clearance or eligibility preferred. \n \n  Our Tech Stack  AWS CDK Linux Python Docker, Kubernetes AWS GitBazel\n      \n ",
        "techs": [
            "satellite imagery",
            "weather data",
            "geospatial datasets",
            "change detection tools",
            "computer vision tools",
            "monitoring tools",
            "custom models",
            "algorithms",
            "cross-disciplinary team",
            "technical writing",
            "communication skills",
            "publication",
            "presentation",
            "software development",
            "data manipulation",
            "data mining",
            "prototype development",
            "python",
            "agile development",
            "software version control",
            "continuous integration",
            "rapid prototyping",
            "data analysis",
            "signal processing",
            "time series analysis",
            "predictive models",
            "deep learning",
            "machine learning",
            "business results",
            "ms/phd",
            "satellite imagery",
            "aerial imagery",
            "geospatial applications",
            "intelligence programs",
            "dod programs",
            "model deployment",
            "cloud computing",
            "distributed computing",
            "agile development practice",
            "git",
            "open-source scientific computation",
            "computer vision",
            "machine learning tools",
            "ml ops",
            "technical volumes",
            "proposals",
            "government-funded projects",
            "secret level clearance",
            "aws",
            "cdk",
            "linux",
            "python",
            "docker",
            "kubernetes",
            "gitbazel"
        ],
        "cleaned_techs": [
            "satellite imagery",
            "weather data",
            "geospatial datasets",
            "change detection tools",
            "computer vision tools",
            "monitoring tools",
            "custom models",
            "algorithms",
            "cross-disciplinary team",
            "technical writing",
            "publication",
            "presentation",
            "software development",
            "data manipulation",
            "data mining",
            "prototype development",
            "python",
            "agile development",
            "software version control",
            "continuous integration",
            "rapid prototyping",
            "signal processing",
            "time series analysis",
            "predictive models",
            "business results",
            "aerial imagery",
            "intelligence programs",
            "dod programs",
            "model deployment",
            "cloud computing",
            "distributed computing",
            "agile development practice",
            "git",
            "open-source scientific computation",
            "computer vision",
            "machine learning tools",
            "ml ops",
            "technical volumes",
            "proposals",
            "government-funded projects",
            "secret level clearance",
            "aws",
            "cdk",
            "linux",
            "docker",
            "kubernetes",
            "gitbazel"
        ]
    },
    "69ac5d8e597209d8": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": 112220.4,
        "salary_max": 142096.03,
        "title": "Machine Learning Engineer \u2013 Work from Home",
        "company": "Capgemini Government Solutions",
        "desc": "of training and educational resources. In addition to our internal learning sites, we partnered with Coursera and Degreed to offer our staff the latest courses from academic institutions around the world. We provide education expense reimbursements as well as sponsor seminars, conferences, and certifications. Our practice leaders work with every team member to chart appropriate career paths and goals to ensure that we all stay innovative and transformative, which maximizes our ability to scale up our solutions, keep up with the cutting edge, and bring the art of what\u2019s possible to the Federal Government. \n  Job Responsibilities \n  As a ML Engineer you will: \n \n Deliver high-quality code components that will power the services, servers, distributed systems, and backend architecture for Microsoft products \n Partner with industry-leading Engineers, Artists, Producers and Designers \n Incorporate the latest AI, Machine Learning and Computer vision capabilities into the design of Microsoft products and services \n Drive ML-related solutions based on evaluation of requirements, resources, and alternatives \n Conduct exploratory data analysis to evaluate data pipelines and construct data stores (structured, semi-structured, and unstructured) as needed to feed frameworks/models \n Develop custom algorithms, frameworks, and models or leverage available tools, libraries, and applications to solve complex problems \n \n \n Deploy ML solutions and develop methodologies to scale up \n Present and articulate findings and present solutions to clients and team members \n Maintain knowledge of advances of ML in industry and academia \n \n \n As needed, collaborate with internal and external stakeholders to identify object detection, optical character recognition, automation, predictive modeling, pattern analysis, natural language processing, fraud detection, and other business cases for using ML \n \n Required Qualifications \n \n U.S. Citizenship is required \n Eligible to obtain and maintain a Government Security Clearance \n \n \n Bachelor\u2019s degree or higher in machine learning, data science, statistics, computer science, economics, mathematics, information systems, or similar field preferred \n \n \n Minimum of two (2) years of professional experience with machine learning-delivery responsibilities such as: \n \n Deliver high-quality code components to power services, servers, distributed systems, and backend architecture \n Partner with industry-leading Engineers, Artists, Producers and Designers \n Incorporate the AI, machine learning, and Computer vision capabilities into solutions and services \n \n Experience in object detection and computer vision models such as YOLO, MMDetection, R-CNN, SSD, FPN, and RetinaNet \n Experience programming in languages such as Python, R, Scala, SQL, JavaScript, C/C++, and Java \n Experience using libraries and frameworks such as TensorFlow, PyTorch, Spark ML/MLlib, and Jupyter \n Excellent verbal and written communication skills \n Ability to multi-task and stay flexible in a dynamic work environment \n \n Nice to have skills/qualifications \n \n Active Government Security clearance \n Data Science, ML, AI, or Cloud certifications \n Experience working in an IT project team following SDLC and DevOps methodologies \n \n \n Experience working with big data distributed programming languages and ecosystems such as Hadoop, MapReduce, Pig, or Kafka \n Experience with designing and building cloud-based databases, data lakes, and data warehouses \n \n \n Experience using tools such as Azure\u2019s Machine Learning and Cognitive Services; AWS\u2019s SageMaker, Polly and Rekognition; DataRobot; or H2O.ai \n \n \n \n \n \n \n \n \n \n \n \n \n \n Life at Capgemini \n Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer: \n \n Flexible work   \n Healthcare including dental, vision, mental health, and well-being programs   \n Financial well-being programs such as 401(k) and Employee Share Ownership Plan   \n Paid time off and paid holidays   \n Paid parental leave   \n Family building benefits like adoption assistance, surrogacy, and cryopreservation   \n Social well-being benefits like subsidized back-up child/elder care and tutoring   \n Mentoring, coaching and learning programs   \n Employee Resource Groups   \n Disaster Relief   \n \n \n About Capgemini \n Capgemini is a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided everyday by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of \u20ac22 billion. \n Get The Future You Want | www.capgemini.com \n Disclaimer \n Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law. \n This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship. \n Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact. \n Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law \n Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini. \n \n \n \n \n \n \n Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities \n  The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor\u2019s legal duty to furnish information. 41 CFR 60-1.35(c)",
        "cleaned_desc": "of training and educational resources. In addition to our internal learning sites, we partnered with Coursera and Degreed to offer our staff the latest courses from academic institutions around the world. We provide education expense reimbursements as well as sponsor seminars, conferences, and certifications. Our practice leaders work with every team member to chart appropriate career paths and goals to ensure that we all stay innovative and transformative, which maximizes our ability to scale up our solutions, keep up with the cutting edge, and bring the art of what\u2019s possible to the Federal Government. \n  Job Responsibilities \n  As a ML Engineer you will: \n \n Deliver high-quality code components that will power the services, servers, distributed systems, and backend architecture for Microsoft products \n Partner with industry-leading Engineers, Artists, Producers and Designers \n Incorporate the latest AI, Machine Learning and Computer vision capabilities into the design of Microsoft products and services \n Drive ML-related solutions based on evaluation of requirements, resources, and alternatives \n Conduct exploratory data analysis to evaluate data pipelines and construct data stores (structured, semi-structured, and unstructured) as needed to feed frameworks/models \n Develop custom algorithms, frameworks, and models or leverage available tools, libraries, and applications to solve complex problems \n \n \n Deploy ML solutions and develop methodologies to scale up \n Present and articulate findings and present solutions to clients and team members \n Maintain knowledge of advances of ML in industry and academia \n \n \n As needed, collaborate with internal and external stakeholders to identify object detection, optical character recognition, automation, predictive modeling, pattern analysis, natural language processing, fraud detection, and other business cases for using ML \n   Required Qualifications \n \n U.S. Citizenship is required \n Eligible to obtain and maintain a Government Security Clearance \n \n \n Bachelor\u2019s degree or higher in machine learning, data science, statistics, computer science, economics, mathematics, information systems, or similar field preferred \n \n \n Minimum of two (2) years of professional experience with machine learning-delivery responsibilities such as: \n \n Deliver high-quality code components to power services, servers, distributed systems, and backend architecture \n Partner with industry-leading Engineers, Artists, Producers and Designers \n Incorporate the AI, machine learning, and Computer vision capabilities into solutions and services \n \n Experience in object detection and computer vision models such as YOLO, MMDetection, R-CNN, SSD, FPN, and RetinaNet \n Experience programming in languages such as Python, R, Scala, SQL, JavaScript, C/C++, and Java \n Experience using libraries and frameworks such as TensorFlow, PyTorch, Spark ML/MLlib, and Jupyter \n Excellent verbal and written communication skills   Ability to multi-task and stay flexible in a dynamic work environment \n \n Nice to have skills/qualifications \n \n Active Government Security clearance \n Data Science, ML, AI, or Cloud certifications \n Experience working in an IT project team following SDLC and DevOps methodologies \n \n \n Experience working with big data distributed programming languages and ecosystems such as Hadoop, MapReduce, Pig, or Kafka \n Experience with designing and building cloud-based databases, data lakes, and data warehouses \n \n \n Experience using tools such as Azure\u2019s Machine Learning and Cognitive Services; AWS\u2019s SageMaker, Polly and Rekognition; DataRobot; or H2O.ai \n \n \n \n \n ",
        "techs": [
            "coursera",
            "degreed",
            "microsoft products",
            "ai",
            "machine learning",
            "computer vision",
            "ml solutions",
            "algorithms",
            "frameworks",
            "models",
            "object detection",
            "optical character recognition",
            "automation",
            "predictive modeling",
            "pattern analysis",
            "natural language processing",
            "fraud detection",
            "yolo",
            "mmdetection",
            "r-cnn",
            "ssd",
            "fpn",
            "retinanet",
            "python",
            "r",
            "scala",
            "sql",
            "javascript",
            "c/c++",
            "java",
            "tensorflow",
            "pytorch",
            "spark ml/mllib",
            "jupyter",
            "hadoop",
            "mapreduce",
            "pig",
            "kafka",
            "azure's machine learning",
            "cognitive services",
            "aws's sagemaker",
            "polly",
            "rekognition",
            "datarobot",
            "h2o.ai"
        ],
        "cleaned_techs": [
            "coursera",
            "degreed",
            "microsoft products",
            "ai",
            "computer vision",
            "ml solutions",
            "algorithms",
            "frameworks",
            "models",
            "object detection",
            "optical character recognition",
            "automation",
            "predictive modeling",
            "pattern analysis",
            "nlp",
            "fraud detection",
            "yolo",
            "mmdetection",
            "r-cnn",
            "ssd",
            "fpn",
            "retinanet",
            "python",
            "r",
            "scala",
            "sql",
            "javascript",
            "c/c++",
            "java",
            "tensorflow",
            "pytorch",
            "spark ml/mllib",
            "jupyter",
            "hadoop",
            "mapreduce",
            "pig",
            "kafka",
            "azure",
            "cognitive services",
            "aws",
            "polly",
            "rekognition",
            "datarobot",
            "h2o.ai"
        ]
    },
    "b37ab7131e205671": {
        "terms": [
            "data science"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Client Partner",
        "company": "Infor",
        "desc": "General information \n       \n \n \n \n \n \n \n        Country \n        \n United States  \n \n \n \n        City \n        \n Remote Location  \n \n \n \n        Department \n        \n Consulting Services  \n \n \n \n        Job ID \n        \n 37210  \n \n \n \n \n \n \n \n \n \n       Description & Requirements \n       \n \n \n \n \n \n \n \n         The Client Partner is responsible for partnering with customers to demonstrate value and deliver business outcomes using Infor solutions. Ensuring on-going high levels of satisfaction and retention of customers is vital. The Client Partner will proactively work with both existing and new customers on a regular basis to uncover opportunities for increased utilization and adoption as well as opportunities for upsell. Additionally, our Client Partner will develop and maintain strong relationships with internal partners and clients to assist in identifying, prioritizing and resolving any challenges or concerns. This position is structured as a remote/home office with moderate travel.\n         \n \n \n  A Day in The Life Typically Includes:\n         \n \n \n  Managing a National Territory with a Large Annual Consulting Services Personal Sales Quota supporting our all software solutions within industry vertical. \n  Supporting \u2018Net New\u2019 Sales Representatives on all opportunities for full sales lifecycle including: coordination of RFP responses, oral presentations and contract negotiation for services elements. \n  Working with sub-contractors to handle additional opportunities. Selling into the existing base in support of new software transactions. \n  Mining current customer base to develop pipeline opportunities in conjunction with inside sales team. \n  Forecasting Responsibility (Monthly, Quarterly, Annually). Identification of Marketing Campaigns to drive opportunities. \n \n \n \n \n  Basic Qualifications:\n         \n \n \n  Experience managing an annual personal quota \n  Consulting and/or License sales experience \n  Experience with consulting methodologies, especially around software/technology solutions \n  Experience supporting license sales and selling the value proposition of consulting services \n  Experience with selling to prospects at the \u2018C\u2019 level \n  Experience with developing pipeline/opportunities from an existing customer base \n  Knowledge of software applications \n \n \n \n \n  Preferred Qualifications:\n         \n \n \n  Consulting/Professional Services Sales experience \n \n \n \n \n  Remote (Dallas, TX; St. Paul, MN; and Alpharetta, GA)\n         \n \n \n \n \n \n \n \n \n         About Infor\n         \n \n \n  Infor is a global leader in business cloud software products for companies in industry specific markets. Infor builds complete industry suites in the cloud and efficiently deploys technology that puts the user experience first, leverages data science, and integrates easily into existing systems. Over 60,000 organizations worldwide rely on Infor to help overcome market disruptions and achieve business-wide digital transformation.\n         \n \n          For more information visit www.infor.com\n         \n \n \n \n \n \n \n \n         Our Values\n         \n \n \n  At Infor, we strive for an environment that is founded on a business philosophy called Principle Based Management\u2122 (PBM\u2122) and eight Guiding Principles: integrity, stewardship & compliance, transformation, principled entrepreneurship, knowledge, humility, respect, self-actualization. Increasing diversity is important to reflect our markets, customers, partners, and communities we serve in now and in the future.\n         \n \n \n  We have a relentless commitment to a culture based on PBM. Informed by the principles that allow a free and open society to flourish, PBM\u2122 prepares individuals to innovate, improve, and transform while fostering a healthy, growing organization that creates long-term value for its clients and supporters and fulfillment for its employees.\n         \n \n \n  Infor is an Equal Opportunity Employer. We are committed to creating a diverse and inclusive work environment. Infor does not discriminate against candidates or employees because of their sex, race, gender identity, disability, age, sexual orientation, religion, national origin, veteran status, or any other protected status under the law.\n          \n \n \n \n \n         At Infor we value your privacy that\u2019s why we created a policy that you can read here.\n         \n \n \n \n \n \n \n \n \n         This employer uses E-Verify. Please visit the following website for additional information: www.kochcareers.com/doc/Everify.pdf",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "613046fe9a13f80a": {
        "terms": [
            "data science"
        ],
        "salary_min": 136678.0,
        "salary_max": 161267.0,
        "title": "Principal IS Analyst - R&D Generative AI",
        "company": "Amgen",
        "desc": "Career Category\n   Information Systems\n  \n \n   Job Description\n  \n \n   HOW MIGHT YOU DEFY IMAGINATION?\n  \n \n   If you feel like you\u2019re part of something bigger, it\u2019s because you are. At Amgen, our shared mission\u2014to serve patients\u2014drives all that we do. It is key to our becoming one of the world\u2019s leading biotechnology companies. We are global collaborators who achieve together\u2014researching, manufacturing, and delivering ever-better products that reach over 10 million patients worldwide. It\u2019s time for a career you can be proud of.\n  \n \n \n   Principal IS Analyst - R&D Generative AI\n  \n \n \n   Live\n  \n \n   What you will do\n  \n \n   Let\u2019s do this. Let\u2019s change the world. In this vital role you will work with our Research and Development (R&D) Digital Technology and Innovation (DTI) group supporting R&D\u2019s Global Regulatory Affairs & Strategy (GRAAS) function in alignment with our business and DTI strategy.\n  \n \n   As a Principal IS Business Systems Analyst at Amgen, you will provide strategic leadership and guidance in analyzing business requirements and designing information systems solutions. You will collaborate with multi-functional teams to understand business needs, identify system enhancements, and drive system implementation projects. Your extensive experience in business analysis, system design, and project management will enable you to deliver innovative and effective solutions that align with the company's strategic goals.\n  \n \n   Responsibilities:\n  \n \n  Serve as a client facing partner and / or product owner working with business and DTI team members to implement and support the Generative AI and Structured Content Management/Authoring initiatives \n  Responsibilities include technical product ownership, process and systems analysis, requirements elicitation, articulating system solutions and approaches for addressing business needs \n  Provide an expert-level understanding of GRAAS business models, processes, and procedures \n  Engage with key vendors and platform owners to understand functionality and how those platforms can be used to tackle the business problems \n  Meet regulatory requirements and adhere to DTI standards including following GxP Quality and SDLC processes \n  The position is located in our offices in Thousand Oaks, CA, or Tampa, FL or we can also consider remote applicants in a United States. \n \n \n \n   Win\n  \n \n   What we expect of you\n  \n \n   We are all different, yet we all use our unique contributions to serve patients. The strategic professional we seek is a critical thinker with these qualifications.\n  \n \n   Basic Qualifications:\n  \n \n  Doctorate degree and 2 years of Information Systems experience \n  OR Master\u2019s degree and 6 years of Information Systems experience \n  OR Bachelor\u2019s degree and 8 years of Information Systems experience \n  OR Associate\u2019s degree and 10 years of Information Systems experience \n  OR High School Diploma /GED and 12 years of Information Systems experience \n \n \n \n   Preferred Qualifications:\n  \n \n  At least 5 years of domain knowledge in health and/or life sciences combined with Information Technology \n  Ability to set expectations with business partners and optimally use governance for a positive business partner experience \n  Strong communications skills in writing, speaking, presenting and time management skills \n  Applied experience and knowledge in the application of Generative AI in the submission/CTD document drafting \n  Understanding, and preferably applied experience and knowledge in the application of Structured Content Management/Authoring in the submission/CTD document space \n  Expert understanding of GRAAS/Regulatory Affairs, and preferably other R&D processes \n  Experience in Agile product development \n  7+ years customer-facing technical consulting experience \n  Experience defining technical roadmaps and driving adoption \n  Either 7+ years of experience with integration architecture, design, and development for geographically distributed enterprise/cloud software or document/content management systems OR development experience with one of the following languages \u2013 Java, .NET, Python, C#, or C++, or database development \n \n \n \n   Thrive\n  \n \n   What you can expect of us\n  \n \n   As we work to develop treatments that take care of others, we also work to care for our teammates\u2019 professional and personal growth and well-being.\n  \n \n   Amgen offers a Total Rewards Plan comprising health and welfare plans for staff and eligible dependents, financial plans with opportunities to save towards retirement or other goals, work/life balance, and career development opportunities including:\n  \n \n  Comprehensive employee benefits package, including a Retirement and Savings Plan with generous company contributions, group medical, dental and vision coverage, life and disability insurance, and flexible spending accounts. \n  A discretionary annual bonus program, or for field sales representatives, a sales-based incentive plan \n  Stock-based long-term incentives \n  Award-winning time-off plans and bi-annual company-wide shutdowns \n  Flexible work models, including remote work arrangements, where possible \n \n \n   Apply now\n  \n \n   for a career that defies imagination\n  \n \n   Objects in your future are closer than they appear. Join us.\n  \n \n   careers.amgen.com\n  \n  .\n  \n   Salary Range\n   136,678.00 USD - 161,267.00 USD",
        "cleaned_desc": "  Ability to set expectations with business partners and optimally use governance for a positive business partner experience \n  Strong communications skills in writing, speaking, presenting and time management skills \n  Applied experience and knowledge in the application of Generative AI in the submission/CTD document drafting \n  Understanding, and preferably applied experience and knowledge in the application of Structured Content Management/Authoring in the submission/CTD document space \n  Expert understanding of GRAAS/Regulatory Affairs, and preferably other R&D processes \n  Experience in Agile product development \n  7+ years customer-facing technical consulting experience \n  Experience defining technical roadmaps and driving adoption \n  Either 7+ years of experience with integration architecture, design, and development for geographically distributed enterprise/cloud software or document/content management systems OR development experience with one of the following languages \u2013 Java, .NET, Python, C#, or C++, or database development \n \n \n \n   Thrive\n  \n \n   What you can expect of us\n  \n \n   As we work to develop treatments that take care of others, we also work to care for our teammates\u2019 professional and personal growth and well-being.\n  \n \n   Amgen offers a Total Rewards Plan comprising health and welfare plans for staff and eligible dependents, financial plans with opportunities to save towards retirement or other goals, work/life balance, and career development opportunities including:",
        "techs": [
            "generative ai",
            "structured content management/authoring",
            "graas/regulatory affairs",
            "agile",
            "java",
            ".net",
            "python",
            "c#",
            "c++",
            "database development"
        ],
        "cleaned_techs": [
            "generative ai",
            "structured content management/authoring",
            "agile",
            "java",
            ".net",
            "python",
            "c#",
            "c++",
            "database development"
        ]
    },
    "8c8a324e9e53ca43": {
        "terms": [
            "data science"
        ],
        "salary_min": 105000.0,
        "salary_max": 115000.0,
        "title": "Digital Marketing Channel Manager (In House Agency)",
        "company": "The NOW HQ",
        "desc": "Founded on the principle that self-care is a necessity, not a luxury, The NOW aims to deliver high-quality massage services in an unparalleled setting. Since our inception in 2015, we've evolved into a sought-after wellness destination with boutiques nationwide. As our franchise system continues to grow, our in-house agency plays a pivotal role in driving success for each franchisee. \n  Position Overview \n  The NOW, a leader in our domain with over 50 thriving franchise boutiques, is scouting for a seasoned Digital Marketing Channel Manager. Your mission will be to devise and deploy sophisticated strategies that not only magnify our brand presence but also cater to the unique digital marketing requisites of each of our franchisees. \n  Your expansive understanding of the digital domain, coupled with an uncanny ability to decode intricate data patterns, orchestrate compelling campaigns, and create synergies among diverse teams, will be paramount in solidifying the growth trajectory of our boutiques. \n  Job Responsibilities: \n \n   Channel Onboarding:  Efficiently introduce and integrate new marketing channels to diversify our digital footprint. \n Strategic Execution:  Formulate and implement cross-platform marketing strategies tailored to resonate with and support the aspirations of our 50+ franchise boutiques. \n Campaign Mastery:  Conceptualize, oversee, and refine campaigns ensuring alignment with individual franchised/boutique objectives and overall brand direction. \n Conversion Optimization:  Delve into conversion metrics, identifying bottlenecks and conceptualizing actionable strategies for enhancement. \n Geographical Campaigning:  Architect robust campaign blueprints for burgeoning territories and prospective boutique locations. \n Insightful Analysis:  Extract actionable intelligence from data, emphasizing the dynamics between creative strategies, geographical performance, and franchise objectives. \n Collaborative Synergy:  Work in tandem with our internal creative team and external partners. Your role will involve molding creative narratives and directives based on proven performance metrics. \n ROI Experiments:  Architect and spearhead experiments to critically assess ROI and optimize future marketing spends. \n In-House Collaboration:  Seamlessly intertwine with our in-house account executives, crafting paid marketing strategies that sync with our overarching business objectives. \n Narrative Reporting:  Elevate performance reporting by weaving in insightful narratives, adding depth and context to raw data. \n Event Logging:  Maintain a precise chronicle of significant events, categorized by specific campaigns or distinct time frames. \n \n   \n Qualifications: \n \n   Education:  Bachelor\u2019s degree in Data Science, Computer Science, or a similar field. \n Experience:  A minimum of 5 years in a relevant role. \n Ad Expertise:  Mastery over ad platform tools like Google Ads Editor, Meta Ads Manager, etc. \n Analytical Acumen:  Proficiency in tools like Google Analytics (UA and GA4) and Google Tag Manager. Familiarity with SQL, JavaScript, Python, or other scripting languages is essential. \n Campaign Volume Management:  A track record of effectively managing an expansive suite of campaigns (100+). \n Multitasking Agility:  Demonstrable proficiency in juggling multiple projects without compromising on quality. \n Certifications:  Desired certifications with Google Ads Search Professional and Meta Certified Media Buying Professional \n \n   \n Benefits & Compensation: \n \n  Pay Range: $105,000 - $115,000 \n Comprehensive benefit package including medical, dental, vision, Life/AD&D, 401K with company match, paid time off, and holidays. \n \n   \n The NOW is an equal opportunity employer and values diversity. All applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. \n  Join our team and be an integral part of our exhilarating digital voyage, ensuring that each of our franchised boutiques achieves unparalleled digital success. \n   \n  This is a remote position.",
        "cleaned_desc": "   Education:  Bachelor\u2019s degree in Data Science, Computer Science, or a similar field. \n Experience:  A minimum of 5 years in a relevant role. \n Ad Expertise:  Mastery over ad platform tools like Google Ads Editor, Meta Ads Manager, etc. \n Analytical Acumen:  Proficiency in tools like Google Analytics (UA and GA4) and Google Tag Manager. Familiarity with SQL, JavaScript, Python, or other scripting languages is essential. \n Campaign Volume Management:  A track record of effectively managing an expansive suite of campaigns (100+). \n Multitasking Agility:  Demonstrable proficiency in juggling multiple projects without compromising on quality. \n Certifications:  Desired certifications with Google Ads Search Professional and Meta Certified Media Buying Professional ",
        "techs": [
            "google ads editor",
            "meta ads manager",
            "google analytics",
            "google tag manager",
            "sql",
            "javascript",
            "python",
            "google ads search professional",
            "meta certified media buying professional"
        ],
        "cleaned_techs": [
            "google ads editor",
            "meta ads manager",
            "google analytics",
            "google tag manager",
            "sql",
            "javascript",
            "python",
            "google ads search professional",
            "meta certified media buying professional"
        ]
    },
    "06bbda25bfbf035e": {
        "terms": [
            "data science"
        ],
        "salary_min": 70000.0,
        "salary_max": 80000.0,
        "title": "Technology Senior Program Associate, Operations",
        "company": "NATIONAL CHILDRENS ALLIANCE",
        "desc": "Description: \n    ABOUT THE ORGANIZATION \n  National Children\u2019s Alliance (NCA) is the national association and accrediting body for Children\u2019s Advocacy Centers (CACs). Formed in 1988, NCA has been providing support, technical assistance, and quality assurance for CACs, while serving as a voice for abused children for more than 30 years. \n  COMMITMENT TO DIVERSITY, EQUITY AND INCLUSION \n  National Children\u2019s Alliance (NCA) is committed to a diversity, equity, and inclusion (DEI) within our staff, board, and Children\u2019s Advocacy Center (CAC) movement. A diverse, equitable, and inclusive movement is critical not only to achieve a just and welcoming work environment for our workforce and partners: it also impossible to effectively serve children and families across this country without a movement of agencies and professionals that reflect them and understand their needs. \n  As the national association and accrediting body for CACs, NCA is committed to achieving universal access to CAC services for children in every community in America and is uniquely positioned to ensure CACs both have the resources they need to truly serve America\u2019s diverse population and also that their accreditation with us requires that they do so equitably and inclusively. \n  POSITION SUMMARY \n  The Technology Senior Program Associate is responsible for the overall project management of NCA\u2019s implementation and ongoing services of an AMS system. This position will assist in the administration of NCA\u2019s technology platforms while providing technical support to internal and external constituents. In addition, this position will support data flows and migrations for a Dynamics 365 AMS and other cloud systems as well as maintaining other technologies provided to NCA staff. \n  ESSENTIAL FUNCTIONS \n \n  Project management of the implementation and ongoing updates to the Dynamics 365 AMS. \n  Assist in implementing robust technology solutions and tools to serve the needs of NCA staff and leadership. \n  Assist in training and preparing staff for new technologies and tech initiatives. \n  Troubleshoot, administer, and provide end-user support for technology tools, platforms, data structures, and a Dynamics 365 AMS. \n  Follow cybersecurity best practices and ensure that all digital environments, devices, and data are protected from security breaches and data losses. \n  Intake, prioritize, analyze, and complete internal and external data and information requests. \n  Develop and maintain documentation on system processes, policies, and procedures. \n  Implement all work in accordance with organizational, local, state, and federal regulations or requirements as well as industry best practices. \n  Maintain a strong awareness and knowledge of relevant technology developments. \n  Other duties as assigned by the Senior Technology Manager, Vice President of Operations, and Chief Executive Officer \n  Requirements: \n    KNOWLEDGE AND SKILLS REQUIRED \n \n  Bachelor\u2019s degree in computer science, data science, software engineering, data analytics, or relevant technical experience is preferred. \n  Demonstrated experience in a project management tool is required (preferred tools are GitHub, Azure DevOps, Microsoft Project, Microsoft Planner, and Asana) \n  Experience in a programming or scripting language (preferred languages are C, C#, C++, Rust, CSS, JavaScript, Python, R, DAX, or SQL). \n  Knowledge of AGILE, DevOps, and CI/CD principles. \n  High degree of integrity, ethical decision making, and commitment to maintaining confidentiality. \n  Detail-oriented with exceptional project management skills. \n  Good written and oral communication skills. \n  A positive, solution-oriented approach to problem-solving. \n  Ability to learn and adapt to new technology systems. \n  Ability to translate complex problems clearly and in non-technical terms. \n  Ability to work both independently and as a team member in a fast-paced remote environment. \n  Experience with ETL processes or tools is preferred. \n  Experience with REST, RESTful, Swagger, or OpenAPI specifications is preferred. \n  Experience or certification in Azure, Dynamics 365, Microsoft 365 administration, or Power Platform is preferred. \n  Having an active public profile or technical certifications on GitHub, HackerRank, DataCamp, LinkedIn Learning, or a similar platform is a plus. \n \n \n  WORK ENVIORNMENT/PHYSICAL REQUIREMENTS \n \n  Remote position. \n  Occasional travel. \n  This position is mainly sedentary. \n  Covid-19 Vaccination required. Reasonable medical accommodation considered. \n  National Children\u2019s Alliance provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.",
        "cleaned_desc": " \n  Project management of the implementation and ongoing updates to the Dynamics 365 AMS. \n  Assist in implementing robust technology solutions and tools to serve the needs of NCA staff and leadership. \n  Assist in training and preparing staff for new technologies and tech initiatives. \n  Troubleshoot, administer, and provide end-user support for technology tools, platforms, data structures, and a Dynamics 365 AMS. \n  Follow cybersecurity best practices and ensure that all digital environments, devices, and data are protected from security breaches and data losses. \n  Intake, prioritize, analyze, and complete internal and external data and information requests. \n  Develop and maintain documentation on system processes, policies, and procedures. \n  Implement all work in accordance with organizational, local, state, and federal regulations or requirements as well as industry best practices.    Maintain a strong awareness and knowledge of relevant technology developments. \n  Other duties as assigned by the Senior Technology Manager, Vice President of Operations, and Chief Executive Officer \n  Requirements: \n    KNOWLEDGE AND SKILLS REQUIRED \n \n  Bachelor\u2019s degree in computer science, data science, software engineering, data analytics, or relevant technical experience is preferred. \n  Demonstrated experience in a project management tool is required (preferred tools are GitHub, Azure DevOps, Microsoft Project, Microsoft Planner, and Asana) \n  Experience in a programming or scripting language (preferred languages are C, C#, C++, Rust, CSS, JavaScript, Python, R, DAX, or SQL). \n  Knowledge of AGILE, DevOps, and CI/CD principles.    High degree of integrity, ethical decision making, and commitment to maintaining confidentiality. \n  Detail-oriented with exceptional project management skills. \n  Good written and oral communication skills. \n  A positive, solution-oriented approach to problem-solving. \n  Ability to learn and adapt to new technology systems. \n  Ability to translate complex problems clearly and in non-technical terms. \n  Ability to work both independently and as a team member in a fast-paced remote environment. \n  Experience with ETL processes or tools is preferred. \n  Experience with REST, RESTful, Swagger, or OpenAPI specifications is preferred. ",
        "techs": [
            "dynamics 365 ams",
            "github",
            "azure devops",
            "microsoft project",
            "microsoft planner",
            "asana",
            "c",
            "c#",
            "c++",
            "rust",
            "css",
            "javascript",
            "python",
            "r",
            "dax",
            "sql",
            "agile",
            "devops",
            "ci/cd",
            "etl",
            "rest",
            "restful",
            "swagger",
            "openapi"
        ],
        "cleaned_techs": [
            "dynamics 365 ams",
            "github",
            "azure",
            "microsoft project",
            "microsoft planner",
            "asana",
            "c",
            "c#",
            "c++",
            "rust",
            "css",
            "javascript",
            "python",
            "r",
            "dax",
            "sql",
            "agile",
            "devops",
            "ci/cd",
            "etl",
            "rest",
            "restful",
            "swagger",
            "openapi"
        ]
    },
    "9eda8e07bb622948": {
        "terms": [
            "data science"
        ],
        "salary_min": 147000.0,
        "salary_max": 189000.0,
        "title": "Senior Manager, Content Marketing",
        "company": "Domino Data Lab",
        "desc": "Who we are \n  Domino Data Lab powers model-driven businesses with its leading Enterprise AI platform trusted by over 20% of the Fortune 100. Domino accelerates the development and deployment of data science work while increasing collaboration and governance. With Domino, enterprises worldwide can develop better medicines, grow more productive crops, build better cars, and much more. Founded in 2013, Domino is backed by Coatue Management, Great Hill Partners, Highland Capital, Sequoia Capital and other leading investors. \n  What we are building \n  The content marketing group at Domino is growing. While AI technology is progressing in leaps and bounds, good content remains few and far. Our goal is to provide AI and data science professionals, leaders, and executives with trusted opinions, reliable information, and proven best practices. Our enterprise customers love the Domino platform as it offers a toolset to thrive with. We now need to increase market awareness of our platform, drive more engagement, and help them understand the value of Domino. Join us and have an immediate impact in building a world-class content engine fueling marketing success \n  The Senior Content Marketing Manager will drive the creation, curation, and evolution of content across multiple platforms to support Domino's strategic plan. The right candidate is an experienced marketer. They are a highly creative and productive writer who crafts and executes content marketing strategies. They will manage the content lifecycle and monitor audience and content metrics. They will help define content strategies that speak to our key audiences in the AI and data science spaces. This role will be pivotal in driving brand awareness, customer engagement, and lead generation through captivating content across all digital platforms. \n  The ideal candidate will have an understanding of data science and artificial intelligence technology. They will be a thought partner to the Sr. Director, Head of Content to define and drive the overall strategic content plan. The plan will span digital and print content, online presence, and events. As AI is a rapidly changing space, we can guarantee no dull moments: the senior manager will frequently learn new concepts and be responsible to explaining them to Domino's audiences in their language and level. If you're excited about AI's potential, this is your opportunity to make an impact with a market leader. \n  What your impact will be \n  In your first year, your impact will be\u2026 \n \n Developing an understanding of Domino, its audience, and its capabilities \n Creating and managing the production of high-quality content including blogs, ebooks, white papers, infographics, videos, and webinars \n Monitoring and improving content search engine and organic traffic performance \n Overseeing Domino's content production processes and editorial calendar, tracking progress and ensuring timely publication across all platforms \n Defining Domino's tone of voice \n Providing an editorial review and quality control for content produced at Domino \n \n What we look for in this role \n \n 5-8 years of content marketing experience, with a proven track record authoring assets across blog posts, white papers, webinars, and more \n Project management experience \n Technology company background and familiarity with analytics, data science, or AI concepts \n Self-starter with initiative to ideate, collaborate, and take ownership of tasks \n Team player who thrives Align with peers across PR, demand generation, events, and partnerships to maximize the reach and effectiveness of content \n Creative and strategic thinking, translating business goals into content initiatives that leverage a single asset into multiple consumption formats \n Flexibility to pivot between assignments and themes based on changing business needs or market conditions \n Passion for ongoing learning, research, and investigation \n Openness to leveraging AI tools to accelerate content creation \n Pragmatic attitude without sacrificing quality; publishing good and iterating to great. \n Content Creation: Excellent writing and editing skills demonstrating attention to detail, ability to spot errors and misses in copy or messaging, and ability to turn in error-free content \n SEO Knowledge: Deep understanding of SEO best practices to ensure content is discoverable and ranks well on search engines \n Trend Awareness: Staying updated with the latest content marketing trends, platform changes, and industry news \n Audience Empathy: Ability to conduct audience research and understand their preferences, pain points, and behaviors to inform content creation \n \n What we value \n \n We strongly believe in the value of growing a diverse team and encourage people of all backgrounds, genders, ethnicities, abilities, and sexual orientations to apply \n We value a growth mindset. High-performing creative individuals who dig into problems and see the opportunities for success \n We believe in individuals who seek truth and speak the truth and can be their whole selves at work \n We value all of you that believe improving is always possible. At Domino, everything is a work in progress \u2013 we can do better at everything \n We emphasize an environment of teaching and learning to equip employees with the tools needed to be successful in their function and the company \n \n \n #LI-Remote \n \n \n  Based on pay transparency guidelines, a reasonable expectation for the salary range is listed below. Information on our competitive total rewards package, including our benefits can be found  \n here \n . Individual salaries are determined by evaluating a variety of factors including geography, cost of labor, experience, skills, education, and internal equity. \n \n  Compensation Range  \n \n   $147,000\u2014$189,000 USD",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "fc4ba5c0588ad045": {
        "terms": [
            "data science"
        ],
        "salary_min": 140000.0,
        "salary_max": -1.0,
        "title": "AWS Developer ( With Artificial Intelligence and Machine Learning Experienc",
        "company": "Trigent Solutions",
        "desc": "Title: AWS Developer ( With Artificial Intelligence and Machine Learning Experience) \n Location: 100% Remote \n Client: FDA \n Duration: Full Time. \n Requirements: \n \n Experience with Artificial Intelligence and Machine Learning is a must. \n AWS Certified Solutions Architect and AWS Certified Developer certifications are highly desirable. \n Proven experience as an AWS Architect/Developer with at least 10 years of hands-on experience in designing and implementing AWS solutions. \n Strong proficiency in AWS services, including EC2, S3, RDS, Lambda, VPC, and more. \n Experience with infrastructure as code (IaC) tools such as AWS CloudFormation, Terraform, or similar. \n Familiarity with security and compliance best practices in AWS. \n Excellent problem-solving and communication skills. \n Strong teamwork and collaboration skills. \n \n Job Type: Full-time \n Pay: From $140,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n Paid time off \n Vision insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Application Question(s): \n \n How much experience do have with AWS. \n How much experience do have with Machine Learning \n How much experience do have with Artificial Intelligence \n \n Work Location: Remote",
        "cleaned_desc": " AWS Certified Solutions Architect and AWS Certified Developer certifications are highly desirable. \n Proven experience as an AWS Architect/Developer with at least 10 years of hands-on experience in designing and implementing AWS solutions. \n Strong proficiency in AWS services, including EC2, S3, RDS, Lambda, VPC, and more. \n Experience with infrastructure as code (IaC) tools such as AWS CloudFormation, Terraform, or similar. \n Familiarity with security and compliance best practices in AWS. \n Excellent problem-solving and communication skills. \n Strong teamwork and collaboration skills. ",
        "techs": [
            "aws certified solutions architect",
            "aws certified developer",
            "aws architect/developer",
            "aws cloudformation",
            "terraform",
            "ec2",
            "s3",
            "rds",
            "lambda",
            "vpc"
        ],
        "cleaned_techs": [
            "aws",
            "terraform",
            "ec2",
            "s3",
            "rds",
            "lambda",
            "vpc"
        ]
    },
    "1314804938af107b": {
        "terms": [
            "data science"
        ],
        "salary_min": 127645.6,
        "salary_max": 161627.78,
        "title": "Principal Statistical Programmer \u2013 Early Dev",
        "company": "SimulStat",
        "desc": "To Apply for this Job Click Here \n \n  Qualifications \n \n \n \n At least 8 years of hands-on experience in Stat Programming, with no employment gaps \n Phase 1, early development experience strongly desired \n At least 4 years hands-on experience working on SDTM and ADaM data sets \n Hands-on experience in oncology and submissions (including preparing metadata for a study and/or working on an ISS or ISE) \n Strong communication skills, both written and verbal \n Demonstrated attention to detail and focus on quality, supported by examples \n Demonstrated proactivity, supported by examples \n Demonstrated analytical skills, for example by creating or providing significant input into specifications as an author or while programming or conducting QC \n  To Apply for this Job Click Here",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "6a0dd20960b17439": {
        "terms": [
            "data science"
        ],
        "salary_min": 50589.098,
        "salary_max": 64057.066,
        "title": "Machine Learning Co-Op",
        "company": "Cohere Health",
        "desc": "Company Overview: \n  Cohere Health is illuminating healthcare for patients, their doctors, and all those who are important in a patient's healthcare experience, both in and out of the doctors office. Founded in August, 2019, we are obsessed with eliminating wasteful friction patients and doctors experience in areas that have nothing to do with health and treatment, particularly for diagnoses that require expensive procedures or medications. To that end, we build software that is expressly designed to ensure the appropriate plan of care is understood and expeditiously approved, so that patients and doctors can focus on health, rather than payment or administrative hassles. \n  Opportunity Overview: \n  As a Machine Learning co-op you will get a front row seat in a fast growing company which will undoubtedly advance your career and give the right candidate an accelerated career path. In this role, you'll work with our growing team of world-class engineers, statisticians, and clinical experts to develop and deploy machine learning algorithms that help automate burdensome administrative clinical practices. This is a unique opportunity to join a new engineering team with great ambition and building on modern technology with zero legacy technical debt. \n  Last but not least: People who succeed here are empathetic teammates who are candid, kind, caring, and embody our core values and principles. We believe that diverse, inclusive teams make the most impactful work. Cohere is deeply invested in ensuring that we have a supportive, growth-oriented environment that works for everyone. \n  What you will do: \n \n Work on reliable and scalable production machine learning systems \n Work as a full fledged member of a fast-paced, autonomous, agile team building a growing suite of machine learning capabilities \n Contribute to feature design, development, testing, and delivery of our machine learning models \n Work cross-functionally across diverse stakeholders, including product managers, statisticians, EHR data specialists and physicians. \n Actively participate in development of machine learning models \n \n Your background & requirements: \n \n You are passionate about building quality products and have end-to-end machine learning experience, leading with the right design and development principles \n Experience developing in python, required (NLP/PyTorch experience preferred) \n You have familiarity with common software development practices such as version control, unit testing, and CI/CD \n You are a team player and are interested in working at a fast-paced startup environment \n At least one prior co-op experience working on a machine learning or data science team \n You are enrolled in (MS/PhD) degree program in computer science, machine learning, computational linguistics, statistics, mathematics or similar field \n Prior experience in healthcare and life sciences is a plus, but is not required \n \n We can't wait to learn more about you and meet you at Cohere Health! \n  Equal Opportunity Statement: \n  Cohere Health is an Equal Opportunity Employer. We are committed to fostering an environment of mutual respect where equal employment opportunities are available to all. To us, it's personal. \n  #LI-Remote \n  #BI-Remote",
        "cleaned_desc": " You are passionate about building quality products and have end-to-end machine learning experience, leading with the right design and development principles \n Experience developing in python, required (NLP/PyTorch experience preferred) \n You have familiarity with common software development practices such as version control, unit testing, and CI/CD \n You are a team player and are interested in working at a fast-paced startup environment \n At least one prior co-op experience working on a machine learning or data science team ",
        "techs": [
            "python",
            "nlp",
            "pytorch",
            "version control",
            "unit testing",
            "ci/cd",
            "machine learning",
            "data science"
        ],
        "cleaned_techs": [
            "python",
            "nlp",
            "pytorch",
            "version control",
            "unit testing",
            "ci/cd",
            "data science"
        ]
    },
    "5cbc2bebd536f96a": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "201 Third Street (61049), United States of America, San Francisco, California\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "201 Third Street (61049), United States of America, San Francisco, California\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "speech",
            "computer vision"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "speech",
            "computer vision"
        ]
    },
    "3be63ef545aaa65a": {
        "terms": [
            "data science",
            "data engineer",
            "machine learning engineer"
        ],
        "salary_min": 107110.914,
        "salary_max": 135626.28,
        "title": "Data Engineer III, IT",
        "company": "Pediatric Associates",
        "desc": "Remote Position  \n PRIMARY FUNCTION \n The Data Engineer III is a senior level data engineer role and is responsible for designing & building a leading-edge Data & Analytics platform for enabling value-based healthcare, population health management, and enterprise analytics. Designs, develops, maintains, and supports the cloud-based (Microsoft Azure) big data platform and uses modern data engineering design patterns and tools. \n ESSENTIAL DUTIES AND RESPONSIBILITIES \n This list may not include all of the duties that may be assigned. \n \n \n Owns solution design blueprints and architecture of the enterprise data platform features and functionality, including data ingestions, data integrations, data pipelines, data models, data quality, data governance. \n \n \n Plays technical leadership role and leads other team members and guides them on solution design blueprints, data solutions development, and best practices for our enterprise data platform. \n \n \n Designs, builds and maintains scalable, automated data pipelines to enable Reporting, Data Visualization, Advanced Analytics, Data Science, and Machine Learning solutions. \n \n \n Supports critical data pipelines with a scalable distributed architecture, including data ingestion (streaming, events, and batch), data integration (ETL, ELT, Azure Data Factory), and distributed data processing using Databricks Data & Analytics and Azure Cloud Technology Stacks. \n \n \n Builds cloud data solutions using multiple technologies, such as SQL, Python, Data Lake (Databricks Delta Lake), Cloud Data Warehouse (Azure Synapse), RDBMS, NoSQL databases. \n \n \n Understands and implements best practices in managing data, including master data, reference data, metadata, data quality, and lineage. \n \n \n Deploys, automates, maintains, and manages cloud-based production systems to ensure the availability, performance, scalability, and security of production systems. \n \n \n Engages with cross-functional stakeholders to identify pain points, business and technical requirements, and to design data solutions using best-practice patterns and modern architecture. \n \n \n Owns end-to-end design and development, testing, the release of critical components using Databricks technology stack and Microsoft Azure cloud platforms and services. \n \n \n Performs other duties as assigned.  \n \n \n QUALIFICATIONS \n EDUCATION:   Minimum BA or BS degree in Computer Science, Information Systems, or related field required. MS in Business Analytics or related discipline preferred.   \n EXPERIENCE \n \n Minimum 6 years of experience required in creating robust enterprise-grade data engineering pipelines using SQL, Python, Apache Spark, ETL, ELT, Databricks Technology Stack, Azure Cloud Services, Cloud-based Data and Analytics platforms required. 8-10 years preferred.  \n Minimum 3 years of experience required in solution design blueprinting and leading technical team members towards delivery of robust enterprise-grade data platform solutions. \n Strong proficiency in SQL and data analysis required. \n Experience in distributed data (structured, semi-structured, unstructured, streaming) processing techniques using Apache Spark, Hadoop, Hive, Kafka, and big data ecosystem technologies preferred. \n Experience in data modeling and design for data warehouse, relational databases, and NoSQL data stores preferred.  \n \n KNOWLEDGE, SKILLS AND ABILITIES \n \n Familiarity with Data Science and Machine Learning technologies, development process, and common Machine Learning libraries (e.g., Scikit-Learn, Tensorflow). \n Strong problem-solving, critical thinking, verbal, and written communication skills. \n Ability to influence decisions related to advanced analytics strategy & roadmaps, business use cases, and data platform capabilities. \n Effective communication and collaboration with internal cross functional teams, leadership team, technology partners & vendors, and end users. \n Excellent analytical, organizational skills and ability to work in a startup environment and to deliver on tight deadlines using Agile practices. \n Healthcare industry experience highly desired. \n \n TYPICAL WORKING CONDITIONS \n \n Non-patient facing \n May be either full time remote/telework or rotate working in the office and remote/telework \n If remote, this job must be U.S. based \n Indoor work; professional office environment \n Operating computer \n Reach outward \n \n OTHER PHYSICAL REQUIREMENTS \n \n Vision \n Sense of sound \n Sense of touch \n Enter all here \n \n PERFORMANCE REQUIREMENTS \n Adhere to all organizational information security policies and protect all sensitive information including but not limited to ePHI and PHI (Protected Health Information) in accordance with organizational policy, Federal, State, and local regulations. \n \n \n \n \n \n \n \n \n \n \n \n  Location: Pediatric Associates \u00b7 Data & Analytics\n   Schedule: Full Time, Days",
        "cleaned_desc": "Remote Position  \n PRIMARY FUNCTION \n The Data Engineer III is a senior level data engineer role and is responsible for designing & building a leading-edge Data & Analytics platform for enabling value-based healthcare, population health management, and enterprise analytics. Designs, develops, maintains, and supports the cloud-based (Microsoft Azure) big data platform and uses modern data engineering design patterns and tools. \n ESSENTIAL DUTIES AND RESPONSIBILITIES \n This list may not include all of the duties that may be assigned. \n \n \n Owns solution design blueprints and architecture of the enterprise data platform features and functionality, including data ingestions, data integrations, data pipelines, data models, data quality, data governance. \n \n \n Plays technical leadership role and leads other team members and guides them on solution design blueprints, data solutions development, and best practices for our enterprise data platform. \n \n \n Designs, builds and maintains scalable, automated data pipelines to enable Reporting, Data Visualization, Advanced Analytics, Data Science, and Machine Learning solutions. \n \n \n Supports critical data pipelines with a scalable distributed architecture, including data ingestion (streaming, events, and batch), data integration (ETL, ELT, Azure Data Factory), and distributed data processing using Databricks Data & Analytics and Azure Cloud Technology Stacks.   \n \n Builds cloud data solutions using multiple technologies, such as SQL, Python, Data Lake (Databricks Delta Lake), Cloud Data Warehouse (Azure Synapse), RDBMS, NoSQL databases. \n \n \n Understands and implements best practices in managing data, including master data, reference data, metadata, data quality, and lineage. \n \n \n Deploys, automates, maintains, and manages cloud-based production systems to ensure the availability, performance, scalability, and security of production systems. \n \n \n Engages with cross-functional stakeholders to identify pain points, business and technical requirements, and to design data solutions using best-practice patterns and modern architecture. \n \n \n Owns end-to-end design and development, testing, the release of critical components using Databricks technology stack and Microsoft Azure cloud platforms and services. \n \n   Performs other duties as assigned.  \n \n \n QUALIFICATIONS \n EDUCATION:   Minimum BA or BS degree in Computer Science, Information Systems, or related field required. MS in Business Analytics or related discipline preferred.   \n EXPERIENCE \n \n Minimum 6 years of experience required in creating robust enterprise-grade data engineering pipelines using SQL, Python, Apache Spark, ETL, ELT, Databricks Technology Stack, Azure Cloud Services, Cloud-based Data and Analytics platforms required. 8-10 years preferred.  \n Minimum 3 years of experience required in solution design blueprinting and leading technical team members towards delivery of robust enterprise-grade data platform solutions. \n Strong proficiency in SQL and data analysis required. \n Experience in distributed data (structured, semi-structured, unstructured, streaming) processing techniques using Apache Spark, Hadoop, Hive, Kafka, and big data ecosystem technologies preferred. \n Experience in data modeling and design for data warehouse, relational databases, and NoSQL data stores preferred.  \n \n KNOWLEDGE, SKILLS AND ABILITIES \n \n Familiarity with Data Science and Machine Learning technologies, development process, and common Machine Learning libraries (e.g., Scikit-Learn, Tensorflow). \n Strong problem-solving, critical thinking, verbal, and written communication skills. ",
        "techs": [
            "sql",
            "python",
            "apache spark",
            "etl",
            "elt",
            "databricks technology stack",
            "azure cloud services",
            "cloud-based data and analytics platforms",
            "distributed data processing",
            "hadoop",
            "hive",
            "kafka",
            "data modeling",
            "data warehouse",
            "relational databases",
            "nosql data stores",
            "data science",
            "machine learning",
            "scikit-learn",
            "tensorflow"
        ],
        "cleaned_techs": [
            "sql",
            "python",
            "apache spark",
            "etl",
            "elt",
            "databricks technology stack",
            "azure",
            "cloud-based data and analytics platforms",
            "distributed data processing",
            "hadoop",
            "hive",
            "kafka",
            "data warehouse",
            "relational databases",
            "nosql",
            "data science",
            "scikit-learn",
            "tensorflow"
        ]
    },
    "ab52012039dea724": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": 165408.58,
        "salary_max": 209444.11,
        "title": "Software Engineer, Machine Learning SDK - US (Remote)",
        "company": "Weights & Biases",
        "desc": "At Weights & Biases, our mission is to build the best developer tools for machine learning. Weights & Biases is a series C company with $250 million in funding and a rapidly growing user base. Our platform is an essential piece of the daily work for machine learning engineers, from academic research institutions like FAIR and UC Berkeley to massive enterprise teams including iRobot, OpenAI, Toyota Research Institute, Samsung, NVIDIA, Salesforce, Blue Cross Blue Shield, Lyft, and more.\n  \n \n \n  The Machine Learning Software Engineer, reporting to the SDK Team Manager, will be responsible for building the software development kit (SDK) which helps our users accelerate their machine learning projects and build better models faster. The role involves exploring the pain points experienced by Machine Learning Engineers who are building state-of-the-art models, designing solutions to address these issues, and implementing efficient and intuitive interfaces which integrate cleanly into the latest Machine Learning frameworks. The role involves working at the intersection of software development, machine learning, and novel product development.\n  \n Responsibilities \n \n  Collaborate with senior engineers to design and build a multi-platform / multi-language Machine Learning SDK in a public github repository \n  Build out support for advanced machine learning workflows, such as distributed training \n  Build, extend, and enhance a highly reliable and performant data ingest SDK \n  Design thoughtful solutions to capture user issues in different client setups, and capture the pertinent info for debugging \n  Creatively improve on current features, and seek out ways to simplify the API and streamline the user experience \n  Improve CLI and SDK interfaces, implement new integrations with popular ML frameworks as they're released \n  Keep up with the latest trends in the ML world and leverage existing tools and frameworks whenever necessary \n \n  Requirements \n \n  Communication and collaboration on cross-functional projects \n  Creative problem solving, and a willingness to dig into details \n  4+ years of professional software development, especially in Python \n  Experience contributing to architecture and systems design \n  Experience with ML frameworks such as PyTorch or TensorFlow \n \n  Our Benefits  \n \n \\uD83C\\uDFDD\ufe0f Flexible time off  \n \\uD83E\\uDE7A Medical, Dental, and Vision for employees and Family Coverage \n  \\uD83C\\uDFE0 Remote first culture with in-office flexibility in San Francisco \n  \\uD83D\\uDCB5 Home office budget with a new high-powered laptop \n  \\uD83E\\uDD47 Truly competitive salary and equity \n  \\uD83D\\uDEBC 12 weeks of Parental leave (U.S. specific) \n  \\uD83D\\uDCC8 401(k) (U.S. specific) \n  Supplemental benefits may be available depending on your location  \n Explore benefits by country \n \n \n   We encourage you to apply even if your experience doesn't perfectly align with the job description as we seek out diverse and creative perspectives. Team members who love to learn and collaborate in an inclusive environment will flourish with us. We are an equal opportunity employer and do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. If you need additional accommodations to feel comfortable during your interview process, reach out at careers@wandb.com.\n  \n \n \n  #LI-Remote\n  \n \n \n  We encourage you to apply even if your experience doesn't perfectly align with the job description as we seek out diverse and creative perspectives. Team members who love to learn and collaborate in an inclusive environment will flourish with us. We are an equal opportunity employer and do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. If you need additional accommodations to feel comfortable during your interview process, reach out at careers@wandb.com.\n  \n \n \n  #LI-Remote",
        "cleaned_desc": "At Weights & Biases, our mission is to build the best developer tools for machine learning. Weights & Biases is a series C company with $250 million in funding and a rapidly growing user base. Our platform is an essential piece of the daily work for machine learning engineers, from academic research institutions like FAIR and UC Berkeley to massive enterprise teams including iRobot, OpenAI, Toyota Research Institute, Samsung, NVIDIA, Salesforce, Blue Cross Blue Shield, Lyft, and more.\n  \n \n \n  The Machine Learning Software Engineer, reporting to the SDK Team Manager, will be responsible for building the software development kit (SDK) which helps our users accelerate their machine learning projects and build better models faster. The role involves exploring the pain points experienced by Machine Learning Engineers who are building state-of-the-art models, designing solutions to address these issues, and implementing efficient and intuitive interfaces which integrate cleanly into the latest Machine Learning frameworks. The role involves working at the intersection of software development, machine learning, and novel product development.\n  \n Responsibilities \n \n  Collaborate with senior engineers to design and build a multi-platform / multi-language Machine Learning SDK in a public github repository    Build out support for advanced machine learning workflows, such as distributed training \n  Build, extend, and enhance a highly reliable and performant data ingest SDK \n  Design thoughtful solutions to capture user issues in different client setups, and capture the pertinent info for debugging \n  Creatively improve on current features, and seek out ways to simplify the API and streamline the user experience \n  Improve CLI and SDK interfaces, implement new integrations with popular ML frameworks as they're released \n  Keep up with the latest trends in the ML world and leverage existing tools and frameworks whenever necessary \n \n  Requirements \n ",
        "techs": [
            "weights & biases",
            "sdk",
            "fair",
            "uc berkeley",
            "irobot",
            "openai",
            "toyota research institute",
            "samsung",
            "nvidia",
            "salesforce",
            "blue cross blue shield",
            "lyft",
            "machine learning frameworks",
            "multi-platform",
            "multi-language",
            "public github repository",
            "distributed training",
            "data ingest sdk",
            "cli",
            "api",
            "ml frameworks"
        ],
        "cleaned_techs": [
            "weights & biases",
            "sdk",
            "fair",
            "uc berkeley",
            "irobot",
            "openai",
            "toyota research institute",
            "samsung",
            "nvidia",
            "salesforce",
            "blue cross blue shield",
            "lyft",
            "machine learning frameworks",
            "multi-platform",
            "multi-language",
            "public github repository",
            "distributed training",
            "data ingest sdk",
            "cli",
            "api",
            "ml frameworks"
        ]
    },
    "0998f5fdeacfc5dd": {
        "terms": [
            "data science",
            "machine learning engineer",
            "mlops"
        ],
        "salary_min": 116180.875,
        "salary_max": 147110.88,
        "title": "Mid MLOps Engineer",
        "company": "EX2 Outcoding",
        "desc": "Become an Outcoder as a Machine Learning Ops Engineer: \n  Position Summary \n  We are seeking a skilled and experienced Software Engineer (contractor) focusing on Large Language Models (LLMs) to develop and test extractive and generative AI capabilities to create impact for our clients. In this role, you will be responsible for developing, testing, and improving applications that can assist our customers, which span a variety of industries. \n What you\u2019ll need to be successful:  \n \n Bachelor\u2019s or Master\u2019s degree in Computer Science, Artificial Intelligence, or related experience \n \n \n 5+ years of Software Engineering experience, including AI engineering and data science.   \n \n \n Expertise in one or more of the following: AWS, Google Cloud, Microsoft Azure   \n Recent experience in evaluating, building, and testing Large Language Models (LLMs) in data or application workflows is a plus.   \n \n \n Strong proficiency in programming languages such as Python, Javascript, Flask ( or other Web development frameworks )   \n \n \n Familiarity with large-scale distributed computing systems and cloud platforms.    \n Strong analytical, problem-solving, and communication skills.   \n Experience with data science and development tooling, especially visual tooling, is a plus.   Ability to quickly learn, understand, and work with new emerging technologies, methodologies, and solutions in the Cloud/IT technology space. \n  \n This will be considered a plus:     \n \n Strong knowledge of AI and machine learning solutions and techniques, including deep learning, natural language processing, MLOps frameworks, and machine learning architectures (e.g., Deep Learning).  \n Experience architecting and developing software or infrastructure for scalable, distributed systems and machine learning technologies. \n Experience with data science and development tooling, especially visual tooling. \n Understanding of responsible AI practice. \n \n  About us:  EX\u00b2 Outcoding is a premier solution provider of a broad range of outsourcing services, combining proven expertise in technology and project execution for companies searching for high-quality software development solutions. We specialize in delivering the best technical solution and enhancing that solution creatively by working closely with stakeholders to understand the business context. \n  #LI-NORELA #LI-REMOTE",
        "cleaned_desc": " \n \n 5+ years of Software Engineering experience, including AI engineering and data science.   \n \n \n Expertise in one or more of the following: AWS, Google Cloud, Microsoft Azure     Recent experience in evaluating, building, and testing Large Language Models (LLMs) in data or application workflows is a plus.   \n \n \n Strong proficiency in programming languages such as Python, Javascript, Flask ( or other Web development frameworks )   \n \n   Familiarity with large-scale distributed computing systems and cloud platforms.    \n Strong analytical, problem-solving, and communication skills.   \n Experience with data science and development tooling, especially visual tooling, is a plus.   Ability to quickly learn, understand, and work with new emerging technologies, methodologies, and solutions in the Cloud/IT technology space. \n  \n This will be considered a plus:     \n   Strong knowledge of AI and machine learning solutions and techniques, including deep learning, natural language processing, MLOps frameworks, and machine learning architectures (e.g., Deep Learning).  \n Experience architecting and developing software or infrastructure for scalable, distributed systems and machine learning technologies. \n Experience with data science and development tooling, especially visual tooling. \n Understanding of responsible AI practice. \n \n  About us:  EX\u00b2 Outcoding is a premier solution provider of a broad range of outsourcing services, combining proven expertise in technology and project execution for companies searching for high-quality software development solutions. We specialize in delivering the best technical solution and enhancing that solution creatively by working closely with stakeholders to understand the business context. ",
        "techs": [
            "aws",
            "google cloud",
            "microsoft azure",
            "python",
            "javascript",
            "flask",
            "large-scale distributed computing systems",
            "cloud platforms",
            "visual tooling",
            "ai",
            "machine learning",
            "deep learning",
            "natural language processing",
            "mlops frameworks",
            "machine learning architectures",
            "responsible ai practice"
        ],
        "cleaned_techs": [
            "aws",
            "gcp",
            "microsoft azure",
            "python",
            "javascript",
            "flask",
            "large-scale distributed computing systems",
            "cloud platforms",
            "visual tooling",
            "ai",
            "nlp",
            "mlops frameworks",
            "machine learning architectures",
            "responsible ai practice"
        ]
    },
    "fc2a1fcacc97b117": {
        "terms": [
            "data science"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Brokerage Ops Specialist",
        "company": "Afterpay",
        "desc": "Company Description \n \n \n \n     It all started with an idea at Block in 2013. Initially built to take the pain out of peer-to-peer payments, Cash App has gone from a simple product with a single purpose to a dynamic ecosystem, developing unique financial products, including Afterpay/Clearpay, to provide a better way to send, spend, invest, borrow and save to our 47 million monthly active customers. We want to redefine the world\u2019s relationship with money to make it more relatable, instantly available, and universally accessible.\n     \n  Today, Cash App has thousands of employees working globally across office and remote locations, with a culture geared toward innovation, collaboration and impact. We\u2019ve been a distributed team since day one, and many of our roles can be done remotely from the countries where Cash App operates. No matter the location, we tailor our experience to ensure our employees are creative, productive, and happy.\n     \n  Check out our locations, benefits, and more at cash.app/careers.\n    \n \n \n \n \n  Job Description \n \n \n  We've recently introduced Cash App Investing and now you can instantly buy stock in your favorite companies with as little as $1 using our free fractional trading feature! We're looking for a world class talent to join our broker-dealer subsidiary, Cash App Investing LLC. The ideal candidate for this role is someone who has deep experience in brokerage operations. \n  You will: \n \n  This role is within the account operations team at Cash App Investing and reports to the brokerage operations accounts lead. \n  Provide subject matter expertise on strategic initiatives related to brokerage functions. \n  Document standard operating procedures and job aids and update as needed. \n  Identify areas for process improvement and work toward implementation with respective teams. \n  Scale processes to operate broker-dealer. \n  Willing to work and get comfortable with ambiguity in a fast paced environment. \n  Works closely with peers to provide support in functional areas of operation and provides feedback \n  Reviews and actions accounts in accordance with firm policy and procedure. \n  Assist in establish controls, metrics, and data in partnership with Data Science for Brokerage operational use. \n  Collaborates with lead, peers, and cross functional teams to identify areas of opportunity and risks. \n  Performs and completes daily operational processing functions and maintains records in accordance with firms Written Supervisory Procedures and operational processes. \n  High-level understanding of SEC / FINRA regulations that pertain to the subject matter. \n  Collaborates with engineering, support, and product teams to facilitate improvements and increase efficiency within the current back office structure for the broker-dealer\u2019s risk operation. \n \n \n \n \n \n  Qualifications \n \n \n  You have: \n \n  5+ years of experience in brokerage operations. \n  Active FINRA Licenses: Series 7 & 63 required. \n  Fundamental SQL knowledge/experience \n  Functional experience with Looker / Tableau or other data analytics tools. \n  Brokerage operations generalist, you have worked in various functions of a brokerage operations team which may include accounts, risk, fraud, trading, etc. \n  Knowledge and respect for SEC/FINRA regulatory and compliance rules and regulations. \n  Well versed in collaborating with peer disciplines such as Product, Engineering, Data Science, Compliance etc \n  Technical experience scaling processes or managing projects. \n  Highly organized and detail oriented. \n  Someone who thrives in a highly collaborative and fast-paced environment. \n  Excellent verbal and written communication skills. \n  A passion for our mission of economic empowerment through serving the underserved \n \n \n \n \n \n  Additional Information \n \n \n  Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate\u2019s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.    Zone A: USD $94,400 - USD $115,400  Zone B: USD $87,800 - USD $107,400  Zone C: USD $80,300 - USD $98,100  Zone D: USD $70,800 - USD $86,600 \n \n  To find a location\u2019s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information. \n  Full-time employee benefits include the following: \n \n  Healthcare coverage (Medical, Vision and Dental insurance) \n  Health Savings Account and Flexible Spending Account \n  Retirement Plans including company match \n  Employee Stock Purchase Program \n  Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance \n  Paid parental and caregiving leave \n  Paid time off (including 12 paid holidays) \n  Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees) \n  Learning and Development resources \n  Paid Life insurance, AD&D, and disability benefits \n  Additional Perks such as WFH reimbursements and free access to caregiving, legal, and discounted resources \n \n  These benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans. \n  US and Canada EEOC Statement \n  We\u2019re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. \n  We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible.  Want to learn more about what we\u2019re doing to build a workplace that is fair and square? \n  Additionally, we consider qualified applicants with criminal histories for employment on our team, and always assess candidates on an individualized basis. \n \n \n  Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.",
        "cleaned_desc": " \n \n \n \n  Qualifications \n \n \n  You have: \n \n  5+ years of experience in brokerage operations. \n  Active FINRA Licenses: Series 7 & 63 required. \n  Fundamental SQL knowledge/experience \n  Functional experience with Looker / Tableau or other data analytics tools. \n  Brokerage operations generalist, you have worked in various functions of a brokerage operations team which may include accounts, risk, fraud, trading, etc. \n  Knowledge and respect for SEC/FINRA regulatory and compliance rules and regulations. \n  Well versed in collaborating with peer disciplines such as Product, Engineering, Data Science, Compliance etc \n  Technical experience scaling processes or managing projects. ",
        "techs": [
            "sql",
            "looker",
            "tableau"
        ],
        "cleaned_techs": [
            "sql",
            "looker",
            "tableau"
        ]
    },
    "2766e6efdad0e143": {
        "terms": [
            "data science",
            "data engineer"
        ],
        "salary_min": 101964.336,
        "salary_max": 129109.57,
        "title": "Data Engineer",
        "company": "AutoAlert",
        "desc": "Data Engineer \n  Location \u2013 Remote\n   Travel Required \u2013 Minimal\n   Classification \u2013 Exempt\n   Salary Range - $90,000 to $113,000\n   Leadership Level \u2013 GS 5 to GS 7\n  \n \n \n Core Competencies associated with this role: Analytical Skills, Continuous Learning, Cooperation, Job Knowledge, Problem Solving, Quality, Teamwork, Use of Technology \n \n  AutoAlert is the original disruptor in the automotive software space. Our Customer Experience Management (CXM) platform is revolutionizing the dealership-customer relationship, creating direct opportunities for meaningful connections and seamless experiences both online and offline.\n  \n \n About AutoAlert \n  AutoAlert offers a portfolio of innovative solutions that maximize dealership profitability by focusing on customer relationships and creating direct opportunities for meaningful data-driven connections. Founded in 2002, AutoAlert is the leading automotive software and data mining provider, enhancing customer relationships that lead to repeat sales, service, and increased loyalty. AutoAlert\u2019s mission is to empower innovative automotive partnerships to improve data-driven customer experiences. AutoAlert is proud to lead the industry in data security, with independently audited high-level security in place via Soc 2 Type 2 and ISO/IEC 27001:2013 certifications.\n  \n \n Role Description \n  At AutoAlert, the Data Engineer will manage the intake, export, and preservation of data as it moves throughout the company. The Data Engineer is expected to have technical skills working with many different relational and non-relational database management systems and have knowledge of how to move data between those systems. ETL is a top-line priority which can span Databases, Big Data architectures, Data Lakes and Datamarts. This role involves execution on data distribution for downstream analysis by Data Scientists and Data Analysts. Data Engineers must balance requirements and current/future use within Data Governance procedures and guidelines. The role provides solutions for errors, data anomalies, system events, reliability, efficiency and quality. \n  \n  You are a team player with strong problem-solving skills and an innovative mindset. You are part of a creative, diverse, collaborative team of hardworking individuals with the goal of increasing brand favorability, customer engagement and advocacy, and revenue growth. \n  \n \n Collaborate with the Data Science team, using Scrum to implement the Software Development Life Cycle (SDLC) and Data Science Life Cycle (DSL) \n  Deliver data related objectives within the team \n  Design Python programs geared towards ETL/ELT \n  Interact with the team by performing code reviews and QA \n  Help drive product vision/enhancement requests \n  Collaborate with other team members and product stakeholders \n  Assemble and model industry best-practices in order to foster best-in-class software development within and across departments \n  Quickly understand functional requirements and work with the team to provide solutions \n  Understand the need for time-to-market \n  Other duties as assigned \n \n \n  Your Experience and Impact \n \n  Bachelor\u2019s degree in Data Science, Computer Science, Software Development, Data or other technical discipline preferred \n  2-5 years of Data Engineering work \n  Strong proficiency in Python is required \n  Strong proficiency in Linux, command line tools, and scripting \n  Strong proficiency with ETL/data transformations \n  Exposure to Amazon Web Services (AWS) \n  Working knowledge of MPPs, ideally AWS Redshift \n  Experience with RDBMS, ideally PostgreSQL and MSSQL \n  Experience with back-end application support \n  Demonstrated proficiency with Data Warehousing design and implementation \n  Experience working with very large data sets \n  Experience working with horizontally scalable, distributed architectures \n \n \n  Supervisory Responsibilities \n  This position has no direct supervisory responsibilities but works with other team members inside and outside of the department\n  \n \n Across all teams, we look for the following Values: \n \n  Be a Role Model \n  Be Passionate About our Partners\u2019 Success \n  Own Working Together \n  Deliver Results \n \n  Living the AutoAlert values is core to all team members\u2019 success. We welcome and encourage all people of diverse backgrounds, experiences, abilities, and perspectives. We are an equal opportunity employer, focused on providing a positive place for you to grow your career.\n  \n \n Accommodations \n  If you require assistance applying for open positions, please reach out to Human Resources at hr@autoalert.com.\n  \n \n Benefits \n  AutoAlert provides a robust benefits package to eligible employees. Eligibility requirements apply to all plans in the United States and Canada. AutoAlert reserves the right to alter benefits offerings at will.\n  \n \n Posting Statement \n  AutoAlert is an Equal Opportunity Employer. Qualified applicants will receive consideration for employment regardless of race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. \n  \n  AutoAlert does not accept unsolicited headhunter and agency resumes. AutoAlert will not pay any third-party agency or company that does not have a signed agreement with AutoAlert. \n  \n  We are unable to sponsor H1-B Visas at this time.",
        "cleaned_desc": "  AutoAlert offers a portfolio of innovative solutions that maximize dealership profitability by focusing on customer relationships and creating direct opportunities for meaningful data-driven connections. Founded in 2002, AutoAlert is the leading automotive software and data mining provider, enhancing customer relationships that lead to repeat sales, service, and increased loyalty. AutoAlert\u2019s mission is to empower innovative automotive partnerships to improve data-driven customer experiences. AutoAlert is proud to lead the industry in data security, with independently audited high-level security in place via Soc 2 Type 2 and ISO/IEC 27001:2013 certifications.\n  \n \n Role Description \n  At AutoAlert, the Data Engineer will manage the intake, export, and preservation of data as it moves throughout the company. The Data Engineer is expected to have technical skills working with many different relational and non-relational database management systems and have knowledge of how to move data between those systems. ETL is a top-line priority which can span Databases, Big Data architectures, Data Lakes and Datamarts. This role involves execution on data distribution for downstream analysis by Data Scientists and Data Analysts. Data Engineers must balance requirements and current/future use within Data Governance procedures and guidelines. The role provides solutions for errors, data anomalies, system events, reliability, efficiency and quality. \n  \n  You are a team player with strong problem-solving skills and an innovative mindset. You are part of a creative, diverse, collaborative team of hardworking individuals with the goal of increasing brand favorability, customer engagement and advocacy, and revenue growth. \n  \n \n Collaborate with the Data Science team, using Scrum to implement the Software Development Life Cycle (SDLC) and Data Science Life Cycle (DSL) \n  Deliver data related objectives within the team \n  Design Python programs geared towards ETL/ELT \n  Interact with the team by performing code reviews and QA \n  Help drive product vision/enhancement requests \n  Collaborate with other team members and product stakeholders    Assemble and model industry best-practices in order to foster best-in-class software development within and across departments \n  Quickly understand functional requirements and work with the team to provide solutions \n  Understand the need for time-to-market \n  Other duties as assigned \n \n \n  Your Experience and Impact \n \n  Bachelor\u2019s degree in Data Science, Computer Science, Software Development, Data or other technical discipline preferred \n  2-5 years of Data Engineering work \n  Strong proficiency in Python is required \n  Strong proficiency in Linux, command line tools, and scripting \n  Strong proficiency with ETL/data transformations \n  Exposure to Amazon Web Services (AWS) \n  Working knowledge of MPPs, ideally AWS Redshift ",
        "techs": [
            "autoalert",
            "soc 2 type 2",
            "iso/iec 27001:2013",
            "data engineer",
            "etl",
            "databases",
            "big data architectures",
            "data lakes",
            "datamarts",
            "data scientists",
            "data analysts",
            "scrum",
            "software development life cycle",
            "data science life cycle",
            "python",
            "code reviews",
            "qa",
            "product vision/enhancement",
            "industry best-practices",
            "time-to-market",
            "bachelor's degree in data science",
            "computer science",
            "software development",
            "data",
            "python proficiency",
            "linux proficiency",
            "command line tools",
            "scripting",
            "etl/data transformations",
            "amazon web services (aws)",
            "aws redshift"
        ],
        "cleaned_techs": [
            "autoalert",
            "soc 2 type 2",
            "iso/iec 27001:2013",
            "data engineer",
            "etl",
            "databases",
            "big data architectures",
            "data lakes",
            "datamarts",
            "data scientists",
            "data analysts",
            "scrum",
            "software development life cycle",
            "data science life cycle",
            "python",
            "code reviews",
            "qa",
            "product vision/enhancement",
            "industry best-practices",
            "time-to-market",
            "computer science",
            "software development",
            "data",
            "linux proficiency",
            "command line tools",
            "scripting",
            "etl/data transformations",
            "aws"
        ]
    },
    "0ae3c7dcaecf5b20": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java"
        ]
    },
    "e5558f5f8f3a6a81": {
        "terms": [
            "data science",
            "data analyst"
        ],
        "salary_min": 60000.0,
        "salary_max": 93150.0,
        "title": "Platforms Data Analyst",
        "company": "Gannett",
        "desc": "Gannett Co., Inc. (NYSE:  GCI) is a subscription-led and digitally-focused media and marketing solutions company committed to empowering communities to thrive. With an unmatched reach at the national and local level, Gannett touches the lives of millions with our Pulitzer Prize-winning content, consumer experiences and benefits, and advertiser products and services. \n \n  Our current portfolio of media assets includes The USA TODAY NETWORK, which includes USA TODAY, and local media organizations in 43 states in the United States, and Newsquest, a wholly-owned subsidiary operating in the United Kingdom. We also own digital marketing services companies under the brand LocaliQ, which provide a cloud-based platform of products to enable small and medium-sized businesses to accomplish their marketing goals. In addition, our portfolio includes one of the largest media-owned events businesses in the U.S., USA TODAY NETWORK Ventures. \n \n  Gannett open roles are featured on various external job boards. When applying to a position at Gannett, you should be completing an application on Gannett Careers via Dayforce. Job postings directing you to complete an application on other external sites may not be valid. \n \n  To connect with us, visit www.gannett.com \n \n  Gannett is looking for a Data Analyst with journalism experience (required) to monitor and advise content strategies on the performance and innovation of the USA TODAY Network\u2019s work on newsletters, social, messaging, video and other emerging platforms. This analyst will also partner across divisions to develop relevant reporting for new experiments and initiatives USA TODAY and local sites are launching. \n \n  Off-platform content is growing fast across at Gannett and a critical way that we reach and engage with younger readers. We are seeking an analyst that understands the nuance of data reporting for these platforms, the audiences, ad revenue, subscription referral and cohorting these users to understand how their readership affects business goals.This analyst must be nimble and willing to adapt and learn as platforms and new delivery methods shift. \n As a key member of our dynamic Content Strategy/Analytics team, you will utilize your expertise in data analytics, reporting tools, and market insights to enhance the audience engagement, revenue generation and digital subscription growth for USA TODAY and local USA TODAY network sites. \n \n  This role can work remote from anywhere in the US except for Alaska & Hawaii. \n \n \n Responsibilities: \n \n \n Help set and monitor KPIs and dashboards, ensuring accuracy, timeliness, and relevance. \n Work with USA Today network leaders in crafting and monitoring strategy, KPIs for video, platforms and experiments. \n Work with regional strategists and product to analyze performance across multiple user journeys. \n Analytics QA for new product deployments and troubleshooting. \n Develop engagement metrics that allow the business to better understand user behavior based on their entry, product use and propensity to subscribe. \n This is about more than reports and spreadsheets. The strategist will exercise judgment in translating data points into actionable insights. The right candidate deeply understands data tools and knows how to apply their analytical mindset to content. This is a hands-on data job, and digging skills are required. But it's just as important that a candidate understand how research relates to modern users of our products, particularly on mobile devices. \n Excellent communication skills, both written and verbal, to effectively convey findings and recommendations to diverse stakeholders. \n \n Requirements: \n \n \n Bachelor's degree in communication, journalism, marketing, data science or analytics, or an equivalent combination of education and experience working with data. \n 3-5 years of experience in analytics, with a preference for content analytics. Journalism experience is mandatory. \n Expertise in Excel, GA4, Looker/Data Studio, Big Query, SQL, Tableau, Google Ad Manager and other analytic and data tools. \n Expertise in platform-specific data tools such as Emplifi, CrowdTangle, Tubular, Exact Target and other analytic and data tools. \n Collaborate with cross-functional teams to analyze audience and revenue data (both consumer and advertising/affiliate), identify trends, and provide timely recommendations that contribute to informed business decisions. \n Experience identifying, analyzing, and using data to independently test theories, confirm assumptions, and measure success via iteration and with attention to detail. \n Familiarity with news products and journalism and understanding of media analytics and ethics. \n Preferred: A passion and experience with storytelling via social, newsletters. \n Employment is contingent on passing a post-offer pre-employment background check. \n \n Application Instructions: \n  We are eager to learn more about you and how you fit this role. When you apply, don\u2019t limit your upload to a resume; show us what you\u2019ve done. To do so, put together a single document file that includes the following, in this order: \n 1. Your resume \u2013 one to two pages. \n 2. A cover letter that outlines how you would approach the job. \n \n  It is important that these items be assembled into a single document and uploaded in PDF format. Completing these steps will ensure that your application receives the highest consideration. \n #Newsgnt \n #LI-NC1 \n #LI-Remote \n The annualized base salary for this role will range between $60,000 and $93,150. Variable compensation is not reflected in these figures and based on the role, may be applicable. Exact compensation may vary based on skills, experience, location, and union representation, if applicable. \u200b \n \n  Gannett Co., Inc. is a proud equal opportunity employer committed to building and maintaining a diverse workforce. As such, we will consider all qualified applicants for employment and do not discriminate in connection with employment decisions on the basis of an applicant or employee\u2019s race, color, national origin, ethnicity, ancestry, citizenship status, sex, gender, gender identity, gender expression, religion, age, marital status, personal appearance (including height and weight), sexual orientation, family responsibilities, physical or mental disability, medical condition, pregnancy status (including childbirth, breastfeeding or related medical conditions), education, genetic characteristics or information, political affiliation, military or veteran status or other classifications protected by applicable federal, state and local laws in the jurisdictions where Gannett employs employees. In addition, Gannett Co., Inc. will provide applicants who require a reasonable accommodation, as a result of an applicant\u2019s disability or religion, to complete this employment application and/or any other process in connection with an individuals\u2019 application for employment with Gannett Co., Inc. Applicants who require such accommodation should contact Gannett Co., Inc.\u2019s Recruitment Department at Recruit@gannett.com.",
        "cleaned_desc": " Bachelor's degree in communication, journalism, marketing, data science or analytics, or an equivalent combination of education and experience working with data. \n 3-5 years of experience in analytics, with a preference for content analytics. Journalism experience is mandatory. \n Expertise in Excel, GA4, Looker/Data Studio, Big Query, SQL, Tableau, Google Ad Manager and other analytic and data tools. \n Expertise in platform-specific data tools such as Emplifi, CrowdTangle, Tubular, Exact Target and other analytic and data tools. \n Collaborate with cross-functional teams to analyze audience and revenue data (both consumer and advertising/affiliate), identify trends, and provide timely recommendations that contribute to informed business decisions. \n Experience identifying, analyzing, and using data to independently test theories, confirm assumptions, and measure success via iteration and with attention to detail. \n Familiarity with news products and journalism and understanding of media analytics and ethics. \n Preferred: A passion and experience with storytelling via social, newsletters. \n Employment is contingent on passing a post-offer pre-employment background check. \n ",
        "techs": [
            "excel",
            "ga4",
            "looker/data studio",
            "big query",
            "sql",
            "tableau",
            "google ad manager",
            "emplifi",
            "crowdtangle",
            "tubular",
            "exact target"
        ],
        "cleaned_techs": [
            "excel",
            "ga4",
            "looker/data studio",
            "big query",
            "sql",
            "tableau",
            "google ad manager",
            "emplifi",
            "crowdtangle",
            "tubular",
            "exact target"
        ]
    },
    "4946a0ef5c10f4c0": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": 139129.06,
        "salary_max": 176168.4,
        "title": "Principal Generative AI Author",
        "company": "Pluralsight",
        "desc": "Job Description:\n  \n \n   Join Pluralsight on our mission to advance the world's technology workforce and bring high-quality instruction to thousands of technologists across the globe! Pluralsight is currently hiring a Principal Generative AI Author to work full-time creating content core to our technology library. This position will include valuable opportunities to maintain your ties with the technology industry, through participation in research, conferences, and other engagements, as well as an opportunity to help educate the next generation of AI professionals.\n  \n \n \n   Role Requirements\n  \n \n   Pluralsight authors are a unique breed of technical professionals, combining years of in-the-trenches practitioner experience with outstanding teaching and authoring skills. As a Principal Generative AI Author you will be accountable for:\n  \n \n \n \n     Authoring content in accordance with the Pluralsight Generative AI Domain content strategy\n    \n \n \n     Developing educational content of the highest quality to be published on the Pluralsight platform. This content spans the following formats:\n    \n \n \n \n       Video courses\n      \n \n \n \n         Develop and structure course outlines, scenarios, and demo environments\n        \n \n \n         Produce slides to a provided set of standards and practices\n        \n \n \n         Narrate, record, and edit screen captures and slide-based presentations\n        \n \n \n \n       Hands-on labs\n      \n \n \n \n         Engineer content and capabilities in support of hands-on learning associated with video content\n        \n \n \n \n \n \n   Pluralsight authors must embrace the mindset of a \u201clifelong learner\u201d and be capable of learning new techniques, processes, and technologies through continuous and rigorous research and self-directed study. It is expected that the research performed will be presented in the content you create.\n  \n \n \n   Pluralsight authors are also expected to:\n  \n \n \n \n     Work with the Pluralsight Marketing teams to help promote yourself, and Pluralsight, as experts and thought leaders\n    \n \n \n     Partner with the Pluralsight Sales teams on customer engagements to present the unique value proposition of our content offerings and your work\n    \n \n \n     Participate in author mentoring programs\n    \n \n \n \n   As a Principal Generative AI Author it is expected you will meet the following requirements:\n  \n \n \n \n     5+ years of practitioner-level technical experience, ideally in software development, data science, machine learning, or artificial intelligence\n    \n \n \n     Previous experience as an author, content creator, instructor, and/or trainer\n    \n \n \n \n   #LI-SW1\n  \n \n   #LI-Remote",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "ae1a80d6f7cd9740": {
        "terms": [
            "data science"
        ],
        "salary_min": 150000.0,
        "salary_max": -1.0,
        "title": "Senior Engineering Manager",
        "company": "Coalition",
        "desc": "About Us \n \n \n Coalition is the world's first Active Insurance provider designed to help prevent digital risk before it strikes. Founded in 2017, Coalition combines broad insurance coverage with a digital risk assessment and continuous security monitoring to help organizations protect themselves in today\u2019s hyper-connected world. \n  Coalition offers its Active Insurance products in the U.S., U.K., and Canada through relationships with leading global insurers including Allianz, Arch Insurance, Lloyd\u2019s of London, Swiss Re and Zurich, as well as cyber capacity through its own carrier, Coalition Insurance Company. Coalition's Active Risk Platform provides automated security alerts, threat intelligence, expert guidance, and cybersecurity tools to help businesses worldwide remain resilient against cyber attacks.     Coalition comprises a team of cybersecurity and technology experts, as well as experienced insurance professionals, who have come together to build a world-class organization with a massive technological advantage. Our secret sauce is bringing these expertise together to create a world-class organization with one mission:  to protect the unprotected as the world digitizes . Today, Coalition is one of the world\u2019s largest commercial insurtechs serving hundreds of thousands of customers worldwide. \n \n  Since its founding, Coalition has raised $755 million in equity funding, including $250 million in June 2022, affirming its ability to deliver profitable growth and cementing its position as a long-term business with a clear competitive advantage. \n  Coalition\u2019s exceptional growth stems from its ability to address real-world problems for organizations of all sizes, and by remaining true to our founding values of character, humility, responsibility, purpose, authenticity and inclusion. We are proud to have been named among Inc.\u2019s Best Workplaces in 2021 and 2023, and one of Fast Company\u2019s Most Innovative Companies in 2022. \n \n \n About the Role \n \n  The Payment Business Systems team is responsible for a critical area of our business. The team is currently building out a system to accept direct payments from policyholders, which we believe will allow us to work with many smaller insurance brokers and thus increase our market penetration. Team responsibilities include invoicing, taxes and surcharges, commissions, and auditing \u2013 across Netsuite and our custom services. This Sr. Engineering Manager role manages software engineers as well as Netsuite developers.\n  \n Responsibilities \n \n Lead and coordinate engineering activities to successfully deliver product features on time while designing for quality and scalability \n Provide technical expertise and software engineering guidance to automate processes and improve team efficiency \n Provide management and mentorship to engineers in order to seek out identification of skill gaps and growth needs \n Balance managing technical debt with new feature development and business priorities \n Ensure full software lifecycle instrumentation from requirement ideation to software development to deployment \n Collaborate with Product and Business staff to balance resources and project schedules \n Maintain transparent communication across all levels of the organization \n Actively participate in all phases of growing the engineering organization through recruiting, team building, etc \n Report to a senior engineering leader \n \n Skills and Qualifications \n \n 7+ years overall software engineering experience with 4+ years experience managing a team of at least 5 people \n Effective in leading and mentoring staff with various technical backgrounds and expertise \n Understanding of software development processes with the experience to apply the best type of engineering methodology to fit a situation or team dynamic \n Experience building high volume distributed systems in a cloud environment \n Proven track record of successfully delivering software and managing projects across multiple teams \n Sufficient level of technical background to objectively evaluate complex project risks and issues \n Strong analytical, planning, and organizational skills with an ability to manage competing priorities \n Ability to have meticulous attention to requirements while also maintaining awareness of the overall business objectives \n Experience building API-first CRUD applications using Python, React, SQL \n Expert knowledge of at least one area of computer science/engineering \n Excellent oral and written communications skills at all levels \n \n Bonus Points \n \n Prior experience with insurance technologies \n Financial systems, especially Netsuite \n In-depth knowledge of AWS infrastructure, CI/CD, and the Atlassian tool suite \n Experience working in the InfoSec, Tech and Cybersecurity markets \n Experience working with data at scale, data science/analytics, and data collection techniques \n Familiar with building service oriented architectures and distributed systems \n \n Compensation \n  Our compensation reflects the cost of labor across several US geographic markets. The US base salary for this position ranges from $150,000/year in our lowest geographic market up to $245,000/year in our highest geographic market. Consistent with applicable laws, an employee's pay within this range is based on a number of factors, which include but are not limited to relevant education, skills, job-related knowledge, qualifications, work experience, credentials, and/or geographic location. Your recruiter can share more on target salary for your location during the interview process. Coalition, Inc. reserves the right to modify this range as needed. \n  Perks \n \n 100% medical, dental and vision coverage \n Flexible PTO policy \n Annual home office stipend and WeWork access \n Mental & physical health wellness programs (One Medical, Headspace, Gympass, and more)! \n Competitive compensation and opportunity for advancement \n \n #LI-Remote \n \n \n \n Why Coalition? \n \n \n \n    We\u2019re a highly fulfilling, mission-driven team who is committed to building a more diverse and inclusive culture. We want to work with people of all different backgrounds and paths in life, and we trust our team members to take responsibility, share ownership and put in the work, no matter how small the task. We are always looking for collaborative, inquisitive and dedicated individuals to join #OurCoalition and help us on our mission to solve digital risk.\n    \n \n \n \n Recent press releases: \n \n \n \n \n Coalition Closes $250 Million in Series F Funding, Valuing the Cyber Insurance Provider at $5 Billion \n Coalition Named to Fast Company\u2019s Annual List of of the World\u2019s Most Innovative Companies for 2022 \n Coalition Launches Active Insurance, Reaches $650M Run Rate GWP \n Coalition launches tech-powered executive risks products with personalized risk assessment for all US small-businesses \n \n \n \n   Coalition's very foundation is built on respecting and encouraging diversity and inclusion across the organization. Coalition is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.\n   \n \n \n   Coalition is committed to ensuring that our hiring process is accessible for persons with disabilities. If you have a disability or limitation, such as those covered by the Americans with Disabilities Act, that requires accommodations to assist you in the search and application process, please email us at candidateaccommodations@coalitioninc.com.\n   \n \n For CA residents, please view important privacy information here. For EU residents, you can view GDPR information here. For any questions regarding CCPA or GDPR, please contact us at privacy@coalitioninc.com.",
        "cleaned_desc": " Provide management and mentorship to engineers in order to seek out identification of skill gaps and growth needs \n Balance managing technical debt with new feature development and business priorities \n Ensure full software lifecycle instrumentation from requirement ideation to software development to deployment \n Collaborate with Product and Business staff to balance resources and project schedules \n Maintain transparent communication across all levels of the organization \n Actively participate in all phases of growing the engineering organization through recruiting, team building, etc \n Report to a senior engineering leader \n \n Skills and Qualifications \n \n 7+ years overall software engineering experience with 4+ years experience managing a team of at least 5 people \n Effective in leading and mentoring staff with various technical backgrounds and expertise \n Understanding of software development processes with the experience to apply the best type of engineering methodology to fit a situation or team dynamic \n Experience building high volume distributed systems in a cloud environment \n Proven track record of successfully delivering software and managing projects across multiple teams \n Sufficient level of technical background to objectively evaluate complex project risks and issues \n Strong analytical, planning, and organizational skills with an ability to manage competing priorities \n Ability to have meticulous attention to requirements while also maintaining awareness of the overall business objectives ",
        "techs": [
            "none"
        ],
        "cleaned_techs": []
    },
    "bccff13cc8f8ea95": {
        "terms": [
            "data science"
        ],
        "salary_min": 103617.63,
        "salary_max": 131203.02,
        "title": "Product Marketing Manager",
        "company": "Canvs",
        "desc": "Canvs AI is on a mission to enable empathy for the world\u2019s leading brands by putting the power of understanding consumers\u2019 feelings in the hands of every business. Fortune 500 companies like CVS, Disney, Church & Dwight, ABInbev and Netflix use the Canvs AI Platform daily to analyze unstructured feedback from both consumers and employees. Over the last year alone, Canvs has analyzed over 850 million pieces of open-ended text from surveys, social media and online comments on behalf of its brands, helping them achieve 20X faster insights while deepening their understanding of customers, markets and employees. \n Canvs AI is looking for a talented Product Marketing Manager to join our growing team. In this role, you will be the expert on Canvs AI\u2019s solution and markets, responsible for developing and executing marketing and sales content and go-to-market strategies to increase awareness and adoption of the Canvs AI insights platform. \n What you\u2019ll do: \n \n Develop positioning and messaging for Canvs AI's products that resonate with target buyers and users \n  Create sales enablement materials and train the sales team on Canvs AI's value proposition, competitive differentiation, and ideal customer profiles \n  Develop case studies, one-pagers, presentations, and other sales collateral to articulate the business value of Canvs AI's products \n  Work closely with product management to analyze market trends and gather customer feedback to inform product enhancements and new features \n  Generate engaging content including blog posts, infographics, and videos to attract and educate potential customers \n  Collaborate with marketing demand generation, sales and customer success teams to optimize conversion rates across the funnel \n  Analyze metrics to measure content performance and iterate on content strategy as needed \n  Stay up-to-date on artificial intelligence and machine learning trends, technologies, and solutions \n \n \n Who you are: \n \n 3+ years experience in B2B tech product marketing, sales enablement, or related field \n  Strong writing and communication skills \n  Analytical mindset with ability to synthesize research and insights into compelling narratives \n  Excellent project management and organizational abilities \n  Passion for artificial intelligence and its business applications \n  Bonus: Background in data science, consumer insights and/or customer experience management.  \n \n If you are an innovative product marketer passionate about AI and evangelizing disruptive technologies, we want to hear from you! This is a great opportunity to join a fast-growing startup and help shape the future of AI. \n \n Benefits we offer: \n \n Compensation in line with experience and title \n  Tremendous opportunity for career growth \n  Strong culture and commitment to values. \n  Flexibility via a fully distributed company (Work From Anywhere!). \n  Generous health benefits. \n  Unlimited paid vacation. \n  Summer Fridays. \n  Monthly (at a minimum) virtual team-building events. \n \n \n The Canvs AI Commitment \n Canvs AI is committed to making the world a more empathetic place, and fills its ranks with people who want to do great work, care about making a difference, and regularly exemplify the company's shared values of: \n \n Openness, honesty, and transparency; \n  Intellectual curiosity; \n  High integrity and honor; and \n  Empathy and compassion. \n \n Canvs AI is also committed to creating a diverse and inclusive environment and is proud to be an equal opportunity employer. Applicants from populations underrepresented in tech are strongly encouraged to apply. All qualified candidates will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, familial status, sexual orientation, national origin, ability, age, or veteran status.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "908c8272e5c01dbf": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "201 Third Street (61049), United States of America, San Francisco, California\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "201 Third Street (61049), United States of America, San Francisco, California\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "apis",
            "sdks",
            "large-language models (llms)",
            "foundation models (fms)",
            "python",
            "go",
            "scala",
            "java",
            "deep neural networks",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems"
        ],
        "cleaned_techs": [
            "apis",
            "sdks",
            "large-language models (llms)",
            "foundation models (fms)",
            "python",
            "go",
            "scala",
            "java",
            "deep neural networks",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems"
        ]
    },
    "6571d90ecae03ac5": {
        "terms": [
            "data science"
        ],
        "salary_min": 75560.836,
        "salary_max": 95676.85,
        "title": "Solutions Analyst - Conversational AI",
        "company": "United Federal Credit Union",
        "desc": "GENERAL   S UMMARY  (What is done and why) \n  The Solutions Analyst \u2013 Conversational AI, directly contributes to the strategic framework through personalized solutions, frictionless experiences, member-centric culture, and process excellence as part of the Enterprise Solutions Team. The focus is on the effective use of technology in the execution of technology personalization strategy, contextual artificial intelligence, and continual process improvement supporting the Enterprise. The Conversational AI Analyst is responsible for business analysis, establishing intents, building entity scopes, and support for cloud-based solutions such as Kore.ai, Open.ai and Microsoft Co-Pilot. \n   \n ESSENTIAL   F UNCTIONS ,   IN   P RIORITY   O RDER  (Majority of duties, but not meant to be all inclusive or prevent other duties from being assigned as necessary) \n \n  Develop a detailed understanding of Conversational AI platforms, Core, and integrated application architecture to implementation approaches, algorithms, and code structure to meet the business requirements and improve member facing and back-office efficiencies.  Time: 20% \n  Administer Conversational AI instance including establishing intent and entity scopes. Perform periodic system maintenance including application configuration, integration, maintenance of products/services and application security for the Conversational AI environment and interrelated systems.  Time: 20% \n  Proactively collaborate with Continuous Improvement to analyze information needs and functional requirements for Conversational AI and deliver the following: functional/business requirements and specification documents, use cases, workflows, proof of concepts, interface designs. Time: 20% \n  Communicate changes, enhancements, and modifications of business requirements\u2013 verbally or through written documentation \u2013 to project managers, sponsors, and other stakeholders so that issues and solutions are understood.  Time: 10%  \n Maintain and monitor tasks associated including but not limited to maintenance of the service catalog, bot behavior, natural language issues and implement approved change requests to ensure natural language understanding performance supports a positive user experience.  Time:10% \n  Create and maintain testing data sets to verify NLU performance, intent accuracy reports, entity accuracy reports, and analyze chat/call history to optimize intent/entity training.  Time: 10% \n  Use knowledge and expertise of Enterprise Solutions to assist with implementation, testing, identifying gaps, and training development, as needed.  Time: 10% \n \n  EDUCATION   (Minimum education required to perform the duties of this position) \n \n  Bachelor\u2019s degree in MIS, Computer Science, Engineering, Physics, Mathematics, Finance or related discipline or three years related work experience. \n \n \n  EXPERIENCE  (Minimum experience required to perform the duties of this position) \n  In addition  to the education requirement: \n \n  Three or more years of experience managing Contextual AI with an Enterprise-level instance, preferred. \n  Experience in systems/ application process, integration and workflow analysis, application/system analysis, and experience working on project teams. \n  The following certifications are considered valuable: Certifications: Six Sigma, ITIL, Microsoft, etc \n \n \n  KNOWLEDGE,   S KILLS  AND  A BILITIES  (Minimum technical and communication skill levels and licenses/certificates normally required to perform the duties of this position) \n \n  Hands-on experience with chatbots and voice assistants \n  A willingness to take on challenges that are not clearly defined \n  An appreciation for constant learning and solutioning \n  Understanding of User Experience principles and techniques \n  In-depth knowledge and/or practical experience with Conversational AI \n  Familiarity with Office 365, and collaboration tools like MS Teams \n  Working technical knowledge of relational database systems such as Oracle, SQL etc. \n  Experience in Financial Services preferred \n  Generates new ideas/approaches to solving problems and/or alternative solutions \n  Can effectively learn diverse business systems; how they function and can apply that knowledge into benefits to better serve the organization. \n  Able to seek out the root cause to a problem and ensure the resolution solves the underlying issue. \n  Proven ability as a change agent; is good at bringing creative ideas of self and others to business \n  Effective in both formal and informal leadership roles in guiding others in support of necessary changes \n  Proficient in the creation of data flow or systems diagrams and the tools used to develop them \n  Technical knowledge of API\u2019s, Web Services, JavaScript, and XML \n  Background in linguistic analysis of conversational language \n \n \n  Required Competencies \n \n  Dealing with Ambiguity \n  Customer Focus \n  Integrity & Trust \n  Interpersonal Savvy \n  Demonstrates Courage \n  Problem Solving \n  Drive for Results \n  Self-Development \n  Time Management \n  Business Acumen \n \n \n  Mental Requirements \n \n  Ability to identify, analyze, present and determine solutions to a variety of technical projects and problems. \n  Ability to use good judgment and make sound decisions quickly. \n  Ability to work under pressure. \n  Proven problem-solving abilities. \n  Ability to recognize security concerns and implement solutions. \n  Ability to analyze and juggle multiple tasks and determine priorities. \n \n \n  Tools and Equipment Used \n \n  All available general office equipment as needed \n  All available computer software, hardware and peripherals as needed \n \n \n  WORKING   R ELATIONSHIPS /C ONTACTS   (Positions with which incumbent has frequent contact) \n \n  Daily, personal/written/phone contact with IT management \n  Daily, personal/written/phone contact with IT staff \n  As necessary, personal/written/phone contact with strategic partners and vendors \n  As necessary, personal/written/phone contact with credit union management and staff. \n \n \n  PHYSICAL   D EMANDS  (Physical effort generally associated with this position) \n  Work involves standing and walking for brief periods of time, but most work is done from a seated position. There is potential for eyestrain from reading detailed reports and computer screen. Deadlines, workloads and pressure to achieve goals may cause increased stress levels. \n \n  WORKING C ONDITIONS  (Typical working conditions associated with this type of work and environmental hazards, if any, that may be encountered in performing the duties of this position) \n  Internal   - Work is normally performed in climate controlled office environment, where exposure to conditions of extreme heat/cold, poor ventilation, fumes and gases is very limited. Noise level is moderate and includes sounds of normal office equipment (computers, telephones, etc). No known environmental hazards are encountered in normal performance of duties. Length of day is normally between 8:30 AM \u2013 5:00 PM; long hours may be required to accommodate deadlines or special meetings. \n  External   - Some travel may be required; however, information on environmental conditions is not available. \n \n  United Federal Credit Union has served its Members since 1949 by helping them to build a sound financial future. United is based in St. Joseph, MI, with additional branches in Arkansas, Indiana, Michigan, Nevada, North Carolina, and Ohio. United, as a not-for-profit company, takes its commitment to both Members and the community to heart by improving lives and bettering local neighborhoods through financial tools and resources. The employees who work at United, known as Team United, are rooted in their communities as friends, family, volunteers, and mentors. For more information visit  www.UnitedFCU.com . \n \n  EEO/AA Employer/Vet/Disabled",
        "cleaned_desc": "  EXPERIENCE  (Minimum experience required to perform the duties of this position) \n  In addition  to the education requirement: \n \n  Three or more years of experience managing Contextual AI with an Enterprise-level instance, preferred. \n  Experience in systems/ application process, integration and workflow analysis, application/system analysis, and experience working on project teams. \n  The following certifications are considered valuable: Certifications: Six Sigma, ITIL, Microsoft, etc \n \n \n  KNOWLEDGE,   S KILLS  AND  A BILITIES  (Minimum technical and communication skill levels and licenses/certificates normally required to perform the duties of this position) \n \n  Hands-on experience with chatbots and voice assistants \n  A willingness to take on challenges that are not clearly defined \n  An appreciation for constant learning and solutioning \n  Understanding of User Experience principles and techniques \n  In-depth knowledge and/or practical experience with Conversational AI \n  Familiarity with Office 365, and collaboration tools like MS Teams \n  Working technical knowledge of relational database systems such as Oracle, SQL etc. \n  Experience in Financial Services preferred ",
        "techs": [
            "contextual ai",
            "enterprise-level instance",
            "systems/application process",
            "integration",
            "workflow analysis",
            "project teams",
            "six sigma",
            "itil",
            "microsoft",
            "chatbots",
            "voice assistants",
            "user experience principles",
            "conversational ai",
            "office 365",
            "ms teams",
            "relational database systems",
            "oracle",
            "sql",
            "financial services"
        ],
        "cleaned_techs": [
            "ai",
            "enterprise-level instance",
            "systems/application process",
            "integration",
            "workflow analysis",
            "project teams",
            "six sigma",
            "itil",
            "microsoft",
            "chatbots",
            "voice assistants",
            "user experience principles",
            "conversational ai",
            "office 365",
            "ms teams",
            "relational database systems",
            "oracle",
            "sql",
            "financial services"
        ]
    },
    "4d5723ae77213571": {
        "terms": [
            "data science"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Product Owner",
        "company": "Medalogix",
        "desc": "Medalogix is searching for a Product Owner to join our dynamic team! \n Company Overview \n Medalogix is a one-of-a-kind data analytics company in the post-acute care space. We are transforming home health and hospice agencies by leveraging cutting edge data science, machine learning, and innovative cloud technology to equip clinicians and agencies to provide the right care at the right time. \n The Company's five machine learning products have demonstrated improved patient outcomes and reduced cost to the healthcare system, including reduced hospitalization, appropriate and timely transitions to end-of-life care, and optimized visit utilization for patients. Medalogix is poised for tremendous growth, as skilled home health and hospice care are positioned as the lowest cost providers in an industry shifting to value-based care. \n Why Medalogix? \n High growth & awesome culture  \u2013 Medalogix has grown 100%+ for three straight years, with continued material growth planned over the next three years. This success is achieved through passionate, high-functioning talent focused on success. We work hard through our core virtues: Be Badass, Dream Big/Act Small, Do No Harm, and Row Together.  \n Meaningful work \u2013  Our products are used by healthcare providers in their day-to-day care programs. Our company\u2019s work directly impacts patient care in the home.  \n Modern technology products  \u2013 At the core Medalogix is a technology company. We build modern, cloud-based SaaS software underpinned by data science and data engineering. Our technology team is passionate about using leading tools and practices in our daily work. We build market leading products that delight our customers. Medalogix products have received recognition from The Boston Globe, Fierce Healthcare, Becker's Hospital Review, HIMSS, and Harvard Business Review. \n Competitive compensation & benefits  \u2013 Medalogix provides big company benefits in a smaller company environment. We provide competitive compensation along with awesome benefits including 401(k) matching. \n Role Summary \n As a Product Owner, you will collaborate with cross-functional teams to define and prioritize product features, translate business requirements into user stories, and ensure efficient product development and delivery. Your role will involve working closely with UX, QA, and stakeholders to create innovative solutions that meet customer needs and drive business objectives. This is an excellent opportunity for a motivated individual with a passion for healthcare and data science to make a significant impact in an Agile development environment. \n Duties & Responsibilities: \n \n Work closely with cross-functional teams to define and prioritize product features based on customer needs, market analysis, and business objectives. \n Translate business requirements into clear user stories, acceptance criteria, and product specifications. \n Work closely with UX to create wireframes, mockups, and prototypes to validate product concepts. \n Participate in sprint planning, daily stand-ups, and sprint reviews to ensure efficient product development and delivery. \n Collaborate with QA teams to define acceptance criteria and conduct user acceptance testing. \n Regularly review and adjust the product backlog based on changing market conditions and stakeholder feedback. \n Monitor key performance indicators (KPIs) and user feedback to assess the product's success and identify areas for improvement. \n \n Experience & Qualifications: \n \n Bachelor's degree in a relevant field (e.g., Computer Science, Business Administration). \n 2-5 years experience as a product owner or similar role in an Agile development environment. \n Strong understanding of software development processes and methodologies. \n Excellent communication, collaboration, and presentation skills. \n Analytical thinking and problem-solving abilities. \n Familiarity with product management tools and software. \n Home health or hospice healthcare experience preferred \n Willingness to work in a highly collaborative, team-based culture \n \n Benefits \n \n Highly sponsored healthcare plans to choose from (PPO, HSA, and FSA) \n Optional Dental and Vision insurance \n 401(k) with company match; immediately vested \n Education Assistance Program/Employee Assistance Program \n Generous maternity / paternity leave \n We celebrate our wins with team events/outings \n Generous Paid Time Off + 8 Paid Holidays + Floating Holiday + Company Sponsored Volunteering Holiday \n Free downtown parking \n \n Medalogix provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",
        "cleaned_desc": " 2-5 years experience as a product owner or similar role in an Agile development environment. \n Strong understanding of software development processes and methodologies. \n Excellent communication, collaboration, and presentation skills. \n Analytical thinking and problem-solving abilities. \n Familiarity with product management tools and software. \n Home health or hospice healthcare experience preferred \n Willingness to work in a highly collaborative, team-based culture \n ",
        "techs": [
            "product owner",
            "agile development",
            "software development processes",
            "methodologies",
            "communication",
            "collaboration",
            "presentation skills",
            "analytical thinking",
            "problem-solving abilities",
            "product management tools",
            "software",
            "home health",
            "hospice healthcare"
        ],
        "cleaned_techs": [
            "agile development",
            "software development processes",
            "methodologies",
            "communication",
            "collaboration",
            "analytical thinking",
            "problem-solving abilities",
            "product management tools",
            "software",
            "home health",
            "hospice healthcare"
        ]
    },
    "7e903524dab076e9": {
        "terms": [
            "data science"
        ],
        "salary_min": 127645.6,
        "salary_max": 161627.78,
        "title": "Principal Statistical Programmer",
        "company": "SimulStat",
        "desc": "To Apply for this Job Click Here \n \n  Qualifications \n \n \n \n At least 8 years of hands-on experience in Stat Programming, with no employment gaps \n At least 4 years hands-on experience working on SDTM and ADaM data sets \n Hands-on experience in oncology and submissions (including preparing metadata for a study and/or working on an ISS or ISE) \n Strong communication skills, both written and verbal \n Demonstrated attention to detail and focus on quality, supported by examples \n Demonstrated proactivity, supported by examples \n Demonstrated analytical skills, for example by creating or providing significant input into specifications as an author or while programming or conducting QC \n  To Apply for this Job Click Here",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "2d892c28d9c58795": {
        "terms": [
            "data science"
        ],
        "salary_min": 132292.25,
        "salary_max": 167511.47,
        "title": "Principal IS Analyst - R&D Generative AI",
        "company": "Amgen",
        "desc": "HOW MIGHT YOU DEFY IMAGINATION? \n  If you feel like you\u2019re part of something bigger, it\u2019s because you are. At Amgen, our shared mission\u2014to serve patients\u2014drives all that we do. It is key to our becoming one of the world\u2019s leading biotechnology companies. We are global collaborators who achieve together\u2014researching, manufacturing, and delivering ever-better products that reach over 10 million patients worldwide. It\u2019s time for a career you can be proud of. \n \n  Principal IS Analyst - R&D Generative AI \n  Live \n  What you will do \n  Let\u2019s do this. Let\u2019s change the world. In this vital role you will work with our Research and Development (R&D) Digital Technology and Innovation (DTI) group supporting R&D\u2019s Global Regulatory Affairs & Strategy (GRAAS) function in alignment with our business and DTI strategy. \n  As a Principal IS Business Systems Analyst at Amgen, you will provide strategic leadership and guidance in analyzing business requirements and designing information systems solutions. You will collaborate with multi-functional teams to understand business needs, identify system enhancements, and drive system implementation projects. Your extensive experience in business analysis, system design, and project management will enable you to deliver innovative and effective solutions that align with the company's strategic goals. \n  Responsibilities: \n \n  Serve as a client facing partner and / or product owner working with business and DTI team members to implement and support the Generative AI and Structured Content Management/Authoring initiatives \n  Responsibilities include technical product ownership, process and systems analysis, requirements elicitation, articulating system solutions and approaches for addressing business needs \n  Provide an expert-level understanding of GRAAS business models, processes, and procedures \n  Engage with key vendors and platform owners to understand functionality and how those platforms can be used to tackle the business problems \n  Meet regulatory requirements and adhere to DTI standards including following GxP Quality and SDLC processes \n  The position is located in our offices in Thousand Oaks, CA, or Tampa, FL or we can also consider remote applicants in a United States. \n \n  Win \n  What we expect of you \n  We are all different, yet we all use our unique contributions to serve patients. The strategic professional we seek is a critical thinker with these qualifications. \n  Basic Qualifications: \n \n  Doctorate degree and 2 years of Information Systems experience \n  OR Master\u2019s degree and 6 years of Information Systems experience \n  OR Bachelor\u2019s degree and 8 years of Information Systems experience \n  OR Associate\u2019s degree and 10 years of Information Systems experience \n  OR High School Diploma /GED and 12 years of Information Systems experience \n \n  Preferred Qualifications: \n \n  At least 5 years of domain knowledge in health and/or life sciences combined with Information Technology \n  Ability to set expectations with business partners and optimally use governance for a positive business partner experience \n  Strong communications skills in writing, speaking, presenting and time management skills \n  Applied experience and knowledge in the application of Generative AI in the submission/CTD document drafting \n  Understanding, and preferably applied experience and knowledge in the application of Structured Content Management/Authoring in the submission/CTD document space \n  Expert understanding of GRAAS/Regulatory Affairs, and preferably other R&D processes \n  Experience in Agile product development \n  7+ years customer-facing technical consulting experience \n  Experience defining technical roadmaps and driving adoption \n  Either 7+ years of experience with integration architecture, design, and development for geographically distributed enterprise/cloud software or document/content management systems OR development experience with one of the following languages \u2013 Java, .NET, Python, C#, or C++, or database development \n \n  Thrive \n  What you can expect of us \n  As we work to develop treatments that take care of others, we also work to care for our teammates\u2019 professional and personal growth and well-being. \n  Amgen offers a Total Rewards Plan comprising health and welfare plans for staff and eligible dependents, financial plans with opportunities to save towards retirement or other goals, work/life balance, and career development opportunities including: \n \n  Comprehensive employee benefits package, including a Retirement and Savings Plan with generous company contributions, group medical, dental and vision coverage, life and disability insurance, and flexible spending accounts. \n  A discretionary annual bonus program, or for field sales representatives, a sales-based incentive plan \n  Stock-based long-term incentives \n  Award-winning time-off plans and bi-annual company-wide shutdowns \n  Flexible work models, including remote work arrangements, where possible \n \n  Apply now \n  for a career that defies imagination \n  Objects in your future are closer than they appear. Join us. \n  careers.amgen.com \n \n  Join Us\n  \n  If you're seeking a career where you can truly make a difference in the lives of others, a career where you can work at the absolute forefront of biotechnology with the top minds in the field, you'll find it at Amgen.\n  \n  Amgen, a biotechnology pioneer, discovers, develops and delivers innovative human therapeutics. Our medicines have helped millions of patients in the fight against cancers, kidney disease, rheumatoid arthritis and other serious illnesses.\n  \n  As an organization dedicated to improving the quality of life for people around the world, Amgen fosters an inclusive environment of diverse, ethical, committed and highly accomplished people who respect each other but compete intensely to win. Together, we live the Amgen values as we continue advancing science to serve patients.\n  \n  Amgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\n  \n  We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",
        "cleaned_desc": "  OR High School Diploma /GED and 12 years of Information Systems experience \n \n  Preferred Qualifications: \n \n  At least 5 years of domain knowledge in health and/or life sciences combined with Information Technology \n  Ability to set expectations with business partners and optimally use governance for a positive business partner experience \n  Strong communications skills in writing, speaking, presenting and time management skills \n  Applied experience and knowledge in the application of Generative AI in the submission/CTD document drafting \n  Understanding, and preferably applied experience and knowledge in the application of Structured Content Management/Authoring in the submission/CTD document space \n  Expert understanding of GRAAS/Regulatory Affairs, and preferably other R&D processes \n  Experience in Agile product development \n  7+ years customer-facing technical consulting experience \n  Experience defining technical roadmaps and driving adoption ",
        "techs": [
            "generative ai",
            "structured content management/authoring",
            "graas/regulatory affairs",
            "r&d processes",
            "agile product development"
        ],
        "cleaned_techs": [
            "generative ai",
            "structured content management/authoring",
            "r&d processes",
            "agile product development"
        ]
    },
    "fa055232f5e4f4ed": {
        "terms": [
            "data science"
        ],
        "salary_min": 96903.46,
        "salary_max": 122701.38,
        "title": "Project Lead, Data Solutions (Remote)",
        "company": "Crum & Forster",
        "desc": "Crum & Forster Company Overview: \n   Crum & Forster (C&F)  Crum & Forster (C&F), with a proud history dating to 1822, provides specialty and standard commercial lines insurance products through our admitted and surplus lines insurance companies. C&F enjoys a financial strength rating of \"A\" (Excellent) by AM Best and is proud of our superior customer service platform. Our claims and risk engineering services are recognized as among the best in the industry.  \n Our most valuable asset is our people: more than 2000 employees in locations throughout the United States. The company is increasingly winning recognition as a great place to work, earning several workplace and wellness awards, including the  October 2022 Great Place to Work \u00ae  Award  for our employee-first focus and our steadfast commitment to diversity, equity and Inclusion.  \n C&F is part of Fairfax Financial Holdings, a global, billion dollar organization. For more information about Crum & Forster, please visit our website: www.cfins.com. \n \n  Job Description:\n  \n  Crum & Forster provides specialty and standard commercial lines insurance products through our admitted and surplus lines insurance companies. Operating out of our corporate offices in Morristown, New Jersey, and ten regional offices, we distribute our products through approximately 1,500 authorized retail and wholesale brokers across the United States. \n  We are looking for innovative, talented, and self-motivated individuals who are interested in working with a cross-functional team focused on production support of our Data & Analytics team. \n \n  We\u2019re seeking an experienced Project Lead/Data Analyst to join our dynamic Data Team. The ideal candidate will have a unique blend of project management skills and a deep understanding of data analytics within the P&C insurance domain. \n \n  What you will do: \n \n  Manage and lead data-driven projects from inception to completion, ensuring alignment with business objectives and timely delivery.  \n Collaborate with stakeholders to define project scopes, objectives, deliverables, and transform requirements into technical specifications.  \n Analyze complex datasets to derive actionable insights, identify trends, and make recommendations. \n  Develop and maintain KPIs, dashboards, and regular reports to monitor project progress and data insights.  \n Interface with the IT and software development teams to integrate and manage insurance data sources and solutions.  \n Ensure data quality and integrity across all projects. \n  Provide mentorship and training to junior team members. \n  Define and oversee data architecture, standards, and governance for P&C insurance initiatives.  \n Design and implement data models optimized for reporting and analytics. \n  Oversee the ETL processes from various data sources.  \n Conduct regular data audits and reconciliation. \n  Stay updated with the latest trends and technologies in data analytics and P&C insurance.  \n Provide solutions to complex data-related problems. \n  Document and maintain architecture blueprints, data dictionaries, and process workflows.  \n Work closely with business teams to understand and predict emerging needs. \n  Evaluate and recommend new tools and technologies for data analytics. \n  Ensure compliance with regulatory standards related to data handling in the P&C domain.  \n Plan and manage data migration projects when necessary. \n  Lead and participate in meetings, providing data-driven insights and recommendations.  \n Work with external vendors or consultants as required. \n \n  What you will bring to C&F: \n \n \n  Bachelor's or Master's degree in Data Science, Computer Science, or a related field such as Data Analytics, Business, or Insurance.  \n Minimum of 8 years of experience (10+ preferred) in leading project and data analytics, preferably in the Property & Casualty (P&C) insurance domain.  \n Proven experience in the P&C insurance domain, with a strong understanding of insurance products, claims, and underwriting processes.  \n Proven ability to manage multiple projects simultaneously and meet tight deadlines. \n  Strong Proficiency in data querying languages like SQL. \n  Strong Expertise in database management systems like SQL Server, Oracle, or MySQL. \n  Strong expertise with ETL (Extract, Transform, Load) tools such as Snowflake, Informatica, or SSIS (SQL Server Integration Services).  \n Strong Knowledge of data modeling tools like ER/Studio or ERwin. \n  Proficiency in MS Excel and equivalent Data Analysis tools \n  Experience with Big Data technologies like Snowflake is good. \n  Deliver projects in a agile environment. \n  Exceptional communication skills, both written and verbal. \n  Demonstrated ability to work independently as well as collaboratively in a team environment. \n  Familiarity with cloud platforms such as AWS (Amazon Web Services), Azure. \n  Knowledge of data visualization tools such as Tableau or Power BI. \n  An innate sense of learning, curiosity and the passion to try and deliver the best. Open to learning new data integration tools. \n \n \n   #LI-MS\n  \n \n \n  #LI-REMOTE\n   What C&F will bring to you: \n  \n Competitive compensation package \n  Generous 401K employer match \n  Employee Stock Purchase plan with employer matching \n  Generous Paid Time Off \n  Excellent benefits that go beyond health, dental & vision. Our programs are focused on your whole family\u2019s wellness, including your physical, mental and financial wellbeing \n  A core C&F tenet is owning your career development, so we provide a wealth of ways for you to keep learning, including tuition reimbursement, industry-related certifications and professional training to keep you progressing on your chosen path \n  A dynamic, ambitious, fun and exciting work environment \n  We believe you do well by doing good and want to encourage a spirit of social and community responsibility, matching donation program, volunteer opportunities, and an employee-driven corporate giving program that lets you participate and support your community \n \n \n   At C&F you will BELONG\n  \n \n \n  If you require special accommodations, please let us know.We value inclusivity and diversity. We are committed to equal employment opportunity and welcome everyone regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you require special accommodations, please let us know\n  \n \n \n  Crum & Forster is committed to ensuring a workplace free from discriminatory pay disparities and complying with applicable pay equity laws. Salary ranges are available for all positions at this location, taking into account roles with a comparable level of responsibility and impact in the relevant labor market and these salary ranges are regularly reviewed and adjusted in accordance with prevailing market conditions. The annualized base pay for the advertised position, located in the specified area, ranges from a minimum of $105,800 to a maximum of 176,300. The actual compensation is determined by various factors, including but not limited to the market pay for the jobs at each level, the responsibilities and skills required for each job, and the employee\u2019s contribution (performance) in that role. To be considered within market range, a salary is at or above the minimum of the range. You may also have the opportunity to participate in discretionary equity (stock) based compensation and/or performance-based variable pay programs.",
        "cleaned_desc": " Analyze complex datasets to derive actionable insights, identify trends, and make recommendations. \n  Develop and maintain KPIs, dashboards, and regular reports to monitor project progress and data insights.  \n Interface with the IT and software development teams to integrate and manage insurance data sources and solutions.  \n Ensure data quality and integrity across all projects. \n  Provide mentorship and training to junior team members. \n  Define and oversee data architecture, standards, and governance for P&C insurance initiatives.  \n Design and implement data models optimized for reporting and analytics. \n  Oversee the ETL processes from various data sources.  \n Conduct regular data audits and reconciliation. \n  Stay updated with the latest trends and technologies in data analytics and P&C insurance.  \n Provide solutions to complex data-related problems. \n  Document and maintain architecture blueprints, data dictionaries, and process workflows.  \n Work closely with business teams to understand and predict emerging needs. \n  Evaluate and recommend new tools and technologies for data analytics. \n  Ensure compliance with regulatory standards related to data handling in the P&C domain.  \n Plan and manage data migration projects when necessary.    Lead and participate in meetings, providing data-driven insights and recommendations.  \n Work with external vendors or consultants as required. \n \n  What you will bring to C&F: \n \n \n  Bachelor's or Master's degree in Data Science, Computer Science, or a related field such as Data Analytics, Business, or Insurance.  \n Minimum of 8 years of experience (10+ preferred) in leading project and data analytics, preferably in the Property & Casualty (P&C) insurance domain.  \n Proven experience in the P&C insurance domain, with a strong understanding of insurance products, claims, and underwriting processes.  \n Proven ability to manage multiple projects simultaneously and meet tight deadlines. \n  Strong Proficiency in data querying languages like SQL. \n  Strong Expertise in database management systems like SQL Server, Oracle, or MySQL. \n  Strong expertise with ETL (Extract, Transform, Load) tools such as Snowflake, Informatica, or SSIS (SQL Server Integration Services).  \n Strong Knowledge of data modeling tools like ER/Studio or ERwin. \n  Proficiency in MS Excel and equivalent Data Analysis tools \n  Experience with Big Data technologies like Snowflake is good.    Deliver projects in a agile environment. \n  Exceptional communication skills, both written and verbal. \n  Demonstrated ability to work independently as well as collaboratively in a team environment. \n  Familiarity with cloud platforms such as AWS (Amazon Web Services), Azure. \n  Knowledge of data visualization tools such as Tableau or Power BI. \n  An innate sense of learning, curiosity and the passion to try and deliver the best. Open to learning new data integration tools. \n \n \n   #LI-MS\n  \n \n \n  #LI-REMOTE\n   What C&F will bring to you: \n  \n Competitive compensation package ",
        "techs": [
            "snowflake",
            "informatica",
            "ssis (sql server integration services)",
            "er/studio",
            "erwin",
            "sql server",
            "oracle",
            "mysql",
            "snowflake",
            "excel",
            "aws (amazon web services)",
            "azure",
            "tableau",
            "power bi"
        ],
        "cleaned_techs": [
            "snowflake",
            "informatica",
            "ssis (sql server integration services)",
            "er/studio",
            "erwin",
            "sql",
            "oracle",
            "mysql",
            "excel",
            "aws",
            "azure",
            "tableau",
            "powerbi"
        ]
    },
    "01e5454fcf84b3ca": {
        "terms": [
            "data science"
        ],
        "salary_min": 75000.0,
        "salary_max": 90000.0,
        "title": "SAS Programmer-Data Warehousing Specialist",
        "company": "Aleto, Inc.",
        "desc": "About Aleto \n  Aleto is a startup company specializing in Federal property management. \n  Aleto is looking to hire someone who will grow with the company. Aleto offers paid vacation, full medical, vision, and dental insurance, and a 401(K) plan. \n  Aleto is an Equal Opportunity Employer. \n  What we are looking for \n  Aleto Inc. is seeking to hire a SAS Programmer \u2013 Mid/Junior Level To assist a federal client, manage and analyze complex data sets so it can provide accurate and useful critical information to internal and external users in a timely fashion. \n  Selection Process \n  If you are selected for an interview, Aleto will contact you to arrange a meeting via an online platform, such as Google Meets. After all, candidates are interviewed, Aleto will notify you of your status. The start date is immediate pending winning the award for the contract. \n  To Apply:  Check out our Careers Page! Please include your cover letter and resume. \n  Data Warehousing Specialist\u2019s Essential Job Duties:  Duties include the following. Other duties may be assigned. \n \n  Support complex Data Analytics and Business Intelligence projects in a Federal government environment. \n  Assist in the design, development, and maintenance of a one-stop service center for scheduled and ad-hoc data reports. \n  Facilitate the Automation and Execution of scheduled Data processes. \n  Create and generate Data products and summary tabulations from multiple data sources. \n  Help prepare Data Requirements, Standard Operating Procedures (SOP), and other technical documentation. \n  Support modernization of program scripts for data manipulation and visualization. \n  Participate in continuous data quality improvement and process streamlining efforts. \n  Accomplish all tasks appropriately assigned or requested. \n  Other duties may be assigned. \n \n  SAS Programmer: Qualifications/Capabilities/Software Knowledge \n  To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions. \n  Education/Experience: \n \n  Minimum of 3 years experience with a bachelor\u2019s degree or minimum of 6 years experience in a technical support role with demonstrated execution of tasks and projects relevant to Data Analytics, Data Science, and Business Intelligence. \n  Experience in translating business requirements into a technical design for data products and Business Intelligence solutions. \n  Prior experience with housing Data is a plus. \n \n  Essential Function/ Job responsibilities: \n \n  Develop and execute database queries and conduct analyses. \n  Hands-on experience with developing ETL processes \n  Experience with performing data quality checks \n  Automate business requirements into SAS programs and support ad hoc requests. \n  Optimize existing SAS/SQL code - Troubleshoot and automate SAS and Python jobs and code. \n  Create visualizations, Dashboard, user stories, and reports for requested projects. \n  Develop and update technical documentation. \n  Maintain/modify SAS/SQL program and/or configuration file changes based on documented requirements. \n  Work with project teams to maintain and enhance source SAS code and process flows. \n  Perform QA checks to ensure data integrity of reporting and dashboard solutions. \n \n \n  Aleto JOB DUTIES \n \n  Compliance with all Aleto processes, standards, and guidelines including utilizing the employee and intranet platforms, clocking in and/or entering time daily, submitting expense reports, providing monthly progress reports, etc. \n  Participate in recurring performance development meetings with your Aleto Team Lead to discuss current job tasks, promote open dialog/feedback, recognize and celebrate wins, and review positive and purposeful approaches for meeting work-related and professional development goals. \n  Attend team meetings, tri-annual company All-Hands Meetings, and other company-sponsored team-building events to foster and support Aleto's core values, vision, and culture. \n \n  Technical Skills: \n \n  Proficient in SAS; Certifications in SAS desired \n  Experience with SAS EG \n  Proficient in writing Oracle PL/SQL \n  Proficient in Python is a plus. \n \n  Soft Skills: \n \n  Ability to communicate with a variety of people, both internal and external. \n  Ability to work collaboratively in a team environment (remote). \n  Proven problem-solving skills with the ability to analyze situations, identify existing or potential problems, and recommend solutions. \n \n  SALARY RANGE:  $75,000-$90,000 \n  Work Location: REMOTE \n  What we offer \n  Aleto is working for you. \n  Our company offers comprehensive benefits to employees who work 30 hours or more each week. A summary of benefits offered: \n \n  Medical, Vision, and Dental Insurance: \n \n  Aleto employees are offered single, single-plus-one-dependent, or family medical, vision, and dental insurance plans. \n \n  Disability Insurance: \n \n  Aleto offers company-paid short-term and long-term disability insurance. \n \n  Life Insurance: \n \n  Aleto offers company-paid life insurance coverage. \n \n  Paid Time Off: \n \n  Aleto\u2019s paid time off includes eleven federal holidays. Full-time employees accrue PTO at the rate of 5 hours per pay period (120 hours per year). \n \n  Retirement Plan: \n \n  Aleto offers full-time employees a 401(k) qualified retirement plan. Employees who have worked for at least three months will be eligible to enroll in the plan on the 15th day of the month of the quarter following their hire anniversary date. The company will match 100% of the first 3% of the employee contribution, then match 50% of the next one-percent match of their contribution to the plan for a total of a 4% employer contribution. \n  Environment and Physical Conditions \n  The environmental factors and/or physical requirements of this position include the following: While performing the duties of this job, the employee is required to have ordinary ambulatory skills sufficient to visit other locations; and the ability to stand, walk, stoop, kneel, crouch, and manipulate (lift, carry, move) medium weights of at least 20 pounds. Requires arm, hand, and finger dexterity, including the ability to grasp, and visual acuity to use a keyboard. The employee frequently is required to be in one position for long periods of screen use, reach with hands and arms, and talk and hear both in person and over devices. The employee interacts in close spaces and frequently with other workers, vendors, and clients. May be in situations that require conflict resolution. The position works in an office environment with artificial light and air. \n  To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed above are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of the position. \n   \n GYCT37LpY0",
        "cleaned_desc": "  Create visualizations, Dashboard, user stories, and reports for requested projects. \n  Develop and update technical documentation. \n  Maintain/modify SAS/SQL program and/or configuration file changes based on documented requirements. \n  Work with project teams to maintain and enhance source SAS code and process flows. \n  Perform QA checks to ensure data integrity of reporting and dashboard solutions. \n \n \n  Aleto JOB DUTIES \n \n  Compliance with all Aleto processes, standards, and guidelines including utilizing the employee and intranet platforms, clocking in and/or entering time daily, submitting expense reports, providing monthly progress reports, etc. \n  Participate in recurring performance development meetings with your Aleto Team Lead to discuss current job tasks, promote open dialog/feedback, recognize and celebrate wins, and review positive and purposeful approaches for meeting work-related and professional development goals. \n  Attend team meetings, tri-annual company All-Hands Meetings, and other company-sponsored team-building events to foster and support Aleto's core values, vision, and culture. \n \n  Technical Skills: \n \n  Proficient in SAS; Certifications in SAS desired \n  Experience with SAS EG \n  Proficient in writing Oracle PL/SQL ",
        "techs": [
            "visualizations",
            "dashboard",
            "user stories",
            "reports",
            "sas",
            "sql",
            "sas eg",
            "oracle pl/sql"
        ],
        "cleaned_techs": [
            "visualizations",
            "dashboard",
            "user stories",
            "reports",
            "sas",
            "sql",
            "oracle"
        ]
    },
    "92da984c9594668d": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": 140000.0,
        "salary_max": -1.0,
        "title": "Principal NLP Engineer",
        "company": "External SambaNova Systems",
        "desc": "The era of pervasive AI has arrived. In this era, organizations will use generative AI to unlock hidden value in their data, accelerate processes, reduce costs, drive efficiency and innovation to fundamentally transform their businesses and operations at scale. \n  SambaNova Suite\u2122 is the first full stack, generative AI platform, from chip to model, optimized for enterprise and government organizations. Powered by the intelligent SN40L chip, the SambaNova Suite is a fully integrated platform, delivered on-premises or in the cloud, combined with state-of-the-art open-source models, which can be easily and securely fine-tuned using customer data for greater accuracy. Once adapted with customer data, customers retain model ownership in perpetuity, so they can turn generative AI into one of their most valuable assets. \n \n  Working at SambaNova \n  This role presents a unique opportunity to shape the future of AI and the value it can unlock across every aspect of an organization's business and operations, including innovating from strategic product pathfinding to large-scale production. We are excited to have talents on board, pushing towards democratizing the modern NLP capability in real-world use cases. \n  Responsibilities \n  SambaNova is hiring a  Principal NLP Engineer   for the Applied Machine Learning (ML) team. \n \n Design and implement large-scale data pipelines that feed billions of high-quality tokens into LLMs. \n Continuously improve SambaNova's LLM by exploring new ideas, including but not limited to new modeling techniques, prompt engineering, instruction tuning, and alignment. \n Curate and crawl the necessary dataset to induce domain specificity. \n Collaborate with product management and executive teams to develop a roadmap for continuous improvement of LLM and incorporate new capabilities. \n Work closely with the product team and our customers to translate product requirements into requisite LLM capabilities. \n Expand LLM capabilities into new languages and domains. \n Develop applications on top of LLMs including but not limited to semantic search, summarization, conversational agents, etc. \n \n Basic Qualifications \n \n Bachelor's or Master's degree in engineering or science fields \n 5-10 years of hands-on engineering experience in machine learning \n \n Additional Required Qualifications \n \n Experience with one or more deep learning frameworks like TensorFlow, PyTorch, Caffe2, or Theano \n A deep theoretical or empirical understanding of deep learning \n Experience building and deploying machine learning models \n Strong analytical and debugging skills \n Experience with either one of Large Language Models, Multilingual Models, Semantic Search, Summarization, Data Pipelines, Domain Adaptation (finance, legal, or bio-medical), and conversational agents. \n Experience in leading small teams. \n Experience in Python and/or C++. \n \n Preferred Qualifications \n \n Experience working in a high-growth startup \n A team player who demonstrates humility \n Action-oriented with a focus on speed & results \n Ability to thrive in a no-boundaries culture & make an impact on innovation \n \n \n \n  Annual Salary Range and Level \n  The base salary for this position ranges from $140,000/year up to $210,000/year. This range is based on role, level, and location and reflects the salary target for new hires in the US. Individual pay within the range will depend on a number of factors, including a candidate's job-related qualifications, skills, competencies and experience, and location. \n \n \n  Benefits Summary \n  SambaNova offers a competitive total rewards package, including the base salary, plus equity and benefits. We cover 95% premium coverage for employee medical insurance, and 80% premium coverage for dependents and offer a Health Savings Account(HSA) with employer contribution. We also offer Dental, Vision, Short/Long term Disability, Basic Life, Voluntary Life, and AD&D insurance plans in addition to Flexible Spending Account(FSA) options like Health Care, Limited Purpose, and Dependent Care. Our library of well-being benefits includes a full subscription to Headspace, access to One Medical, counseling services with an Employee Assistance Program, and much more. \n \n \n  LI-TD1 \n  #LI-Remote \n \n  Submission Guidelines \n  Please note that in order to be considered an applicant for any position at SambaNova Systems you must submit an application form for each position for which you believe you are qualified. \n  If you are a new, recent (within the last two years), or upcoming college graduate and are interested in opportunities with SambaNova Systems, please apply through our University job listings. \n \n \n  EEO Policy \n  SambaNova Systems is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard basis of age (40 and over), color, disability, gender identity, genetic information, marital status, military or veteran status, national origin/ancestry, race, religion, creed, sex (including pregnancy, childbirth, breastfeeding), sexual orientation, and any other applicable status protected by federal, state, or local laws. \n \n \n  Customers turn to SambaNova to quickly deploy state-of-the-art generative AI capabilities within the enterprise. Our purpose-built enterprise-scale AI platform is the technology backbone for the next generation of AI computing. \n  Headquartered in Palo Alto, California, SambaNova Systems was founded in 2017 by industry luminaries, and hardware and software design experts from Sun/Oracle and Stanford University. Investors include SoftBank Vision Fund 2, funds and accounts managed by BlackRock, Intel Capital, GV, Walden International, Temasek, GIC, Redline Capital, Atlantic Bridge Ventures, Celesta, and several others. Visit us at sambanova.ai. Follow SambaNova Systems on Linkedin.",
        "cleaned_desc": "The era of pervasive AI has arrived. In this era, organizations will use generative AI to unlock hidden value in their data, accelerate processes, reduce costs, drive efficiency and innovation to fundamentally transform their businesses and operations at scale. \n  SambaNova Suite\u2122 is the first full stack, generative AI platform, from chip to model, optimized for enterprise and government organizations. Powered by the intelligent SN40L chip, the SambaNova Suite is a fully integrated platform, delivered on-premises or in the cloud, combined with state-of-the-art open-source models, which can be easily and securely fine-tuned using customer data for greater accuracy. Once adapted with customer data, customers retain model ownership in perpetuity, so they can turn generative AI into one of their most valuable assets. \n \n  Working at SambaNova \n  This role presents a unique opportunity to shape the future of AI and the value it can unlock across every aspect of an organization's business and operations, including innovating from strategic product pathfinding to large-scale production. We are excited to have talents on board, pushing towards democratizing the modern NLP capability in real-world use cases. \n  Responsibilities \n  SambaNova is hiring a  Principal NLP Engineer   for the Applied Machine Learning (ML) team. \n \n Design and implement large-scale data pipelines that feed billions of high-quality tokens into LLMs. \n Continuously improve SambaNova's LLM by exploring new ideas, including but not limited to new modeling techniques, prompt engineering, instruction tuning, and alignment. \n Curate and crawl the necessary dataset to induce domain specificity. \n Collaborate with product management and executive teams to develop a roadmap for continuous improvement of LLM and incorporate new capabilities.   A deep theoretical or empirical understanding of deep learning \n Experience building and deploying machine learning models \n Strong analytical and debugging skills \n Experience with either one of Large Language Models, Multilingual Models, Semantic Search, Summarization, Data Pipelines, Domain Adaptation (finance, legal, or bio-medical), and conversational agents. \n Experience in leading small teams. \n Experience in Python and/or C++. \n \n Preferred Qualifications \n \n Experience working in a high-growth startup \n A team player who demonstrates humility \n Action-oriented with a focus on speed & results ",
        "techs": [
            "sambanova suite",
            "sn40l chip",
            "open-source models",
            "nlp capability",
            "large-scale data pipelines",
            "llm (large language models)",
            "prompt engineering",
            "instruction tuning",
            "alignment",
            "domain specificity",
            "machine learning models",
            "large language models",
            "multilingual models",
            "semantic search",
            "summarization",
            "data pipelines",
            "domain adaptation (finance",
            "legal",
            "or bio-medical)",
            "conversational agents",
            "python",
            "c++"
        ],
        "cleaned_techs": [
            "sambanova suite",
            "sn40l chip",
            "open-source models",
            "nlp",
            "large-scale data pipelines",
            "llm",
            "prompt engineering",
            "instruction tuning",
            "alignment",
            "domain specificity",
            "large language models",
            "multilingual models",
            "semantic search",
            "summarization",
            "data pipelines",
            "domain adaptation (finance",
            "legal",
            "or bio-medical)",
            "conversational agents",
            "python",
            "c++"
        ]
    },
    "dc90f7b4093de39a": {
        "terms": [
            "data science"
        ],
        "salary_min": 10.0,
        "salary_max": 14.0,
        "title": "AI Professional for SAAS Development",
        "company": "Kealee Construction",
        "desc": "About Us: \n Kealee Construction is a full-service construction tech company dedicated to revolutionizing the architecture, civil engineering, and structural engineering industries. We are passionate about innovation and are seeking a highly skilled AI Professional to join our team. In this role, you will play a pivotal part in the development of a cutting-edge Software as a Service (SAAS) platform that leverages artificial intelligence to automate, optimize, and transform the construction and engineering processes. \n Job Description: \n We are looking for an AI Professional with a deep understanding of AI technologies, machine learning, and data analysis to lead the development of our SAAS platform. As part of the Kealee team, you will have the opportunity to create AI solutions that will drive innovation in architecture, civil engineering, and structural engineering fields. \n Responsibilities: \n \n Collaborate with our development team to define the AI strategy and technical requirements for the SAAS platform, emphasizing architecture, civil engineering, and structural engineering. \n Develop and implement machine learning and deep learning algorithms to automate tasks in architecture, civil engineering, and structural engineering, such as design optimization, structural analysis, project analysis, and predictive modeling. \n Create and maintain a scalable and efficient AI infrastructure that can handle large datasets and complex computations in architecture, civil engineering, and structural engineering. \n Collaborate with domain experts in architecture, civil engineering, and structural engineering to understand the unique challenges and opportunities in these fields. \n Implement computer vision and natural language processing techniques to process and analyze architectural, engineering, and structural data. \n Ensure the platform's AI components are robust, accurate, and continuously improved through testing, monitoring, and feedback loops. \n Stay up-to-date with the latest advancements in AI and apply innovative technologies to enhance the SAAS platform in the contexts of architecture, civil engineering, and structural engineering. \n Contribute to the development of user-friendly interfaces that allow professionals to interact with and benefit from the AI capabilities in architecture, civil engineering, and structural design, analysis, and optimization. \n \n Qualifications: \n \n Bachelor's or higher degree in Computer Science, Artificial Intelligence, Machine Learning, or a related field. \n Proven experience in AI development, with a focus on machine learning, deep learning, and data analysis. \n Strong programming skills in languages such as Python and proficiency in AI libraries and frameworks like TensorFlow, PyTorch, and scikit-learn. \n Experience with computer vision, natural language processing, and other AI subfields. \n Ability to work collaboratively in a team and communicate effectively with both technical and non-technical stakeholders. \n Knowledge of cloud computing platforms (e.g., AWS, Azure, or Google Cloud) and database management. \n Experience in software development and an understanding of SAAS architecture is a plus. \n \n How to Apply: \n If you are excited about transforming the architecture, civil engineering, and structural engineering industries through AI innovation and meet the qualifications listed above, we encourage you to apply. Please send your resume, a cover letter outlining your relevant experience, and any relevant project portfolios to Reina@kealee.com \n Job Type: Full-time \n Pay: $10.00 - $14.00 per hour \n Expected hours: 8 per week \n Experience level: \n \n 2 years \n \n Schedule: \n \n 8 hour shift \n \n Experience: \n \n Python: 1 year (Preferred) \n SQL: 1 year (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": " Create and maintain a scalable and efficient AI infrastructure that can handle large datasets and complex computations in architecture, civil engineering, and structural engineering. \n Collaborate with domain experts in architecture, civil engineering, and structural engineering to understand the unique challenges and opportunities in these fields. \n Implement computer vision and natural language processing techniques to process and analyze architectural, engineering, and structural data. \n Ensure the platform's AI components are robust, accurate, and continuously improved through testing, monitoring, and feedback loops. \n Stay up-to-date with the latest advancements in AI and apply innovative technologies to enhance the SAAS platform in the contexts of architecture, civil engineering, and structural engineering. \n Contribute to the development of user-friendly interfaces that allow professionals to interact with and benefit from the AI capabilities in architecture, civil engineering, and structural design, analysis, and optimization. \n \n Qualifications:   \n Bachelor's or higher degree in Computer Science, Artificial Intelligence, Machine Learning, or a related field. \n Proven experience in AI development, with a focus on machine learning, deep learning, and data analysis. \n Strong programming skills in languages such as Python and proficiency in AI libraries and frameworks like TensorFlow, PyTorch, and scikit-learn. \n Experience with computer vision, natural language processing, and other AI subfields. \n Ability to work collaboratively in a team and communicate effectively with both technical and non-technical stakeholders. \n Knowledge of cloud computing platforms (e.g., AWS, Azure, or Google Cloud) and database management. \n Experience in software development and an understanding of SAAS architecture is a plus. ",
        "techs": [
            "architecture",
            "civil engineering",
            "structural engineering",
            "computer vision",
            "natural language processing",
            "ai components",
            "testing",
            "monitoring",
            "feedback loops",
            "saas platform",
            "user-friendly interfaces",
            "ai capabilities",
            "computer science",
            "artificial intelligence",
            "machine learning",
            "ai development",
            "machine learning",
            "deep learning",
            "data analysis",
            "python",
            "tensorflow",
            "pytorch",
            "scikit-learn",
            "cloud computing platforms",
            "aws",
            "azure",
            "google cloud",
            "database management",
            "software development",
            "saas architecture"
        ],
        "cleaned_techs": [
            "architecture",
            "civil engineering",
            "structural engineering",
            "computer vision",
            "nlp",
            "ai",
            "testing",
            "monitoring",
            "feedback loops",
            "saas platform",
            "user-friendly interfaces",
            "computer science",
            "python",
            "tensorflow",
            "pytorch",
            "scikit-learn",
            "cloud computing platforms",
            "aws",
            "azure",
            "gcp",
            "database management",
            "software development",
            "saas architecture"
        ]
    },
    "766c155e71d4fd8c": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "77 West Wacker Dr (35012), United States of America, Chicago, Illinois\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "77 West Wacker Dr (35012), United States of America, Chicago, Illinois\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "apis",
            "sdks",
            "large-language models (llms)",
            "foundation models (fms)",
            "python",
            "go",
            "scala",
            "java",
            "deep neural networks",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems",
            "neural network models",
            "api security",
            "observability",
            "cloud access control",
            "privacy best practices."
        ],
        "cleaned_techs": [
            "apis",
            "sdks",
            "large-language models (llms)",
            "foundation models (fms)",
            "python",
            "go",
            "scala",
            "java",
            "deep neural networks",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems",
            "neural network models",
            "observability",
            "cloud access control"
        ]
    },
    "e32a1ddceeaaa2fb": {
        "terms": [
            "data science"
        ],
        "salary_min": 139382.47,
        "salary_max": 176489.28,
        "title": "AI/ML Lead 4655",
        "company": "MetroStar",
        "desc": "As a  AI/ML Lead , you'll analyze complex datasets to assess performance or find new patterns and apply machine learning techniques to data sets related to immigration, with the goal to make an impact across the federal government. \n \n \n We know that you can't have great technology services without amazing people. At MetroStar, we are obsessed with our people and have led a two-decade legacy of building the best and brightest teams. Because we know our future relies on our deep understanding and relentless focus on our people, we live by our mission: A passion for our people. Value for our customers. \n \n \n If you think you can see yourself delivering our mission and pursuing our goals with us, then check out the job description below! \n \n \n \n What  you'll  do: \n \n \n \n Lead architectural design, technical support, and advisement services to ensure identity management system technologies are integrated and meeting the appropriate security requirements \n Present results to a diverse audience in presentation or report form and support leadership who engage with senior-level executives at a public-facing Federal agency and provide subject matter expertise in security architecture and other key domain areas. \n \n \n \n \n \n What  you'll  need to succeed: \n \n \n \n A minimum of 10 years of IT experience, focusing on enterprise data architecture and management. \n At least 8 years of experience in Conceptual/Logical/Physical data modeling. \n Experience in modeling solutions in AWS that us AI/ML algorithms and in in cloud architecture, specifically AWS, as it relates to data processing (i.e., EC2, S3, Redshift, etc.). \n At least 10 years of proven expertise in Relational and Dimensional Data Modeling. \n Ability to define & maintain BI/Data Warehouse methodologies, standards, and industry best practices. \n Experience leading and architecting enterprise-wide initiatives, specifically system integration, data migration, transformation, data warehouse build, data mart build, data lakes implementation/support, as well as O&M etc. for a large enterprise. \n Expertise in large-scale database requirements supporting diverse data types and eperience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior levels of management. \n A bachelor's degree (Data Science, Statistics, Computer Science, Information Technology Management or Engineering, or other comparable degree or experience) and the ability to obtain and maintain a DHS Suitability. \n \n \n \n \n \n Like we said,  we are  big fans of our people. That's why  we offer  a generous benefits package, professional growth, and valuable time to recharge. Learn more about our company culture code and benefits. Plus, check out our accolades. \n \n \n \n Don't  meet every single requirement ? \n \n \n Studies have shown that women, people of color and the LGBTQ+ community are less likely to apply to jobs unless they meet every single qualification. At MetroStar we are dedicated to building a diverse, inclusive, and authentic culture, so, if you're excited about this role, but your previous experience doesn't align perfectly with every qualification in the job description, we encourage you to go ahead and apply. We pride ourselves on making great matches, and you may be the perfect match for this role or another one we have. Best of luck! \u2013 The MetroStar People & Culture Team \n \n \n What we want you to know: \n \n \n In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire. \n \n \n MetroStar Systems is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. The statements herein are intended to describe the general nature and level of work being performed by employees and are not to be construed as an exhaustive list of responsibilities, duties, and skills required of personnel so classified. Furthermore, they do not establish a contract for employment and are subject to change at the discretion of MetroStar Systems. \n \n \n Not ready to apply now? \n \n \n Sign up to join our newsletter here.",
        "cleaned_desc": " \n A minimum of 10 years of IT experience, focusing on enterprise data architecture and management. \n At least 8 years of experience in Conceptual/Logical/Physical data modeling. \n Experience in modeling solutions in AWS that us AI/ML algorithms and in in cloud architecture, specifically AWS, as it relates to data processing (i.e., EC2, S3, Redshift, etc.). \n At least 10 years of proven expertise in Relational and Dimensional Data Modeling. \n Ability to define & maintain BI/Data Warehouse methodologies, standards, and industry best practices. \n Experience leading and architecting enterprise-wide initiatives, specifically system integration, data migration, transformation, data warehouse build, data mart build, data lakes implementation/support, as well as O&M etc. for a large enterprise. \n Expertise in large-scale database requirements supporting diverse data types and eperience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior levels of management. \n A bachelor's degree (Data Science, Statistics, Computer Science, Information Technology Management or Engineering, or other comparable degree or experience) and the ability to obtain and maintain a DHS Suitability. \n \n \n ",
        "techs": [
            "aws",
            "ai/ml algorithms",
            "ec2",
            "s3",
            "redshift"
        ],
        "cleaned_techs": [
            "aws",
            "ai",
            "ec2",
            "s3",
            "redshift"
        ]
    },
    "cb1dc70d8951a983": {
        "terms": [
            "data science",
            "data analyst"
        ],
        "salary_min": 81920.0,
        "salary_max": 122880.0,
        "title": "Sr. Business / Data Analyst",
        "company": "LPL Financial",
        "desc": "Are you a team player? Are you curious to learn? Are you interested in working in meaningful projects? Do you want to work with cutting-edge technology? Are you interested in being part of a team that is working to transform and do things differently? If so, LPL Financial is the place for you! \n \n  LPL Financial (Nasdaq: LPLA) was founded on the principle that the firm should work for the advisor, and not the other way around. Today, LPL is a leader* in the markets we serve, supporting more than 18,000 financial advisors, 800 institution-based investment programs and 450 independent RIA firms nationwide. We are steadfast in our commitment to the advisor-centered model and the belief that Americans deserve access to personalized guidance from a financial advisor. At LPL, independence means that advisors have the freedom they deserve to choose the business model, services, and technology resources that allow them to run their perfect practice. And they have the freedom to manage their client relationships, because they know their clients best. Simply put, we take care of our advisors, so they can take care of their clients. \n \n  Job Overview: \n  At LPL Financial we consider it our mission to take care of our advisors so they can take care of their client. Joining as Senior Business/Data Analyst, you will be responsible for building and maintaining the sustainable, scalable products and capabilities that support our advisors to deliver world-class services for their investors. \n \n  Responsibilities: \n \n  Understand the business needs of the users; document the analysis using business process descriptions, use cases, scenarios, tasks, and workflow. \n  Generate requirements from users; use advanced problem-solving techniques (both diagnostic and prognostic) to analyze the problem and generate requirements with the users and document those. \n  Schedule and lead design discussions with Architecture, Infrastructure, and different Technical teams of all levels throughout the company including executives \n  Innovate an out-of-the-box solution that leverages the holistic knowledge of a variety of use cases. \n  Collaborate with subject matter experts and developers to establish the technical vision and analyze tradeoffs between usability and performance needs \n  Perform query and reporting analysis; document data sources and mappings to support reporting and other data requirements \n  Be a liaison between the Business Users, Application development team, Architecture and other technology teams (infrastructure, info security etc.), and project and relationship managers \n  Manage and ensure requirements are met not only through the entire SDLC process by participating in design discussions, QA, UAT, and deployment, but also in all aspects of regulatory requirements \n  Work with the project team to translate Business requirements into a solution, help the team for Technical design documentation, and analyze tradeoffs between effort and return on value. Socialize these tradeoffs with the project team and stakeholders \n  Actively work to resolve issues with users when requirements clarification is needed or to facilitate sign-off on test cases \n \n \n  What are we looking for? \n  We want  strong collaborators   who can deliver a world-class client experience . We are looking for people who thrive in a  fast-paced environment ,  are client-focused ,  team-oriented , and are able to execute in a way that encourages  creativity  and  continuous improvement . \n \n  Requirements: \n \n  7 years of experience functioning as a business analyst within a mid to large organization \n  4+ years of experience writing epics stories, and tasks in Agile (Scrum is preferred) using JIRA and Waterfall project approaches. \n  2+ years of experience with SQL queries, hands-on in SQL knowledge report writing and presenting findings \n  2+ years of working with JIRA, TFS, HPQC, Aha! and similar other project management tools \n  1+ years of experience in data structures, data modeling, db schema is required. Knowledge of technologies including API, XML, and web application concepts. \n \n \n  Core Competencies: \n \n  Detail-oriented and critical thinker. Engage in multiple initiatives simultaneously. \n  Comfortable working independently, adapting to shifting priorities, demands, and timelines in a fast-paced environment and demonstrating leadership skills. \n  Keen attention to detail and complex problem-solving abilities from operational and technical perspectives \n  Expert written and oral communication skills, strong interpersonal skills, and the ability to interact professionally with diverse groups, executives, managers, and subject matter experts. \n \n \n  Preferences: \n \n  Good Product acumen, can easily fit into product owner\u2019s role to deliver the product, must have worked on building/integrating products on various Fintech platforms \n  4+ years of writing user stories. \n  2+ years of writing SQL queries with simple joins \n  Wealth Management Experience or Investment Asset Management experience preferred or Financial Industry experience \n  Expert understanding of the broker-dealer business model, various financial asset classes, various industry-standard applications and tools used by financial advisors \n \n \n  #LI-Remote \n \n  Pay Range:  $81,920-$122,880/year\n  \n  Actual base salary varies based on factors, including but not limited to, relevant skill, prior experience, education, base salary of internal peers, demonstrated performance, and geographic location. Additionally, LPL Total Rewards package is highly competitive, designed to support your success at work, at home, and at play \u2013 such as 401K matching, health benefits, employee stock options, paid time off, volunteer time off, and more. Your recruiter will be happy to discuss all that LPL has to offer!\n  \n  Why LPL?  \n \n At LPL, we believe that objective financial guidance is a fundamental need for everyone. As the nation\u2019s leading independent broker-dealer, we offer an integrated platform of proprietary technology, brokerage, and investment advisor services. We provide you with a work environment that encourages your creativity and growth, a leadership team that is supportive and responsive, and the opportunity to create a career that has no limits, only amazing potential. \n \n  We are  one team on one mission.  We take care of our advisors, so they can take care of their clients. \n \n  Because our company is not too big and not too small, you can seize the opportunity to make a real impact. We are committed to supporting workplace equality, and we embrace the different perspectives and backgrounds of our employees. We also care for our communities, and we encourage our employees to do the same. This creates an environment in which you can do your best work. \n \n  Want to hear from our employees on what it\u2019s like to work at LPL? Watch this! \n \n  We take social responsibility seriously. Learn more here \n \n  Want to see info on our benefits? Learn more here \n \n  Join the LPL team and help us make a difference by turning life\u2019s aspirations into financial realities. Please log in or create an account to apply to this position. Principals only. EOE. \n \n  Information on Interviews: \n \n  LPL will only communicate with a job applicant directly from an  @lp lfinancial.com  email address and will never conduct an interview online or in a chatroom forum. During an interview, LPL will not request any form of payment from the applicant, or information regarding an applicant\u2019s bank or credit card. Should you have any questions regarding the application process, please contact LPL\u2019s Human Resources Solutions Center at (800) 877-7210.",
        "cleaned_desc": "  2+ years of experience with SQL queries, hands-on in SQL knowledge report writing and presenting findings \n  2+ years of working with JIRA, TFS, HPQC, Aha! and similar other project management tools \n  1+ years of experience in data structures, data modeling, db schema is required. Knowledge of technologies including API, XML, and web application concepts. \n \n \n  Core Competencies: \n \n  Detail-oriented and critical thinker. Engage in multiple initiatives simultaneously. \n  Comfortable working independently, adapting to shifting priorities, demands, and timelines in a fast-paced environment and demonstrating leadership skills. \n  Keen attention to detail and complex problem-solving abilities from operational and technical perspectives \n  Expert written and oral communication skills, strong interpersonal skills, and the ability to interact professionally with diverse groups, executives, managers, and subject matter experts. \n \n \n  Preferences: ",
        "techs": [
            "sql queries",
            "jira",
            "tfs",
            "hpqc",
            "aha!",
            "data structures",
            "data modeling",
            "db schema",
            "api",
            "xml",
            "web application concepts"
        ],
        "cleaned_techs": [
            "jira",
            "tfs",
            "hpqc",
            "aha!",
            "data structures",
            "db schema",
            "api",
            "xml",
            "web application concepts"
        ]
    },
    "0631e989757b9de3": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "apis",
            "sdks",
            "large-language models (llms)",
            "foundation models (fms)",
            "python",
            "go",
            "scala",
            "java",
            "deep neural networks",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems",
            "neural network models."
        ],
        "cleaned_techs": [
            "apis",
            "sdks",
            "large-language models (llms)",
            "foundation models (fms)",
            "python",
            "go",
            "scala",
            "java",
            "deep neural networks",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems",
            "neural network models."
        ]
    },
    "a844a79c87351af5": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "77 West Wacker Dr (35012), United States of America, Chicago, Illinois\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "77 West Wacker Dr (35012), United States of America, Chicago, Illinois\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems"
        ]
    },
    "49d8ad3e52f0092f": {
        "terms": [
            "data science"
        ],
        "salary_min": 52100.0,
        "salary_max": 119000.0,
        "title": "Frontend Web Developer, Mid",
        "company": "Booz Allen Hamilton",
        "desc": "Job Description \n \n \n \n \n \n \n \n \n \n \n         Location: \n         \n \n         Chantilly,VA,US \n         \n \n \n \n         Remote Work: \n         \n \n         Hybrid \n         \n \n \n \n         Job Number: \n         \n \n         R0182323\n         \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n         Frontend Web Developer, Mid\n           The Opportunity: \n  The right interface can make a tool, system, or site easy to use, encourage early adoption, and save time and resources. We\u2019re looking for you, a web developer who will use equal parts skill and vision to create an experience that delivers functionality and efficiency. Bring your passion for creating an amazing user experience to Booz Allen. \n \n  This is an opportunity to meet challenges in the mission area and customer industry by collaborating with a development team to develop a type of system, tool, or web application with a user-centric design. You\u2019ll work with UI/UX designers and back-end developers to create a seamless user experience using React, Angular, or Ember. \n \n  On our team, you\u2019ll work with the development team to ensure accessibility for all users by developing a front end that functions across browsers, platforms, and devices while meeting accessibility and security requirements. With mentoring, positive code reviews, and opportunities to learn new tools and skills, we focus on growing as a team to make the best solutions for our customers. \n \n  Work with us as we shape systems to change the mission area and customer industry for the better. \n \n  Join us. The world can\u2019t wait. \n \n  You Have: \n \n  4+ years of experience in working with Web development or design, including HTML or CSS \n  2+ years of experience with using Angular, React, or Vue and their ecosystems, including Gulp, Angular CLI, integrating NPM libraries and modularizing code for reuse across multiple projects \n  Experience with responsive design libraries, including Bootstrap and Material Design \n  Experience in working with configuration management and source code control, including Git or SVN \n  TS/SCI clearance \n  Bachelor's degree \n \n \n  Nice If You Have: \n \n  4+ years of experience in working with Web or data analysis languages, including Java or Python \n  Experience with JS visualization libraries, including Cytoscape, D3, Ogma, or AmCharts \n  Experience with CI/CD pipelines, including Jenkins, Kubernetes, and Docker \n  Experience in working with RESTful Web services \n  Experience with advanced data structures and algorithms that apply to them, including connected graph analysis \n  Master's degree in Data Science or Operations Research \n \n \n  Clearance: \n  Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; TS/SCI clearance is required. \n \n  Create Your Career: \n \n  Grow With Us \n  Your growth matters to us\u2014that\u2019s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. \n \n  A Place Where You Belong \n  Diverse perspectives cultivate collective ingenuity. Booz Allen\u2019s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you\u2019ll build your community in no time. \n \n  Support Your Well-Being \n  Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we\u2019ll support you as you pursue a balanced, fulfilling life\u2014at work and at home. \n \n  Your Candidate Journey \n  At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we\u2019ve compiled a list of resources so you\u2019ll know what to expect as we forge a connection with you during your journey as a candidate with us. \n \n  Compensation \n  At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen\u2019s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. \n  Salary at Booz Allen is determined by various factors, including but not limited to location, the individual\u2019s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $52,100.00 to $119,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen\u2019s total compensation package for employees.\n          \n  Work Model  Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. \n \n  If this position is listed as remote or hybrid, you\u2019ll periodically work from a Booz Allen or client site facility. \n  If this position is listed as onsite, you\u2019ll work with colleagues and clients in person, as needed for the specific role. \n \n \n  EEO Commitment \n  We\u2019re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change \u2013 no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.",
        "cleaned_desc": "  Experience with responsive design libraries, including Bootstrap and Material Design \n  Experience in working with configuration management and source code control, including Git or SVN \n  TS/SCI clearance \n  Bachelor's degree \n \n \n  Nice If You Have: \n \n  4+ years of experience in working with Web or data analysis languages, including Java or Python \n  Experience with JS visualization libraries, including Cytoscape, D3, Ogma, or AmCharts \n  Experience with CI/CD pipelines, including Jenkins, Kubernetes, and Docker \n  Experience in working with RESTful Web services \n  Experience with advanced data structures and algorithms that apply to them, including connected graph analysis \n  Master's degree in Data Science or Operations Research \n \n \n  Clearance: \n  Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; TS/SCI clearance is required. \n \n  Create Your Career: \n \n  Grow With Us ",
        "techs": [
            "bootstrap",
            "material design",
            "git",
            "svn",
            "java",
            "python",
            "cytoscape",
            "d3",
            "ogma",
            "amcharts",
            "jenkins",
            "kubernetes",
            "docker",
            "restful web services",
            "connected graph analysis"
        ],
        "cleaned_techs": [
            "bootstrap",
            "material design",
            "git",
            "svn",
            "java",
            "python",
            "cytoscape",
            "d3",
            "ogma",
            "amcharts",
            "jenkins",
            "kubernetes",
            "docker",
            "restful web services",
            "connected graph analysis"
        ]
    },
    "9d09c0d4f96ff75d": {
        "terms": [
            "data science"
        ],
        "salary_min": 151109.5,
        "salary_max": 191338.31,
        "title": "Distinguished Engineer - AI/ML",
        "company": "Cisco Systems",
        "desc": "Who We Are   \n \n \n \n \n  We are a part of the Outshift Group focused on identifying breakthrough emerging solutions that create new markets and businesses for Cisco. We incubate these new opportunities in partnership with our business groups, partners and other startups. The team also continues to progress Cisco\u2019s important work with Standards bodies and manage research partnerships with leading-edge Universities. \n  \n \n  Our organization is anticipating high growth. We are seeking talent with the agility and creativity to explore opportunities and fill in needs as they arise across our teams. Applicants should be seeking a flexible role, in which they not only provide leadership in their primary function, but also contribute in meaningful ways to other projects. We are looking for individuals who are excited about pivoting quickly to different domains that may be outside of their normal scope of responsibilities.\n   \n \n \n \n \n  Learn more about us at https://eti.cisco.com/\n   \n \n \n \n \n Who You'll Work With   \n \n \n \n \n  The team is a highly visible group which reports directly to Cisco\u2019s CEO. Outshift focuses on the next wave of innovation by anticipating, investing in, and incubating new technologies and business ventures. You will be part of the incubation team to develop new products and bring them to market in a startup-like environment.\n   \n \n \n \n \n What You'll Do   \n \n \n \n \n  In your role as a Distinguished Engineer, you will guide Outshift from the front on AI/ML technologies for products and products for AI enablement and help build a pipeline of talent and shape up Cisco\u2019s AI strategies for the next decade.\n   \n \n \n \n \n  You will be driving multi-functional engagements to architect, design, build, validate, deliver and successfully deploy new products.\n   \n \n \n \n \n  Lead a diverse community of engineers and ensure alignment across all functions of the team.\n   \n \n \n \n \n  Take ambiguous technical problems, Customer challenges and refine that to a clear solution with architecture and design. Ensure the solution gets deployed successfully.\n   \n \n \n \n \n  Synthesize manual-on experience into feedback to Product Engineering Leadership.\n  \n \n \n  Build and present executive level collateral on the business value of technology.\n   \n \n \n \n \n The Impact You\u2019ll Make   \n \n \n \n \n  If you want the challenge of fast-paced growth, the satisfaction of seeing your thoughtful ideas come to life, and the pride in helping grow a best-in-class AI team, this is the place for you.\n   \n \n \n \n \n Your Responsibilities Will Include   \n \n \n \n \n Drive technology strategy for the AI/ML products.  \n Drive multi-functional engagements to architect, design, build and validate end-to-end generative AI driven products.  \n Lead a diverse community of engineers and ensure alignment across all functions of the team.  \n Work with peer leaders in Outshift and wider Cisco Senior Tech Talent community (PE, DE, Fellows) from other business groups to align strategies and collaborate on AI/ML platforms and capabilities. \n Lead due diligence and selection of AI technology platforms including LLMs.  \n Take ambiguous technical problems, customer challenges and refine that to a clear solution with architecture and design. Ensure the solution gets deployed successfully.  \n Stay up to date with the latest advancements in the field of AI and guide teams to apply them to develop innovative solutions.  \n Conduct research and develop ideas to enable adoption of innovative AI models using state-of-the-art techniques and tools.  \n Build and present executive level collateral on the business value of technology. \n \n \n \n  Minimum Requirements   \n \n \n \n \n Seasoned engineer and a team leader with degree and corresponding engineering experience (below) with a consistent track record of developing and delivering complex, distributed, enterprise class software, and creating successful business outcomes.  \n Expert in Software Engineering principles, System Architecture.  \n Expert knowledge and practical experience in programming languages like C, C++, Python.  \n 5+ years of experience in core ML/AI technologies with deep understanding of ML/AI application development. Knowledge of deep learning frameworks is preferred.  \n Ability to lead and mentor engineers globally, be an advocate for creative technical trends. Acknowledged for driving decisions collaboratively, resolving challenges and ensuring follow through.   \n \n \n \n \n  Bachelors degree 17+ yrs engineering experience OR\n  \n \n   Masters degree with 14+ yrs engineering experience OR\n  \n \n   PhD with 10+ yrs engineering experience\n  \n \n \n  Preferred Requirements   \n \n \n \n \n Visibility (eg: conference speaker, publications) and strong personal network across ML/AI industry ecosystem.  \n Extraordinarily resourceful & rigorous, you can operate successfully among forward-thinking and charismatic people.  \n Equally comfortable and capable of interacting with technologists as with business executives.  \n Excellent verbal and written communication skills.  \n Strong problem-solving skills and ability to work independently or in a team.   \n \n \n \n \n Why Cisco?   \n \n \n \n \n  #WeAreCisco. We are all unique, but collectively we bring our talents to work as a team, to develop innovative technology and power a more inclusive, digital future for everyone. How do we do it? Well, for starters \u2013 with people like you! \n  \n \n  Nearly every internet connection around the world touches Cisco. We\u2019re the Internet\u2019s optimists. Our technology makes sure the data travelling at light speed across connections does so securely, yet it\u2019s not what we make but what we make happen which marks us out. We\u2019re helping those who work in the health service to connect with patients and each other; schools, colleges, and universities to teach in even the most challenging of times. We\u2019re helping businesses of all shapes and size to connect with their employees and customers in new ways, providing people with access to the digital skills they need and connecting the most remote parts of the world \u2013 whether through 5G, or otherwise.\n   \n \n \n \n \n  We tackle whatever challenges come our way. We have each other\u2019s backs, we recognize our accomplishments, and we grow together. We celebrate and support one another \u2013 from big and small things in life to big career moments. And giving back is in our DNA (we get 10 days off each year to do just that).\n   \n \n \n \n \n  We know that powering an inclusive future starts with us. Because without diversity and a dedication to equality, there is no moving forward. Our 30 Inclusive Communities, that bring people together around commonalities or passions, are leading the way. Together we\u2019re committed to learning, listening, caring for our communities, whilst supporting the most vulnerable with a collective effort to make this world a better place either with technology, or through our actions. \n  \n \n  So, you have colorful hair? Don\u2019t care. Tattoos? Show off your ink. Like polka dots? That\u2019s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us! #WeAreCisco\n   \n \n \n \n \n  #LI-TA2 \n  \n \n  #LI-Remote\n  \n \n \n \n \n Message to applicants applying to work in the U.S.: \n \n \n \n  When available, the salary range posted for this position reflects the projected hiring range for new hire, full-time salaries in U.S. locations, not including equity or benefits. For non-sales roles the hiring ranges reflect base salary only; employees are also eligible to receive annual bonuses. Hiring ranges for sales positions include base and incentive compensation target. Individual pay is determined by the candidate's hiring location and additional factors, including but not limited to skillset, experience, and relevant education, certifications, or training. Applicants may not be eligible for the full salary range based on their U.S. hiring location. The recruiter can share more details about compensation for the role in your location during the hiring process.\n  \n \n  U.S. employees have \n   access  to quality medical, dental and vision insurance, a 401(k) plan with a Cisco matching contribution, short and long-term disability coverage, basic life insurance and numerous wellbeing offerings. Employees receive up to twelve paid holidays per calendar year, which includes one floating holiday, plus a day off for their birthday. Employees accrue up to 20 days of Paid Time Off (PTO) each year and have access to paid time away to deal with critical or emergency issues without tapping into their PTO. We offer additional paid time to volunteer and give back to the community. Employees are also able to purchase company stock through our Employee Stock Purchase Program.\n  \n \n  Employees on sales plans earn performance-based incentive pay on top of their base salary, which is split between quota and non-quota components. For quota-based incentive pay, Cisco pays at the standard rate of 1% of incentive target for each 1% revenue attainment against the quota up to 100%. Once performance exceeds 100% quota attainment, incentive rates may increase up to five times the standard rate with no cap on incentive compensation. For non-quota-based sales performance elements such as strategic sales objectives, Cisco may pay up to 125% of target. Cisco sales plans do not have a minimum threshold of performance for sales incentive compensation to be paid.",
        "cleaned_desc": " \n  If you want the challenge of fast-paced growth, the satisfaction of seeing your thoughtful ideas come to life, and the pride in helping grow a best-in-class AI team, this is the place for you.\n   \n \n \n \n \n Your Responsibilities Will Include   \n \n \n \n \n Drive technology strategy for the AI/ML products.  \n Drive multi-functional engagements to architect, design, build and validate end-to-end generative AI driven products.  \n Lead a diverse community of engineers and ensure alignment across all functions of the team.  \n Work with peer leaders in Outshift and wider Cisco Senior Tech Talent community (PE, DE, Fellows) from other business groups to align strategies and collaborate on AI/ML platforms and capabilities. \n Lead due diligence and selection of AI technology platforms including LLMs.  \n Take ambiguous technical problems, customer challenges and refine that to a clear solution with architecture and design. Ensure the solution gets deployed successfully.  \n Stay up to date with the latest advancements in the field of AI and guide teams to apply them to develop innovative solutions.  \n Conduct research and develop ideas to enable adoption of innovative AI models using state-of-the-art techniques and tools.  \n Build and present executive level collateral on the business value of technology. \n \n \n \n  Minimum Requirements   \n \n \n \n \n Seasoned engineer and a team leader with degree and corresponding engineering experience (below) with a consistent track record of developing and delivering complex, distributed, enterprise class software, and creating successful business outcomes.  \n Expert in Software Engineering principles, System Architecture.  \n Expert knowledge and practical experience in programming languages like C, C++, Python.  \n 5+ years of experience in core ML/AI technologies with deep understanding of ML/AI application development. Knowledge of deep learning frameworks is preferred.  \n Ability to lead and mentor engineers globally, be an advocate for creative technical trends. Acknowledged for driving decisions collaboratively, resolving challenges and ensuring follow through.   \n \n \n ",
        "techs": [
            "c",
            "c++",
            "python"
        ],
        "cleaned_techs": [
            "c",
            "c++",
            "python"
        ]
    },
    "9cd7b3e1ccdca604": {
        "terms": [
            "data science"
        ],
        "salary_min": 165101.89,
        "salary_max": 209055.8,
        "title": "Head of Quantitative Products and Solutions - HYBRID",
        "company": "Inclusively",
        "desc": "Inclusively is partnering with a  company description   to hire a  Head of Quantitative Products and Solutions - HYBRID . \n ABOUT INCLUSIVELY: \n Inclusively is a digital tech platform that connects candidates with disabilities, who may benefit from workplace accommodations, to inclusive employers. This includes all disabilities under the ADA, including mental health conditions (e.g. anxiety, depression, PTSD), chronic illnesses (e.g. diabetes, Long COVID), and neurodivergence (e.g. autism, ADHD).  Applicants with one or more of these conditions are encouraged to apply; Inclusively does not require applicants to disclose their specific disability. \n Job Description: \n The Role \n You will be leading the Quantitative Products and Solutions (QPS) team within the Quantitative Research & Investments (QRI) division. You will be responsible for the daily production lifecycle of the quantitative models used in the systematic investment strategies managed by QRI. The QPS team works alongside quant researchers to migrate research models into production, build the model QA framework, and monitor and ensure data quality and model integrity as part of the investment process. In addition, the team partners with the QRI Technology team (QRIT) in developing the data platform underpinning quant research. \n The QPS team partners with and supports all teams within QRI that manage and develop new quantitative products, including the Systematic Equity Strategies and Systematic Fixed Income Strategies teams, the Quant Index Solutions team, and the Advanced Strategies and Research team. \n The Value You Deliver \n You will be leading the data and operations teams that support QRI products and solutions, including: \n \n Data Services: Build and manage a world class data operations team that responds to data quality issues in overnight feeds, enabling fast and seamless responses to upstream issues and insulating production and research. You will drive operational excellence by fostering a culture that encourages and rewards proactivity and accountability, driving to zero defects through continuous improvement. \n Proprietary Indexing support: Partner with Quantitative Index and Solutions (QIS) team in validating and implementing models used in the construction of beta and thematic proprietary indices. The QPS team is responsible for the daily validation of holdings for over 100+ indices. \n Investment Products support: Partner with quant portfolio managers and quant researchers in designing and monitoring the QA and control framework around all model output used in portfolio construction, including alpha generation, risk, and transaction cost models. The quant development team works closely with QRI quant research teams in converting research into production code and implementing daily model DQ checks. The team owns the production implementation of all research code and helps troubleshoot issues flagged by the daily QC processes. \n Hire, mentor and develop top tier quant talent: This role includes direct management responsibilities for a team of 10+ quantitative specialists and data experts. \n \n The Skills You Bring \n \n Technical understanding of quantitative research and the production lifecycle, including signal research, optimization, backtesting, and portfolio construction \n Ability to design and build production quality systems and processes that can simultaneously meet the daily needs of multiple investment teams \n Highly analytical with the ability to quickly comprehend complex data and process workflows, with the ability to identify and implement the right quality controls for these investment processes \n Strong operations and process driven mindset with exceptional attention to detail \n Skilled at building a high performing operations team with a singular focus on operational excellence and total quality management \n Skilled at communicating concisely with senior management while simultaneously being able to get extremely granular with technical SMEs in root cause analysis \n Strong hands-on quant research, data and infrastructure skills including model management and data quality \n Excellent written and verbal communication skills; experience working with both technical and investment teams \n Ability to work with and manage teams whose skill sets span quant research, data-science and quant-development across multiple asset classes including equity, bonds, and derivatives \n \n Education and Experience \n \n 10+ years of relevant work experience in the financial industry \n Advanced degree in Engineering, Computer Science, or a closely related field is encouraged \n Investment operations experience in developing robust workflow and controls and managing production processes supporting systematic investing \n Experience implementing a total quality management framework to investment operations \n Experience leading global data services, operations and quantitative developer teams, in a peer firm with a demonstrable track record in both building and managing such teams \n Proven track record of working with complex data environments and associated technology and analytics infrastructure needed to support these environments \n A demonstrated ability to partner with quantitative researchers, database engineers, investment services, and portfolio managers \n You are a seasoned leader with strong interpersonal, communication, negotiation, organization and client management skills \n \n Job Type: Full-time \n Schedule: \n \n Monday to Friday \n \n Work Location: Remote",
        "cleaned_desc": " You will be leading the data and operations teams that support QRI products and solutions, including: \n \n Data Services: Build and manage a world class data operations team that responds to data quality issues in overnight feeds, enabling fast and seamless responses to upstream issues and insulating production and research. You will drive operational excellence by fostering a culture that encourages and rewards proactivity and accountability, driving to zero defects through continuous improvement. \n Proprietary Indexing support: Partner with Quantitative Index and Solutions (QIS) team in validating and implementing models used in the construction of beta and thematic proprietary indices. The QPS team is responsible for the daily validation of holdings for over 100+ indices. \n Investment Products support: Partner with quant portfolio managers and quant researchers in designing and monitoring the QA and control framework around all model output used in portfolio construction, including alpha generation, risk, and transaction cost models. The quant development team works closely with QRI quant research teams in converting research into production code and implementing daily model DQ checks. The team owns the production implementation of all research code and helps troubleshoot issues flagged by the daily QC processes. \n Hire, mentor and develop top tier quant talent: This role includes direct management responsibilities for a team of 10+ quantitative specialists and data experts. \n \n The Skills You Bring   \n Technical understanding of quantitative research and the production lifecycle, including signal research, optimization, backtesting, and portfolio construction \n Ability to design and build production quality systems and processes that can simultaneously meet the daily needs of multiple investment teams \n Highly analytical with the ability to quickly comprehend complex data and process workflows, with the ability to identify and implement the right quality controls for these investment processes \n Strong operations and process driven mindset with exceptional attention to detail \n Skilled at building a high performing operations team with a singular focus on operational excellence and total quality management \n Skilled at communicating concisely with senior management while simultaneously being able to get extremely granular with technical SMEs in root cause analysis \n Strong hands-on quant research, data and infrastructure skills including model management and data quality   Excellent written and verbal communication skills; experience working with both technical and investment teams \n Ability to work with and manage teams whose skill sets span quant research, data-science and quant-development across multiple asset classes including equity, bonds, and derivatives \n \n Education and Experience \n \n 10+ years of relevant work experience in the financial industry \n Advanced degree in Engineering, Computer Science, or a closely related field is encouraged \n Investment operations experience in developing robust workflow and controls and managing production processes supporting systematic investing   Experience implementing a total quality management framework to investment operations \n Experience leading global data services, operations and quantitative developer teams, in a peer firm with a demonstrable track record in both building and managing such teams \n Proven track record of working with complex data environments and associated technology and analytics infrastructure needed to support these environments \n A demonstrated ability to partner with quantitative researchers, database engineers, investment services, and portfolio managers \n You are a seasoned leader with strong interpersonal, communication, negotiation, organization and client management skills \n \n Job Type: Full-time \n Schedule: ",
        "techs": [
            "quantitative research",
            "data operations",
            "data quality",
            "data services",
            "operational excellence",
            "continuous improvement",
            "proprietary indexing",
            "validation",
            "model implementation",
            "investment products",
            "portfolio construction",
            "alpha generation",
            "risk models",
            "transaction cost models",
            "quant talent",
            "quantitative specialists",
            "data experts",
            "signal research",
            "optimization",
            "backtesting",
            "production quality systems",
            "process workflows",
            "quality controls",
            "total quality management",
            "operations team",
            "senior management",
            "root cause analysis",
            "model management",
            "data science",
            "quant development",
            "asset classes",
            "engineering",
            "computer science",
            "investment operations",
            "workflow and controls",
            "systematic investing",
            "data services",
            "quantitative developer teams",
            "global data services",
            "complex data environments",
            "technology infrastructure",
            "analytics infrastructure",
            "quantitative researchers",
            "database engineers",
            "investment services",
            "portfolio managers",
            "interpersonal skills",
            "communication skills",
            "negotiation skills",
            "organization skills",
            "client management skills."
        ],
        "cleaned_techs": [
            "quantitative research",
            "data operations",
            "data quality",
            "data services",
            "operational excellence",
            "continuous improvement",
            "validation",
            "model implementation",
            "investment products",
            "portfolio construction",
            "alpha generation",
            "risk models",
            "transaction cost models",
            "quant talent",
            "quantitative specialists",
            "data experts",
            "signal research",
            "optimization",
            "backtesting",
            "production quality systems",
            "process workflows",
            "quality controls",
            "total quality management",
            "operations team",
            "senior management",
            "root cause analysis",
            "model management",
            "data science",
            "quant development",
            "asset classes",
            "engineering",
            "computer science",
            "investment operations",
            "workflow and controls",
            "systematic investing",
            "quantitative developer teams",
            "global data services",
            "complex data environments",
            "technology infrastructure",
            "analytics infrastructure",
            "quantitative researchers",
            "database engineers",
            "investment services",
            "portfolio managers"
        ]
    },
    "933bae56820cc5bf": {
        "terms": [
            "data science"
        ],
        "salary_min": 143165.95,
        "salary_max": 181279.98,
        "title": "Data Modeler/Data Architect w/Databricks - Empower (remote/virtual - US based)",
        "company": "Hitachi Solutions",
        "desc": "Company Description\n   Company Overview \n  Hitachi Solutions is a global solutions integrator passionate about designing, developing, and delivering cutting edge cloud solutions to help our clients innovative across their entire business. Our firm develops the business services and technology powering some of the products you use every day \u2013 and is closely aligned with Microsoft and other leaders in the cloud computing space. \n  What sets Hitachi Solutions apart is both our industry focus, and the intellectual property that we bring to our customers. Recognized for our achievements year after year, we strive to be the trusted advisor of large and medium sized enterprises alike \u2013 helping them move fast to achieve strategic business initiatives with distinguished engineering, hard work, and compassion. With over 3,000 team members across 14 countries, in our 18 years of focus our company has seen explosive growth and high customer satisfaction. This has allowed us to offer exceptionally compelling salaries, 401k match, family leave, and health benefits. And no \u2013 we will not make you come into an office or ask for an inflexible work schedule. \n  A part of Hitachi, Ltd., our company has a long and rich history of innovation, financial strength, and international presence of one of the world\u2019s largest companies. Since 1910, Hitachi, Ltd. has been a leader in manufacturing innovative products and solutions that support industry and social infrastructure around the globe supported by 303,000 employees in over 100 countries and across 864 companies. \n \n \n \n Job Description\n   Please note:  Although this is a Remote / Virtual / Work-From-Home career opportunity, candidates  MUST  reside, and be authorized to work without sponsorship, in the United States. \n \n  This is a full-time role in our New Product organization for professionals with a proven history of execution, and a desire to rapidly expand a product organization. Our team is seeking a Senior Data Architect, with a strong background in data definition, modeling, and implementation. \n \n  This is a hands-on architect role where the individual will be responsible for the overall design of solutions within the data ecosystem serving our customers. This role will require someone who is experienced in designing data schemas using Kimball/Dimensional modeling and pipelines in Databricks for data warehouses/lakehouses and reporting systems, is able to evaluate multiple technologies and technical solutions, and be a hands-on contributor for engineering goals for multiple engineering teams. Additionally, as a technical leader within the team, you will mentor and coach your team members; therefore, top candidates have experience as either a team lead or manager. \n \n  Responsibilities \n \n  Scope and execute with independence. Work with the product team to understand platform capabilities and leverage those capabilities to bring customer\u2019s data estate. \n  Identify and articulate technical and operational requirements for data pipelines and the models they populate; be able to optimize these pipelines for high performance/resiliency workloads. \n  Instill excellence into the processes, methodologies, standards and technology choices embraced by the team with customer teams, and junior staff. \n  Lead requirements discovery and delivery workshops \u2013 helping customers understand their options and guiding them to the best options. \n  Be an advocate for the customer, and support delivery leadership (Director, VPs) in managing client expectations. \n  Dedicate time to continuous learning to keep the team and customers appraised of the latest developments in the space. \n  A committed teacher and someone who enjoys developing technical maturity across the company. \n  Experience supporting analytics, data science and/or engineering teams and understand their unique needs and challenges. \n \n \n \n \n Qualifications\n  \n \n  8+ overall years of professional experience including 4+ years\u2019 experience in designing high-scale, Big Data, Datalakehouse Solutions is  REQUIRED \n  4+ years of experience with data modeling (Kimball/Dimensional modeling), schema design patterns and modern data access patterns (including API, streams, data lake) is  REQUIRED \n  2+ years as a proven leader interacting with customers (client facing) to collect requirements and manage a technical data backlog is  REQUIRED ; therefore you must have excellent interpersonal skills, both written and spoken, as well as a consultative/collaborative style. \n  2+ years with Databricks and Spark framework are  REQUIRED \n  2+ years of experience building data applications, microservices and/or APIs using Python, Scala or an equivalent language is  REQUIRED \n  2+ years of experience with SQL, knowledgeable in complex queries and joins is  REQUIRED ; experience with UDF and/or Stored Procedure development is HIGHLY DESIRED \n  2+ years Azure Data Services including Azure Data Factory, ADLS, and Synapse is  HIGHLY DESIRED ; strong experience with AWS or other major cloud platform will be considered in lieu of Azure. \n \n \n  #REMOTE \n  #LI-CA1 \n  #azure \n  #databricks \n  #datalakehouse \n  #datamodeling \n  Additional Information\n   We are an equal opportunity employer. All applicants will be considered for employment without attention to age, race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status. \n \n  Beware of scams \n  Our recruiting team may communicate with candidates via our @hitachisolutions.com domain email address and/or via our SmartRecruiters (Applicant Tracking System) notification@smartrecruiters.com domain email address regarding your application and interview requests. \n  All offers will originate from our @hitachisolutions.com domain email address. If you receive an offer or information from someone purporting to be an employee of Hitachi Solutions from any other domain, it may not be legitimate.",
        "cleaned_desc": " \n  This is a full-time role in our New Product organization for professionals with a proven history of execution, and a desire to rapidly expand a product organization. Our team is seeking a Senior Data Architect, with a strong background in data definition, modeling, and implementation. \n \n  This is a hands-on architect role where the individual will be responsible for the overall design of solutions within the data ecosystem serving our customers. This role will require someone who is experienced in designing data schemas using Kimball/Dimensional modeling and pipelines in Databricks for data warehouses/lakehouses and reporting systems, is able to evaluate multiple technologies and technical solutions, and be a hands-on contributor for engineering goals for multiple engineering teams. Additionally, as a technical leader within the team, you will mentor and coach your team members; therefore, top candidates have experience as either a team lead or manager. \n \n  Responsibilities \n \n  Scope and execute with independence. Work with the product team to understand platform capabilities and leverage those capabilities to bring customer\u2019s data estate. \n  Identify and articulate technical and operational requirements for data pipelines and the models they populate; be able to optimize these pipelines for high performance/resiliency workloads. \n  Instill excellence into the processes, methodologies, standards and technology choices embraced by the team with customer teams, and junior staff.    \n \n  8+ overall years of professional experience including 4+ years\u2019 experience in designing high-scale, Big Data, Datalakehouse Solutions is  REQUIRED \n  4+ years of experience with data modeling (Kimball/Dimensional modeling), schema design patterns and modern data access patterns (including API, streams, data lake) is  REQUIRED \n  2+ years as a proven leader interacting with customers (client facing) to collect requirements and manage a technical data backlog is  REQUIRED ; therefore you must have excellent interpersonal skills, both written and spoken, as well as a consultative/collaborative style. \n  2+ years with Databricks and Spark framework are  REQUIRED \n  2+ years of experience building data applications, microservices and/or APIs using Python, Scala or an equivalent language is  REQUIRED \n  2+ years of experience with SQL, knowledgeable in complex queries and joins is  REQUIRED ; experience with UDF and/or Stored Procedure development is HIGHLY DESIRED \n  2+ years Azure Data Services including Azure Data Factory, ADLS, and Synapse is  HIGHLY DESIRED ; strong experience with AWS or other major cloud platform will be considered in lieu of Azure. \n ",
        "techs": [
            "databricks",
            "spark framework",
            "python",
            "scala",
            "sql",
            "azure data services",
            "azure data factory",
            "adls",
            "synapse"
        ],
        "cleaned_techs": [
            "databricks",
            "spark framework",
            "python",
            "scala",
            "sql",
            "azure",
            "adls",
            "synapse"
        ]
    },
    "60d38564e0fd0004": {
        "terms": [
            "data science"
        ],
        "salary_min": 91158.59,
        "salary_max": 115427.09,
        "title": "Sr. Evaluator - Immediate need",
        "company": "Eagle Technologies, Inc.",
        "desc": "Job Title: Senior Evaluator \u2013 Immediate Need \n  Job Description: \n  Eagle Technologies, Inc., has an immediate need for an experienced Senior Evaluator to support critical behavioral health program evaluation projects for the Substance Abuse and Mental Health Services Administration (SAMHSA), U.S. Department of Health and Human Services, and to engage in other research activities. Eagle\u2019s Research and Evaluation experts provide government decision-makers with deep, data-driven insights into the effectiveness, impact, and limitations of federal programs. Eagle possesses comprehensive experience in managing large-scale projects for the federal government, and expert knowledge of evaluation design and implementation, and data analytics. \n  The Senior Evaluator will work closely with other scientists to design and implement program evaluations and research studies and perform advanced analytics. They will coordinate with team members and clients in support of various project tasks and work under tight deadlines to fulfill client requirements.  Applicants must have experience conducting program evaluations as described below.  The successful candidate will have strong communication and writing skills and be able to effectively present data to various consumers including the U.S. Department of Health and Human Services, academia, and the public. \n  Responsibilities: \n \n  Assist with client account management to understand needs, manage expectations, and build long-term relationships (including business development activities) \n \n \n  Author reports, data briefs, executive summaries, and peer-reviewed articles summarizing evaluation or research findings \n \n \n  Conduct and supervise interviews \n \n \n  Conduct quality control on the results of other researchers \n \n \n  Design survey instruments and interview protocols \n \n \n  Design, implement, and help lead complex health evaluations \n \n \n  Design, perform, and interpret research studies \n \n \n  Develop conceptual and logic models  \n \n \n  Help manage relationships with community stakeholders and technical experts \n \n \n  Lead qualitative data coding and analysis \n \n \n  Manage large-scale data collection efforts \n \n \n  Perform literature reviews and environmental scans \n \n \n  Prepare memos, technical assistance tools/products, and webinar presentations \n \n  Requirements: \n \n  Demonstrated experience in all stages of evaluation design and implementation that include literature review, conceptual and logic model development, instrument and measures development, cognitive testing, and quantitative and qualitative data collection, management, and analysis \n \n \n  Demonstrated experience in advanced analytics for social science including multiple imputation, longitudinal analysis, factor analysis, and structural equation modeling \n \n \n  Prior evaluation, research, or lived experience with communities experiencing health or social disparities \n \n \n  Experience with natural language processing (NLP) techniques for qualitative data analysis \n \n \n  High level of proficiency in SAS, Stata, or both Strong knowledge of traditional qualitative data analysis \n \n \n  High level of proficiency in MS Office Suite and Adobe Acrobat \n \n \n  Attention to detail, with strong deductive and inductive reasoning skills \n \n \n  Ability to manage multiple assignments at once \n \n \n  Ability to mentor junior staff \n \n \n  Ability to assess, understand, and adapt to new approaches quickly \n \n \n  Ability to work effectively independently as well as within a team environment \n \n \n  Ability to clearly express ideas orally and in writing \n \n  Minimum qualifications and experience: \n \n  Master\u2019s degree in public health, behavioral or social sciences, statistics, data analytics, data science, or other relevant disciplines \n \n \n  Three years of relevant professional experience \n \n \n  Two or more years of experience leading complex health/program evaluation projects \n \n \n  Three or more years of experience participating in complex health/program evaluation projects \n \n  Preferred qualifications and experience: \n \n  Doctoral degree in public health, behavioral or social sciences, statistics, data analytics, data science, or other relevant disciplines \n \n \n  Knowledge of U.S. behavioral health programs and policy, and/or SAMHSA programs/data \n \n \n  Authored a minimum of two peer-reviewed publications in public health, the social sciences, or statistics \n \n \n  Designed survey instruments and interview protocols \n \n \n  Demonstrated expert ability to work with large data sets and extract meaningful insights \n \n \n \n \n  Demonstrated expertise in research and advanced statistical analysis methods \n \n \n  Used SAS, Stata, or open-source software to perform imputation, matching/weighting, and univariate and multivariate analyses including longitudinal analysis, factor analysis, structural equation modeling, and causal effects modeling \n \n  Other preferred: Please submit a representative research publication you have authored. \n  Work Authorization: \n  Applicants must be authorized to work in the U.S. without sponsorship, both now and in the future. \n  Background Check:  \n Candidates who receive a conditional offer of employment will be subject to a background check. \n \n \n  Eagle Technologies, Inc. is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, sexual orientation, gender identity, or any other protected categories covered under local law.",
        "cleaned_desc": " \n \n  Develop conceptual and logic models  \n \n \n  Help manage relationships with community stakeholders and technical experts \n \n \n  Lead qualitative data coding and analysis \n \n \n  Manage large-scale data collection efforts \n \n \n  Perform literature reviews and environmental scans \n \n \n  Prepare memos, technical assistance tools/products, and webinar presentations \n \n  Requirements: \n \n  Demonstrated experience in all stages of evaluation design and implementation that include literature review, conceptual and logic model development, instrument and measures development, cognitive testing, and quantitative and qualitative data collection, management, and analysis \n \n \n  Demonstrated experience in advanced analytics for social science including multiple imputation, longitudinal analysis, factor analysis, and structural equation modeling   \n \n  Prior evaluation, research, or lived experience with communities experiencing health or social disparities \n \n \n  Experience with natural language processing (NLP) techniques for qualitative data analysis \n \n \n  High level of proficiency in SAS, Stata, or both Strong knowledge of traditional qualitative data analysis \n \n \n  High level of proficiency in MS Office Suite and Adobe Acrobat \n \n \n  Attention to detail, with strong deductive and inductive reasoning skills \n \n \n  Ability to manage multiple assignments at once \n \n \n  Ability to mentor junior staff \n \n \n  Ability to assess, understand, and adapt to new approaches quickly \n   \n  Ability to work effectively independently as well as within a team environment \n \n \n  Ability to clearly express ideas orally and in writing \n \n  Minimum qualifications and experience: \n \n  Master\u2019s degree in public health, behavioral or social sciences, statistics, data analytics, data science, or other relevant disciplines \n \n \n  Three years of relevant professional experience \n \n \n  Two or more years of experience leading complex health/program evaluation projects \n \n \n  Three or more years of experience participating in complex health/program evaluation projects \n \n  Preferred qualifications and experience: \n \n  Doctoral degree in public health, behavioral or social sciences, statistics, data analytics, data science, or other relevant disciplines \n \n \n  Knowledge of U.S. behavioral health programs and policy, and/or SAMHSA programs/data ",
        "techs": [
            "conceptual and logic models",
            "qualitative data coding and analysis",
            "large-scale data collection",
            "literature reviews and environmental scans",
            "natural language processing (nlp) techniques",
            "sas",
            "stata",
            "ms office suite",
            "adobe acrobat",
            "multiple imputation",
            "longitudinal analysis",
            "factor analysis",
            "structural equation modeling",
            "traditional qualitative data analysis",
            "deductive and inductive reasoning skills",
            "mentorship",
            "assessing and adapting to new approaches",
            "public health",
            "behavioral or social sciences",
            "statistics",
            "data analytics",
            "data science",
            "health/program evaluation projects",
            "u.s. behavioral health programs and policy",
            "samhsa programs/data"
        ],
        "cleaned_techs": [
            "conceptual and logic models",
            "qualitative data coding and analysis",
            "large-scale data collection",
            "literature reviews and environmental scans",
            "nlp",
            "sas",
            "stata",
            "microsoft",
            "adobe",
            "multiple imputation",
            "longitudinal analysis",
            "factor analysis",
            "structural equation modeling",
            "traditional qualitative data analysis",
            "mentorship",
            "assessing and adapting to new approaches",
            "public health",
            "behavioral or social sciences",
            "statistics",
            "data analytics",
            "data science",
            "health/program evaluation projects",
            "u.s. behavioral health programs and policy",
            "samhsa programs/data"
        ]
    },
    "146538acc6acec2a": {
        "terms": [
            "data science"
        ],
        "salary_min": 85000.0,
        "salary_max": 90000.0,
        "title": "Oracle HCM Cloud Fusion Consultant",
        "company": "Strategic Innovation Group LLC",
        "desc": "Strategic Innovation Group  (SIG) seeks a motivated, technical resource with experience in Oracle HCM Cloud Fusion. You will be reporting to SIG's VP of Technology and working with our clients in Baltimore City. \n  $85,000 to $90,000 per year \n  SIG is a fast growing 8(a) government contractor based in Arlington, Virginia. We offer a broad range of technical expertise and experience in Digital Transformation, Data Management/Data Science, and Systems Modernization. At SIG, our people are our mission. Come join our team! A successful candidate will be offered the following: \n \n  Great work/life balance \n  Eligibility for performance-based participation in cash bonuses \n  Potential to participate in growth of the company through incentives \n  Relocation assistance may be available \n  Excellent benefits, including health, dental, vision, generous PTO, a 401(k) with match, life insurance, short- and long-term disability, and a health savings account (HSA) \n \n  Additionally, this position is expected to be fully remote. \n  ESSENTIAL DUTIES AND RESPONSIBILITIES \n  The essential functions include, but are not limited to the following: \n \n  Provide Oracle HCM Cloud Fusion functional/technical consulting services by acting as subject matter expert and assist clients through the implementation lifecycle for Oracle HCM Cloud Fusion projects \n  Advise client on options, risks, and any impacts on other processes or systems \n  Configure the Oracle HCM Cloud Fusion Applications to meet client requirements and document application set-ups \n  Write business requirement documents for reports, interfaces, data conversions and application extensions for Oracle HCM Cloud Fusion projects \n  Develop reports, interfaces, etc. based on existing client requirements \n  Assist in the development and execution of security and validation strategies for Oracle HCM Cloud Fusion projects \n  Assist client in preparing validation scripts, testing scenarios and develop test scripts for Oracle HCM Cloud Fusion projects \n  Support clients with the execution of test scripts \n  Validate configuration and data for Oracle HCM Cloud Fusion modules \n  Complete tasks efficiently and in a timely manner \n  Interact with the project team members responsible for developing reports, interfaces, data conversion programs, and application extensions \n  Provide status and issue reports to the project manager/client on a regular basis \n  Share knowledge to continually improve implementation methodology for Oracle HCM Cloud Fusion projects Required Qualifications \n \n  REQUIRED EXPERIENCE/QUALIFICATIONS \n  Minimum of five years experience performing development/configuration activities \n \n  Oracle HCM Cloud Fusion experience in at least one or more of these modules: \n \n  Core Human Resources, Benefits, Payroll, Time & Labor, Absence Management, Goal Management, Performance Management, Talent Review and Succession Management, Workforce Compensation \n \n  Hands-on experience with Fusion release 10 and above \u2022 Experience in Oracle Cloud Implementations \n  Upgrade and implementation experience \n  Ability to work on all project phases including, but not limited to, Configuration, Testing, and Production Support Preferred Qualifications \n  Effectively manages scope and customer expectations on individual assignments \u2022 Follows through on all assignments and takes ownership of client issues \n  Communicates clearly and effectively with clients and project managers \n  Consistently produces clear, concise status reports \n  Displays effective analytical skills \n  Effectively communicate and drive project deliverables for Oracle HCM Cloud Fusion modules \n \n  SPECIAL REQUIREMENTS \n \n  Must work the lesser of 1920 or allowable billable hours under contract \n  Vaccination requirements and testing are subject to the customer site requirements; testing is at the cost of the employee. \n \n  The company is an Equal Opportunity Employer, drug free workplace, and complies with ADA regulations as applicable.",
        "cleaned_desc": "  Develop reports, interfaces, etc. based on existing client requirements \n  Assist in the development and execution of security and validation strategies for Oracle HCM Cloud Fusion projects \n  Assist client in preparing validation scripts, testing scenarios and develop test scripts for Oracle HCM Cloud Fusion projects \n  Support clients with the execution of test scripts \n  Validate configuration and data for Oracle HCM Cloud Fusion modules \n  Complete tasks efficiently and in a timely manner \n  Interact with the project team members responsible for developing reports, interfaces, data conversion programs, and application extensions \n  Provide status and issue reports to the project manager/client on a regular basis \n  Share knowledge to continually improve implementation methodology for Oracle HCM Cloud Fusion projects Required Qualifications ",
        "techs": [
            "oracle hcm cloud fusion"
        ],
        "cleaned_techs": [
            "oracle"
        ]
    },
    "8f88cef711f4b4b0": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "201 Third Street (61049), United States of America, San Francisco, California\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "201 Third Street (61049), United States of America, San Francisco, California\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java"
        ]
    },
    "df7fd6c0a6917026": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "llms",
            "fms",
            "apis",
            "sdks",
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems",
            "neural networks",
            "ai"
        ],
        "cleaned_techs": [
            "llm",
            "fms",
            "apis",
            "sdks",
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems",
            "neural networks",
            "ai"
        ]
    },
    "276b84746413c821": {
        "terms": [
            "data science"
        ],
        "salary_min": 112312.72,
        "salary_max": 142212.92,
        "title": "Senior Statistical Programmer",
        "company": "Katalyst Healthcares & Life Sciences Inc",
        "desc": "Responsibilities: \n \n In-depth knowledge of data structures and relevant programming languages for data manipulation and reporting. Knowledge of SAS is required, may include SAS, etc.. \n Expert knowledge of Define.xml, SDTM aCRF. \n Expert knowledge of data structures (e.g., CDISC SDTM, CDISC CT) and their implementation. \n Experience in working with SDTM Conformance Checks \n SDTM mapping is a must. \n Preparing and validating submission packages, i.e., define.xml, Reviewers Guide \n Serving as team player, with a willingness to go the extra distance to get results, meet deadlines, etc. \n Being adaptable and flexible when priorities change \n Strong SAS data manipulation, analysis, and reporting skills. \n Ability to implement the latest CDISC SDTM standards (production/validation). \n Experience and or familiar with Pinnacle21 \n Experience supporting Medical Affairs, Immunology, Cardiovascular, Oncology, Infectious Disease strongly desirable. \n Excellent analytical & troubleshooting skills. \n Experience with study design and application of statistical methods for clinical trials \n Experience with CDISC standard datasets (SDTM and ADaM) is desirable. \n Submissions experience utilizing define.xml and other submission. \n Familiarity with drug development life cycle and experience with the manipulation, analysis, and reporting of clinical trials' data \n \n Requirements: \n \n BSc/ MSC degree or equivalent, preferably in a scientific discipline such as Statistics, Computer Science, Mathematics & Life Sciences etc. \n Must have relevant industry experience3-5 years' experience. \n Proficient with R and SAS (Base SAS, SAS/STAT, SAS Macro and SAS SQL) programming \n SAS Certified Base Programmer and Advanced Programmer are preferred, but not required. \n Knowledge of MS Excel, Word, Access, and PowerPoint is essential. \n Excellent organizational and communication skills and strong attention to details \n Ability to manage multiple tasks and work independently. \n \n Job Type: Contract \n Experience level: \n \n 4 years \n \n Schedule: \n \n Day shift \n Monday to Friday \n \n Experience: \n \n Informatica: 1 year (Preferred) \n SQL: 1 year (Preferred) \n Data warehouse: 1 year (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": "Responsibilities: \n \n In-depth knowledge of data structures and relevant programming languages for data manipulation and reporting. Knowledge of SAS is required, may include SAS, etc.. \n Expert knowledge of Define.xml, SDTM aCRF. \n Expert knowledge of data structures (e.g., CDISC SDTM, CDISC CT) and their implementation. \n Experience in working with SDTM Conformance Checks \n SDTM mapping is a must. \n Preparing and validating submission packages, i.e., define.xml, Reviewers Guide \n Serving as team player, with a willingness to go the extra distance to get results, meet deadlines, etc.   Being adaptable and flexible when priorities change \n Strong SAS data manipulation, analysis, and reporting skills. \n Ability to implement the latest CDISC SDTM standards (production/validation). \n Experience and or familiar with Pinnacle21 \n Experience supporting Medical Affairs, Immunology, Cardiovascular, Oncology, Infectious Disease strongly desirable. \n Excellent analytical & troubleshooting skills. \n Experience with study design and application of statistical methods for clinical trials \n Experience with CDISC standard datasets (SDTM and ADaM) is desirable. \n Submissions experience utilizing define.xml and other submission.   Familiarity with drug development life cycle and experience with the manipulation, analysis, and reporting of clinical trials' data \n \n Requirements: \n \n BSc/ MSC degree or equivalent, preferably in a scientific discipline such as Statistics, Computer Science, Mathematics & Life Sciences etc. \n Must have relevant industry experience3-5 years' experience. \n Proficient with R and SAS (Base SAS, SAS/STAT, SAS Macro and SAS SQL) programming \n SAS Certified Base Programmer and Advanced Programmer are preferred, but not required. \n Knowledge of MS Excel, Word, Access, and PowerPoint is essential. ",
        "techs": [
            "sas",
            "define.xml",
            "sdtm",
            "cdisc sdtm",
            "cdisc ct",
            "sdtm conformance checks",
            "sdtm mapping",
            "define.xml",
            "pinnacle21",
            "medical affairs",
            "immunology",
            "cardiovascular",
            "oncology",
            "infectious disease",
            "cdisc sdtm standards",
            "adam",
            "drug development life cycle",
            "r",
            "base sas",
            "sas/stat",
            "sas macro",
            "sas sql",
            "sas certified base programmer",
            "sas certified advanced programmer",
            "ms excel",
            "ms word",
            "ms access",
            "ms powerpoint."
        ],
        "cleaned_techs": [
            "sas",
            "define.xml",
            "sdtm",
            "cdisc sdtm",
            "cdisc ct",
            "sdtm conformance checks",
            "sdtm mapping",
            "pinnacle21",
            "medical affairs",
            "immunology",
            "cardiovascular",
            "oncology",
            "infectious disease",
            "cdisc sdtm standards",
            "adam",
            "drug development life cycle",
            "r",
            "base sas",
            "excel",
            "microsoft",
            "ms access",
            "ms powerpoint."
        ]
    },
    "7e3ba144998b50f2": {
        "terms": [
            "data science"
        ],
        "salary_min": 85000.0,
        "salary_max": 102000.0,
        "title": "T&E Engineer/Analyst",
        "company": "Man-Machine Systems Assessment",
        "desc": "Overview: \n At MSA, you will be working with a diverse team of experts working to strengthen the preparedness of our Nation through innovation in technical and analytical services. While our typical team member has a master\u2019s degree in his or her field of expertise, we also have a mix of Ph.D. scientists, analysts, and technicians \u2013 many with backgrounds in military or government service. We look for candidates who want to embrace our culture, our philosophy, and our mission to prepare and strengthen our nation and our communities. \n Principal Duties and Responsibilities: \n Plan and perform engineering research, design development, and other assignments in conformance with design, engineering, and customer specifications in the arena of Enterprise Data Management (EDM). May be responsible for the technical/engineering part of a project and expected to work with a team. May be responsible for coordinating the activities of technicians assigned to specific engineering projects. \n Location: \n Remote work with the need for occasional meetings onsite at DOT&E in DC. Strong preference for candidates to be within commuting distance to NCR. \n Required Education/Experience: \n STEM master\u2019s degree (e.g., operations research, engineering, applied mathematics, computer science, physics, data science, or other related technical field), with at least 1 year of experience related to Defense Test and Evaluation (T&E); or a STEM bachelor\u2019s degree with at least 3 years of experience related to Defense T&E. Experience that also includes Defense T&E policy is preferred. \n Ability to acquire a DoD security clearance is required, with preference given to candidates currently holding, at minimum, a SECRET level clearance. \n About MSA: \n Our team has been recognized repeatedly for excellence in job performance and client satisfaction. Whether you\u2019re a client or an employee, your success is our success. \n We don\u2019t just say we want our employees to thrive; we create an environment that fosters growth and provides stability. That\u2019s why we partnered with the top carriers in the nation to build one of the most comprehensive benefits packages in our industry. \n - Carefirst BCBS Medical plans (PPO, POS, HMO) - Paid Time Off and Holidays - 401(k) retirement matching programs - Employer-paid dental and vision insurance - Employer-paid short-term and long-term disability insurance - Employer-paid Life and AD&D Insurance - TriCare Supplemental Insurance - Employee Assistance Program (EAP) - Tuition and Training Reimbursement \n The secret to our success is attracting and retaining high-caliber talent. If you have a passion for learning, love to tackle challenges and want to work for a company that values you and your contributions \u2013 MSA is your place! \n MSA is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, sexual orientation, gender identity, or any other characteristic protected by law. \n This position has a December start date. \n Job Type: Full-time \n Pay: $85,000.00 - $102,000.00 per year \n Benefits: \n \n 401(k) \n 401(k) matching \n Dental insurance \n Employee assistance program \n Health insurance \n Life insurance \n Paid time off \n Parental leave \n Professional development assistance \n Referral program \n Vision insurance \n \n Schedule: \n \n Monday to Friday \n \n Supplemental pay types: \n \n Bonus opportunities \n \n Education: \n \n Bachelor's (Required) \n \n Security clearance: \n \n Secret (Preferred) \n \n Work Location: Hybrid remote in Washington, DC 20301",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "9722a420780b591b": {
        "terms": [
            "data science"
        ],
        "salary_min": 123659.58,
        "salary_max": 156580.58,
        "title": "Senior Statistical Programmer (Remote)",
        "company": "Everest Clinical Research USA",
        "desc": "Everest Clinical Research (\u201cEverest\u201d) is a full-service contract research organization (CRO) providing a broad range of expertise-based clinical research services to worldwide pharmaceutical, biotechnology, and medical device industries. We serve some of the best-known companies and work with many of the most advanced drugs, biologics, and medical devices in development today. \n  Everest has been an independent CRO since 2004 with a strong foundation as a statistical and data management center of excellence. Building on this foundation, Everest has successfully developed and established itself as a full-service CRO. Everest\u2019s headquarters are located in Markham (Greater Toronto Area), Ontario, Canada with additional sites in Bridgewater (Greater New York City Area), New Jersey, USA, Shanghai (Pudong Zhangjiang New District), China and Taipei, Taiwan. \n  Everest is known in the industry for its high quality deliverables, superior customer service, and flexibility in meeting clients\u2019 needs. A dynamic organization with an entrepreneurial origin, Everest continues to experience exceptional growth and great success. \n  Quality is our backbone, customer-focus is our tradition, flexibility is our strength\u2026that\u2019s us\u2026that\u2019s Everest. \n  To drive continued success in this exciting clinical research field, we are seeking committed, skilled, and customer-focused individuals to join our winning team as  Senior Statistical Programmers  for our Bridgewater, New Jersey, USA on-site location, or remotely from a home-based office anywhere in the USA in accordance with our Work from Home policy. \n  Key Accountabilities: \n \n \n Design and specify study data tabulation models and analysis data models for clinical trials or for the integration of clinical trial data from multiple trials (e.g. ISS and ISE datasets). These designs and specifications are completed in accordance with common industry standards and conventions, statistical requirements and specifications, and/or clinical trial sponsor\u2019s requirements. \n \n \n Generate complete and efficient analysis data models following approved dataset designs or specifications.  Perform independent validation of datasets created by other programmers or statisticians. \n \n \n Develop SAS programming codes and generate complete and accurate statistical output reports of trial data in well-defined formats. \n \n \n Develop SAS programming codes to independently validate statistical output reports of trial data generated by other programmers or statisticians. \n \n \n Develop and test SAS codes for clinical trial database logical checks and reports for ongoing data review. \n \n \n Document data and programming information in accordance with corporate SOPs and guidelines. \n \n \n Archive clinical trial data (study data tabulation models and analysis data models) and programming information in accordance with corporate archival SOPs and guidelines. \n \n \n Develop and provide expertise in other programming and system administration areas when required. \n \n \n Provide technical guidance and support to less experienced Statistical Programmers. \n \n \n Contribute to the continuous improvement of the statistical programming processes and procedures. \n \n \n Contribute to the establishment of therapeutic area programming standards/conventions/procedures within assigned therapeutic areas as a lead programmer. \n \n \n Contribute to the establishment of client specific programming standards/conventions/procedures for assigned clients when required. \n \n \n Qualifications and Experience: \n \n \n A Master\u2019s in Computer Science or Math/Statistics with at least five years experience \n \n \n \n \n  To find out more about Everest Clinical Research and to review other opportunities, please visit our website at www.ecrscorp.com \n  We thank all interested applicants, however, only those selected for an interview will be contacted. \n  Everest is committed to upholding the principles of dignity, independence, integration, and equal opportunity. We welcome and encourage applications from people with disabilities, and upon request we will provide accommodations for candidates participating in any part of our recruitment and selection process. \n \n  Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities \n  The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor\u2019s legal duty to furnish information. 41 CFR 60-1.35(c)",
        "cleaned_desc": " Generate complete and efficient analysis data models following approved dataset designs or specifications.  Perform independent validation of datasets created by other programmers or statisticians. \n \n \n Develop SAS programming codes and generate complete and accurate statistical output reports of trial data in well-defined formats. \n \n \n Develop SAS programming codes to independently validate statistical output reports of trial data generated by other programmers or statisticians. \n \n \n Develop and test SAS codes for clinical trial database logical checks and reports for ongoing data review. \n   \n Document data and programming information in accordance with corporate SOPs and guidelines. \n \n \n Archive clinical trial data (study data tabulation models and analysis data models) and programming information in accordance with corporate archival SOPs and guidelines. \n \n \n Develop and provide expertise in other programming and system administration areas when required. \n \n \n Provide technical guidance and support to less experienced Statistical Programmers. ",
        "techs": [
            "sas programming",
            "statistical output reports",
            "trial data",
            "dataset designs",
            "specifications",
            "data review",
            "clinical trial database logical checks",
            "corporate sops",
            "archival sops",
            "system administration. "
        ],
        "cleaned_techs": [
            "sas",
            "statistical output reports",
            "trial data",
            "dataset designs",
            "specifications",
            "data review",
            "clinical trial database logical checks",
            "corporate sops",
            "archival sops",
            "system administration. "
        ]
    },
    "2da4950af8d598e2": {
        "terms": [
            "data science",
            "machine learning engineer",
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Sr Distinguished Engineer, Generative AI Systems - (Remote- Eligible)",
        "company": "Capital One",
        "desc": "Locations: VA - McLean, United States of America, McLean, Virginia\n   Sr Distinguished Engineer, Generative AI Systems - (Remote- Eligible)\n  \n  Sr Distinguished Engineer, Generative AI Systems \n \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Senior Distinguished Engineer, AI Systems, to help us build the foundations of our enterprise AI Capabilities. You will work on a wide range of initiatives, whether that\u2019s designing robust, secure infrastructure, building large-scale distributed training clusters, deploying LLMs on GPU instances for real-time use cases, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work with a team of AI engineers and researchers to envision the target state of our capabilities while helping to design and implement key services. Examples of projects you will work on include: \n \n \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build infrastructure for serving large ML models, in our public cloud. \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 9 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 6 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud. \n \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $315,100 - $359,700 for Sr Distinguished Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $333,900 - $381,000 for Sr Distinguished Machine Learning Engineer\n   Remote (Regardless of Location): $267,100 - $304,800 for Sr Distinguished Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 9 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 6 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud.   \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n ",
        "techs": [
            "llms and fms",
            "mlops",
            "python",
            "c/c++",
            "ai and ml frameworks",
            "public cloud",
            "aws",
            "azure",
            "gcp",
            "gpu clusters",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "llm hosting and fine-tuning",
            "neural networks",
            "distributed training",
            "sysml."
        ],
        "cleaned_techs": [
            "llm",
            "mlops",
            "python",
            "c/c++",
            "ai",
            "public cloud",
            "aws",
            "azure",
            "gcp",
            "gpu clusters",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "neural networks",
            "distributed training",
            "sysml."
        ]
    },
    "cfdc4b02fb0d7c48": {
        "terms": [
            "data science"
        ],
        "salary_min": 111431.586,
        "salary_max": 141097.22,
        "title": "Director of Business Intelligence",
        "company": "Love in Faith",
        "desc": "This is a fully remote position within the United States. \n  About Love in Faith \n  Love in Faith is a thriving faith-based clothing and accessories startup. Here, we're not just a company; we're a team of enthusiasts, dreamers, and doers that strive to deliver exceptional products with a customer-focused experience you can trust. With fully remote positions, you can work from anywhere while contributing to our exciting growth story. \n  Here's why you should consider us: \n \n Rapid Growth: We're not your typical startup. Our exponential growth speaks volumes about the opportunities that await you. Be part of something big! \n Remote Flexibility: Enjoy the freedom to work from your preferred location, without sacrificing your career goals. \n Culture: Our team knows how to work hard and play harder. We value creativity, innovation, and a positive work environment. Join us in celebrating successes, big and small. \n \n \n \n About the Role \n  Love in Faith is seeking a Director of Business Intelligence to lead our efforts in gathering, analyzing, and transforming data into actionable insights. The Director of Business Intelligence will play a crucial role in helping the company make data-driven decisions and will proactively identify opportunities for omni business performance insights. If you are a strategic thinker with a passion for data and analytics, we invite you to join our team. \n  Responsibilities: \n \n Business Intelligence Strategy: \n \n Proactively identify value-add opportunities for omni business performance insights based on guidance from senior management and ongoing dialogue with cross-functional business users. \n Define the vision and strategic direction for the Business Intelligence function to align with the company's goals and objectives. \n \n Data Management and Governance: \n \n Establish and maintain data governance principles, ensuring data quality, accuracy, and consistency. \n Oversee data integration, data warehousing, and data architecture to support BI initiatives. \n \n Reporting and Analysis: \n \n Define appropriate timelines and cadences of business reporting within the company's data sharing principles. \n Develop and maintain a comprehensive suite of reports and dashboards to monitor key performance indicators (KPIs) and provide insights to stakeholders. \n \n Team Leadership: \n \n Manage a team of BI analysts and data scientists, providing leadership, coaching, and mentorship. \n Delegate responsibilities, set performance goals, and conduct regular performance reviews. \n \n Project Management: \n \n Develop concrete strategies and project plans for the team to execute business requests, data analysis, and reporting initiatives. \n Ensure projects are delivered on time, within budget, and meet quality standards. \n \n Cross-Functional Collaboration: \n \n Collaborate with various departments to understand their data and reporting needs. \n Establish strong working relationships with stakeholders to ensure that BI solutions align with business goals. \n \n Continuous Improvement: \n \n Stay current with industry best practices, emerging technologies, and trends in business intelligence. \n Drive innovation and improvement in BI tools, methodologies, and processes. \n \n \n Requirements \n \n Bachelor's degree in Business, Computer Science, Data Science, or a related field (Master's degree preferred). \n Proven experience in business intelligence, data analysis, and data-driven decision-making, preferably in the retail or fashion industry. \n Strong leadership and team management skills. \n Proficiency in data visualization tools (e.g., Tableau, Power BI) and data warehousing. \n Experience with SQL and database management. \n Excellent analytical and problem-solving abilities. \n Effective communication and presentation skills. \n Strong project management skills and the ability to manage multiple initiatives simultaneously. \n \n \n \n Physical Demands \n  While performing the duties of this job, the employee routinely is required to sit; walk; talk and hear; use hands to keyboard, finger, handle, and feel; stoop, kneel, crouch, twist, reach, and stretch. Speaking and hearing ability sufficient to communicate in person, over the telephone, and/or via video conference. \n \n The ability to stand, walk, and sit in a computer chair for long periods of time. \n The ability to see and respond to dangerous situations. \n Speaking and hearing ability sufficient to communicate in person, over the telephone and/or via video conferences. \n Sufficient hand, arm, and finger dexterity to operate a computer keyboard and other office equipment. \n \n Emotional Demands \n \n Data analysis & interpretation. You will have to process and review a lot of information from a variety of sources, understand how data is collected, assess data quality, and how to use the information. \n Communication skills. You need to have advanced communication skills in both oral and written form. Emails and written communication with colleagues and external partners, written reports for senior executives. Able to communicate complex financial information to people outside of the finance department. \n Comfortable with technology. Able to navigate data through computers, mobile, software, databases. Stay up to date with technology advances. \n Oriented to detail. Financial forecasts rely on projections which can be impacted by even minor changes in sales patterns, consumer sentiment, economic shifts, competitor changes, etc. You will need to be attuned to small changes in all streams of data. \n Confident decision-making skills. You will need to review data and make sound decisions on what actions to take and make confident recommendations to senior management. You may need to make decisions quickly with limited amounts of information in urgent situations. \n \n \n \n For Applicants with Disabilities \n  Reasonable accommodation will be made for those that qualified during application process. If you need accommodations during the hiring process, please let us know when you submit your application, and we\u2019ll do our very best to adjust as needed. \n  Benefits \n \n Medical, Dental, Vision \n Health Savings (HSA) & Flexible Spending Account (FSA) \n Company Paid Life Insurance \n Supplemental Benefits Available\n    \n Accident Insurance \n Critical Illness Insurance \n \n Unlimited PTO Policy \n Paid Parental Leave \n Other Company Perks\n    \n Monthly Utility Stipend \n Team Meetings Coffee/Food Stipend \n Health & Wellness Stipend \n Education & Professional Development Stipend \n Charitable Gift Matching",
        "cleaned_desc": " Data Management and Governance: \n \n Establish and maintain data governance principles, ensuring data quality, accuracy, and consistency. \n Oversee data integration, data warehousing, and data architecture to support BI initiatives. \n \n Reporting and Analysis: \n \n Define appropriate timelines and cadences of business reporting within the company's data sharing principles. \n Develop and maintain a comprehensive suite of reports and dashboards to monitor key performance indicators (KPIs) and provide insights to stakeholders. \n \n Team Leadership: \n \n Manage a team of BI analysts and data scientists, providing leadership, coaching, and mentorship. \n Delegate responsibilities, set performance goals, and conduct regular performance reviews. \n \n Project Management: \n \n Develop concrete strategies and project plans for the team to execute business requests, data analysis, and reporting initiatives. \n Ensure projects are delivered on time, within budget, and meet quality standards. \n   Cross-Functional Collaboration: \n \n Collaborate with various departments to understand their data and reporting needs. \n Establish strong working relationships with stakeholders to ensure that BI solutions align with business goals. \n \n Continuous Improvement: \n \n Stay current with industry best practices, emerging technologies, and trends in business intelligence. \n Drive innovation and improvement in BI tools, methodologies, and processes. \n \n \n Requirements \n \n Bachelor's degree in Business, Computer Science, Data Science, or a related field (Master's degree preferred). \n Proven experience in business intelligence, data analysis, and data-driven decision-making, preferably in the retail or fashion industry. \n Strong leadership and team management skills. \n Proficiency in data visualization tools (e.g., Tableau, Power BI) and data warehousing. \n Experience with SQL and database management. \n Excellent analytical and problem-solving abilities. \n Effective communication and presentation skills.   Strong project management skills and the ability to manage multiple initiatives simultaneously. \n \n \n \n Physical Demands \n  While performing the duties of this job, the employee routinely is required to sit; walk; talk and hear; use hands to keyboard, finger, handle, and feel; stoop, kneel, crouch, twist, reach, and stretch. Speaking and hearing ability sufficient to communicate in person, over the telephone, and/or via video conference. \n \n The ability to stand, walk, and sit in a computer chair for long periods of time. \n The ability to see and respond to dangerous situations. \n Speaking and hearing ability sufficient to communicate in person, over the telephone and/or via video conferences. \n Sufficient hand, arm, and finger dexterity to operate a computer keyboard and other office equipment. \n \n Emotional Demands \n \n Data analysis & interpretation. You will have to process and review a lot of information from a variety of sources, understand how data is collected, assess data quality, and how to use the information. \n Communication skills. You need to have advanced communication skills in both oral and written form. Emails and written communication with colleagues and external partners, written reports for senior executives. Able to communicate complex financial information to people outside of the finance department. \n Comfortable with technology. Able to navigate data through computers, mobile, software, databases. Stay up to date with technology advances. \n Oriented to detail. Financial forecasts rely on projections which can be impacted by even minor changes in sales patterns, consumer sentiment, economic shifts, competitor changes, etc. You will need to be attuned to small changes in all streams of data. \n Confident decision-making skills. You will need to review data and make sound decisions on what actions to take and make confident recommendations to senior management. You may need to make decisions quickly with limited amounts of information in urgent situations. \n ",
        "techs": [
            "tableau",
            "power bi",
            "sql",
            "database management"
        ],
        "cleaned_techs": [
            "tableau",
            "powerbi",
            "sql",
            "database management"
        ]
    },
    "570f6e195ceeb9bc": {
        "terms": [
            "data science"
        ],
        "salary_min": 138457.1,
        "salary_max": 175317.53,
        "title": "Sr. Software Engineer - AI Specialist",
        "company": "Software AG",
        "desc": "Software AG simplifies the connected world. Founded in 1969 it helps deliver the experiences that employees, partners, and customers now expect. Its technology creates the digital backbone that integrates applications, devices, data, and clouds; empowers streamlined processes; and connects \u201cthings\u201d like sensors, devices, and machines. It helps 10,000+ organizations to become a truly connected enterprises and make smarter decisions, faster.\n  \n \n \n   Our story goes beyond technology. We put people first \u2013 employees, customers, and partners. We build strong teams and cultivate relationships that last. We provide incomparable products, solutions, services, and technical excellence for our customers. We are a team of over 5,000 colleagues across 70+ countries who value inclusion, integrity, and innovation. Our size means everyone has an impact and every voice is valued. We are big enough to compete and small enough to care.\n  \n \n \n   Be you, join us.\n  \n \n   We are currently seeking a Senior Software Engineer - AI Specialist. You will take ownership of creating high-quality software solutions utilizing advanced generative AI technologies and will work closely with our global, cross-functional teams, influencing technical decisions and ensuring the success of our product delivery.\n  \n \n \n   Essential functions:\n  \n \n \n \n     You\u2019ll act as an individual contributor with the ownership and initiative of a leader\u2013 not as a people manager, but as a technical flag-bearer of our webMethods AI\u2019s core generative AI development with an urgency to ship.\n    \n \n \n     Develop and execute innovative software solutions using not only traditional technologies such as AWS, Azure, Kubernetes, Node.js, Python, or Golang but also embracing the latest generative AI technologies and various large language models (LLMs). This will involve leveraging these technologies to create advanced, scalable solutions that drive business objectives.\n    \n \n \n     Collaborate with product managers, architects, and stakeholders to translate business needs into technical requirements.\n    \n \n \n     Adhere to coding standards and best practices to write efficient, maintainable code.\n    \n \n \n     Ensure software application reliability, performance, and security through comprehensive testing and debugging.\n    \n \n \n     Be involved in the complete software development lifecycle from requirements gathering to maintenance post-deployment.\n    \n \n \n     Work collaboratively with cross-functional teams to resolve software dependencies and ensure smooth integration of components.\n    \n \n \n     Keep abreast of emerging generative AI technologies and industry trends to evaluate their potential implementation in our solutions.\n    \n \n \n     Work with the team to identify and solve technical challenges innovatively and practically.\n    \n \n \n     Mentor junior developers, fostering a culture of knowledge sharing and professional growth.\n    \n \n \n     Improve engineering processes, tools, and methodologies continuously to boost productivity and efficiency\n    \n \n \n \n   Minimum Requirements\n  \n \n \n \n     Bachelor's or Master's degree in Computer Science, Software Engineering, or related discipline.\n    \n \n \n     At least 6 years of professional experience in software development, focused on creating scalable, robust applications.\n    \n \n \n     Proficiency in AWS, Azure, Kubernetes, Node.js, Python, or Golang.\n    \n \n \n     Solid understanding of software engineering and generative AI principles, design patterns, and best practices.\n    \n \n \n     Experience with Agile development methodologies and tools.\n    \n \n \n     Ability to write clean, efficient, and maintainable code.\n    \n \n \n     Strong analytical and problem-solving skills.\n    \n \n \n     Excellent communication skills, with the ability to collaborate effectively with cross-functional teams.\n    \n \n \n     Experience with containerization and orchestration tools like Docker and Kubernetes.\n    \n \n \n     Familiarity with DevOps practices and CI/CD pipelines.\n    \n \n \n     Knowledge of SQL and NoSQL database technologies.\n    \n \n \n     Exposure to frontend technologies like React or Angular is a plus.\n    \n \n \n     Experience with cloud-native architectures and serverless computing is desirable.\n    \n \n \n \n   Nice to Haves\n  \n \n \n \n     OpenAI APIs and models\n    \n \n \n     LangChain\n    \n \n \n     Vector databases like Pinecone\n    \n \n \n     Prompt engineering for LLMs (large language models)\n    \n \n \n     Other AI/ML models, frameworks, and tools\n    \n \n \n \n   What\u2019s in it for you?\n  \n \n \n \n     Earn competitive total compensation and receive comprehensive country-specific medical and other benefits.\n    \n \n \n     Enjoy time and location flexibility with our Hybrid Working Model, which allows a remote workshare of up to 60%. Work anywhere in your country or abroad for up to 10 days per year.\n    \n \n \n     Set yourself up for success in your new role by upgrading your home office space using your one-time hybrid work payment.\n    \n \n \n     Lean on the Employee Assistance Program for support during some of life\u2019s most common but difficult challenges.\n    \n \n \n \n   At Software AG we are committed to providing an environment of mutual respect and fairness where equal employment opportunities are available to all applicants and employees without regard to race, colour, religion, gender, pregnancy, national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, and any other characteristic protected by applicable law.\n  \n \n   We believe that diversity, equity, and inclusion is critical to our success as a global company, and we seek to recruit, compensate, develop, promote, and retain the most talented people from a diverse candidate pool.\n  \n \n \n   #LI-AS1\n  \n \n   #LI-Remote\n  \n \n \n   It is the policy of the Company that employment decisions shall be based on merit, qualifications, and competence. Employment practices shall not be influenced or affected by virtue of an applicant\u2019s or Employee\u2019s age, race, color, gender, gender identity or expression, genetics, sex, sexual orientation, marital status, pregnancy, national origin, ancestry, religion, disability, protected veteran status and other protected classifications. In addition, it is Company policy to provide an environment that is free of unlawful harassment of any kind, including that which is sexual, age-related, or ethnic. This policy governs all aspects of employment, promotion, assignment, discharge, and other terms and conditions of employment.\n    Software AG is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration without regard to race, age, religion, color, marital status, national origin, gender, gender identity or expression, sexual orientation, disability, or veteran status. It is the policy of the Company that employment decisions shall be based on merit, qualifications, and competence. Employment practices shall not be influenced or affected by virtue of an applicant\u2019s or Employee\u2019s age, race, color, gender, gender identity or expression, genetics, sex, sexual orientation, marital status, pregnancy, national origin, ancestry, religion, disability, protected veteran status and other protected classifications. In addition, it is Company policy to provide an environment that is free of unlawful harassment of any kind, including that which is sexual, age-related, or ethnic. This policy governs all aspects of employment, promotion, assignment, discharge, and other terms and conditions of employment.",
        "cleaned_desc": "     At least 6 years of professional experience in software development, focused on creating scalable, robust applications.\n    \n \n \n     Proficiency in AWS, Azure, Kubernetes, Node.js, Python, or Golang.\n    \n \n \n     Solid understanding of software engineering and generative AI principles, design patterns, and best practices.\n    \n \n \n     Experience with Agile development methodologies and tools.\n    \n \n \n     Ability to write clean, efficient, and maintainable code.\n    \n \n \n     Strong analytical and problem-solving skills.\n    \n \n \n     Excellent communication skills, with the ability to collaborate effectively with cross-functional teams.\n    \n \n \n     Experience with containerization and orchestration tools like Docker and Kubernetes.\n    \n \n \n     Familiarity with DevOps practices and CI/CD pipelines.\n    \n \n \n     Knowledge of SQL and NoSQL database technologies.     \n \n \n     Exposure to frontend technologies like React or Angular is a plus.\n    \n \n \n     Experience with cloud-native architectures and serverless computing is desirable.\n    \n \n \n \n   Nice to Haves\n  \n \n \n \n     OpenAI APIs and models\n    \n \n \n     LangChain\n    \n \n \n     Vector databases like Pinecone\n    \n \n \n     Prompt engineering for LLMs (large language models)\n    \n \n \n     Other AI/ML models, frameworks, and tools\n    \n \n ",
        "techs": [
            "aws",
            "azure",
            "kubernetes",
            "node.js",
            "python",
            "golang",
            "docker",
            "kubernetes",
            "sql",
            "nosql",
            "react",
            "angular",
            "openai apis",
            "langchain",
            "pinecone (vector databases)",
            "prompt engineering for llms",
            "other ai/ml models",
            "frameworks",
            "and tools"
        ],
        "cleaned_techs": [
            "aws",
            "azure",
            "kubernetes",
            "node.js",
            "python",
            "golang",
            "docker",
            "sql",
            "nosql",
            "react",
            "angular",
            "openai apis",
            "langchain",
            "pinecone (vector databases)",
            "prompt engineering for llms",
            "other ai/ml models",
            "frameworks",
            "and tools"
        ]
    },
    "dd2b85252a650d20": {
        "terms": [
            "data science",
            "machine learning engineer",
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Distinguished Engineer, Generative AI Systems (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Locations: NY - New York, United States of America, New York, New York\n   Distinguished Engineer, Generative AI Systems (Remote-Eligible)\n  \n  Distinguished Engineer, Generative AI Systems \n \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Senior Distinguished Engineer, AI Systems, to help us build the foundations of our enterprise AI Capabilities. You will work on a wide range of initiatives, whether that\u2019s designing robust, secure infrastructure, building large-scale distributed training clusters, deploying LLMs on GPU instances for real-time use cases, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work with a team of AI engineers and researchers to envision the target state of our capabilities while helping to design and implement key services. Examples of projects you will work on include: \n \n \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build infrastructure for serving large ML models, in our public cloud. \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 7 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 5 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud. \n \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $269,400 - $307,500 for Distinguished Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $285,400 - $325,700 for Distinguished Machine Learning Engineer\n   Remote (Regardless of Location): $228,300 - $260,600 for Distinguished Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 7 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 5 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud.   \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n ",
        "techs": [
            "llms",
            "fms",
            "mlops",
            "python",
            "c/c++",
            "ai and ml frameworks",
            "public cloud",
            "aws",
            "azure",
            "gcp",
            "gpu clusters",
            "ml compilers",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "llm hosting",
            "fine-tuning",
            "neural networks",
            "distributed training",
            "sysml."
        ],
        "cleaned_techs": [
            "llm",
            "fms",
            "mlops",
            "python",
            "c/c++",
            "ai",
            "public cloud",
            "aws",
            "azure",
            "gcp",
            "gpu clusters",
            "ml compilers",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "fine-tuning",
            "neural networks",
            "distributed training",
            "sysml."
        ]
    },
    "af17fadda600ff8e": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing \n  At least 4 years of experience with HPCs, vector embedding, or semantic search technologies \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing systems for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n    Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ],
        "cleaned_techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ]
    },
    "9a8c7a0b76116feb": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing \n  At least 4 years of experience with HPCs, vector embedding, or semantic search technologies \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing systems for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n    Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ],
        "cleaned_techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ]
    },
    "f570cd587b1ada36": {
        "terms": [
            "data science"
        ],
        "salary_min": 49800.0,
        "salary_max": 102000.0,
        "title": "Frontend Web Developer, Junior",
        "company": "Booz Allen Hamilton",
        "desc": "Job Description \n \n \n \n \n \n \n \n \n \n \n         Location: \n         \n \n         Sterling,VA,US \n         \n \n \n \n         Remote Work: \n         \n \n         Hybrid \n         \n \n \n \n         Job Number: \n         \n \n         R0182327\n         \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n         Frontend Web Developer, Junior\n           The Opportunity: \n  The right interface can make a tool, system, or site easy to use, encourage early adoption, and save time and resources. We\u2019re looking for you, a web developer who will use equal parts skill and vision to create an experience that delivers functionality and efficiency. \n \n  Bring your passion for creating an amazing user experience to Booz Allen. This is an opportunity to contribute to a project that meets challenges in the mission area and customer industry by collaborating with the development team to build a type of system, tool, or web applications with user-centric design. You\u2019ll work with a team of UI/UX designers and developers to create a seamless user experience using React, Angular, or Ember. \n \n  On our team, you\u2019ll hone your skills and learn how to ensure accessibility for all users by developing a front end that functions across browsers, platforms, and devices while meeting accessibility and security requirements. With mentoring, positive code reviews, and opportunities to learn new tools and skills, we focus on growing as a team to make the best solutions for our customers. \n \n  Work with us as we shape systems to change the mission area and customer industry for the better. \n \n  Join us. The world can\u2019t wait. \n \n  You Have: \n \n  2+ years of experience in working with Web development or design, including HTML or CSS \n  Experience with using Angular, React, or Vue and their ecosystems, including Gulp, Angular CLI, integrating NPM libraries and modularizing code for reuse across multiple projects \n  Experience with responsive design libraries, including Bootstrap and Material Design \n  Experience in working with configuration management and source code control, including Git or SVN \n  TS/SCI clearance \n  Bachelor's degre \n \n \n  Nice If You Have: \n \n  2+ years of experience in working with Web or data analysis languages, including Java or Python \n  Experience with JS visualization libraries, including Cytoscape,D3, Ogma, or AmCharts \n  Experience with CI/CD pipelines., including Jenkins, Kubernetes and Docker \n  Experience in working with RESTful Web services \n  Experience with advanced data structures and algorithms that apply to them, including connected graph analysis \n  Master's degree in Data Science or Operations Research \n \n \n  Clearance: \n  Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; TS/SCI clearance is required. \n \n  Create Your Career: \n \n  Grow With Us \n  Your growth matters to us\u2014that\u2019s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. \n \n  A Place Where You Belong \n  Diverse perspectives cultivate collective ingenuity. Booz Allen\u2019s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you\u2019ll build your community in no time. \n \n  Support Your Well-Being \n  Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we\u2019ll support you as you pursue a balanced, fulfilling life\u2014at work and at home. \n \n  Your Candidate Journey \n  At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we\u2019ve compiled a list of resources so you\u2019ll know what to expect as we forge a connection with you during your journey as a candidate with us. \n \n  Compensation \n  At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen\u2019s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. \n  Salary at Booz Allen is determined by various factors, including but not limited to location, the individual\u2019s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $49,800.00 to $102,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen\u2019s total compensation package for employees.\n          \n  Work Model  Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. \n \n  If this position is listed as remote or hybrid, you\u2019ll periodically work from a Booz Allen or client site facility. \n  If this position is listed as onsite, you\u2019ll work with colleagues and clients in person, as needed for the specific role. \n \n \n  EEO Commitment \n  We\u2019re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change \u2013 no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.",
        "cleaned_desc": "  Experience with responsive design libraries, including Bootstrap and Material Design \n  Experience in working with configuration management and source code control, including Git or SVN \n  TS/SCI clearance \n  Bachelor's degre \n \n \n  Nice If You Have: \n \n  2+ years of experience in working with Web or data analysis languages, including Java or Python \n  Experience with JS visualization libraries, including Cytoscape,D3, Ogma, or AmCharts \n  Experience with CI/CD pipelines., including Jenkins, Kubernetes and Docker \n  Experience in working with RESTful Web services \n  Experience with advanced data structures and algorithms that apply to them, including connected graph analysis \n  Master's degree in Data Science or Operations Research \n \n \n  Clearance: \n  Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; TS/SCI clearance is required. \n \n  Create Your Career: \n \n  Grow With Us ",
        "techs": [
            "bootstrap",
            "material design",
            "git",
            "svn",
            "java",
            "python",
            "cytoscape",
            "d3",
            "ogma",
            "amcharts",
            "jenkins",
            "kubernetes",
            "docker",
            "restful web services"
        ],
        "cleaned_techs": [
            "bootstrap",
            "material design",
            "git",
            "svn",
            "java",
            "python",
            "cytoscape",
            "d3",
            "ogma",
            "amcharts",
            "jenkins",
            "kubernetes",
            "docker",
            "restful web services"
        ]
    },
    "2ff36cfb23302953": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)",
        "company": "Capital One",
        "desc": "314 Main Street (21020), United States of America, Cambridge, Massachusetts\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing \n  At least 4 years of experience with HPCs, vector embedding, or semantic search technologies \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing systems for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "314 Main Street (21020), United States of America, Cambridge, Massachusetts\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n    Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ],
        "cleaned_techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ]
    },
    "b663f0ff1f132410": {
        "terms": [
            "data science"
        ],
        "salary_min": 105000.0,
        "salary_max": 132000.0,
        "title": "Senior T&E Engineer/Analyst",
        "company": "Man-Machine Systems Assessment",
        "desc": "Overview: \n At MSA, you will be working with a diverse team of experts working to strengthen the preparedness of our Nation through innovation in technical and analytical services. While our typical team member has a master\u2019s degree in his or her field of expertise, we also have a mix of Ph.D. scientists, analysts, and technicians \u2013 many with backgrounds in military or government service. We look for candidates who want to embrace our culture, our philosophy, and our mission to prepare and strengthen our nation and our communities. \n Principal Duties and Responsibilities: \n Plan and perform engineering research, design development, and other assignments in conformance with design, engineering, and customer specifications in the arena of Enterprise Data Management (EDM). May be responsible for the technical/engineering part of a project and expected to work with a team. May be responsible for coordinating the activities of technicians assigned to specific engineering projects. \n Location: \n Remote work with the need for occasional meetings onsite at DOT&E in DC. Strong preference for candidates to be within commuting distance to the Pentagon. \n Required Education/Experience: \n STEM master\u2019s degree (e.g., operations research, engineering, applied mathematics, computer science, physics, data science, or other related technical field), with at least 5 years of experience related to Defense Test and Evaluation (T&E); or a STEM bachelor\u2019s degree with at least 7 years of experience related to Defense T&E. Experience that also includes data management methods and systems is preferred. \n Ability to acquire a DoD security clearance is required, with preference given to candidates currently holding, at minimum, a SECRET level clearance. \n About MSA: \n Our team has been recognized repeatedly for excellence in job performance and client satisfaction. Whether you\u2019re a client or an employee, your success is our success. \n We don\u2019t just say we want our employees to thrive; we create an environment that fosters growth and provides stability. That\u2019s why we partnered with the top carriers in the nation to build one of the most comprehensive benefits packages in our industry. \n - Carefirst BCBS Medical plans (PPO, POS, HMO) - Paid Time Off and Holidays - 401(k) retirement matching programs - Employer-paid dental and vision insurance - Employer-paid short-term and long-term disability insurance - Employer-paid Life and AD&D Insurance - TriCare Supplemental Insurance - Employee Assistance Program (EAP) - Tuition and Training Reimbursement \n The secret to our success is attracting and retaining high-caliber talent. If you have a passion for learning, love to tackle challenges and want to work for a company that values you and your contributions \u2013 MSA is your place! \n MSA is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, sexual orientation, gender identity, or any other characteristic protected by law. \n This position is fully funded and on an active contract. \n Job Type: Full-time \n Pay: $105,000.00 - $132,000.00 per year \n Benefits: \n \n 401(k) \n 401(k) matching \n Dental insurance \n Employee assistance program \n Health insurance \n Life insurance \n Paid time off \n Parental leave \n Professional development assistance \n Referral program \n Vision insurance \n \n Schedule: \n \n Monday to Friday \n \n Supplemental pay types: \n \n Bonus opportunities \n \n Education: \n \n Bachelor's (Required) \n \n Security clearance: \n \n Secret (Preferred) \n \n Work Location: Hybrid remote in Washington, DC 20301",
        "cleaned_desc": "Overview: \n At MSA, you will be working with a diverse team of experts working to strengthen the preparedness of our Nation through innovation in technical and analytical services. While our typical team member has a master\u2019s degree in his or her field of expertise, we also have a mix of Ph.D. scientists, analysts, and technicians \u2013 many with backgrounds in military or government service. We look for candidates who want to embrace our culture, our philosophy, and our mission to prepare and strengthen our nation and our communities. \n Principal Duties and Responsibilities: \n Plan and perform engineering research, design development, and other assignments in conformance with design, engineering, and customer specifications in the arena of Enterprise Data Management (EDM). May be responsible for the technical/engineering part of a project and expected to work with a team. May be responsible for coordinating the activities of technicians assigned to specific engineering projects. \n Location: \n Remote work with the need for occasional meetings onsite at DOT&E in DC. Strong preference for candidates to be within commuting distance to the Pentagon. \n Required Education/Experience: \n STEM master\u2019s degree (e.g., operations research, engineering, applied mathematics, computer science, physics, data science, or other related technical field), with at least 5 years of experience related to Defense Test and Evaluation (T&E); or a STEM bachelor\u2019s degree with at least 7 years of experience related to Defense T&E. Experience that also includes data management methods and systems is preferred. \n Ability to acquire a DoD security clearance is required, with preference given to candidates currently holding, at minimum, a SECRET level clearance. ",
        "techs": [
            "enterprise data management (edm)",
            "operations research",
            "engineering",
            "applied mathematics",
            "computer science",
            "physics",
            "data science",
            "defense test and evaluation (t&e)",
            "data management methods and systems",
            "dod security clearance",
            "secret level clearance"
        ],
        "cleaned_techs": [
            "enterprise data management (edm)",
            "operations research",
            "engineering",
            "applied mathematics",
            "computer science",
            "physics",
            "data science",
            "defense test and evaluation (t&e)",
            "data management methods and systems",
            "secret level clearance"
        ]
    },
    "4137d316d602ebd9": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 1 (19052), United States of America, McLean, Virginia\n   Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Lead Generative AI Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services that enable real-time customer-facing applications.  \n \n Examples of projects you will work on include: \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience. \n  Enable our users to build new GenAI capabilities. \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field \n  At least 6 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 2 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  At this time, Capital One will not sponsor a new applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $197,400 - $225,300 for Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $209,200 - $238,700 for Lead Machine Learning Engineer\n   Remote (Regardless of Location): $167,400 - $191,000 for Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 1 (19052), United States of America, McLean, Virginia\n   Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Lead Generative AI Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services that enable real-time customer-facing applications.  \n \n Examples of projects you will work on include: \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.    Enable our users to build new GenAI capabilities. \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field \n  At least 6 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 2 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n   \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  At this time, Capital One will not sponsor a new applicant for employment authorization for this position. \n ",
        "techs": [
            "apis",
            "sdks",
            "large-language models (llms)",
            "foundation models (fms)",
            "python",
            "go",
            "scala",
            "java",
            "deep neural networks",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems",
            "neural network models",
            "api security",
            "observability",
            "cloud access control",
            "privacy best practices"
        ],
        "cleaned_techs": [
            "apis",
            "sdks",
            "large-language models (llms)",
            "foundation models (fms)",
            "python",
            "go",
            "scala",
            "java",
            "deep neural networks",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems",
            "neural network models",
            "observability",
            "cloud access control"
        ]
    },
    "21d8cf86b12909fa": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "77 West Wacker Dr (35012), United States of America, Chicago, Illinois\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "77 West Wacker Dr (35012), United States of America, Chicago, Illinois\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems"
        ]
    },
    "58b273b01c0d3a59": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "77 West Wacker Dr (35012), United States of America, Chicago, Illinois\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "77 West Wacker Dr (35012), United States of America, Chicago, Illinois\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java",
            "api",
            "sdk",
            "llms",
            "fms",
            "distributed computing",
            "cache optimization techniques",
            "deep neural networks",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems",
            "api security",
            "observability",
            "cloud access control",
            "privacy best practices"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java",
            "api",
            "sdk",
            "llm",
            "fms",
            "distributed computing",
            "cache optimization techniques",
            "deep neural networks",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems",
            "observability",
            "cloud access control"
        ]
    },
    "527a650c55c7a42d": {
        "terms": [
            "data science"
        ],
        "salary_min": 97123.27,
        "salary_max": 122979.71,
        "title": "Associate Manager, Consumer Marketplace Insights & Innovation",
        "company": "Altria",
        "desc": "Overview: \n  \n  Are you looking for an opportunity to join a Fortune 200 company where your consumer and analytics expertise will influence our dynamic sales and marketing organizations and help craft our future? Do you thrive on transforming data into insights? If so, we want to speak with you! \n \n \n  We are currently looking for an \n   Associate Manager, Consu \n mer Marketplace Insights & Innovation  to join us in\n    Richmond, VA,  or we are \n   open to remote work arrangements. \n \n \n  What you will be doing: \n \n \n Generate consumer and marketplace insights using a combination of analytical expertise and business knowledge to provide significant recommendations to our internal brand clients. \n Serve as a critical thinking partner for the brand team by anticipating business questions and opportunities and delivering learnings to the business in ways that make significant impact.  \n Integrate and analyze a variety of data sources (e.g., retail point-of-sale, consumer, shipments, financial) to proactively identify consumer centric insights and business opportunities  \n Synthesize, translate, and communicate analyses to inform and influence business decisions  \n Collaborate with a variety of internal and external partners to develop a holistic view of marketplace dynamics. \n \n \n \n  We want you to have: \n \n \n Bachelor\u2019s degree in Analytics, Marketing, Data Science, or a related field. Advanced degree preferred  \n 4+ years of relevant data analytics experience; working knowledge of consumer research is a plus  \n Demonstrated application of technical skills with consumer data sets: SQL, Python or R a plus  \n Working knowledge of introductory statistics \n Excellent written and verbal communication skills with demonstrated ability to communicate data and technical aspects to partners having varying technical and data knowledge  \n Demonstrated experience leading cross-functional teams and influencing others without authority  \n Demonstrated strategic and creative thinking skills  \n Previous experience in visualization tools such as Tableau, Power BI a plus Consultative skills, and the courage and confidence to share your point of view \n \n \n \n \n  In addition to the opportunity to apply and develop your skills toward key business objectives, we offer an excellent compensation package including a competitive base salary, comprehensive health/vision/dental insurance, participation in our deferred profit sharing and incentive compensation programs as well as a relocation assistance. \n  Sponsorship: Immigration Sponsorship is not available for this role. Company Overview: Altria has a leading portfolio of tobacco products for U.S. tobacco consumers 21+. Our tobacco companies \u2013 which have been the undisputed market leaders in the U.S. tobacco industry for decades \u2013 include some of the most enduring names in American business. In combustibles, we own Philip Morris USA, the maker of Marlboro cigarettes and John Middleton, manufacturer of Black & Mild cigars. Our smoke-free portfolio includes ownership of U.S. Smokeless Tobacco Company, the maker of Copenhagen and Skoal, and Helix Innovations, the maker of on! oral nicotine pouches. Additionally, we have a majority-owned joint venture with JT Group, Horizon Innovations, for the U.S. marketing and commercialization of heated tobacco stick products. Through a separate agreement with Philip Morris International, we have the exclusive U.S. commercialization rights to the IQOS* Tobacco Heating System\u00ae and Marlboro HeatSticks\u00ae through April 2024. Our equity investments include Anheuser-Busch InBev SA/NV, the world\u2019s largest brewer and Cronos Group, a leading Canadian cannabinoid company. Each Altria company is an equal opportunity employer. We are committed to providing individuals with criminal records, including formerly incarcerated individuals and individuals with conviction records, a fair chance at employment. Learn more about Altria at www.altria.com and follow us on Twitter, Facebook and LinkedIn",
        "cleaned_desc": " Demonstrated application of technical skills with consumer data sets: SQL, Python or R a plus  \n Working knowledge of introductory statistics \n Excellent written and verbal communication skills with demonstrated ability to communicate data and technical aspects to partners having varying technical and data knowledge  \n Demonstrated experience leading cross-functional teams and influencing others without authority  \n Demonstrated strategic and creative thinking skills  \n Previous experience in visualization tools such as Tableau, Power BI a plus Consultative skills, and the courage and confidence to share your point of view \n ",
        "techs": [
            "sql",
            "python",
            "r",
            "tableau",
            "power bi"
        ],
        "cleaned_techs": [
            "sql",
            "python",
            "r",
            "tableau",
            "powerbi"
        ]
    },
    "c0e9a8a6de07d364": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 1 (19052), United States of America, McLean, Virginia\n   Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Lead Generative AI Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services that enable real-time customer-facing applications.  \n \n Examples of projects you will work on include: \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience. \n  Enable our users to build new GenAI capabilities. \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field \n  At least 6 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 2 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  At this time, Capital One will not sponsor a new applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $197,400 - $225,300 for Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $209,200 - $238,700 for Lead Machine Learning Engineer\n   Remote (Regardless of Location): $167,400 - $191,000 for Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 1 (19052), United States of America, McLean, Virginia\n   Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Lead Generative AI Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services that enable real-time customer-facing applications.  \n \n Examples of projects you will work on include: \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.    Enable our users to build new GenAI capabilities. \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field \n  At least 6 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 2 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n   \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  At this time, Capital One will not sponsor a new applicant for employment authorization for this position. \n ",
        "techs": [
            "python",
            "go",
            "scala",
            "java"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java"
        ]
    },
    "11d460d368cab30c": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing \n  At least 4 years of experience with HPCs, vector embedding, or semantic search technologies \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing systems for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n    Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ],
        "cleaned_techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ]
    },
    "71aac7db0fdbca2a": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing \n  At least 4 years of experience with HPCs, vector embedding, or semantic search technologies \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing systems for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n    Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ],
        "cleaned_techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ]
    },
    "9319bd37330d4f84": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing \n  At least 4 years of experience with HPCs, vector embedding, or semantic search technologies \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing systems for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 3 (19075), United States of America, McLean, Virginia\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n    Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "tensorflow",
            "pytorch"
        ],
        "cleaned_techs": [
            "tensorflow",
            "pytorch"
        ]
    },
    "4bfefc62ea95866c": {
        "terms": [
            "data science",
            "machine learning engineer",
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Sr Distinguished Engineer, Generative AI Systems - (Remote- Eligible)",
        "company": "Capital One",
        "desc": "Locations: VA - McLean, United States of America, McLean, Virginia\n   Sr Distinguished Engineer, Generative AI Systems - (Remote- Eligible)\n  \n  Sr Distinguished Engineer, Generative AI Systems \n \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Senior Distinguished Engineer, AI Systems, to help us build the foundations of our enterprise AI Capabilities. You will work on a wide range of initiatives, whether that\u2019s designing robust, secure infrastructure, building large-scale distributed training clusters, deploying LLMs on GPU instances for real-time use cases, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work with a team of AI engineers and researchers to envision the target state of our capabilities while helping to design and implement key services. Examples of projects you will work on include: \n \n \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build infrastructure for serving large ML models, in our public cloud. \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 9 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 6 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud. \n \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $315,100 - $359,700 for Sr Distinguished Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $333,900 - $381,000 for Sr Distinguished Machine Learning Engineer\n   Remote (Regardless of Location): $267,100 - $304,800 for Sr Distinguished Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 9 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 6 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud.   \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n ",
        "techs": [
            "llms",
            "fms",
            "mlops",
            "hpc",
            "ml",
            "python",
            "c/c++",
            "ai",
            "ml",
            "ml frameworks",
            "public cloud",
            "engineering",
            "computer science",
            "distributed computing",
            "large-scale ml systems",
            "ml algorithms",
            "ml development lifecycle",
            "ai techniques",
            "distributed platforms",
            "cloud environments",
            "aws",
            "azure",
            "gcp",
            "cloud systems",
            "security",
            "availability",
            "performance",
            "scalability",
            "cost",
            "mlops life cycle",
            "gpu clusters",
            "ml compilers",
            "distributed training frameworks",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "llm hosting",
            "fine-tuning",
            "research publications",
            "neural networks",
            "distributed training",
            "sysml."
        ],
        "cleaned_techs": [
            "llm",
            "fms",
            "mlops",
            "hpc",
            "ml",
            "python",
            "c/c++",
            "ai",
            "ml frameworks",
            "public cloud",
            "engineering",
            "computer science",
            "distributed computing",
            "large-scale ml systems",
            "ml algorithms",
            "distributed platforms",
            "cloud environments",
            "aws",
            "azure",
            "gcp",
            "cloud systems",
            "availability",
            "performance",
            "scalability",
            "cost",
            "mlops life cycle",
            "gpu clusters",
            "ml compilers",
            "distributed training frameworks",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "fine-tuning",
            "research publications",
            "neural networks",
            "distributed training",
            "sysml."
        ]
    },
    "f56528eb184dba9e": {
        "terms": [
            "data science",
            "machine learning engineer",
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Sr Distinguished Engineer, Generative AI Systems - (Remote- Eligible)",
        "company": "Capital One",
        "desc": "Locations: VA - McLean, United States of America, McLean, Virginia\n   Sr Distinguished Engineer, Generative AI Systems - (Remote- Eligible)\n  \n  Sr Distinguished Engineer, Generative AI Systems \n \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Senior Distinguished Engineer, AI Systems, to help us build the foundations of our enterprise AI Capabilities. You will work on a wide range of initiatives, whether that\u2019s designing robust, secure infrastructure, building large-scale distributed training clusters, deploying LLMs on GPU instances for real-time use cases, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work with a team of AI engineers and researchers to envision the target state of our capabilities while helping to design and implement key services. Examples of projects you will work on include: \n \n \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build infrastructure for serving large ML models, in our public cloud. \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 9 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 6 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud. \n \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $315,100 - $359,700 for Sr Distinguished Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $333,900 - $381,000 for Sr Distinguished Machine Learning Engineer\n   Remote (Regardless of Location): $267,100 - $304,800 for Sr Distinguished Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 9 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 6 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud.   \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n ",
        "techs": [
            "benchmarks",
            "llms",
            "fms",
            "mlops",
            "python",
            "c/c++",
            "ml development lifecycle",
            "ai frameworks",
            "public cloud",
            "master's degree",
            "phd",
            "cloud environments",
            "aws",
            "azure",
            "gcp",
            "security",
            "availability",
            "performance",
            "scalability",
            "cost",
            "mlops life cycle",
            "gpu clusters",
            "storage",
            "networking",
            "ml compilers",
            "distributed training frameworks",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "llm hosting",
            "fine-tuning",
            "research publications",
            "neural networks",
            "distributed training",
            "sysml"
        ],
        "cleaned_techs": [
            "benchmarks",
            "llm",
            "fms",
            "mlops",
            "python",
            "c/c++",
            "ai",
            "public cloud",
            "cloud environments",
            "aws",
            "azure",
            "gcp",
            "availability",
            "performance",
            "scalability",
            "cost",
            "mlops life cycle",
            "gpu clusters",
            "storage",
            "networking",
            "ml compilers",
            "distributed training frameworks",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "fine-tuning",
            "research publications",
            "neural networks",
            "distributed training",
            "sysml"
        ]
    },
    "93513ea2bff64d73": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)",
        "company": "Capital One",
        "desc": "314 Main Street (21020), United States of America, Cambridge, Massachusetts\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing \n  At least 4 years of experience with HPCs, vector embedding, or semantic search technologies \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing systems for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "314 Main Street (21020), United States of America, Cambridge, Massachusetts\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n    Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml",
            "gpu clusters",
            "public cloud"
        ],
        "cleaned_techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml",
            "gpu clusters",
            "public cloud"
        ]
    },
    "c3ff7035a34cd642": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 1 (19052), United States of America, McLean, Virginia\n   Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Lead Generative AI Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services that enable real-time customer-facing applications.  \n \n Examples of projects you will work on include: \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience. \n  Enable our users to build new GenAI capabilities. \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field \n  At least 6 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 2 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  At this time, Capital One will not sponsor a new applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $197,400 - $225,300 for Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $209,200 - $238,700 for Lead Machine Learning Engineer\n   Remote (Regardless of Location): $167,400 - $191,000 for Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 1 (19052), United States of America, McLean, Virginia\n   Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Lead Generative AI Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services that enable real-time customer-facing applications.  \n \n Examples of projects you will work on include: \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.    Enable our users to build new GenAI capabilities. \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field \n  At least 6 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 2 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n   \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  At this time, Capital One will not sponsor a new applicant for employment authorization for this position. \n ",
        "techs": [
            "apis",
            "sdks",
            "large-language models (llms)",
            "foundation models (fms)",
            "python",
            "go",
            "scala",
            "java",
            "deep neural networks",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems"
        ],
        "cleaned_techs": [
            "apis",
            "sdks",
            "large-language models (llms)",
            "foundation models (fms)",
            "python",
            "go",
            "scala",
            "java",
            "deep neural networks",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems"
        ]
    },
    "153340c97d1a550a": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)",
        "company": "Capital One",
        "desc": "314 Main Street (21020), United States of America, Cambridge, Massachusetts\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing \n  At least 4 years of experience with HPCs, vector embedding, or semantic search technologies \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing systems for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "314 Main Street (21020), United States of America, Cambridge, Massachusetts\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n    Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ],
        "cleaned_techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ]
    },
    "1da6a289f51f443a": {
        "terms": [
            "data science",
            "machine learning engineer",
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Distinguished Engineer, Generative AI Systems (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Locations: NY - New York, United States of America, New York, New York\n   Distinguished Engineer, Generative AI Systems (Remote-Eligible)\n  \n  Distinguished Engineer, Generative AI Systems \n \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Senior Distinguished Engineer, AI Systems, to help us build the foundations of our enterprise AI Capabilities. You will work on a wide range of initiatives, whether that\u2019s designing robust, secure infrastructure, building large-scale distributed training clusters, deploying LLMs on GPU instances for real-time use cases, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work with a team of AI engineers and researchers to envision the target state of our capabilities while helping to design and implement key services. Examples of projects you will work on include: \n \n \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build infrastructure for serving large ML models, in our public cloud. \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 7 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 5 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud. \n \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $269,400 - $307,500 for Distinguished Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $285,400 - $325,700 for Distinguished Machine Learning Engineer\n   Remote (Regardless of Location): $228,300 - $260,600 for Distinguished Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 7 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 5 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud.   \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n ",
        "techs": [
            "benchmarks",
            "ai capabilities",
            "llms",
            "fms",
            "mlops",
            "distributed computing",
            "hpc",
            "ml systems",
            "ai algorithms",
            "ml algorithms",
            "python",
            "c/c++",
            "ml development lifecycle",
            "ai frameworks",
            "ml frameworks",
            "public cloud",
            "large-scale distributed platforms",
            "cloud environments",
            "aws",
            "azure",
            "gcp",
            "cloud systems",
            "security",
            "availability",
            "performance",
            "scalability",
            "cost",
            "gpu clusters",
            "storage",
            "networking",
            "ml compilers",
            "distributed training frameworks",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "llm hosting",
            "fine-tuning",
            "research publications",
            "neural networks",
            "sysml"
        ],
        "cleaned_techs": [
            "benchmarks",
            "ai",
            "llm",
            "fms",
            "mlops",
            "distributed computing",
            "hpc",
            "ml systems",
            "ml algorithms",
            "python",
            "c/c++",
            "ml frameworks",
            "public cloud",
            "large-scale distributed platforms",
            "cloud environments",
            "aws",
            "azure",
            "gcp",
            "cloud systems",
            "availability",
            "performance",
            "scalability",
            "cost",
            "gpu clusters",
            "storage",
            "networking",
            "ml compilers",
            "distributed training frameworks",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "fine-tuning",
            "research publications",
            "neural networks",
            "sysml"
        ]
    },
    "829c93c449167a55": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": 70.0,
        "salary_max": 80.0,
        "title": "NLP Data Scientist",
        "company": "Primary Talent Partners",
        "desc": "***********W2 Requirement - USC or GC required - NO C2C AT ALL************       Primary Talent Partners has 1 opening for an NLP Data Scientist to join our client. This is a 6-month W2 contract opening, contract extensions are based on performance, budget, and business needs. \n \n Pay: $70/hr \n Duration: 6-month W2 contract, no sponsorship & no c2c \n Location: Hybrid in St. Louis, MO or Remote \n We're seeking a talented and innovative NLP/Information Retrieval Scientist to join our team. In this role, you will play a pivotal part in enhancing Large Language Models (LLMs) to provide more accurate, context-aware, and creatively curated responses with real-world applications.\n  \n  As an NLP/Information Retrieval Scientist, you will develop and to improve the usability and creativity of our LLM responses. You will collaborate closely with our multidisciplinary team of researchers and engineers to not only advance the technical aspects but also bring real-world relevance and creativity into our language models.\n  \n  The successful candidate will use the latest innovations in NLP and LLM to propose software solutions to improve customer experience. The Remote Sensing Data Scientist will design and test algorithms, conduct prototyping to evaluate possible scenarios leveraging computational and statistical techniques for the development of Client approaches for remotely sensed data.\n  \n \n Required Skills: \n \n Ph.D. (or MS with 4+ years of post MS experience), Computer Science, Electrical Engineering, Physics, Mathematics, Statistics or an Analytics discipline. \n Expertise with NLP or Information \n Proficiency in Python or in another high-level programming language \n Experience in developing statistical, and machine learning models for environmental and agronomical applications \n Familiairty with LLM \n Experience analyzing and presenting complex data and proven problem-solving abilities \n Strong publication record in leading scientific journals \n \n Desired Skills: \n \n Experience working with agricultural/biological scientific data is highly desired \n Drive for translating business problems into research initiatives that deliver business value \n Creativity in defining challenging exploratory projects \n \n \n Primary Talent Partners is an Equal Opportunity / Affirmative Action employer committed to diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, age, national origin, disability, protected veteran status, gender identity, or any other factor protected by applicable federal, state, or local laws. \n \n  #PTPJobs",
        "cleaned_desc": "  \n  As an NLP/Information Retrieval Scientist, you will develop and to improve the usability and creativity of our LLM responses. You will collaborate closely with our multidisciplinary team of researchers and engineers to not only advance the technical aspects but also bring real-world relevance and creativity into our language models.\n  \n  The successful candidate will use the latest innovations in NLP and LLM to propose software solutions to improve customer experience. The Remote Sensing Data Scientist will design and test algorithms, conduct prototyping to evaluate possible scenarios leveraging computational and statistical techniques for the development of Client approaches for remotely sensed data.\n  \n   Required Skills: \n \n Ph.D. (or MS with 4+ years of post MS experience), Computer Science, Electrical Engineering, Physics, Mathematics, Statistics or an Analytics discipline. \n Expertise with NLP or Information \n Proficiency in Python or in another high-level programming language \n Experience in developing statistical, and machine learning models for environmental and agronomical applications   Familiairty with LLM \n Experience analyzing and presenting complex data and proven problem-solving abilities \n Strong publication record in leading scientific journals \n \n Desired Skills: \n ",
        "techs": [
            "nlp",
            "llm",
            "python",
            "statistical techniques",
            "machine learning models"
        ],
        "cleaned_techs": [
            "nlp",
            "llm",
            "python",
            "statistical techniques"
        ]
    },
    "1767636be52dd2d8": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "77 West Wacker Dr (35012), United States of America, Chicago, Illinois\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "77 West Wacker Dr (35012), United States of America, Chicago, Illinois\n   Senior Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities, to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services of our capabilities and enable real-time customer-facing applications powered by these capabilities. Examples of projects you will work on include: \n \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.. \n  Enable our users to build new AI capabilities    Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field. \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques. \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks   \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java",
            "nlp",
            "speech",
            "computer vision",
            "recommendation systems"
        ]
    },
    "66298b9c49650d80": {
        "terms": [
            "data science",
            "data engineer"
        ],
        "salary_min": 50.0,
        "salary_max": 55.0,
        "title": "Big Data Engineer",
        "company": "eTeam Inc",
        "desc": "Post1 \n Job Title: Big Data Engineer Duration: 6 Months (Contract to hire) Location: Remote \n Job Opportunity: \n \n This job designs and engineers solutions associated with analytic data for the organization and, working closely with the business, analytic and IT teams, assists with the build and upkeep for these solutions. This includes coding data ingestion, transformation, and delivery programs/locic for analysts to access operational, derived, and external data sets. \n Expected deliverables will include; coding of delivery frameworks to load and transform raw source data into enhanced analytic assets, being a key resource for analytical and big data efforts, working with architects, analysts and data scientists as needed. \n The incumbent is responsible for the operation and execution of projects related to Big Data or other analytic platforms. Leverages experience in analyzing and delivering large data sets by using a variety of delivery tools to perform tasks. \n Works in cross-functional teams from different organizations (both technical and non-technical) on projects. Provides guidance and education to Seniors and Intermediate level staff. Responsible for maintaining customer relationships. Technologies such as, but not limited to: Hadoop, Hive, NoSQL, Spark, Python, SAS, Teradata, Oracle, Informatica. \n Work closely with IT, architect and engineer solutions that provide views for Enterprise Data Objects or other analytic ecosystems. This would include working with the appropriate teams, leading the design and building out the design, and providing upkeep for the solution. Contribute to creating high performance Big Data (and traditional) systems to be used with analytic applications. \n Code, test, process, and maintain data resources for the analytics organizations. This will include working to maintain data sourcing, transformation and delivery, for key analytic platforms throughout the organization. (ETL/ELT) \n Work with alternative analytic data systems to incorporate them into the operational data flow for the Analytics Teams. Work with data science teams and strategic partners on capabilities of core platform. This may include products purchased by the organization that must be ingested or modeled/derived data maintained by analytic teams. \n Contribute to large functional efforts for programs across multiple projects. Manage relationships with customers of the function. Attend meetings with customers on a stand-alone basis or with team as needed. \n Follow standards and patters for high performance data ingestion, transformation, and delivery of data analytic needs. Keep current with Big Data technologies in order to recommend best tools in order to perform current and future work. \n \n Required: \n \n Bachelor's Degree in Computer Systems Analysis, Computer Engineering, Data Processing, Healthcare Informatics or Management Information Systems \n \n Preferred: \n \n Master's Degree in Management Information Systems, Healthcare Informatics or Computer Engineering . \n \n Experience: Required - \n \n 2 years in Data platform development, data engineering, software development, or data science \n 2 years in Big data or cloud data platform \n \n Preferred  - \n \n 2 years in the Healthcare Industry \n 2 years in Data Warehousing \n 2 years in Database Administration \n \n Licenses And Certifications: \n \n Cloud certification (GCP, Azure, AWS) \n \n Required Skills: \n \n Demonstrated skills in SQL \n Data Warehousing \n Problem-Solving \n Communication Skills \n Analytical Skills \n Demonstrated skills in Spark or Python or related tool \n Demonstrated skills in Google Cloud Technologies \n Databricks \n \n Job Type: Contract \n Salary: $50.00 - $55.00 per hour \n Benefits: \n \n Referral program \n \n Experience level: \n \n 6 years \n \n Schedule: \n \n 8 hour shift \n \n Experience: \n \n Data science: 5 years (Preferred) \n Big data: 5 years (Required) \n Data warehouse: 5 years (Required) \n Healthcare: 5 years (Required) \n SQL: 5 years (Required) \n \n Work Location: Remote",
        "cleaned_desc": "Post1 \n Job Title: Big Data Engineer Duration: 6 Months (Contract to hire) Location: Remote \n Job Opportunity: \n \n This job designs and engineers solutions associated with analytic data for the organization and, working closely with the business, analytic and IT teams, assists with the build and upkeep for these solutions. This includes coding data ingestion, transformation, and delivery programs/locic for analysts to access operational, derived, and external data sets. \n Expected deliverables will include; coding of delivery frameworks to load and transform raw source data into enhanced analytic assets, being a key resource for analytical and big data efforts, working with architects, analysts and data scientists as needed. \n The incumbent is responsible for the operation and execution of projects related to Big Data or other analytic platforms. Leverages experience in analyzing and delivering large data sets by using a variety of delivery tools to perform tasks. \n Works in cross-functional teams from different organizations (both technical and non-technical) on projects. Provides guidance and education to Seniors and Intermediate level staff. Responsible for maintaining customer relationships. Technologies such as, but not limited to: Hadoop, Hive, NoSQL, Spark, Python, SAS, Teradata, Oracle, Informatica. \n Work closely with IT, architect and engineer solutions that provide views for Enterprise Data Objects or other analytic ecosystems. This would include working with the appropriate teams, leading the design and building out the design, and providing upkeep for the solution. Contribute to creating high performance Big Data (and traditional) systems to be used with analytic applications. \n Code, test, process, and maintain data resources for the analytics organizations. This will include working to maintain data sourcing, transformation and delivery, for key analytic platforms throughout the organization. (ETL/ELT) \n Work with alternative analytic data systems to incorporate them into the operational data flow for the Analytics Teams. Work with data science teams and strategic partners on capabilities of core platform. This may include products purchased by the organization that must be ingested or modeled/derived data maintained by analytic teams. \n Contribute to large functional efforts for programs across multiple projects. Manage relationships with customers of the function. Attend meetings with customers on a stand-alone basis or with team as needed. \n Follow standards and patters for high performance data ingestion, transformation, and delivery of data analytic needs. Keep current with Big Data technologies in order to recommend best tools in order to perform current and future work. \n   Required: \n \n Bachelor's Degree in Computer Systems Analysis, Computer Engineering, Data Processing, Healthcare Informatics or Management Information Systems \n \n Preferred: \n \n Master's Degree in Management Information Systems, Healthcare Informatics or Computer Engineering . \n \n Experience: Required - \n \n 2 years in Data platform development, data engineering, software development, or data science \n 2 years in Big data or cloud data platform \n \n Preferred  -   \n 2 years in the Healthcare Industry \n 2 years in Data Warehousing \n 2 years in Database Administration \n \n Licenses And Certifications: \n \n Cloud certification (GCP, Azure, AWS) \n \n Required Skills: \n \n Demonstrated skills in SQL \n Data Warehousing \n Problem-Solving ",
        "techs": [
            "hadoop",
            "hive",
            "nosql",
            "spark",
            "python",
            "sas",
            "teradata",
            "oracle",
            "informatica"
        ],
        "cleaned_techs": [
            "hadoop",
            "hive",
            "nosql",
            "spark",
            "python",
            "sas",
            "teradata",
            "oracle",
            "informatica"
        ]
    },
    "a81c3b5fd2d70225": {
        "terms": [
            "data science",
            "machine learning engineer",
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Sr Distinguished Engineer, Generative AI Systems - (Remote- Eligible)",
        "company": "Capital One",
        "desc": "Locations: VA - McLean, United States of America, McLean, Virginia\n   Sr Distinguished Engineer, Generative AI Systems - (Remote- Eligible)\n  \n  Sr Distinguished Engineer, Generative AI Systems \n \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Senior Distinguished Engineer, AI Systems, to help us build the foundations of our enterprise AI Capabilities. You will work on a wide range of initiatives, whether that\u2019s designing robust, secure infrastructure, building large-scale distributed training clusters, deploying LLMs on GPU instances for real-time use cases, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work with a team of AI engineers and researchers to envision the target state of our capabilities while helping to design and implement key services. Examples of projects you will work on include: \n \n \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build infrastructure for serving large ML models, in our public cloud. \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 9 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 6 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud. \n \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $315,100 - $359,700 for Sr Distinguished Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $333,900 - $381,000 for Sr Distinguished Machine Learning Engineer\n   Remote (Regardless of Location): $267,100 - $304,800 for Sr Distinguished Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 9 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 6 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud.   \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n ",
        "techs": [
            "llms",
            "fms",
            "mlops",
            "python",
            "c/c++",
            "ai",
            "ml",
            "ml frameworks",
            "public cloud",
            "aws",
            "azure",
            "gcp",
            "gpu clusters",
            "storage",
            "networking",
            "ml compilers",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "llm hosting",
            "fine-tuning",
            "research publications",
            "neural networks",
            "distributed training",
            "sysml."
        ],
        "cleaned_techs": [
            "llm",
            "fms",
            "mlops",
            "python",
            "c/c++",
            "ai",
            "ml",
            "ml frameworks",
            "public cloud",
            "aws",
            "azure",
            "gcp",
            "gpu clusters",
            "storage",
            "networking",
            "ml compilers",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "fine-tuning",
            "research publications",
            "neural networks",
            "distributed training",
            "sysml."
        ]
    },
    "6ae1ba34ab2af352": {
        "terms": [
            "data science"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 1 (19052), United States of America, McLean, Virginia\n   Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Lead Generative AI Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services that enable real-time customer-facing applications.  \n \n Examples of projects you will work on include: \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience. \n  Enable our users to build new GenAI capabilities. \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field \n  At least 6 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 2 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  At this time, Capital One will not sponsor a new applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $197,400 - $225,300 for Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $209,200 - $238,700 for Lead Machine Learning Engineer\n   Remote (Regardless of Location): $167,400 - $191,000 for Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 1 (19052), United States of America, McLean, Virginia\n   Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Lead Generative AI Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services that enable real-time customer-facing applications.  \n \n Examples of projects you will work on include: \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.    Enable our users to build new GenAI capabilities. \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field \n  At least 6 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 2 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n   \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  At this time, Capital One will not sponsor a new applicant for employment authorization for this position. \n ",
        "techs": [
            "python",
            "go",
            "scala",
            "java"
        ],
        "cleaned_techs": [
            "python",
            "go",
            "scala",
            "java"
        ]
    },
    "d7f53ed930fa9e45": {
        "terms": [
            "data science",
            "machine learning engineer",
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Distinguished Engineer, Generative AI Systems (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Locations: NY - New York, United States of America, New York, New York\n   Distinguished Engineer, Generative AI Systems (Remote-Eligible)\n  \n  Distinguished Engineer, Generative AI Systems \n \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Senior Distinguished Engineer, AI Systems, to help us build the foundations of our enterprise AI Capabilities. You will work on a wide range of initiatives, whether that\u2019s designing robust, secure infrastructure, building large-scale distributed training clusters, deploying LLMs on GPU instances for real-time use cases, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work with a team of AI engineers and researchers to envision the target state of our capabilities while helping to design and implement key services. Examples of projects you will work on include: \n \n \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build infrastructure for serving large ML models, in our public cloud. \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 7 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 5 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud. \n \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $269,400 - $307,500 for Distinguished Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $285,400 - $325,700 for Distinguished Machine Learning Engineer\n   Remote (Regardless of Location): $228,300 - $260,600 for Distinguished Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 7 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 5 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud.   \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n ",
        "techs": [
            "python",
            "c/c++",
            "aws",
            "azure",
            "gcp",
            "pytorch",
            "tensorflow",
            "lightning"
        ],
        "cleaned_techs": [
            "python",
            "c/c++",
            "aws",
            "azure",
            "gcp",
            "pytorch",
            "tensorflow",
            "lightning"
        ]
    },
    "f572c189c8ab6c1d": {
        "terms": [
            "data science",
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)",
        "company": "Capital One",
        "desc": "314 Main Street (21020), United States of America, Cambridge, Massachusetts\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing \n  At least 4 years of experience with HPCs, vector embedding, or semantic search technologies \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing systems for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "314 Main Street (21020), United States of America, Cambridge, Massachusetts\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n    Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml",
            "gpu clusters",
            "public cloud"
        ],
        "cleaned_techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml",
            "gpu clusters",
            "public cloud"
        ]
    },
    "4862a045dd914a87": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 70000.0,
        "salary_max": 115000.0,
        "title": "Data Analyst II",
        "company": "Ascensus",
        "desc": "At Ascensus, technology is more than just a solution. It powers the business that helps millions of people save for what matters\u2014retirement, education, and healthcare. Our technology experts tackle exciting challenges in collaborative teams, but work in an environment where individual and career development is always valued. Technology associates leverage their talents and passion, building new and innovative platforms, creating programs founded in automation in agile frameworks, and driving existing and new markets\u2014all of which supports the rapid growth of a dynamic industry leader. \n \n  Position Purpose : The Business Systems Analyst II (BSA II) leads the requirements and documentation efforts for financial software applications within Ascensus College Savings. Responsibilities include leading the requirements scoping and documentation effort, as well as coordinating with internal contacts and external partners. This position requires prior experience as a BSA I, Project Manager, or Test Analyst within financial services or participant recordkeeping. \n \n  Responsibilities: \n \n  Requirements scoping and definition (25%)\n    \n Serve as key person in ensuring project scope is accurately captured and maintained \n Ensure that requirements are complete and feasible \n Gather requirements from internal and external system users   \n \n Coordination (Internal and External) (25%)\n    \n Discuss project solutions/issues with ACS business and tech personnel \n Lead internal Requirements Reviews to share/discuss functional requirements \n Communicate with external partners to define/discuss/document customer needs   \n \n Documentation (20%)\n    \n Ensure ACS requirements documentation standards are met \n Identify and resolve open requirements issues \n Accurately capture and communicate information throughout all project phases   \n \n Data Conversion (15%)\n    \n Leads data analysis, planning, and execution of data conversions, including mock conversion efforts \n Develop solutions to data mapping issues when converting from another system   \n \n Recommend solutions based on experience (15%)\n    \n Analyze tradeoffs of various solution approaches \n Identify impacts of proposed solutions on rest of system functionality \n   \n \n Identify impacts of proposed solutions on user procedures \n \n  The national average salary range for this role is 70k-115k in base pay, exclusive of any bonuses and benefits. This base salary range represents the low and high end of the salary range for this position. Actual salary offered will vary and may be above or below the range based on various factors including but not limited to location, experience, performance, and internal pay alignment. We do not anticipate that candidates hired will begin at the top of the range however, from time to time, it may occur on a case-by-case basis. Other rewards and benefits may include: 401(k) match, Medical, Dental, Vision, Paid-Time-Off, etc. For more information, please visit careers.ascensus.com/#Benefits .",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "af7b77afe40501f6": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 65000.0,
        "salary_max": 75000.0,
        "title": "Data Analyst",
        "company": "National Commission for the Certification of Crane Operators",
        "desc": "ABOUT THE NATIONAL COMMISSION FOR THE CERTIFICATION OF CRANE OPERATORS (CCO): \n CCO was formed in January 1995 as a non-profit organization with a mission to develop effective performance standards; provide fair, valid, and reliable assessments; act as an industry resource; and be the leader in providing certifications for those who work in and around load handling equipment. CCO is headquartered in Fairfax, VA, and maintains offices in Murray, UT and Palm Harbor, FL. \n POSITION SUMMARY: \n As the certification leader in its field, CCO\u2019s scope of national personnel certification programs has expanded, and we have an immediate opening for a  Data Analyst . The successful candidate for this position will be a part of the certification team. This team maintains existing certification programs and builds new high-stakes certification examination programs, supporting compliance with ISO/IEC 17024 accreditation. This position\u2019s primary responsibility is maintaining databases of candidate and item performance across the examinations and assisting in basic psychometric data analyses necessary to maintain the quality of assessments associated with these certification programs. \n ESSENTIAL DUTIES AND RESPONSIBILITIES: \n To perform this job successfully, an individual must be able to perform each essential duty and responsibility satisfactorily. Reasonable accommodation may be made to enable an individual with disabilities to perform the essential functions. Other duties may be assigned to meet business needs. \n \n Maintain candidate performance database on all CCO certification programs and the associated written and performance-based exams which are maintained according to the ISO/IEC 17024 Standards and certification industry best practices. \n Download candidate performance data from multiple sources and compile the data files. \n Clean the downloaded candidate performance data for duplicates, errors, or outliers to ensure that the psychometric interpretations are accurate. \n Perform basic psychometric analysis in preparation for meetings such as for cut score (standard setting) meetings. \n Maintain CCO\u2019s test item bank by timely uploading the item performance data and linking the data with the items in the bank. \n Create and manage surveys for job analysis studies for new and existing programs. \n Assist psychometrician in preparing technical reports and statistical analysis reports. \n Prepare data for data-forensic investigations to identify potential test fraud. \n Provide feedback on and refine existing data and operational systems to support the organization to serve the candidates and examination committees more effectively. \n Work with the certification team in preparing and filing ANAB application submissions and responding to inquiries from ANAB and its assessors to maintain ISO/IEC 17024 accreditation. \n Perform other job-related duties as assigned. \n \n QUALIFICATIONS/REQUIREMENTS: \n Education and/or Experience: \n \n Three (3) years of full-time work experience directly related to the duties and responsibilities specified. \n Bachelor's degree in statistics, mathematics, or a related field. Master's degree preferred. \n \n Knowledge, skills, and abilities: \n \n Mastery of Excel (pivot tables, formulas, etc.). \n Mastery of Microsoft Access (or SQL). \n Experience in downloading and uploading of data files. \n Experience in working on R programs. \n Familiarity with item banking software. \n Basic understanding of statistics related to reporting of candidate performance. \n Solid computer software skills, including item banking, spreadsheet, and Word. \n Strong time-management skills and ability to manage multiple tasks. \n Excellent attention to detail. \n Self-directed and comfortable working with a remote team, often independently. \n Ability to maintain a positive attitude under stress. \n Fluency in English (proficiency in Spanish an advantage). \n Willingness and ability to travel (approximately 10 to 15%). \n \n JOB CONDITIONS: \n CCO is seeking qualified applicants who reflect and understand our values: Quality, Integrity, Fairness, Excellence, Service and Respect. CCO is a fast-paced, high-energy organization with an ambitious agenda and staff that is highly motivated. This position may experience high work demands under tight timelines. \n CCO is an equal opportunity employer. \n BENEFITS \n \n Monday \u2013 Friday Schedule \n Paid Time Off (PTO) \n 14 Paid Holidays \n 401k and Matching \n Profit Sharing \n Annual Merit Reviews \n Medical, Dental and Vision Insurance \n Pet Insurance \n Life Insurance \n Short/Long Term Disability Coverage \n Celebrating 28 years as an organization \n \n Job Type: Full-time \n Pay: $65,000.00 - $75,000.00 per year \n Benefits: \n \n 401(k) \n 401(k) 3% Match \n AD&D insurance \n Dental insurance \n Flexible spending account \n Health insurance \n Life insurance \n Paid holidays \n Paid time off \n Vision insurance \n Work from home \n \n Compensation package: \n \n Profit sharing \n Yearly bonus \n \n Experience level: \n \n 3 years \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Education: \n \n Bachelor's (Required) \n \n Experience: \n \n SQL/Access: 2 years (Preferred) \n full-time statistics: 3 years (Preferred) \n Microsoft Excel: 2 years (Required) \n \n Work Location: Remote",
        "cleaned_desc": " QUALIFICATIONS/REQUIREMENTS: \n Education and/or Experience: \n \n Three (3) years of full-time work experience directly related to the duties and responsibilities specified. \n Bachelor's degree in statistics, mathematics, or a related field. Master's degree preferred. \n \n Knowledge, skills, and abilities: \n \n Mastery of Excel (pivot tables, formulas, etc.). \n Mastery of Microsoft Access (or SQL). \n Experience in downloading and uploading of data files. \n Experience in working on R programs. \n Familiarity with item banking software. \n Basic understanding of statistics related to reporting of candidate performance. \n Solid computer software skills, including item banking, spreadsheet, and Word. \n Strong time-management skills and ability to manage multiple tasks. \n Excellent attention to detail. \n Self-directed and comfortable working with a remote team, often independently. \n Ability to maintain a positive attitude under stress. ",
        "techs": [
            "excel (pivot tables",
            "formulas",
            "etc.)",
            "microsoft access (or sql)",
            "r programs",
            "item banking software"
        ],
        "cleaned_techs": [
            "excel",
            "formulas",
            "etc.)",
            "microsoft access (or sql)",
            "r programs",
            "item banking software"
        ]
    },
    "57fdf59ef23c685d": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 125000.0,
        "salary_max": 145000.0,
        "title": "Data Analyst Associate",
        "company": "FSS Government Solutions",
        "desc": "Must be a U.S. Citizen. \n Will be a W2 or hourly W2 employee. \n Data Analysts enable businesses to maximize the value of their data assets by using Microsoft Power BI. As a subject matter expert, the Data Analyst Associate will have a minimum of three (3) years of experience building interactive dashboards and reports from enterprise data stores. Microsoft certification is required. \n Qualifying experience \n Individual with three (3) years\u2019 operational experience designing and building scalable data models, cleaning and transforming data, and enabling advanced analytic capabilities that provide meaningful business value through easy-to-comprehend data visualizations. \n Associate-level skills \n \n Design and build scalable data models \n Cleaning and transforming data \n Enabling advances analytic capabilities \n Provide meaningful business value \n \n Job Type: Full-time \n Pay: $125,000.00 - $145,000.00 per year \n Benefits: \n \n Paid time off \n \n Experience level: \n \n 3 years \n \n Schedule: \n \n 8 hour shift \n \n Education: \n \n Bachelor's (Required) \n \n Experience: \n \n relevant: 3 years (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": "Must be a U.S. Citizen. \n Will be a W2 or hourly W2 employee. \n Data Analysts enable businesses to maximize the value of their data assets by using Microsoft Power BI. As a subject matter expert, the Data Analyst Associate will have a minimum of three (3) years of experience building interactive dashboards and reports from enterprise data stores. Microsoft certification is required. \n Qualifying experience \n Individual with three (3) years\u2019 operational experience designing and building scalable data models, cleaning and transforming data, and enabling advanced analytic capabilities that provide meaningful business value through easy-to-comprehend data visualizations. \n Associate-level skills ",
        "techs": [
            "microsoft power bi",
            "microsoft certification"
        ],
        "cleaned_techs": [
            "powerbi",
            "microsoft certification"
        ]
    },
    "bc52eee877f32e05": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 61009.676,
        "salary_max": 77251.84,
        "title": "Data Analyst Associate",
        "company": "FSS Government Solutions",
        "desc": "Must be U.S. Citizen \n Will be a W2 or hourly W2 employee. \n Data Analysts enable businesses to maximize the value of their data assets by using Microsoft Power BI. As a subject matter expert, the Data Analyst Associate will have a minimum of three (3) years of experience building interactive dashboards and reports from enterprise data stores. Microsoft certification is required. \n Qualifying experience \n Individual with three (3) years\u2019 operational experience designing and building scalable data models, cleaning and transforming data, and enabling advanced analytic capabilities that provide meaningful business value through easy-to-comprehend data visualizations. \n Associate-level skills \n \n Design and build scalable data models \n Cleaning and transforming data \n Enabling advances analytic capabilities \n Provide meaningful business value \n \n Job Type: Full-time \n Experience level: \n \n 3 years \n \n Schedule: \n \n 8 hour shift \n \n Education: \n \n Bachelor's (Required) \n \n Experience: \n \n relevant: 3 years (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": "Must be U.S. Citizen \n Will be a W2 or hourly W2 employee. \n Data Analysts enable businesses to maximize the value of their data assets by using Microsoft Power BI. As a subject matter expert, the Data Analyst Associate will have a minimum of three (3) years of experience building interactive dashboards and reports from enterprise data stores. Microsoft certification is required. \n Qualifying experience \n Individual with three (3) years\u2019 operational experience designing and building scalable data models, cleaning and transforming data, and enabling advanced analytic capabilities that provide meaningful business value through easy-to-comprehend data visualizations. ",
        "techs": [
            "microsoft power bi",
            "microsoft certification"
        ],
        "cleaned_techs": [
            "powerbi",
            "microsoft certification"
        ]
    },
    "a36e31d0eba2f9f9": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 65068.06,
        "salary_max": 78361.54,
        "title": "Data Analyst",
        "company": "Securance Consulting",
        "desc": "For data entry projects that require manual key entry and/or data capture through scanning. This can be either a W-2 or 1099 or C2C opportunity; no H1B Visa candidates. \n Years of Relevant Experience: 2 to 4 years \n Preferred Education: Associate degree or equivalent \n Role Description : \u2022 Good written and oral communication skills. \n \n Strong technical documentation skills. \n \n \n Good interpersonal skills. \n \n \n Ability to conduct research into data management issues, practices, and products as \n \n required. \n \n Ability to present ideas in a user-friendly language. \n \n \n Keen attention to detail. \n \n \n Proven analytical and problem-solving abilities. \n \n \n Ability to effectively prioritize and execute tasks in a high-pressure environment. \n \n \n Strong customer service orientation. \n \n \n Experience working in a team-oriented, collaborative environment. \n \n \n Strong understanding of data models, structures, theories, principles, and practices. \n \n \n Excellent knowledge of data modeling tools such as SQL. \n \n \n Strong familiarity with data preparation, processing, classification, and forecasting. \n \n \n Working technical experience with relational database servers, including Access \n \n Database. \n \n Direct experience with data management techniques. \n \n \n Hands-on database tuning and troubleshooting experience. \n \n \n Project management experience. \n \n \n Good understanding of the organization\u2019s goals and objectives. \n \n \n Involvement in inter-departmental projects and experience communicating with various \n \n stakeholders \n \n Strong technical documentation skills. \n \n \n Good interpersonal skills. \n \n \n Ability to conduct research into data management issues, practices, and products as \n \n required. \n \n Ability to present ideas in a user-friendly language. \n \n \n Keen attention to detail. \n \n \n Proven analytical and problem-solving abilities. \n \n \n Ability to effectively prioritize and execute tasks in a high-pressure environment. \n \n \n Strong customer service orientation. \n \n \n Experience working in a team-oriented, collaborative environment. \n \n \n Strong understanding of data models, structures, theories, principles, and practices. \n \n \n Excellent knowledge of data modeling tools such as SQL. \n \n \n Strong familiarity with data preparation, processing, classification, and forecasting. \n \n \n Working technical experience with relational database servers, including Access \n \n Database. \n \n Direct experience with data management techniques. \n \n \n Hands-on database tuning and troubleshooting experience. \n \n \n Project management experience. \n \n \n Good understanding of the organization\u2019s goals and objectives. \n \n \n Involvement in inter-departmental projects and experience communicating with various \n \n stakeholders \n About Securance Consulting: \n Securance Consulting is an IT professional services firm dedicated to independent assessments of\u202fcybersecurity, risk, compliance, and efficiency of IT operations. For over 21 years, we have helped clients\u202fleverage the advantage of insight to safeguard data, mitigate risks, and align their security programs\u202fwith business objectives. \n Securance is proud to provide equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. \n This policy applies to all terms and conditions of employment, including recruiting, hiring, placement,\u202fpromotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. \n Job Type: Full-time \n Pay: $65,068.06 - $78,361.54 per year \n Benefits: \n \n 401(k) \n 401(k) matching \n Dental insurance \n Dependent health insurance coverage \n Disability insurance \n Flexible spending account \n Health insurance \n Health savings account \n Life insurance \n Paid holidays \n Paid time off \n Vision insurance \n Work from home \n \n Work Location: Remote",
        "cleaned_desc": "For data entry projects that require manual key entry and/or data capture through scanning. This can be either a W-2 or 1099 or C2C opportunity; no H1B Visa candidates. \n Years of Relevant Experience: 2 to 4 years \n Preferred Education: Associate degree or equivalent \n Role Description : \u2022 Good written and oral communication skills. \n \n Strong technical documentation skills. \n \n \n Good interpersonal skills. \n \n \n Ability to conduct research into data management issues, practices, and products as \n \n required. \n \n Ability to present ideas in a user-friendly language. \n \n \n Keen attention to detail. \n \n \n Proven analytical and problem-solving abilities. \n \n \n Ability to effectively prioritize and execute tasks in a high-pressure environment. \n \n \n Strong customer service orientation.   \n \n Experience working in a team-oriented, collaborative environment. \n \n \n Strong understanding of data models, structures, theories, principles, and practices. \n \n \n Excellent knowledge of data modeling tools such as SQL. \n \n \n Strong familiarity with data preparation, processing, classification, and forecasting. \n \n \n Working technical experience with relational database servers, including Access \n \n Database. \n \n Direct experience with data management techniques. \n \n \n Hands-on database tuning and troubleshooting experience. \n \n \n Project management experience. \n \n \n Good understanding of the organization\u2019s goals and objectives.   \n \n Involvement in inter-departmental projects and experience communicating with various \n \n stakeholders \n \n Strong technical documentation skills. \n \n \n Good interpersonal skills. \n \n \n Ability to conduct research into data management issues, practices, and products as \n \n required. \n \n Ability to present ideas in a user-friendly language. \n \n \n Keen attention to detail. \n \n \n Proven analytical and problem-solving abilities. \n \n \n Ability to effectively prioritize and execute tasks in a high-pressure environment. \n \n   Strong customer service orientation. \n \n \n Experience working in a team-oriented, collaborative environment. \n \n \n Strong understanding of data models, structures, theories, principles, and practices. \n \n \n Excellent knowledge of data modeling tools such as SQL. \n \n \n Strong familiarity with data preparation, processing, classification, and forecasting. \n \n \n Working technical experience with relational database servers, including Access \n \n Database. \n \n Direct experience with data management techniques. \n \n \n Hands-on database tuning and troubleshooting experience. \n \n \n Project management experience. \n \n ",
        "techs": [
            "sql",
            "access database"
        ],
        "cleaned_techs": [
            "sql",
            "access database"
        ]
    },
    "b02c770691361103": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 57622.965,
        "salary_max": 72963.516,
        "title": "Customer Operations Analyst",
        "company": "Ocean Spray",
        "desc": "Ocean Spray is hiring for a(n) Customer Operations Analyst in Lakeville! We\u2019re a team of farmers, thinkers, creators, and doers. Whatever your title, whatever your role \u2014 it always comes back to this: we\u2019re a farmer-owned co-op where everyone rolls up their sleeves to get the job done. Three maverick farmers started it all \u2014 and we\u2019ve been making our own way ever since.\n  \n \n   Position location: We're all about flexibility! This will be a \n   hybrid role  based out of our corporate headquarters in Lakeville, MA or our Seaport Boston, MA location with Mondays & Fridays remote.\n  \n \n \n   A Day in the Life...\n  \n \n   Ocean Spray is hiring a Customer Operations Analyst to join our Team!\n  \n \n \n   Position location: Remote\n  \n \n  A Day in the Life of a Customer Operations Analyst:\n  \n \n  Resolve customer order exceptions in the areas of pricing, credit, systems, inventory and transportation, balancing cost and service \n  Proactively implement improvements across customer base to minimize exceptions in systems, pricing credit, supply chain and transportation, while driving cost savings to the organization \n  Analyze data from multiple sources and proactively identify areas of opportunities for improved service levels or improved cost to serve and create organizational buy in and execute changes as agreed \n  Meet periodically with customers and/or brokers to solve issues, present business updates, build relationships and to review metrics. \n  Manage carryover and deduction balance to ensure specific metrics are achieved \n  Complete Tasks and or projects as assigned \n \n \n \n   What We\u2019re Looking For:\n  \n \n  Bachelor\u2019s Degree required \n  1 year of experience in a Customer Operations position \n  Detail oriented problem solver with the ability to multitask \n  Working knowledge of MS Office and ERP Software (SAP preferred) \n  Excellent communication and interpersonal skills; influences and persuades others internally and externally \n \n \n \n   Compensation/Benefits\n  \n \n  Competitive Base Salary \n  Medical, Dental, and Vision Insurance on day one \n  Performance Bonus \n  401K with up to 10% in matching contributions \n  Bright Horizons Family Solutions \u2013 Back up care, tutoring, etc. \n  Transgender benefits \n  Fertility benefits \n  Tuition Reimbursement Program \n  Paid holidays and vacation days \n  Up to $300 Fitness Reimbursement \n  Up to $300 Massage Reimbursement \n  Health Resources including Free Headspace membership, 1:1 Health Coaching and more. Check out our benefits at oceanspraybenefits.com \n \n \n \n   What We Are Looking For:\n  \n \n \n   Education:\n  \n  No Minimum Education Specified\n  \n \n   Work Experience:\n   Years of Experience: No Experience Necessary\n  \n \n   Benefits:\n  \n \n  Complete insurance package on Day-1 that includes a plethora of health and wellness programs\n    \n  Health, Dental and Vision insurance \n  Health savings account \n  Flexible spending account \n  Life and accident insurance \n  Employee assistance program \n  Telehealth services \n  Fertility benefits \n  Transgender benefits \n  1:1 health coaching and more \n \n  401(k) with up to 6% Company matching; additional potential discretionary match at year-end \n  Short-Term Incentive/Performance bonuses \n  Flexible scheduling options \n  Vacation pay, up to three weeks of time (pro-rated for your first year of employment) \n  Holiday pay for 12 holidays \n  Career development and growth opportunities \n  Tuition/Education assistance programs \n  Access to LinkedIn Learning \n  Scholarship programs for children of employees \n  Parental leave \n  Bright Horizons Family Solutions \u2013 Back-up care, tutoring, etc. \n  Adoption assistance \n  Bereavement leave \n  Up to $300 fitness reimbursement \n  Up to $300 massage reimbursement \n  Employee appreciation events \n  Employee discounts \n  Charitable giving \n \n \n \n   Who We Are:\n  \n \n   You might have our iconic cranberry juice in your fridge or have gotten into heated holiday debate about what\u2019s better - canned or fresh cranberry sauce. But did you know that the hardworking people growing the superfruit in our products are 700 family farmers that own our cooperative? They entrust us with what is most precious to them to create new and innovative products that will delight consumers and grow this beloved brand today and into the future.\n   \n  Team members, farmers, consumers and communities alike-we value what makes us unique and strive to connect our farms to families for a better life by living our values:\n  \n \n  Grower Mindset \u2013 We embrace our grower-owners innovative spirit and heritage through confidence, learning and focus on the future. \n  Sustainable Results \u2013 Guided by purpose, we are focused on delivering results for our grower-owners. \n  Integrity Above All \u2013 We are ethical, doing the right thing for our grower-owners, customers, consumers and each other \n  Inclusive Teamwork \u2013 We build diverse and inclusive teams that strengthen our cooperative. \n \n \n \n \n \n \n \n \n \n \n  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "9f88e3a87a69903e": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 22.44,
        "salary_max": 24.64,
        "title": "EDI Analyst",
        "company": "Nava",
        "desc": "EDI Analyst \n Remote(Atlanta, GA) \n 12+ months, ongoing contract \n Job Requirements: \n \n 3+ years as an EDI Analyst with working knowledge of Order-to-Cash & Sales Order Processes and Transaction sets, including 850 \u2013 Purchase Order, 810 \u2013 Invoices, 855 \u2013 PO Confirmation) \n Experience with SAP iDOCs or Sterling Integrator would be a plus \n Ability to work part-time on as-needed basis (25-30 hours a week, regular work hours EST) \n Ability to work onsite in Atlanta would be a plus, not required \n \n Job Type: Part-time \n Pay: $22.44 - $24.64 per hour \n Expected hours: 25 \u2013 30 per week \n Experience level: \n \n 3 years \n \n Experience: \n \n EDI Analyst: 3 years (Preferred) \n Order-to-Cash & Sales Order Processes and Transaction sets: 3 years (Preferred) \n SAP iDOCs or Sterling Integrator: 1 year (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "ea4e1ce152339d20": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 73608.14,
        "salary_max": 93204.31,
        "title": "Data Analyst and Creative Consultant",
        "company": "Frontier Design",
        "desc": "Description: \n   Frontier Design seeks an experienced data analyst and creative consultant to join our team of management consultants, designers, policy advisors, researchers, and organizational coaches. We are growing quickly and seek an experienced, analytic, and proactive problem solver who can collaborate well with creative thinkers and designers and can immediately jump into a variety of projects across Frontier\u2019s diverse client base. New team members will hit the ground running to match our fast pace, working collaboratively on project teams and independently on deliverables. \n  You will work directly with team members with different mindsets, superpowers, and perspectives and clients with a variety of challenges spanning diverse consulting disciplines. These include, but aren\u2019t limited to, strategic planning; organizational design and change management; research and data analysis; facilitation and training; performance management; and process improvement. Successful candidates will be structured problem solvers who can turn big-picture client ideas into concrete, detailed and actionable processes or deliverables. \n  Responsibilities \n \n  Conduct quantitative and qualitative research and can quickly analyze and synthesize patterns into findings. \n  Design and implement surveys using multiple software platforms. \n  Conduct statistical analysis using tools, such as Stata, Python, spreadsheets (Excel and Google Sheets), or R. \n  Communicates findings compellingly and visually through dashboards, reports, briefing decks, stories, scorecards, executive briefings, etc. for different audiences using different software, such as Power BI, Tableau, and Looker Studio. \n  Work closely with team using human-centered design problem solving approaches (empathy mindset and user-focused, diverging and converging iteratively, prototyping and testing to learn). \n  Prototypes solutions and rapidly tests with clients to get feedback and adapt solutions as needed. \n  Presents results of data analysis in a way that clients with non-technical backgrounds can understand and make decisions based on the data. \n  Anticipates what clients and colleagues need  next  and dives in to make it or take the first cut. \n  Thinks strategically, is focused on the big picture and long-term project impact but will also sweat the details of implementation. \n  Conducts interviews to identify challenges and opportunities facing client organizations. \n  Uses expertise and problem-solving abilities in service to client's problems and opportunities. \n \n  About Us \n  Frontier Design is a design and impact advisory firm, dedicated to serving mission-driven organizations and leaders transforming the world. Our core capabilities include measuring and improving  impact , co-creating and implementing innovative  strategies , leading human-centered  change   and transformation , and developing and supporting  leaders   and teams . How we deliver these services is unique, rooted in the mindset, tools and approaches of human-centered design and organizational psychology. \n  We sit at the intersection of management consulting and human-centered design\u2014what we call  Design Advising . Our interdisciplinary team is both creative and analytical, combining our expertise as engineers, designers, evaluators, strategists, innovators and storytellers. We believe in delivering  radical value , delighting our clients with exceptional quality and wowing them with our cost-effectiveness. \n  We work globally and locally in the U.S., with a wide variety of world class organizations across industries including federal and state governments, non-profit organizations, academia, foundations and the private sector. While headquartered in Alexandria, VA, our organizational construct is largely \u201cvirtual\u201d, which offers us the opportunity to employ remarkable talent regardless of physical location. For team members in the Washington, DC area, we utilize shared office spaces for in-person meetings as needed. To learn more about Frontier Design please visit www.imaginefrontier.com. \n  Type of position:  Full-time \n  Compensation:  Competitive salary and benefits commensurate with experience and education  Requirements: \n  \n Bachelor\u2019s degree from an accredited college or university in a relevant field \n  2+ years of experience or 1+ years of experience with a Master\u2019s degree \n  Strong quantitative and analytical skills, including statistics, sampling methodologies, statistical analysis software, data collection, synthesis, and data validation \n  Ability to leverage rigorous data analysis and synthesis skills to develop creative and compelling visualizations, tools, and reporting mechanisms for diverse client needs \n  Proven ability to use empathy and others' perspectives to creatively frame engagements and surface possible solutions \n  Well-organized, attentive to detail, and able to handle multiple tasks simultaneously \n  Ability to problem-solve complex issues using strong analytical skills and proactively anticipate work requirements with minimal supervision \n  Ability to communicate effectively through verbal, visual, and written means with diverse stakeholders \n  Capable of producing thorough, high-quality work within a tight timeframe \n  Excellent interpersonal skills and ability to work with a geographically and culturally diverse range of stakeholders from various industries \n  Ability to develop effective working relationships with key internal and external contacts \n  Must be a U.S. Citizen",
        "cleaned_desc": "  Conduct statistical analysis using tools, such as Stata, Python, spreadsheets (Excel and Google Sheets), or R. \n  Communicates findings compellingly and visually through dashboards, reports, briefing decks, stories, scorecards, executive briefings, etc. for different audiences using different software, such as Power BI, Tableau, and Looker Studio. \n  Work closely with team using human-centered design problem solving approaches (empathy mindset and user-focused, diverging and converging iteratively, prototyping and testing to learn). \n  Prototypes solutions and rapidly tests with clients to get feedback and adapt solutions as needed. \n  Presents results of data analysis in a way that clients with non-technical backgrounds can understand and make decisions based on the data. \n  Anticipates what clients and colleagues need  next  and dives in to make it or take the first cut. \n  Thinks strategically, is focused on the big picture and long-term project impact but will also sweat the details of implementation.    Type of position:  Full-time \n  Compensation:  Competitive salary and benefits commensurate with experience and education  Requirements: \n  \n Bachelor\u2019s degree from an accredited college or university in a relevant field \n  2+ years of experience or 1+ years of experience with a Master\u2019s degree \n  Strong quantitative and analytical skills, including statistics, sampling methodologies, statistical analysis software, data collection, synthesis, and data validation \n  Ability to leverage rigorous data analysis and synthesis skills to develop creative and compelling visualizations, tools, and reporting mechanisms for diverse client needs    Proven ability to use empathy and others' perspectives to creatively frame engagements and surface possible solutions \n  Well-organized, attentive to detail, and able to handle multiple tasks simultaneously \n  Ability to problem-solve complex issues using strong analytical skills and proactively anticipate work requirements with minimal supervision \n  Ability to communicate effectively through verbal, visual, and written means with diverse stakeholders \n  Capable of producing thorough, high-quality work within a tight timeframe \n  Excellent interpersonal skills and ability to work with a geographically and culturally diverse range of stakeholders from various industries \n  Ability to develop effective working relationships with key internal and external contacts ",
        "techs": [
            "stata",
            "python",
            "spreadsheets (excel and google sheets)",
            "r",
            "power bi",
            "tableau",
            "looker studio"
        ],
        "cleaned_techs": [
            "stata",
            "python",
            "spreadsheets (excel and google sheets)",
            "r",
            "powerbi",
            "tableau",
            "looker studio"
        ]
    },
    "ba2bfa389bb2b047": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 83041.164,
        "salary_max": 105148.62,
        "title": "Business Analyst",
        "company": "Preludesys",
        "desc": "Job Description: \n Job Summary : As a Technical Business Analyst, you will be responsible for analyzing and optimizing our CRM, ERP, and Supply Chain processes. You will work closely with cross-functional teams to identify business requirements, design technical solutions, and ensure seamless integration with our systems. Your experience in the automobile industry, particularly in the EV sector, will be a significant advantage in this role.We are seeking a highly skilled Technical Business Analyst with expertise in CRM, ERP, or Supply Chain systems, and a background in the automobile industry, preferably in the electric vehicle sector. \n \n Stay up to date with industry trends and best practices in CRM, ERP, and Supply Chain management. Key Responsibilities: Collaborate with stakeholders to gather and document business requirements.  Analyze and evaluate existing CRM, ERP, and Supply Chain systems.  Identify opportunities for process improvement and system enhancements.  Design and document technical specifications for system modifications and integrations.  Act as a liaison between business units and IT teams to ensure clear communication.  Lead and participate in system testing and quality assurance efforts.  Provide ongoing support and training to end-users. \n \n Job Type: Full-time \n Work Location: Remote",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "f21fcedd1b3773a9": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 0.0,
        "salary_max": 40.0,
        "title": "Contract Analyst",
        "company": "Careers of New England",
        "desc": "Bach Degree Required, prefer JD and 2+ years of exp \n Top technical skills \n a. Contractual terms and conditions and budget drafting and negotiation experience b. Prior experience in the healthcare industry c. Vendor and consulting agreement types experience d. Legal degree preferred e. Bilingual a plus \n Administers, extends, negotiates and terminates standard and nonstandard contracts. Conducts proposal preparation, contract negotiation, contract administration, and customer contact activities to provide for proper contract acquisition and fulfillment in accordance with company policies, legal requirements, and customer specifications. Examines estimates of material, equipment services, production costs, performance requirements, and delivery schedules to ensure accuracy and completeness. Prepares bids, processes specifications, progress, and other reports; advises management of contractual rights and obligations; compiles and analyzes data and maintains historical information. \n Job Type: Contract \n Pay: Up to $40.00 per hour \n Schedule: \n \n 8 hour shift \n Day shift \n Monday to Friday \n \n Experience: \n \n Negotiation: 1 year (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "ea4490358afc28a4": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 45400.0,
        "salary_max": 93000.0,
        "title": "Data Analyst",
        "company": "Booz Allen Hamilton",
        "desc": "Job Description \n \n \n \n \n \n \n \n \n \n \n         Location: \n         \n \n         McLean,VA,US \n         \n \n \n \n         Remote Work: \n         \n \n         Hybrid \n         \n \n \n \n         Job Number: \n         \n \n         R0182342\n         \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n         Data Analyst\n           The Opportunity: \n  As an analytics professional, you love diving into data and turning it into meaningful insights. With the abundance of structured and unstructured data, you understand the importance of transforming complex data sets into useful information to answer questions and solve challenges. As a data analyst at Booz Allen, you can use your skills to support a mission and use data for good. \n \n  As a data analyst on our Contracts, Procurement, and Pricing (CP&P) team, you\u2019ll contribute to a variety of data quality and data analysis assignments. You\u2019ll have the opportunity to develop your knowledge of both internal and external systems used in the management of government contracts. Not only will you extract and evaluate data from these systems, but you\u2019ll also help the organization identify trends to inform people, process and technology improvements. \n \n  What You\u2019ll Do: \n \n  As a data analyst on our team, you\u2019ll: \n \n  Apply basic data compliance principles and analytical best practice to data collection and analysis efforts \n  Read through complex contractual documents, analyzing them and extracting crucial data to compare to what is in business systems \n  Provide timely customer service to end users and clients, escalating issues to leadership as required \n  Manage the firm\u2019s business registrations within SAM.gov, including our SAM Unique Entity IDs (SAM UEI) and CAGE Codes, and support related reporting requirements \n  Assist management with ad hoc data compliance requests as needed \n  Manage a large set of reference data that is used throughout several internal business systems \n \n \n  Bring your analytical mindset and passion for change to help transform current and future processes. Due to the nature of work performed within this facility, U.S. citizenship is required. \n \n  Join us. The world can\u2019t wait. \n \n  You Have: \n \n  Experience with data compliance \n  Experience with customer service, troubleshooting, and problem resolution \n  Experience using Microsoft Office products, including Point, Outlook, Word, Excel, and PowerPoint \n  Ability to work with large data sets, extracting important data points, and ensuring data is accurate \n  Ability to effectively respond to user inquiries \n  Ability to leverage analytical and problem-solving expertise \n  Ability to multi-task and respond to tight deadlines \n  Bachelor\u2019s degree \n \n \n  Nice If You Have: \n \n  Knowledge of SAM.gov, SAM UEIs and CAGE Codes \n  Ability to learn new software tools and techniques quickly \n  Ability to work independently while accomplishing tasks assigned within tight deadlines \n  Ability to be self-motivated and directed \n  Possession of strong customer service skills \n  Possession of excellent oral and written communication skills \n  Possession of excellent organizational skills \n \n \n  Create Your Career: \n \n  Grow With Us \n  Your growth matters to us\u2014that\u2019s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. \n \n  A Place Where You Belong \n  Diverse perspectives cultivate collective ingenuity. Booz Allen\u2019s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you\u2019ll develop your community in no time. \n \n  Support Your Well-Being \n  Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we\u2019ll support you as you pursue a balanced, fulfilling life\u2014at work and at home. \n \n  Your Candidate Journey \n  At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we\u2019ve compiled a list of resources so you\u2019ll know what to expect as we forge a connection with you during your journey as a candidate with us. \n \n  Compensation \n  At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen\u2019s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. \n  Salary at Booz Allen is determined by various factors, including but not limited to location, the individual\u2019s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $45,400.00 to $93,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen\u2019s total compensation package for employees.\n          \n  Work Model  Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. \n \n  If this position is listed as remote or hybrid, you\u2019ll periodically work from a Booz Allen or client site facility. \n  If this position is listed as onsite, you\u2019ll work with colleagues and clients in person, as needed for the specific role. \n \n \n  EEO Commitment \n  We\u2019re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change \u2013 no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.",
        "cleaned_desc": "  You Have: \n \n  Experience with data compliance \n  Experience with customer service, troubleshooting, and problem resolution \n  Experience using Microsoft Office products, including Point, Outlook, Word, Excel, and PowerPoint \n  Ability to work with large data sets, extracting important data points, and ensuring data is accurate \n  Ability to effectively respond to user inquiries \n  Ability to leverage analytical and problem-solving expertise \n  Ability to multi-task and respond to tight deadlines \n  Bachelor\u2019s degree \n \n \n  Nice If You Have: \n \n  Knowledge of SAM.gov, SAM UEIs and CAGE Codes \n  Ability to learn new software tools and techniques quickly \n  Ability to work independently while accomplishing tasks assigned within tight deadlines \n  Ability to be self-motivated and directed \n  Possession of strong customer service skills \n  Possession of excellent oral and written communication skills \n  Possession of excellent organizational skills \n \n \n  Create Your Career: ",
        "techs": [
            "data compliance",
            "microsoft office products (point",
            "outlook",
            "word",
            "excel",
            "powerpoint)",
            "large data sets",
            "analytical expertise",
            "problem-solving expertise",
            "multi-tasking",
            "user inquiries",
            "sam.gov",
            "sam ueis",
            "cage codes",
            "new software tools",
            "working independently",
            "self-motivation",
            "customer service skills",
            "oral and written communication skills",
            "organizational skills"
        ],
        "cleaned_techs": [
            "data compliance",
            "microsoft",
            "outlook",
            "word",
            "excel",
            "powerpoint)",
            "large data sets",
            "analytical expertise",
            "problem-solving expertise",
            "multi-tasking",
            "user inquiries",
            "sam.gov",
            "sam ueis",
            "cage codes",
            "new software tools",
            "working independently",
            "self-motivation"
        ]
    },
    "c4015d61484e66f0": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 60000.0,
        "salary_max": 69000.0,
        "title": "Sustainability Data Analyst",
        "company": "JLL",
        "desc": "JLL supports the Whole You, personally and professionally. \n  Our people at JLL are shaping the future of real estate for a better world by combining world class services, advisory and technology to our clients. We are committed to hiring the best, most talented people in our industry; and we support them through professional growth, flexibility, and personalized benefits to manage life in and outside of work. Whether you\u2019ve got deep experience in commercial real estate, skilled trades, and technology, or you\u2019re looking to apply your relevant experience to a new industry, we empower you to shape a brighter way forward so you can thrive professionally and personally. \n \n  Duties & responsibilities \n  The Sustainability Data Analyst role will join JLL\u2019s Sustainability Data and Reporting team to support our data management, platform, compliance and reporting functions. The role will work across our global client portfolio delivering data management services, reporting and advice in a high performance, outcomes driven team. This is a remote and/or hybrid role that can be located anywhere in the U.S. \n \n  The position is required to work collaboratively across internal global business lines including JLL\u2019s Client Account, Technology and Operations teams to help manage stakeholder expectations and maintain high quality service delivery. \n  The candidate will have experience in delivering multiple programs of works in parallel. The role will be responsible for several tasks including: \n \n  Assist in delivery of client projects by assisting them to meet their sustainability reporting objectives e.g. DJSI, GRESB, Modern Slavery, TCFD, GRI, RE100, NGER, SASB, CDP and other reporting frameworks. \n  Develop a detailed understanding of JLL\u2019s sustainability reporting application, Canopy, and how we support our clients in measuring their sustainability performance. \n  Participate in new client transition and delivery activities as required, to ensure projects are delivered on time and to a high standard. \n  Work with client delivery and development teams to ensure program milestones are being met. \n  Develop a detailed understanding of data structures within client\u2019s data. \n \n \n  Performance objectives \n \n  The role requires the ability to actively manage concurrent projects and a strong talent for project coordination. \n  Regularly communicate in a clear and non-technical way to internal JLL and client users. \n  Be an integral part of the data and reporting team, contributing to the efficient monitoring and management of support requests, following internal processes and on-going performance improvement of the team. \n  Contribute the successful migration of existing client base to new application within agreed timelines. \n  Identify areas of the platform for improvement to enhance user experience. \n  Identify support, training and management processes that can be improved to increase scalability and efficiency. \n \n \n  Key skills \n \n  The ability to manage multiple deliverables with inter-related dependencies \n  Strong organisational skills and process-driven, with an orientation toward continuous process improvement \n  Strong emotional intelligence and capacity to deliver an excellent client experience. \n  Strong ability to clearly identify issues with data and raise them to the appropriate JLL team member. \n  Ability to work to a defined milestone dates and raise any concerns early and often. \n \n \n  Candidate specification \n \n  1-3 years\u2019 experience in similar analyst role. \n  Basic understanding of HVAC and building energy systems and/or Green House Gas Emission calculations. \n  Familiarity with sustainability related certification and benchmarking tool \n  High proficiency in Microsoft Excel and data management. \n  Proficient in using MS Excel, Access and SQL to combine and analyze data from various sources, and generate reports and graphics. Must be able to combine large volumes of data for consolidated reporting. \n  Ability to perform data analysis and identify the gaps and trends in data. \n  Excellent communication skills including the ability to identify and describe data anomalies and provide solutions accordingly. \n  Lateral thinking and problem-solving skills. \n  Ability to multi-task and manage priorities to meet deadlines. \n  A strong desire to deliver a quality outcome for the client and end users. \n  Experience and understanding of sustainability and carbon emissions reporting will be a strong advantage. \n  Project management experience would be an advantage. \n  This role requires a high attention to detail and a strong process-driven approach. \n \n \n  Estimated compensation for this position is: \n  60,000.00 \u2013 69,000.00 USD\n  \n  The pay range listed is a total compensation range including bonus, if applicable. The provided range is an estimate and not guaranteed. An employment offer is based on applicant\u2019s education, experience, skills, abilities, geographic location, internal equity and alignment with market data. \n \n  Location: \n  Remote \u2013Austin, TX, Charlotte, NC, Chicago, IL, Dallas, TX, Los Angeles, CA, New York, NY, Pittsburgh, PA, Raleigh, NC, San Diego, CA\n  \n  Job Tags: \n \n  If this job description resonates with you, we encourage you to apply, even if you don\u2019t meet all the requirements. We\u2019re interested in getting to know you and what you bring to the table! \n  Personalized benefits that support personal well-being and growth: \n \n  JLL recognizes the impact that the workplace can have on your wellness, so we offer a supportive culture and comprehensive benefits package that prioritizes mental, physical and emotional health. Some of these benefits may include: \n \n  401(k) plan with matching company contributions \n  Comprehensive Medical, Dental & Vision Care \n  Paid parental leave at 100% of salary \n  Paid Time Off and Company Holidays \n  Flexible and Remote Work Arrangements may be available \n \n \n  About JLL  \u2013 \n \n  For over 200 years, JLL (NYSE: JLL), a leading global commercial real estate and investment management company, has helped clients buy, build, occupy, manage and invest in a variety of commercial, industrial, hotel, residential and retail properties. A Fortune 500\u00ae company with annual revenue of $20.9 billion and operations in over 80 countries around the world, our more than 103,000 employees bring the power of a global platform combined with local expertise. Driven by our purpose to shape the future of real estate for a better world, we help our clients, people and communities SEE A BRIGHTER WAY. JLL is the brand name, and a registered trademark, of Jones Lang LaSalle Incorporated. For further information, visit jll.com. \n \n  JLL Privacy Notice \n  Jones Lang LaSalle (JLL), together with its subsidiaries and affiliates, is a leading global provider of real estate and investment management services. We take our responsibility to protect the personal information provided to us seriously. Generally the personal information we collect from you are for the purposes of processing in connection with JLL\u2019s recruitment process. We endeavour to keep your personal information secure with appropriate level of security and keep for as long as we need it for legitimate business or legal reasons. We will then delete it safely and securely. \n \n  For more information about how JLL processes your personal data, please view our Candidate Privacy Statement. \n \n  For additional details please see our career site pages for each country. \n \n  For candidates in the United States, please see a full copy of our Equal Employment Opportunity and Affirmative Action policy here. \n \n  This position may require you to be fully vaccinated against COVID-19. If required, you\u2019ll be asked to provide proof that you\u2019re fully vaccinated upon your start date. You\u2019re considered fully vaccinated two weeks after you receive the second dose of a two-dose vaccine series (e.g., Pfizer or Moderna) or two weeks after a single-dose vaccine (e.g., Johnson & Johnson/Janssen). Failure to provide proof of vaccination may result in termination. \n \n  Jones Lang LaSalle (\u201cJLL\u201d) is an Equal Opportunity Employer and is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the employment process \u2013 including the online application and/or overall selection process \u2013 you may contact us at Accommodation Requests. This email is only to request an accommodation. Please direct any other general recruiting inquiries to our Contact Us page > I want to work for JLL. \n \n  Pursuant to the Arizona Civil Rights Act, criminal convictions are not an absolute bar to employment. \n \n  Pursuant to Illinois Law, applicants are not obligated to disclose sealed or expunged records of conviction or arrest. \n \n  Pursuant to Columbia, SC ordinance, this position is subject to a background check for any convictions directly related to its duties and responsibilities. Only job-related convictions will be considered and will not automatically disqualify the candidate. \n \n  California Residents only \n  If you are a California resident as defined in the California Consumer Privacy Act (CCPA) please view our  Supplemental Privacy Statement  which describes your rights and disclosures about your personal information. If you are viewing this on a mobile device you may want to view the CCPA version on a larger device. \n \n  Pursuant to the Los Angeles Fair Chance Initiative for Hiring Ordinance, JLL will consider for employment all qualified Applicants, including those with Criminal Histories, in a manner consistent with the requirements of applicable state and local laws, including the City of Los Angeles\u2019 Fair Chance Initiative for Hiring Ordinance. \n \n  Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.",
        "cleaned_desc": "  Be an integral part of the data and reporting team, contributing to the efficient monitoring and management of support requests, following internal processes and on-going performance improvement of the team. \n  Contribute the successful migration of existing client base to new application within agreed timelines. \n  Identify areas of the platform for improvement to enhance user experience. \n  Identify support, training and management processes that can be improved to increase scalability and efficiency. \n \n \n  Key skills \n \n  The ability to manage multiple deliverables with inter-related dependencies \n  Strong organisational skills and process-driven, with an orientation toward continuous process improvement \n  Strong emotional intelligence and capacity to deliver an excellent client experience. \n  Strong ability to clearly identify issues with data and raise them to the appropriate JLL team member. \n  Ability to work to a defined milestone dates and raise any concerns early and often. \n \n \n  Candidate specification \n \n  1-3 years\u2019 experience in similar analyst role. \n  Basic understanding of HVAC and building energy systems and/or Green House Gas Emission calculations. \n  Familiarity with sustainability related certification and benchmarking tool    High proficiency in Microsoft Excel and data management. \n  Proficient in using MS Excel, Access and SQL to combine and analyze data from various sources, and generate reports and graphics. Must be able to combine large volumes of data for consolidated reporting. \n  Ability to perform data analysis and identify the gaps and trends in data. \n  Excellent communication skills including the ability to identify and describe data anomalies and provide solutions accordingly. \n  Lateral thinking and problem-solving skills. \n  Ability to multi-task and manage priorities to meet deadlines. \n  A strong desire to deliver a quality outcome for the client and end users. \n  Experience and understanding of sustainability and carbon emissions reporting will be a strong advantage. \n  Project management experience would be an advantage. \n  This role requires a high attention to detail and a strong process-driven approach. \n \n \n  Estimated compensation for this position is: \n  60,000.00 \u2013 69,000.00 USD\n  \n  The pay range listed is a total compensation range including bonus, if applicable. The provided range is an estimate and not guaranteed. An employment offer is based on applicant\u2019s education, experience, skills, abilities, geographic location, internal equity and alignment with market data. \n \n  Location: \n  Remote \u2013Austin, TX, Charlotte, NC, Chicago, IL, Dallas, TX, Los Angeles, CA, New York, NY, Pittsburgh, PA, Raleigh, NC, San Diego, CA\n  ",
        "techs": [
            "microsoft excel",
            "access",
            "sql"
        ],
        "cleaned_techs": [
            "excel",
            "access",
            "sql"
        ]
    },
    "a4a09926f8272f97": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 60.0,
        "salary_max": 62.0,
        "title": "Treasure Business Analyst (Quantum)",
        "company": "Diverse Team LLC",
        "desc": "Job Role: Treasure Business Analyst \n Location: San Jose, CA (Remote) \n Position Type: Contract \n Job Description:- \n \u00b7 Responsible for the delivery of the execution and oversight of effective Treasury system risk management activities within Finance with key responsibilities: \n \u00b7 Act as a subject matter expert for the  Quantum  Treasury management system, eBAMf (bank account management system) and SWIFT. \n \u00b7 Collaborate with Treasury, IT and Finance teams to identify, analyze and implement process improvements related to cash management and SWIFT messaging. \n \u00b7 Partner with technology team and users to develop and maintain SLA\u2019s on various activities \n \u00b7 Support the Treasury team in the efficient system utilization of daily cash management activities, including bank reconciliation, cash forecasting and investment management, identify and implement improvements as required. \n \u00b7 Work closely with IT to support Quantum upgrade and enhancements to improve automation and efficiency in Treasury operations. \n \u00b7 Provide training and support to Treasury team members on the use of Quantum and SWIFT. \n \u00b7 Stay current with new developments in Treasury management systems, SAP and SWIFT messaging, and recommend new technologies to improve operations. \n Knowledge, Skills, and Abilities: \n \u00b7 Bachelor's degree in Finance, Accounting or related field. \n \u00b7 Minimum of 7 years of experience in Treasury operations, including experience with Quantum Treasury management system and SWIFT messaging. \n \u00b7 Strong understanding of cash management, investment management processes and related Accounting rules. \n \u00b7 Excellent analytical and problem-solving skills, with the ability to think creatively and outside the box. \n \u00b7 Ability to work independently and effectively prioritize tasks in a fast-paced environment. \n \u00b7 Excellent written and verbal communication skills, with the ability to present complex information in a clear and concise manner. \n \u00b7 Strong project management skills, with the ability to lead cross-functional teams and drive projects to completion. \n Rakesh |Technical Recruiter| \n Contact: +1 832-645-6862 \n Email:  Rakesh.j@diverseteam.com \n LinkedIn:  https://www.linkedin.com/in/rakesh-jangra-8a91121a4/ \n Office Address: 330 Washington St, PMB 390, Hoboken, New Jersey, NJ 07030| \n Job Type: Contract \n Salary: $60.00 - $62.00 per hour \n Schedule: \n \n 8 hour shift \n \n Work Location: Remote",
        "cleaned_desc": " Knowledge, Skills, and Abilities: \n \u00b7 Bachelor's degree in Finance, Accounting or related field. \n \u00b7 Minimum of 7 years of experience in Treasury operations, including experience with Quantum Treasury management system and SWIFT messaging. \n \u00b7 Strong understanding of cash management, investment management processes and related Accounting rules. \n \u00b7 Excellent analytical and problem-solving skills, with the ability to think creatively and outside the box. \n \u00b7 Ability to work independently and effectively prioritize tasks in a fast-paced environment.   \u00b7 Excellent written and verbal communication skills, with the ability to present complex information in a clear and concise manner. \n \u00b7 Strong project management skills, with the ability to lead cross-functional teams and drive projects to completion. \n Rakesh |Technical Recruiter| \n Contact: +1 832-645-6862 \n Email:  Rakesh.j@diverseteam.com \n LinkedIn:  https://www.linkedin.com/in/rakesh-jangra-8a91121a4/ ",
        "techs": [
            "quantum treasury management system",
            "swift messaging"
        ],
        "cleaned_techs": [
            "quantum treasury management system",
            "swift messaging"
        ]
    },
    "4609c1c868f4f295": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 68130.664,
        "salary_max": 86268.6,
        "title": "Master Data Analyst",
        "company": "US LBM Holdings",
        "desc": "US LBM is one of the leading and fastest growing distributors of specialty building materials in the United States, with a team of over 15,000 employees located throughout the country. Since our founding in 2009, we have acquired over 70 companies and have expanded to more than 500 locations serving 37 states. US LBM is a progressive organization that promotes a unique culture that focuses on the value of its customers and associates. Developing our people is critical to our strategy and fostering our culture of empowerment.\n  \n  The Master Data Analyst will work with business owners to provide analysis and recommend solutions to data challenges. This position will define key data quality metrics and indicators and facilitate the development and implementation of supporting standards. This role will focus on data quality, accuracy, and reliability based on stakeholder needs. This is a remote role with 10% travel.\n  \n \n Things you should know about working at US LBM: \n \n \n  We are all about teamwork! All positions are hands-on, and we band together when necessary. \n  We support each other. We have local and corporate team members to help you along the way and partner on projects as appropriate. \n  We are in a relaxed atmosphere; this is not a suit-and-tie environment. \n  We work hard. We are a continuous improvement-driven organization, and we are focused on keeping organized and on task. \n \n \n  How you will spend your days as the Master Data Analyst: \n \n \n  Work with business owners on data challenges, providing analysis and expertise in business data to recommend solutions. \n  Execute enterprise data best practices, metadata standardization, lineage, data deduplication, mapping and \n  transformation, and business validations. \n  Execute Data Standards as well as KPIs to track ongoing data quality performance. \n  Define key data quality metrics and indicators and facilitate the development and implementation of supporting \n  standards. \n  Develops automation and cycle audits to expedite data quality improvements. \n  Manages medium to large projects and/or cross functional, gives direction to data analysts as needed. \n  Communicates internally and with third parties to provide status updates, to resolve conflicts and to provide feedback & recommend business process changes. \n  Understand internal and external stakeholder data needs and expectations for information. \n  Ensures the data in systems supports the needs of stakeholders. \n  Maintains and develop quality control processes for managing data. \n  Collaborate with subject matter experts and other data analysts to ensure the enterprise data is accurate, complete, and reliable. \n  Applies critical thinking in reviewing provided data in various formats for accuracy. \n  Proposes and implements innovative solutions to increase team efficiency. \n  Prepares product data to upload into ERP system. \n  Data standardization across ERP systems. \n  Identifies data issues that require recurring/ad-hoc audits & implement business process to address. \n  Complies with Company's attendance policy by maintaining regular and predictable attendance. \n  Performs other duties as assigned by Management. \n  Coordinates and validates out-sourced work as part of Master Data Management. \n  Meets throughput, quality and customer satisfaction standards. \n  Effectively works in a team environment with ability to work independently and remotely. \n \n \n  We offer... \n \n \n  Health care benefits, starting the first of the month after 30 days of employment \n  Monday - Friday schedule \n  401(k) with company match \n  Closed holidays \n  Paid time off \n  Employee discount \n  Relocation \n  We'll support your educational and career goals with our continuing education programs \n \n \n  We want you to join the team if you can check these boxes: \n \n \n  Bachelor's degree in a business field or equivalent work experience required. \n  Minimum 3-5 years of experience in data management and/or business expertise. \n  Exhibits proficiency with data governance/management concepts. \n  Solid understanding of and experience working in an MDM and/or PIM system is required. \n  Advanced proficiency with Microsoft Word, Excel, Access, Project, and Outlook. \n  Proficient in relational databases (Microsoft Access, SQL, etc.). \n  Experience with Epicor ERP Systems is a plus. \n  Strong interpersonal, verbal, and written communication skills. \n  Strong analytical and time management skills, with strong attention to detail. \n  Inquisitive and curious nature; committed problem-solver. \n  Intermediate facilitation skills with the ability to drive issues to closure. \n  Self-motivated and able to handle tasks with minimal supervision or questions. \n  Ability to deliver a high level of customer service and ability to meet deadlines. \n  Experience working with IT project managers, architects, and developers, translating business data needs into technical language, and ensuring effective QA / UAT of data solution delivery. \n  Ability to thrive in a fast-paced, responsive work environment. \n  Foster company success through a professional appearance, being courteous to customers with a positive attitude. \n  Initiate and execute continuous improvement processes. \n  Able to effectively lead meetings and medium/large projects. \n  Collaborates effectively with peers and leaders and must be comfortable leading meetings, delegating work to analysts, and influencing strategy. \n  Comfortable working with stakeholders to establish or clarify data ownership, conduct validation, manage conflict, etc. \n  Demonstrated ability to complete established goals with limited guidance and work in an unstructured environment. \n  Authorized to work in the United States without sponsorship. \n \n  #USLBMCORP\n  \n  US LBM Holdings, LLC, is an equal-opportunity employer. We do not discriminate on the basis of race, color, religion, creed, national origin or ancestry, sex, age, physical or mental disability, veteran or military status, genetic information, sexual orientation, gender identity, marital status, military status, order of protection status, or any other legally recognized protected basis under federal, state, or local law.",
        "cleaned_desc": " \n  Work with business owners on data challenges, providing analysis and expertise in business data to recommend solutions. \n  Execute enterprise data best practices, metadata standardization, lineage, data deduplication, mapping and \n  transformation, and business validations. \n  Execute Data Standards as well as KPIs to track ongoing data quality performance. \n  Define key data quality metrics and indicators and facilitate the development and implementation of supporting \n  standards. \n  Develops automation and cycle audits to expedite data quality improvements. \n  Manages medium to large projects and/or cross functional, gives direction to data analysts as needed. \n  Communicates internally and with third parties to provide status updates, to resolve conflicts and to provide feedback & recommend business process changes. \n  Understand internal and external stakeholder data needs and expectations for information. \n  Ensures the data in systems supports the needs of stakeholders. \n  Maintains and develop quality control processes for managing data. \n  Collaborate with subject matter experts and other data analysts to ensure the enterprise data is accurate, complete, and reliable. \n  Applies critical thinking in reviewing provided data in various formats for accuracy. \n  Proposes and implements innovative solutions to increase team efficiency.    Experience with Epicor ERP Systems is a plus. \n  Strong interpersonal, verbal, and written communication skills. \n  Strong analytical and time management skills, with strong attention to detail. \n  Inquisitive and curious nature; committed problem-solver. \n  Intermediate facilitation skills with the ability to drive issues to closure. \n  Self-motivated and able to handle tasks with minimal supervision or questions. \n  Ability to deliver a high level of customer service and ability to meet deadlines. \n  Experience working with IT project managers, architects, and developers, translating business data needs into technical language, and ensuring effective QA / UAT of data solution delivery. \n  Ability to thrive in a fast-paced, responsive work environment. \n  Foster company success through a professional appearance, being courteous to customers with a positive attitude. \n  Initiate and execute continuous improvement processes. \n  Able to effectively lead meetings and medium/large projects. \n  Collaborates effectively with peers and leaders and must be comfortable leading meetings, delegating work to analysts, and influencing strategy. \n  Comfortable working with stakeholders to establish or clarify data ownership, conduct validation, manage conflict, etc. \n  Demonstrated ability to complete established goals with limited guidance and work in an unstructured environment. \n  Authorized to work in the United States without sponsorship. ",
        "techs": [
            "epicor erp systems",
            "it project managers",
            "architects",
            "developers",
            "qa/uat",
            "metadata standardization",
            "data deduplication",
            "mapping and transformation",
            "business validations",
            "data standards",
            "kpis",
            "data quality metrics",
            "data quality indicators",
            "automation",
            "cycle audits"
        ],
        "cleaned_techs": [
            "epicor erp systems",
            "it project managers",
            "architects",
            "developers",
            "qa/uat",
            "metadata standardization",
            "data deduplication",
            "mapping and transformation",
            "business validations",
            "data standards",
            "kpis",
            "data quality metrics",
            "data quality indicators",
            "automation",
            "cycle audits"
        ]
    },
    "641e943125aa0940": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 59480.656,
        "salary_max": 75315.766,
        "title": "Data Analyst",
        "company": "Aetos Systems",
        "desc": "Who We Are! \n Aetos Systems, Inc. was founded in early 2007 to provide a unique work experience. Employees are the foundation of our business. Our leaders work hard every day to empower and support our employees in the development of their careers, giving back to their community and providing their expertise and innovations to our customers - solving real world business problems. Our culture focuses on our people - our strongest asset - ensuring they have an environment to provide best-in-class service and solutions to our customers. We always strive to do the right thing. \n Have you imagined working for a dynamic small business where you are heard, highly regarded and able to do what you love all in one package? This is your opportunity! Join now! \n  Aetos Systems, Inc. is looking for a highly talented Data Analyst with advanced analytical, critical thinking and problem-solving skills to support NASA\u2019s Office of STEM Engagement in strengthening involvement with educational institutions to ensure that NASA can meet future workforce needs in the fields of Science, Technology, Engineering and Mathematics (STEM). The Office of STEM Engagement seeks to foster the high-quality engagement of students, faculty and minority serving institutions (MSI) and the broader community in NASA\u2019s missions and research. \n  Responsibilities \n  Aetos\u2019 data analyst will support NASA\u2019s internal and external stakeholders with MSI data, analysis, and reporting to facilitate communication, networking, and collaboration. They will perform data analysis at a level commensurate with executive and legislative decision makers and for the purposes of informing decision making at a national level.  \n The position primarily resides off -site and follows a traditional work schedule incorporating 9:00 am \u2013 5:00 pm Eastern Standard Time (core hours). \n  Additional responsibilities include: \n \n Perform quantitative and qualitative data analysis. \n Function as an internal consultant to the organization. \n Develop reporting solutions and analytic methodologies. \n Establish and present data analysis for internal and external customers. \n Responsible for the process of business needs collection and documentation; data and systems analysis; report design and creation; data findings formatting, aggregation, and presentation; results analysis; documentation of processes and findings. \n Work directly with clients, project, and business leaders to identify analytical requirements. \n \n \n Provide data analysis for NASA, Congressional, and White House Initiative reports and assist in the creation of reports to include graphs and tables to convey information to diverse audiences for different purposes.  \n Leverage enterprise resources, tools, and platforms to support scheduled and AD HOC MSI reporting. \n Report, analyze, visualize, and synthesize data.  \n Create reports to inform goals, metrics, decisions, and presentations.  \n Support the analysis of legacy data to standardize reporting analytics.  \n \n \n Requirements \n \n  Bachelor's degree with specialization in a related field and at least two (2) year of relevant experience \n \n  Skills \n \n Microsoft Office (word, PowerPoint, Excel, OneNote, Outlook, etc.) \n Collaboration software (Microsoft Teams, G-Suite, SharePoint, etc.) \n NASA reporting platforms (Business Objects Warehouse (BOBJ), NASA Procurement Data View (NPDV), Federal Data Procurement System (FDPS), etc.) \n Ability to analyze, interpret and visualize data for internal/external reports. \n Ability to adapt to changing requirements for analysis and reporting. \n Ability to learn new skills to report and analyze data. \n \n Must be a U.S. citizen with ability to pass a background investigation. \n  Benefits \n  What we offer: \n \n \n Competitive salaries \n Education and professional development assistance \n Multiple healthcare benefit packages & 24/7 virtual on-demand doctors\u2019 visits \n 401K \n Dave Ramsey\u2019s SmartDollar Financial Wellness program. \n Civic Leave \u2013 time off to support your favorite charity or community. \n Paid time off for personal leave and holidays.",
        "cleaned_desc": " Perform quantitative and qualitative data analysis. \n Function as an internal consultant to the organization. \n Develop reporting solutions and analytic methodologies. \n Establish and present data analysis for internal and external customers. \n Responsible for the process of business needs collection and documentation; data and systems analysis; report design and creation; data findings formatting, aggregation, and presentation; results analysis; documentation of processes and findings. \n Work directly with clients, project, and business leaders to identify analytical requirements. \n \n \n Provide data analysis for NASA, Congressional, and White House Initiative reports and assist in the creation of reports to include graphs and tables to convey information to diverse audiences for different purposes.    \n  Skills \n \n Microsoft Office (word, PowerPoint, Excel, OneNote, Outlook, etc.) \n Collaboration software (Microsoft Teams, G-Suite, SharePoint, etc.) \n NASA reporting platforms (Business Objects Warehouse (BOBJ), NASA Procurement Data View (NPDV), Federal Data Procurement System (FDPS), etc.) \n Ability to analyze, interpret and visualize data for internal/external reports. \n Ability to adapt to changing requirements for analysis and reporting. \n Ability to learn new skills to report and analyze data. ",
        "techs": [
            "microsoft office (word",
            "powerpoint",
            "excel",
            "onenote",
            "outlook),\ncollaboration software (microsoft teams",
            "g-suite",
            "sharepoint),\nnasa reporting platforms (business objects warehouse (bobj)",
            "nasa procurement data view (npdv)",
            "federal data procurement system (fdps))"
        ],
        "cleaned_techs": [
            "microsoft",
            "powerpoint",
            "excel",
            "onenote",
            "outlook),\ncollaboration software (microsoft teams",
            "g-suite",
            "sharepoint),\nnasa reporting platforms (business objects warehouse (bobj)",
            "nasa procurement data view (npdv)",
            "federal data procurement system (fdps))"
        ]
    },
    "e6b9738450d6a7bb": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 55000.0,
        "salary_max": 65000.0,
        "title": "People Operations Analyst",
        "company": "Alpine IQ",
        "desc": "Company Overview: \n  Join Alpine IQ, where innovation, collaboration, and a passion for excellence are at the core of everything we do. Our cutting-edge solutions empower organizations to achieve their goals, and our commitment to our employees ensures that we maintain a vibrant and inclusive workplace. If you are a data-driven HR professional with a knack for process improvement and a thirst for data analysis, we want you to be part of our People Operations team. \n  Position Overview: \n  Are you a master of HR data analysis, process optimization, and HR technology? Are you excited about the opportunity to make an impact on our people operations while working in a forward-thinking, fast-paced environment? We are seeking a People Operations Analyst who will play a key role in driving our HR strategies, ensuring data-driven decision-making, and enhancing the employee experience. \n  Key Responsibilities: \n \n Data-Driven Insights: Uncover valuable insights from HR data to drive informed decision-making. You'll be our data wizard, turning numbers into actionable strategies that optimize our HR processes and enhance our employee experience. \n Process Optimization: Identify and implement process improvements within our HR functions to ensure efficiency and compliance. We value your expertise in streamlining workflows and eliminating bottlenecks. \n Recruitment Analysis: Help us attract and retain top talent by analyzing recruitment data, including time-to-fill, sourcing channels, and candidate quality. Your insights will guide our recruitment strategies. \n Employee Relations: Collect and analyze data related to employee relations matters, providing insights that improve employee satisfaction and resolve workplace issues. \n Benefits & Compensation: Support the administration of our employee benefits and compensation programs. Stay ahead of market trends and provide recommendations for competitive compensation packages. \n Technology Management: Assist in implementing and maintaining our HR software and systems. You'll be the go-to person for ensuring our HR tech stack is user-friendly and efficient. \n \n Qualifications: \n \n Bachelor's degree in Human Resources, Business, Data Analytics, or related field. \n 2-4 years of HR or data analysis experience, preferably in a SaaS or tech company. \n Strong analytical skills and proficiency with data analysis tools and HRIS systems. \n Excellent communication and interpersonal skills. \n A keen understanding of HR policies, procedures, and employment laws. \n \n What We Offer: \n \n A dynamic and innovative work environment. \n Opportunities for professional growth and development. \n Competitive compensation and benefits package. \n The chance to work with a passionate and collaborative team. \n \n Are you ready to make a meaningful impact in a rapidly evolving SaaS company? If you are excited about leveraging data and process optimization to drive HR excellence, we want to hear from you. Please submit your resume and a cover letter explaining why you'd be a great fit for our People Operations Analyst position. \n \n  Join us in revolutionizing how we manage our people, data, and processes. Your expertise will drive our success, and together, we'll improve the workplace experience for everyone. \n  Alpine IQ is an equal opportunity employer and welcomes applicants from all backgrounds. We look forward to meeting you!",
        "cleaned_desc": " \n Qualifications: \n \n Bachelor's degree in Human Resources, Business, Data Analytics, or related field. \n 2-4 years of HR or data analysis experience, preferably in a SaaS or tech company. \n Strong analytical skills and proficiency with data analysis tools and HRIS systems. ",
        "techs": [
            "data analysis tools",
            "hris systems"
        ],
        "cleaned_techs": [
            "data analysis tools",
            "hris systems"
        ]
    },
    "e90db1db95676bd9": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 118000.0,
        "salary_max": 157000.0,
        "title": "Lead Data Analyst, Go-To-Market",
        "company": "WEX Inc.",
        "desc": "Lead Data Analyst\n  \n \n \n   WEX is an innovative payments and technology company leading the way in a rapidly changing environment. Our goal is to simplify the business of running a business for our customers and free them to spend more time, with less worry, on the things they love. We are on a journey to build a unified, world-class user experience across our products and services and leverage customer-driven innovation to power our growth and strategic initiatives.\n  \n \n \n   This is a really exciting time to be working in Data & Analytics at WEX. We are driving towards our True North vision of providing the right data, in the right place, at the right time so that we can enable and accelerate the journey to being a data-driven organization. There is strong executive support for all things data and it is viewed as a critical component for our enterprise transformation activities. What we need now is someone with a passion for solving problems and driving actionable outcomes with data. You will do this through performing and developing the following competencies: Insights Driven, Stakeholder Aligned, Results Focused, Dynamic Collaboration, Quality Mindset.\n  \n \n \n   About the team:\n  \n \n   Centralized within the Digital organization, the Data & Analytics team is growing rapidly in order to support new initiatives that will enable business stakeholders to more effectively leverage data to support their daily activities. We are a lively, collaborative group of data enthusiasts who find great joy in delivering high value data solutions to our internal customers. We work on the top priority efforts across the organization and are key to enabling data-driven decision making.\n  \n \n \n   What you\u2019ll do:\n  \n \n \n \n     Act as an analytics translator between data consumers & producers to drive better business outcomes with sound, data-driven recommendations\n    \n \n \n     Work collaboratively with senior business stakeholders to identify, define, & prioritize opportunities for data solutions to improve & enhance business processes\n    \n \n \n     Perform complex data analysis and exploration, leading to concise and digestible data storytelling\n    \n \n \n     Translate business data needs into elegant and executable plans and designs; conceptualize and develop reusable tools, techniques, metrics, and dashboards for insights\n    \n \n \n     Coach and mentor other data analysts on best practices and approaches to problem-solving\n    \n \n \n     Clearly explain complex analytical concepts & outcomes, as well as how they can be leveraged by business stakeholders to achieve business objectives\n    \n \n \n     Plan, define, & prioritize data delivery activities within an analytics team (eg. create efficient automated dashboards for long-term self-service use by business)\n    \n \n \n     Cultivate a data-driven culture through the creation of analytics deliverables that delight the customer and have a high degree of data integrity\n    \n \n \n     Participate in the development of standards & best practices for analytics delivery, and model this behavior for other analytics practitioners across the organization\n    \n \n \n \n   How you\u2019ll engage:\n  \n \n \n \n  Insights Driven:  Clear hypothesis and objective driven analytics that help drive our business decisions and ongoing metrics\n    \n \n \n  Stakeholder Aligned:  Understand the needs and audience for deliverables with a succinct and tailored message to maximize impact\n    \n \n \n  Results Focused:  Rigorous focus on how analytics drive the end to end experiences with clear path to production and measurable impact\n    \n \n \n  Dynamic Collaboration:  Drive continual improvement of our teams best practices and processes to power collaboration\n    \n \n \n  Quality Mindset:  Trust in our findings is critical so data and analytic quality is understood and accounted for from the beginning\n    \n \n \n \n   Experience you\u2019ll bring:\n  \n \n \n \n     8+ years of experience working in an analytics function, with a balanced mix of working with business stakeholders and getting in the weeds technically, with at least a portion of that time spent as an analytics project lead\n    \n \n \n     5+ years building data visualizations with one or more modern reporting tools (Tableau, Power BI, Looker, etc.)\n    \n \n \n     Advanced working knowledge and hands-on experience with SQL, preferably on Snowflake or other cloud data platforms\n    \n \n \n     Knowledge and experience with product development processes, methodologies, and tooling (JIRA)\n    \n \n \n     Excellent communication, analytical, and problem-solving skills\n    \n \n \n     Strong ability to tell stories with data & visualizations to both technical and non-technical audiences, targeting the strategic level\n    \n \n \n     Ability to perform in-depth data & business analysis and nurture those skills in more junior analysts\n    \n \n \n     Business acumen to elicit and understand complex business requirements, and ensure that analyses properly answer meaningful questions\n    \n \n \n     Specific subject area expertise in go-to-market domains (Sales, Marketing, and Customer Success)\n    \n \n \n \n   What would make you stand out:\n  \n \n \n \n     Experience in data consulting or as an analytics project lead, focusing on translating business needs into data-driven solutions as well as integrating data into business processes\n    \n \n \n     Experience collaborating with technical partners to build data solutions that meet the needs of business priorities\n    \n \n \n     Experience building automated and scalable solutions for self-service through Tableau or other tools to enable greater data self-sufficiency for a broad data consumer audience\n    \n \n \n     Experience participating in an Analytics Center of Excellence\n    \n \n \n     Analytics data preparation/build experience with tools like Dataiku, Tableau Prep, Alteryx, dbt, etc.\n    \n \n \n     Fintech/payments industry experience\n    \n \n  The base pay range represents the anticipated low and high end of the pay range for this position. Actual pay rates will vary and will be based on various factors, such as your qualifications, skills, competencies, and proficiency for the role. Base pay is one component of WEX's total compensation package. Most sales positions are eligible for commission under the terms of an applicable plan. Non-sales roles are typically eligible for a quarterly or annual bonus based on their role and applicable plan. WEX's comprehensive and market competitive benefits are designed to support your personal and professional well-being. Benefits include health, dental and vision insurances, retirement savings plan, paid time off, health savings account, flexible spending accounts, life insurance, disability insurance, tuition reimbursement, and more. For more information, check out the \"About Us\" section.\n   Salary Pay Range: $118,000.00 - $157,000.00",
        "cleaned_desc": " \n     Perform complex data analysis and exploration, leading to concise and digestible data storytelling\n    \n \n \n     Translate business data needs into elegant and executable plans and designs; conceptualize and develop reusable tools, techniques, metrics, and dashboards for insights\n    \n \n \n     Coach and mentor other data analysts on best practices and approaches to problem-solving\n    \n \n \n     Clearly explain complex analytical concepts & outcomes, as well as how they can be leveraged by business stakeholders to achieve business objectives\n    \n \n \n     Plan, define, & prioritize data delivery activities within an analytics team (eg. create efficient automated dashboards for long-term self-service use by business)\n    \n \n \n     Cultivate a data-driven culture through the creation of analytics deliverables that delight the customer and have a high degree of data integrity\n    \n \n \n     Participate in the development of standards & best practices for analytics delivery, and model this behavior for other analytics practitioners across the organization\n    \n \n \n \n   How you\u2019ll engage:     \n \n \n     5+ years building data visualizations with one or more modern reporting tools (Tableau, Power BI, Looker, etc.)\n    \n \n \n     Advanced working knowledge and hands-on experience with SQL, preferably on Snowflake or other cloud data platforms\n    \n \n \n     Knowledge and experience with product development processes, methodologies, and tooling (JIRA)\n    \n \n \n     Excellent communication, analytical, and problem-solving skills\n    \n \n \n     Strong ability to tell stories with data & visualizations to both technical and non-technical audiences, targeting the strategic level\n    \n \n \n     Ability to perform in-depth data & business analysis and nurture those skills in more junior analysts\n    \n \n \n     Business acumen to elicit and understand complex business requirements, and ensure that analyses properly answer meaningful questions\n    \n \n       Specific subject area expertise in go-to-market domains (Sales, Marketing, and Customer Success)\n    \n \n \n \n   What would make you stand out:\n  \n \n \n \n     Experience in data consulting or as an analytics project lead, focusing on translating business needs into data-driven solutions as well as integrating data into business processes\n    \n \n \n     Experience collaborating with technical partners to build data solutions that meet the needs of business priorities\n    \n \n \n     Experience building automated and scalable solutions for self-service through Tableau or other tools to enable greater data self-sufficiency for a broad data consumer audience\n    \n \n \n     Experience participating in an Analytics Center of Excellence\n    \n \n \n     Analytics data preparation/build experience with tools like Dataiku, Tableau Prep, Alteryx, dbt, etc.\n    \n \n \n     Fintech/payments industry experience",
        "techs": [
            "tableau",
            "power bi",
            "looker",
            "sql",
            "snowflake",
            "jira",
            "dataiku",
            "tableau prep",
            "alteryx",
            "dbt"
        ],
        "cleaned_techs": [
            "tableau",
            "powerbi",
            "looker",
            "sql",
            "snowflake",
            "jira",
            "dataiku",
            "tableau prep",
            "alteryx",
            "dbt"
        ]
    },
    "cf87bc729ea42b14": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 82943.586,
        "salary_max": 105025.055,
        "title": "BUSINESS ANALYST",
        "company": "Transition Technologies PSC Sp. z o. o.",
        "desc": "_ Business Analyst  \n \n \n \n _ What will you do? \n \n Responsibilities : \n  You will be a member of project team that implements ITSM solutions for external customers. The solution is based on third-party system and aligned with ITIL best practices. During the project you will combine the role of business analyst with ITSM specialist by: \n \n Working closely with external customers: identify needs, gather requirements and analyzing customer\u2019s expectations; \n Modeling business processes with focus on IT Service Management Area; \n Suggesting optimizations and areas for improvement; \n Hosting workshops and meetings with the customer, presenting solutions to stakeholders; \n Creating a concept of final solutions based on third-party vendors\u2019 systems; \n Participation in presales activities like assessing feasibility of the requirements and preparing content of the commercial offer; \n Close cooperation with technical teams; \n Verification whether delivered solution meets business needs; \n Collaboration within a team of business analysts. \n \n \n \n \n _ Who are we looking for? \n \n \n Requirements : \n \n Place of work: Poland \n At least 3 years of experience in IT Service Management (Service Manager, Service Delivery Manager, Service Desk Manager, Incident Manager, Asset Manager etc); \n Good knowledge of at least one of ITSM tool: ServiceNow, BMC, Atlassian/Jira, Manage Engine/Service Desk+; \n Very good understanding of ITIL concept, experience in working with ITIL best practices; \n Ability to suggest how to implement an ITIL-compliant processes in the organization; \n Strong communication and presentation skills; \n Ability to share knowledge, focus on learning new skills independently; \n Effective communication skills in English and Polish \u2013 at least (B2/C1) level; \n \n Nice to have : \n \n ITIL Certificate; \n Experience in Business Analysis or Project Management area; \n Technical IT background; \n Familiar with Jira and Confluence; \n Readiness to public presentations. \n \n \n \n \n \n \n \n \n _ Why is it worth it? \n \n \n What can we offer : \n \n Flexible forms of employment and working hours (CoE or B2B); \n An interesting, challenging job in the dynamically developing Capital Group company; \n Work on innovative projects using modern technologies; \n Direct impact on shaping the image of the Capital Group\u2019s companies on the market; \n Possibility to develop competences in a wide range; \n Attractive salary; \n Stability of employment and a friendly work atmosphere; \n Cool benefits, among others integration meetings, internal company competitions, fruit Tuesdays, sweet Thursdays and much more;",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "347e2f75f6e385c0": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 60000.0,
        "salary_max": 93150.0,
        "title": "Platforms Data Analyst",
        "company": "Gannett",
        "desc": "Gannett Co., Inc. (NYSE:  GCI) is a subscription-led and digitally-focused media and marketing solutions company committed to empowering communities to thrive. With an unmatched reach at the national and local level, Gannett touches the lives of millions with our Pulitzer Prize-winning content, consumer experiences and benefits, and advertiser products and services. \n \n  Our current portfolio of media assets includes The USA TODAY NETWORK, which includes USA TODAY, and local media organizations in 43 states in the United States, and Newsquest, a wholly-owned subsidiary operating in the United Kingdom. We also own digital marketing services companies under the brand LocaliQ, which provide a cloud-based platform of products to enable small and medium-sized businesses to accomplish their marketing goals. In addition, our portfolio includes one of the largest media-owned events businesses in the U.S., USA TODAY NETWORK Ventures. \n \n  Gannett open roles are featured on various external job boards. When applying to a position at Gannett, you should be completing an application on Gannett Careers via Dayforce. Job postings directing you to complete an application on other external sites may not be valid. \n \n  To connect with us, visit www.gannett.com \n \n  Gannett is looking for a Data Analyst with journalism experience (required) to monitor and advise content strategies on the performance and innovation of the USA TODAY Network\u2019s work on newsletters, social, messaging, video and other emerging platforms. This analyst will also partner across divisions to develop relevant reporting for new experiments and initiatives USA TODAY and local sites are launching. \n \n  Off-platform content is growing fast across at Gannett and a critical way that we reach and engage with younger readers. We are seeking an analyst that understands the nuance of data reporting for these platforms, the audiences, ad revenue, subscription referral and cohorting these users to understand how their readership affects business goals.This analyst must be nimble and willing to adapt and learn as platforms and new delivery methods shift. \n As a key member of our dynamic Content Strategy/Analytics team, you will utilize your expertise in data analytics, reporting tools, and market insights to enhance the audience engagement, revenue generation and digital subscription growth for USA TODAY and local USA TODAY network sites. \n \n  This role can work remote from anywhere in the US except for Alaska & Hawaii. \n \n \n Responsibilities: \n \n \n Help set and monitor KPIs and dashboards, ensuring accuracy, timeliness, and relevance. \n Work with USA Today network leaders in crafting and monitoring strategy, KPIs for video, platforms and experiments. \n Work with regional strategists and product to analyze performance across multiple user journeys. \n Analytics QA for new product deployments and troubleshooting. \n Develop engagement metrics that allow the business to better understand user behavior based on their entry, product use and propensity to subscribe. \n This is about more than reports and spreadsheets. The strategist will exercise judgment in translating data points into actionable insights. The right candidate deeply understands data tools and knows how to apply their analytical mindset to content. This is a hands-on data job, and digging skills are required. But it's just as important that a candidate understand how research relates to modern users of our products, particularly on mobile devices. \n Excellent communication skills, both written and verbal, to effectively convey findings and recommendations to diverse stakeholders. \n \n Requirements: \n \n \n Bachelor's degree in communication, journalism, marketing, data science or analytics, or an equivalent combination of education and experience working with data. \n 3-5 years of experience in analytics, with a preference for content analytics. Journalism experience is mandatory. \n Expertise in Excel, GA4, Looker/Data Studio, Big Query, SQL, Tableau, Google Ad Manager and other analytic and data tools. \n Expertise in platform-specific data tools such as Emplifi, CrowdTangle, Tubular, Exact Target and other analytic and data tools. \n Collaborate with cross-functional teams to analyze audience and revenue data (both consumer and advertising/affiliate), identify trends, and provide timely recommendations that contribute to informed business decisions. \n Experience identifying, analyzing, and using data to independently test theories, confirm assumptions, and measure success via iteration and with attention to detail. \n Familiarity with news products and journalism and understanding of media analytics and ethics. \n Preferred: A passion and experience with storytelling via social, newsletters. \n Employment is contingent on passing a post-offer pre-employment background check. \n \n Application Instructions: \n  We are eager to learn more about you and how you fit this role. When you apply, don\u2019t limit your upload to a resume; show us what you\u2019ve done. To do so, put together a single document file that includes the following, in this order: \n 1. Your resume \u2013 one to two pages. \n 2. A cover letter that outlines how you would approach the job. \n \n  It is important that these items be assembled into a single document and uploaded in PDF format. Completing these steps will ensure that your application receives the highest consideration. \n #Newsgnt \n #LI-NC1 \n #LI-Remote \n The annualized base salary for this role will range between $60,000 and $93,150. Variable compensation is not reflected in these figures and based on the role, may be applicable. Exact compensation may vary based on skills, experience, location, and union representation, if applicable. \u200b \n \n  Gannett Co., Inc. is a proud equal opportunity employer committed to building and maintaining a diverse workforce. As such, we will consider all qualified applicants for employment and do not discriminate in connection with employment decisions on the basis of an applicant or employee\u2019s race, color, national origin, ethnicity, ancestry, citizenship status, sex, gender, gender identity, gender expression, religion, age, marital status, personal appearance (including height and weight), sexual orientation, family responsibilities, physical or mental disability, medical condition, pregnancy status (including childbirth, breastfeeding or related medical conditions), education, genetic characteristics or information, political affiliation, military or veteran status or other classifications protected by applicable federal, state and local laws in the jurisdictions where Gannett employs employees. In addition, Gannett Co., Inc. will provide applicants who require a reasonable accommodation, as a result of an applicant\u2019s disability or religion, to complete this employment application and/or any other process in connection with an individuals\u2019 application for employment with Gannett Co., Inc. Applicants who require such accommodation should contact Gannett Co., Inc.\u2019s Recruitment Department at Recruit@gannett.com.",
        "cleaned_desc": " Bachelor's degree in communication, journalism, marketing, data science or analytics, or an equivalent combination of education and experience working with data. \n 3-5 years of experience in analytics, with a preference for content analytics. Journalism experience is mandatory. \n Expertise in Excel, GA4, Looker/Data Studio, Big Query, SQL, Tableau, Google Ad Manager and other analytic and data tools. \n Expertise in platform-specific data tools such as Emplifi, CrowdTangle, Tubular, Exact Target and other analytic and data tools. \n Collaborate with cross-functional teams to analyze audience and revenue data (both consumer and advertising/affiliate), identify trends, and provide timely recommendations that contribute to informed business decisions. \n Experience identifying, analyzing, and using data to independently test theories, confirm assumptions, and measure success via iteration and with attention to detail. \n Familiarity with news products and journalism and understanding of media analytics and ethics. \n Preferred: A passion and experience with storytelling via social, newsletters. \n Employment is contingent on passing a post-offer pre-employment background check. \n ",
        "techs": [
            "excel",
            "ga4",
            "looker/data studio",
            "big query",
            "sql",
            "tableau",
            "google ad manager",
            "emplifi",
            "crowdtangle",
            "tubular",
            "exact target"
        ],
        "cleaned_techs": [
            "excel",
            "ga4",
            "looker/data studio",
            "big query",
            "sql",
            "tableau",
            "google ad manager",
            "emplifi",
            "crowdtangle",
            "tubular",
            "exact target"
        ]
    },
    "6a982c4eb5a753cf": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 65.0,
        "salary_max": -1.0,
        "title": "NextGen Support Analyst",
        "company": "Tekshapers Software Solutions Pvt Ltd",
        "desc": "Role : NextGen Support Analyst \n Location : Los Angeles, CA (Remote) \n Type : Contract \n Job Summary: \n We are looking for a dedicated and experienced NextGen Revenue Cycle Legacy Application Support Specialist to join our team during the organization's transition to Epic. In this role, you will be responsible for maintaining the functionality of the legacy NextGen Revenue Cycle application, ensuring a smooth transition to Epic, and providing support to end-users. Your expertise will play a vital role in bridging the gap between legacy systems and the new Epic implementation. \n Responsibilities: \n Supporting Next Gen Revenue Cycle Legacy Application while the organization implements Epic \n Application Maintenance: Ensure the stability and optimal performance of the NextGen Revenue Cycle Legacy Application. \n Support Transition: Assist in the organization's transition to Epic, ensuring that data and processes are smoothly transferred from the legacy system. \n Issue Resolution: Diagnose and resolve technical issues and inquiries related to the NextGen Revenue Cycle application. \n End-User Support: Provide support to end-users, addressing their queries, issues, and training needs related to the legacy application. \n Documentation: Maintain detailed records of support requests, issue resolutions, and troubleshooting steps. \n Testing: Collaborate with cross-functional teams to conduct testing and quality assurance to validate data and processes during the transition to Epic. \n Training: Develop and provide training and guidance to end-users on the NextGen Revenue Cycle Legacy Application to ensure a smooth transition. \n Data Integrity: Ensure data integrity and compliance with healthcare regulations throughout the transition. \n Qualifications: \n Bachelor's degree in Computer Science, Information Technology, or a related field. \n Proven experience in providing support for NextGen Healthcare solutions and applications. \n Strong knowledge of NextGen EHR and practice management software. \n Excellent problem-solving and analytical skills. \n Effective communication and client-facing skills. \n Ability to work both independently and as part of a team. \n Familiarity with healthcare industry standards and regulations is a plus. \n Job Type: Contract \n Pay: From $65.00 per hour \n Schedule: \n \n 8 hour shift \n \n Experience: \n \n Total: 10 years (Required) \n Revenue Cycle and Data Analyst: 5 years (Required) \n NextGen: 5 years (Required) \n \n Work Location: Remote",
        "cleaned_desc": " Data Integrity: Ensure data integrity and compliance with healthcare regulations throughout the transition. \n Qualifications: \n Bachelor's degree in Computer Science, Information Technology, or a related field. \n Proven experience in providing support for NextGen Healthcare solutions and applications. \n Strong knowledge of NextGen EHR and practice management software. \n Excellent problem-solving and analytical skills. \n Effective communication and client-facing skills. ",
        "techs": [
            "nextgen healthcare solutions",
            "nextgen ehr",
            "practice management software"
        ],
        "cleaned_techs": [
            "nextgen healthcare solutions",
            "nextgen ehr",
            "practice management software"
        ]
    },
    "dfe9deb71ef882b1": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 81996.75,
        "salary_max": 103826.16,
        "title": "Business Analyst",
        "company": "Redan LLC",
        "desc": "Business   Analyst \n  Redan LLC is looking for a mid-senior level Business Analyst to join our growing team! \n  Redan's Business Analyst will identify and articulate the need for change for the product and facilitate that change. \n  The role operates in an Agile environment that continuously deploys code and requires a solid understanding of the full software lifecycle, including DevSecOps, Software Development, System Integration, Infrastructure Management, and Cyber Security related to public-facing government products. \n  What You'll Be Doing  \n As the Business Analyst at Redan, you will: \n \n  Be responsible for user stories, spikes, and bugs: create, include details and attachments, labels, tags, testing steps, ensure that stories adhere to definition of done, conduct actual testing of stories, including 508 testing. \n  Work to become a subject matter expert for the products that are supported. \n  Assist with testing/communication/de-bugging with external integration partner system teams. \n  Work independently to continue to learn the system and products by reviewing resources and documentation, testing, and engaging with design, development, Product Owner and stakeholders in great detail. \n  Obtain DHS TT5 Trusted Tester certification within first 3 months, test stories that impact 508 accessibility. \n  Support technical team as needed. \n  Develop and provide training as needed. \n  Develop technical documentation as needed.  \n Facilitate Agile ceremonies, such as sprint planning, daily stand-ups, sprint reviews, and retrospectives. \n \n  Profile of Success: \n \n  3-5 Years of Experience (YOE) working in an Agile and Scrum software development environment performing the duties of Business Analyst. \n  Intermediate+ skills in Jira and Confluence. \n  Strong aptitude for writing user stories and leading backlog refinement. \n  Exceptional documentation and attention to detail. \n  Confident facilitator and strong communications skills. \n  Must be able to pass the DHS TT5 Trusted Tester certification within first 3 months. \n  Experience working in an Agile software development environment and familiar with Scrum structure and ceremonies. \n  Familiar with and experience using tools like Jira, Confluence, Mural, and Teams to communicate requirements, design, processes, and content. \n  Strong oral and written communication skills. \n  Able to communicate effectively with Product Owners (POs) and stakeholders to \n  understand requirements/scope and be able to translate that into user stories. \n  Able to effectively communicate and coordinate between stakeholders, POs, and team members including designers, business analysts, developers, and Scrum Masters across other development teams. \n  Able to prepare for and facilitate sprint ceremonies effectively. \n  Proactive approach to asking questions to understand gaps, and to help identify and support the best design for product needs \n \n  Our IDEAL Business Analyst Will Have:  \n \n USCIS or DHS Experience. \n \n  Conditions of Employment  \n \n US Citizenship is required \n  Bachelor's Degree required \n  You must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future \n \n  Compensation \n  Pay and benefits information for this position will be provided to interested candidates that apply. Redan offers a package of compensation and benefits to full-time salaried employees. \n  Redan, LLC is an Equal Opportunity Employer and we highly value diversity of our workforce. We accept resumes from all interested parties and consider applicants for all positions without regard to race, color, religion, sex, national origin, age, marital status, sexual preference, personal appearance, family responsibility, the presence of a non-job-related medical condition or physical disability, matriculation, political affiliation, veteran status, or any other legally protected status.",
        "cleaned_desc": " \n  3-5 Years of Experience (YOE) working in an Agile and Scrum software development environment performing the duties of Business Analyst. \n  Intermediate+ skills in Jira and Confluence. \n  Strong aptitude for writing user stories and leading backlog refinement. \n  Exceptional documentation and attention to detail. \n  Confident facilitator and strong communications skills. \n  Must be able to pass the DHS TT5 Trusted Tester certification within first 3 months. \n  Experience working in an Agile software development environment and familiar with Scrum structure and ceremonies. \n  Familiar with and experience using tools like Jira, Confluence, Mural, and Teams to communicate requirements, design, processes, and content. ",
        "techs": [
            "jira",
            "confluence",
            "mural",
            "teams"
        ],
        "cleaned_techs": [
            "jira",
            "confluence",
            "mural",
            "teams"
        ]
    },
    "ba1e2db423326890": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 57400.414,
        "salary_max": 72681.71,
        "title": "Regulatory Reporting Analyst",
        "company": "AmeriHealth Caritas",
        "desc": "Your career starts now. We\u2019re looking for the next generation of health care leaders. \n  At AmeriHealth Caritas, we\u2019re passionate about helping people get care, stay well and build healthy communities. As one of the nation's leaders in health care solutions, we offer our associates the opportunity to impact the lives of millions of people through our national footprint of products, services and award-winning programs. AmeriHealth Caritas is seeking talented, passionate individuals to join our team. Together we can build healthier communities. If you want to make a difference, we\u2019d like to hear from you. \n  Headquartered in Newtown Square, AmeriHealth Caritas is a mission-driven organization with more than 30 years of experience. We deliver comprehensive, outcomes-driven care to those who need it most. We offer integrated managed care products, pharmaceutical benefit management and specialty pharmacy services, behavioral health services, and other administrative services. Discover more about us at  www.amerihealthcaritas.com . \n \n \n \n \n \n \n Responsibilities: \n  Under the general supervision of the Manager, Regulatory Reporting, the Analyst is responsible for the continued enhancement of the Regulatory Reporting process through the improvement and completion of existing state mandated reporting requirements while maintaining a high level of accuracy, integrity and timeliness of data reports. \n \n Develops and maintains a complete understanding of all data sources (including, but not limited to the Data Warehouse, the Facets System and the Reports Database) available for reporting. \n Completes all regulatory reporting requirements assigned, accurately and meeting all regulatory deadlines. Failure to do so may result in the company incurring financial penalties, decreased satisfaction by client (e.g., expansion plans) and compromised relationship with state or other regulatory agencies. \n Performs data verification through trending, benchmarking or other analyses to ensure data integrity. \n Maintains electronic and paper documentation supporting the completion of each report for each reporting period within departmental guidelines. \n Utilizing team approach, works cooperatively with other staff members, sharing specific technical or business expertise. \n Develops and maintains strong knowledge of business practices, claims coding and the managed care industry in general and applies knowledge to determining data and report solutions. \n Develops and maintains strong working knowledge of state and other regulatory agency statutory reporting policies and procedures. \n Uses independent judgment (with supervisory guidance as necessary or appropriate) to ensure reporting is completed as accurately and efficiently as possible. \n Appropriately identifies and researches any issues or problems potentially compromising the accuracy or integrity of the data reported, utilizing on-line help, the Facets system or other reference materials. \n Identifies and analyzes data and/or reporting anomalies. Researches, tests and validates results. Provides explanations for variances and changes. Documents agreed upon explanations in \u201cnotes to files\u201d, copying statutory management. \n Communicates with management/stakeholders to determine appropriate courses of action prior to enacting changes in reporting processes. \n \n Education/ Experience: \n \n Bachelor\u2019s Degree or equivalent. \n 1 to 3 years Data Analysis, Reporting, Claims, Managed Care or Healthcare environment required. \n Basic working knowledge of business computer programs, applications and systems, database structures and data files required. \n Experience with Microsoft Access or other Relational Databases required. \n Basic understanding of Claims Processing/ Clinical Coding and business practices. \n Ability to appropriately extract and manipulate data from a variety of sources and databases. \n Proficiency in Microsoft Access, Excel, Word and Power Point. \n Strong communication (both written and verbal), analytical and problem solving skills. \n Excellent organizational skills. \n Ability to identify issues, research, investigates and document findings.",
        "cleaned_desc": " Basic working knowledge of business computer programs, applications and systems, database structures and data files required. \n Experience with Microsoft Access or other Relational Databases required. \n Basic understanding of Claims Processing/ Clinical Coding and business practices. \n Ability to appropriately extract and manipulate data from a variety of sources and databases. \n Proficiency in Microsoft Access, Excel, Word and Power Point. \n Strong communication (both written and verbal), analytical and problem solving skills. \n Excellent organizational skills. ",
        "techs": [
            "microsoft access",
            "relational databases",
            "claims processing",
            "clinical coding",
            "microsoft excel",
            "microsoft word",
            "microsoft power point"
        ],
        "cleaned_techs": [
            "microsoft access",
            "relational databases",
            "claims processing",
            "clinical coding",
            "excel",
            "microsoft",
            "microsoft power point"
        ]
    },
    "3ba1f3b229830b18": {
        "terms": [
            "data analyst"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Associate Financial Analyst - Lymphoma-Myeloma",
        "company": "MD Anderson Cancer Center",
        "desc": "Associate Financial Analyst \n \n \n MISSION STATEMENT \n \n  The mission of The University of Texas M. D. Anderson Cancer Center is to eliminate cancer in Texas, the nation, and the world through outstanding programs that integrate patient care, research and prevention, and through education for undergraduate and graduate students, trainees, professionals, employees and the public. \n  \n \n SUMMARY \n \n  Provides accounting support for Clinical Research financial and operational activities of the Department of Lymphoma/Myeloma. \n  \n \n JOB SPECIFIC COMPETENCIES \n \n  Experience with financial data analysis, using OBIEE, reconciling clinical research study accounts and experience with Epic account reconciliation is preferred. \n  \n  1. \n  Financial Analysis & Reporting \n \n \n \n Prepare and maintain reports for all industry sponsored studies belonging to assigned PIs that show the financial status of each study, including both current revenue and expenses along with anticipated revenue and expenses.  \n \n \n \n Prepare and maintain reports for all assigned PIs summarizing free balances for all ancillary accounts (30s, 40s, 80s, 90s, etc.).  \n \n \n \n Provide account information when study team members and PIs need to make purchases or request services or new hires.  \n \n \n \n Regularly review all invoices from other departments within MD Anderson for accuracy and ensure that they are paid in a timely manner - IDSR, CTRC, etc.  \n \n \n \n Review and complete monthly Statistical Sampling entries for assigned PIs by institutional deadlines.  \n \n \n \n Review and analyze data from above reports to identify trends or problem areas and formulate strategic solutions to address any deficiencies.  \n \n \n \n Meet regularly with assigned faculty to review financial account status and provide updates regarding invoicing and payments, as well as discuss personnel financing and plan for their continued support.  \n \n \n \n Provide guidance and advice to faculty regarding the appropriate use of funds according to institutional policies and procedures.  \n \n \n \n Maintain projections of current and anticipated personnel spending along with anticipated personnel funding.  \n \n \n \n Create break even report for PI clinical trial portfolio and determine their team's monthly burn rate.  \n \n \n \n Review all sources of personnel funding to ensure all personnel are being charged to the correct accounts for invoicing and effort reporting purposes.  \n \n \n \n Prepare and process ePAFs for new hires, staffing changes, or to correct any errors on study deficits.  \n \n \n \n Regularly calculate personnel money earned on each sponsored project, and regularly prepare IDTs to move that personnel funding from individual study accounts to fund the personnel Infrastructure Accounts.  \n \n \n \n Assist in the preparation of other monthly and annual financial reports as assigned.  \n \n \n \n Manage Alliance trials for faculty and provide quarterly expense report to the sponsor or CRO  \n \n  2. \n  Invoicing \n \n \n \n Review all sponsored contracts and invoice as appropriate on a routine basis to ensure invoicing occurs as outlined in the sponsored contract.  \n \n \n \n Download study charges from epic and reconcile against coverage analysis and budget to invoice sponsor for expense reimbursement  \n \n \n \n Track payments to ensure they have been received and reflected to study account as expected.  \n \n \n \n Coordinate with Grants & Contracts to make sure that the accounting for all invoices is accurate and all payments are posted to the correct accounts.  \n \n \n \n Review hospital billing records, study accounts, and coordinate with study teams to be sure that all invoiceables listed on the contract are properly invoiced to the sponsor.  \n \n \n \n Review patient travel reimbursement submissions from the patient, review for accordance with sponsor's contract and institutional rules, prepare check request submissions, prepare invoices for sponsor reimbursement, and monitor accounts to be sure that all patients have been properly reimbursed and all reimbursements have been paid by the sponsor.  \n \n \n \n Coordinate with Pre-Award on contract amendments for those studies where the existing contract does not provide adequate coverage for the expenses incurred.  \n \n  3. \n  Quarterly Effort Reporting/Certification \n \n \n \n Coordinate the quarterly effort certification process with PI and study team members in accordance with institutional policy.  \n \n \n \n Review all effort cards for alignment with department records.  \n \n \n \n Meet with principal investigators to review their own effort cards as well as the cards for all employees whose salary is supported from their sponsored projects.  \n \n \n \n Monitor ECC system to track timely certification and processing of reports.  \n \n \n \n Process all ePAFs/retros needed to ensure that the effort certified matches actual payroll.  \n \n \n \n Maintain high level of expertise for effort certification through educational opportunities and interactions with Grants & Contracts personnel.  \n \n \n 4. \n Monthly Fee processing of  \n Institutional Start-up Fees and Department Core Rees related to clinical research projects and other studies  \n \n \n \n Process start-up fees on a monthly (or quarterly) basis ensuring timely and accurate recording and reporting of financial data.  \n \n \n \n Process department core fees acquired from clinical trials to ensure salaries are covered for regulatory accounting personnel and clinical research supervisors. \n \n \n \n Other duties as assigned. \n \n  Education Required - Bachelor's degree in Business Administration or related business specialty. \n  \n  Experience Required - None. May substitute required education degree with additional years of equivalent experience on a one to one basis. \n  \n  It is the policy of The University of Texas MD Anderson Cancer Center to provide equal employment opportunity without regard to race, color, religion, age, national origin, sex, gender, sexual orientation, gender identity/expression, disability, protected veteran status, genetic information, or any other basis protected by institutional policy or by federal, state or local laws unless such distinction is required by law. http://www.mdanderson.org/about-us/legal-and-policy/legal-statements/eeo-affirmative-action.html\n  \n  Additional Information \n  \n \n Requisition ID: 162768  \n Employment Status: Full-Time  \n Employee Status: Regular  \n Work Week: Days  \n Minimum Salary: US Dollar (USD) 58,000  \n Midpoint Salary: US Dollar (USD) 72,500  \n Maximum Salary : US Dollar (USD) 87,000  \n FLSA: non-exempt and eligible for overtime pay  \n Fund Type: Soft  \n Work Location: Remote (within Texas only)  \n Pivotal Position: No  \n Referral Bonus Available?: No  \n Relocation Assistance Available?: No  \n Science Jobs: No  \n \n  #LI-Remote",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "b425373e0e29ad76": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 50000.0,
        "salary_max": 60000.0,
        "title": "Marketing Data Analyst",
        "company": "The Media Manager",
        "desc": "Marketing Data Analyst \n Media Manager is at the forefront of the advertising industry, driven by data and transparency. As we continue to generate momentum and ROI for our clients, we're seeking a skilled Data Analyst. This role requires expertise in Excel, SQL, and Python, with an emphasis on data-driven decision-making. Beyond technical skills, we value critical thinking and clear communication. Join our team to transform ad spend effectiveness and unveil compelling stories behind the numbers. \n About the Media Manager \n Media Manager specializes in performance marketing media buying and analytics for direct-to-consumer products. Performance Marketing is a highly technical industry where return on investment is paramount to a sustained campaign. All our clients depend on our knowledge and expertise to deliver the highest ROI and the most efficient media buys. \n Why work at the Media Manager? \n At Media Manager, we\u2019re leading the way in a changing advertising environment. This begins with our people. We\u2019ve worked hard to build a company culture that supports each individual\u2019s work/life balance, their professional growth, and the community we share. \n \n Media Manager has grown 25% or more in each of the last 6 years. \n Ranked on the 2023, 2022, 2021, 2019, and 2018 Inc. 5000 list of the nation\u2019s fastest-growing private companies. \n Flexible scheduling to fit a work/life balance. Retirement, Health Insurance, Life Insurance, PTO, company outings, company provided laptop, casual work environment, quarterly bonus\u2019 and more. \n We take pride in our company culture and are looking for someone who can build on our culture. \n Work hard, play hard, win together attitude. \n \n Hard Skills: \n \n Excel: This is a fundamental tool for any data analyst. \n Pivot Tables: Ability to organize and summarize data efficiently. \n Advanced formulas and functions like VLOOKUP, INDEX/MATCH, etc. \n Data Visualization: Proficiency in creating charts and graphs. \n Power Query: For importing and transforming data. \n Power Pivot: For more advanced data modeling. \n SQL: Ability to write and optimize complex queries, understand joins, subqueries, stored procedures, etc. \n Python: Basic understanding of Python, especially libraries like: \n Pandas: For data manipulation and analysis. \n NumPy: For numerical computations. \n Statistical Analysis: Familiarity with statistical tests, distributions, and regression analysis can be valuable. \n \n Recommended Additional Skills: \n \n Data Warehousing: Basic understanding of concepts like ETL (Extract, Transform, Load) processes and tools like Apache Kafka, Talend, etc. \n Cloud Platforms: Familiarity with Google Cloud Platform, Azure, especially services related to data storage, processing, and analysis. \n APIs: Ability to fetch data from various sources using APIs. \n \n Soft Skills: \n \n Critical Thinking: Ability to approach problems logically and make sound decisions. \n Communication: Ability to convey complex data findings in simple terms to stakeholders or team members. \n Attention to Detail: Ensuring data accuracy and consistency. \n Problem-Solving: Ability to tackle unexpected challenges and think outside the box. \n Collaboration: Being a team player, working seamlessly with various departments. \n Time Management: Efficiently juggling multiple tasks or projects. \n Continuous Learning: The data field is always evolving, so a willingness to learn and adapt is crucial. \n \n Minimum Requirements \n \n Bachelor or Associates degree in Business, Analytics, Computer Science, Marketing, Advertising, or any related field. \n 2+ years of professional business experience. \n A high level of ethics, accuracy, dependability, enthusiasm, and confidentiality. \n Strong technical and organizational skills in addition to excellent written and verbal communication skills. \n This position is a remote position. \n \n Media Manager Employee Testimonial \n \u201cI love working at the Media Manager because of the people and the work we do for our clients. The Media Manager\u2019s culture is special and is the true definition of work hard, play hard. Throughout my time at Media Manager, my role has evolved, and I\u2019ve been able to grow both personally and professionally. I always feel appreciated and rewarded for the work I do. There\u2019s never a dull day in the office whether that may be our monthly event or fun chatter amongst co-workers. Media Manager is a great place to build your career.\u201d \n Job Type: Full-time \n Pay: $50,000.00 - $60,000.00 per year \n Benefits: \n \n 401(k) \n 401(k) matching \n Dental insurance \n Flexible schedule \n Health insurance \n Paid time off \n Parental leave \n Professional development assistance \n Vision insurance \n \n Schedule: \n \n Monday to Friday \n \n COVID-19 considerations: One the candidate is further along in the process , we will share our office COVID-19 Preparedness Plan. \n Education: \n \n Bachelor's (Required) \n \n Work Location: Remote",
        "cleaned_desc": " Excel: This is a fundamental tool for any data analyst. \n Pivot Tables: Ability to organize and summarize data efficiently. \n Advanced formulas and functions like VLOOKUP, INDEX/MATCH, etc. \n Data Visualization: Proficiency in creating charts and graphs. \n Power Query: For importing and transforming data. \n Power Pivot: For more advanced data modeling. \n SQL: Ability to write and optimize complex queries, understand joins, subqueries, stored procedures, etc. \n Python: Basic understanding of Python, especially libraries like: \n Pandas: For data manipulation and analysis. \n NumPy: For numerical computations. \n Statistical Analysis: Familiarity with statistical tests, distributions, and regression analysis can be valuable. \n \n Recommended Additional Skills: \n \n Data Warehousing: Basic understanding of concepts like ETL (Extract, Transform, Load) processes and tools like Apache Kafka, Talend, etc.   Cloud Platforms: Familiarity with Google Cloud Platform, Azure, especially services related to data storage, processing, and analysis. \n APIs: Ability to fetch data from various sources using APIs. \n \n Soft Skills: \n \n Critical Thinking: Ability to approach problems logically and make sound decisions. \n Communication: Ability to convey complex data findings in simple terms to stakeholders or team members. \n Attention to Detail: Ensuring data accuracy and consistency. \n Problem-Solving: Ability to tackle unexpected challenges and think outside the box. \n Collaboration: Being a team player, working seamlessly with various departments. \n Time Management: Efficiently juggling multiple tasks or projects. \n Continuous Learning: The data field is always evolving, so a willingness to learn and adapt is crucial. \n \n Minimum Requirements \n ",
        "techs": [
            "excel",
            "pivot tables",
            "advanced formulas",
            "vlookup",
            "index/match",
            "data visualization",
            "power query",
            "power pivot",
            "sql",
            "python",
            "pandas",
            "numpy",
            "statistical analysis",
            "data warehousing",
            "etl",
            "apache kafka",
            "talend",
            "cloud platforms",
            "google cloud platform",
            "azure",
            "apis",
            "critical thinking",
            "communication",
            "attention to detail",
            "problem-solving",
            "collaboration",
            "time management",
            "continuous learning"
        ],
        "cleaned_techs": [
            "excel",
            "pivot tables",
            "advanced formulas",
            "vlookup",
            "index/match",
            "data visualization",
            "power query",
            "power pivot",
            "sql",
            "python",
            "pandas",
            "numpy",
            "statistical analysis",
            "data warehousing",
            "etl",
            "apache kafka",
            "talend",
            "cloud platforms",
            "gcp",
            "azure",
            "apis",
            "critical thinking",
            "communication",
            "attention to detail",
            "problem-solving",
            "collaboration",
            "time management",
            "continuous learning"
        ]
    },
    "ba58f09380489de9": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 78897.63,
        "salary_max": 99901.99,
        "title": "HRIS Business Analyst II",
        "company": "Yale New Haven Health",
        "desc": "Provide client support services on Manager Self Service and Employee Self Service applications. Develops training materials, user documentation and conduct training sessions for functional users. Develops training materials in conjunction with systems analysts for Employee Self Service users. Assists in the testing maintenance of code table changes. Assists in testing changes and maintaining Infor/Lawson HR code tables. Assists in reviewing and processing security requests for HR system applications. Tests and validates all HR Infor security set up. Maintains and updates HR codes and tables in the HR system, ensuring data integrity. In collaboration with the Manager, sets up new leave plans, modifying existing plans and documenting methodology for exceptions. Coordinates all Position Budget Manager activities such as, but not limited to, troubleshooting FTE conversions, budget restrict error messages. \n EEO/AA/Disability/Veteran \n Qualifications \n EDUCATION:  Bachelor's degree in Human Resources or related discipline or training and work related experience or equivalent. \n EXPERIENCE:  Five (5) to seven (7) years of experience with analysis activities in Human Resources, with at least three (3) years of experience coding HR systems required. \n SPECIAL SKILLS:  In-depth knowledge of Infor/Lawson or comparable HR/Payroll system set up. Understanding of impact of shared systems. Ability to develop training materials and conduct training sessions. Working knowledge in MS Access, SQL or TOAD to build queries for research and issue resolution. Excellent interpersonal and communications skills required. \n Responsibilities: \n \n 1. Provides client support services on Manager Self Service and Employee Self Service applications. Researches and solves issues with Managers experiencing Manager Self Services problems or HR applications issues on Employee Self Service. Responds to Helpdesk tickets and HRConnect cases. \n 2. In collaboration with the Manager, sets up new leave plans, modifying existing plans and documenting methodology for exceptions. Ensures thorough testing and implementation of new and revised plans. Works closely with the HRIS technical team, Benefits, ITS and Payroll teams to ensure plans are set up correctly \n 3. Maintains and updates HR codes and tables in the HR System, ensuring data integrity. \n 4. Responsible for coordinating the testing of HR systems upgrades, patches and issues with all HR users. Coordinates the documentation of test results. Develops test scripts for all HR system users. Maintains Infor/Lawson HR code table changes, as needed. \n 5. Reviews, coordinates and processes end user security access requests for all HR Systems related features in Infor GHR, BI Reports, Position Control reports and Crystal Reports. Assists in processing security access requests for HR/Payroll applications. \n 6. Completes transactions that cannot be processed through Manager Space such as executive new hires, organizational transfers and transactions with appropriate documentation. Provides back-up to other HRIS staff as need arises, including running critical jobs. Assists functional users in researching HR system issues, as they arise. Validates automated jobs for completeness and corrects errors, as needed. \n 7. Researches HR system issues with Manager Space, Employee Space and ESS-HRIS applications, and other HR systems, as they arise. Works with the local functional users to test solutions and documents changes, as needed. \n 8. Works in conjunction with HR functional areas and the Infor Core team to define system change requests and report requests. \n 9. Provides support to all levels of management on Position Budget Manager issues. Works in collaboration with Talent Acquisition and the Budget departments to ensure all facets of Position Budget are considered when resolving an issue. \n 10. Develops training materials and conducts training sessions on enhancements to the HR system for Infor HR clients and HR functional areas. \n 11. Assists in training of HRIS business staff. Assesses current business processes to identify improvement opportunities, makes recommendations for improvements and implements approved enhancements. \n 12. In conjunction with Corporate HRIS systems analysts, develops training materials and user documentation for HRIS web applications. Tests new HR applications as they are developed. \n 13. Assists departments with quick HRDB report requests as needed. \n 14. Provides consultation to clients on procedures and processes affecting HR network and applications. \n 15. Keeps abreast of recent developments in the Information Systems field as related to HR products. Participates in professional development programs. Participates in the evaluation and recommendation of HR software products. \n 16. Performs other job related duties as required. \n \n Job Type: Full-time \n Benefits: \n \n 401(k) matching \n Dental insurance \n Disability insurance \n Family leave \n Health insurance \n Paid time off \n Parental leave \n Retirement plan \n Tuition reimbursement \n Vision insurance \n \n Experience level: \n \n 5 years \n \n Schedule: \n \n Day shift \n Monday to Friday \n \n Work Location: Remote",
        "cleaned_desc": "Provide client support services on Manager Self Service and Employee Self Service applications. Develops training materials, user documentation and conduct training sessions for functional users. Develops training materials in conjunction with systems analysts for Employee Self Service users. Assists in the testing maintenance of code table changes. Assists in testing changes and maintaining Infor/Lawson HR code tables. Assists in reviewing and processing security requests for HR system applications. Tests and validates all HR Infor security set up. Maintains and updates HR codes and tables in the HR system, ensuring data integrity. In collaboration with the Manager, sets up new leave plans, modifying existing plans and documenting methodology for exceptions. Coordinates all Position Budget Manager activities such as, but not limited to, troubleshooting FTE conversions, budget restrict error messages. \n EEO/AA/Disability/Veteran \n Qualifications \n EDUCATION:  Bachelor's degree in Human Resources or related discipline or training and work related experience or equivalent. \n EXPERIENCE:  Five (5) to seven (7) years of experience with analysis activities in Human Resources, with at least three (3) years of experience coding HR systems required. \n SPECIAL SKILLS:  In-depth knowledge of Infor/Lawson or comparable HR/Payroll system set up. Understanding of impact of shared systems. Ability to develop training materials and conduct training sessions. Working knowledge in MS Access, SQL or TOAD to build queries for research and issue resolution. Excellent interpersonal and communications skills required. \n Responsibilities: \n \n 1. Provides client support services on Manager Self Service and Employee Self Service applications. Researches and solves issues with Managers experiencing Manager Self Services problems or HR applications issues on Employee Self Service. Responds to Helpdesk tickets and HRConnect cases. ",
        "techs": [
            "manager self service",
            "employee self service",
            "infor/lawson hr",
            "ms access",
            "sql",
            "toad"
        ],
        "cleaned_techs": [
            "manager self service",
            "employee self service",
            "infor/lawson hr",
            "ms access",
            "sql",
            "toad"
        ]
    },
    "0ddd20ce62ab77c7": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 30.0,
        "salary_max": 30.0,
        "title": "Middle Level Business Analyst",
        "company": "Innovative Computer Solutions Group, Inc",
        "desc": "Business Analyst (Mid)  Experience in leading large development projects from a requirements/business perspective. Familiar with a range of digital/web services and solutions, ideally where open source and cloud technologies and agile development methodologies have been applied. Must possess an eye for detail, excellent communication skills, ability to rationalize complex information to make it understandable for others to work, and ability to assess and report information where inconsistencies are found.\n  \n  Minimums\n  \n \n \n BA/BS in Computer Science \n 6 years professional IT experience with agile methodologies (Scrum) \n 2 years of mid-level IT experience \n Strong and effective communication skills \n Understands the Software Development Life Cycle (SDLC) \n Use analytical skills to uncover an issue\u2019s root causes \n Test Driven Development (TDD) and unit testing \n \n \n CA Agile Central \n Subversion (SVN) \n Jenkins \n SolarQube \n Selenium \n Postman \n Jmeter \n \n  Responsibilities:\n  \n \n \n Work within an Agile team and DevSecOps culture \n Conduct business cost/benefit analyses to align business systems, solutions and initiatives \n Analyze propositions and decision-making factors: strategic alignment, cost/benefit, and risk \n Define skill requirements and map internal, agency, and external (partners/contractors) resources \n Ensure client has the budget and resources to execute the proposed approach during delivery \n Analyze what resources/provisions the client has for ongoing running costs \n Analyze and map the risks of product approach and propose mitigation solutions \n Define how the expected user and cost benefits can be realized \n Devise how performance will be measured \n Maximize user/customer satisfaction \n Create customer-centric solutions with the end in mind \n Think and act outside of comfort-zone and maximize team members\u2019 talent to deliver results \n Communicate credibly with a wide range of IT delivery disciplines and talent \n Develop creative ideas to solve complex problems \n Proactively identify and address risks \n Encourage communication, collaboration, and knowledge sharing between people and teams \n \n \n  This is a remote position.",
        "cleaned_desc": "Business Analyst (Mid)  Experience in leading large development projects from a requirements/business perspective. Familiar with a range of digital/web services and solutions, ideally where open source and cloud technologies and agile development methodologies have been applied. Must possess an eye for detail, excellent communication skills, ability to rationalize complex information to make it understandable for others to work, and ability to assess and report information where inconsistencies are found.\n  \n  Minimums\n  \n \n \n BA/BS in Computer Science \n 6 years professional IT experience with agile methodologies (Scrum) \n 2 years of mid-level IT experience ",
        "techs": [
            "business analyst",
            "open source technologies",
            "cloud technologies",
            "agile development methodologies",
            "scrum",
            "computer science"
        ],
        "cleaned_techs": [
            "open source technologies",
            "cloud technologies",
            "agile development methodologies",
            "scrum",
            "computer science"
        ]
    },
    "5ee93fca18ffa690": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 45.0,
        "salary_max": 45.0,
        "title": "Senior Business Analyst",
        "company": "Innovative Computer Solutions Group, Inc",
        "desc": "Business Analyst (Senior) \n 100% Remote Position \u2013 Candidate must reside in the United States \n \n  Experience in leading large development projects from a requirements/business perspective. Familiar with a range of digital/web services and solutions, ideally where open source and cloud technologies and agile development methodologies have been applied. Must possess an eye for detail, excellent communication skills, ability to rationalize complex information to make it understandable for others to work, and ability to assess and report information where inconsistencies are found.\n  \n  Minimums\n  \n \n \n BA/BS in Computer Science \n 10 years professional IT experience with agile methodologies (Scrum) \n 4 years of senior-level lead IT experience \n Validate requirements \n Refine estimates \n Capable of communicating directly with our end customers \n Understands the Software Development Life Cycle (SDLC) \n Quick and effective troubleshooting skills to uncover an issue\u2019s root causes \n Test Driven Development (TDD) and unit testing \n \n \n CA Agile Central \n Subversion (SVN) \n Jenkins \n SolarQube \n Selenium scripting \n Postman \n Jmeter \n \n  Responsibilities:\n  \n \n \n Work within an Agile team and DevSecOps culture \n Conduct business cost/benefit analyses to align business systems, solutions and initiatives \n Analyze propositions and decision-making factors: strategic alignment, cost/benefit, and risk \n Define skill requirements and map internal, agency, and external (partners/contractors) resources \n Ensure client has the budget and resources to execute the proposed approach during delivery \n Analyze what resources/provisions the client has for ongoing running costs \n Analyze and map the risks of product approach and propose mitigation solutions \n Define how the expected user and cost benefits can be realized \n Devise how performance will be measured \n Maximize user/customer satisfaction \n Create customer-centric solutions with the end in mind \n Think and act outside of comfort-zone and maximize team members\u2019 talent to deliver results \n Communicate credibly with a wide range of IT delivery disciplines and talent \n Develop creative ideas to solve complex problems \n Proactively identify and address risks \n Encourage communication, collaboration, and knowledge sharing between people and teams \n \n \n  This is a remote position.",
        "cleaned_desc": "Business Analyst (Senior) \n 100% Remote Position \u2013 Candidate must reside in the United States \n \n  Experience in leading large development projects from a requirements/business perspective. Familiar with a range of digital/web services and solutions, ideally where open source and cloud technologies and agile development methodologies have been applied. Must possess an eye for detail, excellent communication skills, ability to rationalize complex information to make it understandable for others to work, and ability to assess and report information where inconsistencies are found.\n  \n  Minimums\n  \n \n \n BA/BS in Computer Science ",
        "techs": [
            "none"
        ],
        "cleaned_techs": []
    },
    "dc9d05061e87f12d": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 90386.305,
        "salary_max": 114449.21,
        "title": "Data Analyst - Product Analytics - UK or Portugal",
        "company": "Intermedia.net, Inc.",
        "desc": "Are you looking for a company where  YOUR VOICE  is heard? Where you can  MAKE A DIFFERENCE ? Do you  THRIVE  in a  FAST-PACED  work environment? Do you wake every morning  EXCITED  to work with  GREAT   PEOPLE  and create  SUCCESS   TOGETHER ? Then Intermedia is the place for you. \n \n Intermedia has established itself as a leading provider of cloud communications and collaboration tech that allows companies to connect better. We have a strong track record of growth, profitability, and creating an environment where everyone matters. Everyone. While we are fast-paced and admittedly a bit intense, we promise that you won\u2019t be bored. You will find Intermedia is a place where you can indulge your passion for creating and supporting great cloud technology. What\u2019s more, we always look to promote from within and have many employees who have been with us 10, 15, and 20+ years! \n \n Culture at Intermedia is built on teamwork and transparency. We hold each other accountable and always have each other\u2019s back! \n \n About the role : \n \n This is a new role for the Product Analytics team. The Analytics Products currently collects raw data from Intermedia\u2019s products and processes that data to generate reports showing how our products are used by customers. These reports are then made available for customers to view. \n \n RESPONSIBILITIES: \n \n Build dashboards and reports that allow customers to view and explore product analytics \n Deeply understand customer problems and address solutions according to the business requirements \n Work with the Analytics Product Manager to define dashboards and reports that meet customer needs, with the correct metrics and KPIs \n Build business-centric KPIs from raw data \n Build data models that support the dashboards and reports \n Work with development teams from other products to understand their data structures \n Work with the Development team to ensure the correct raw data is available for the dashboards and identify any gaps \n Work with the UX designers on a product to ensure the dashboards and reports comply with branding guidelines and integrate with the product consistently \n Participate in internal reviews within UX/UI team \n Conduct customer research, UI testing \n \n \n REQUIREMENTS: \n \n 3+ years of experience as a data analyst in a product analytics role \n Strong technical aptitude, analysis skills and problem-solving skills \n Good knowledge of usability, UX/UI and human factors principles \n Experience of databases such as ElasticSearch, SQL \n Ability to learn quickly, solve problems, multi-task and work exceptionally well with diverse co-workers across all functional areas \n Excellent organizational skills with the ability to manage multiple priorities and deadlines \n Experience working with Scrum or similar Agile frameworks \n Work in a large team, listen to feedback from users and colleagues \n Working knowledge of the Software Development Process \n Bachelor\u2019s degree preferably with emphasis on Computer Science or related field; or equivalent, relevant work experience \n Initiative and leadership \n English: Upper-Intermediate and higher \n \n \n WILL BE A PLUS: \n \n Experience with data visualizations tools, particularly MicroStrategy or equivalent BI tools such as Tableau or PowerBI \n Experience with Figma \n Knowledge of the basics of accessibility \n Knowledge of Unified Communications Services \n Knowledge of Contact Centre operations \n \n \n Diversity, Inclusion, and Equal Opportunity \n We hire, promote, and compensate employees based on their ability to perform their job responsibilities, without regard to race, color, creed, religion, sex, gender, marital status, national origin, ancestry, age, citizenship, physical or mental disability, sexual orientation, or any other basis protected by applicable law (collectively referred to in our Code of Conduct as \u201cProtected Classes\u201d). We do not tolerate employment discrimination in the workplace, and we are committed to making reasonable accommodations for identified disabilities or other limitations as required by all applicable laws. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status",
        "cleaned_desc": " Participate in internal reviews within UX/UI team \n Conduct customer research, UI testing \n \n \n REQUIREMENTS: \n \n 3+ years of experience as a data analyst in a product analytics role \n Strong technical aptitude, analysis skills and problem-solving skills \n Good knowledge of usability, UX/UI and human factors principles \n Experience of databases such as ElasticSearch, SQL   Ability to learn quickly, solve problems, multi-task and work exceptionally well with diverse co-workers across all functional areas \n Excellent organizational skills with the ability to manage multiple priorities and deadlines \n Experience working with Scrum or similar Agile frameworks \n Work in a large team, listen to feedback from users and colleagues \n Working knowledge of the Software Development Process \n Bachelor\u2019s degree preferably with emphasis on Computer Science or related field; or equivalent, relevant work experience \n Initiative and leadership \n English: Upper-Intermediate and higher \n \n ",
        "techs": [
            "elasticsearch",
            "sql"
        ],
        "cleaned_techs": [
            "elasticsearch",
            "sql"
        ]
    },
    "6787b8c35045cf0e": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 84201.695,
        "salary_max": 106618.1,
        "title": "Sr Business Analyst",
        "company": "Exelon Corporation",
        "desc": "Description   We're powering a cleaner, brighter future.\n  \n  Exelon is leading the energy transformation, and we're calling all problem solvers, innovators, community builders and change makers. Work with us to deliver solutions that make our diverse cities and communities stronger, healthier and more resilient.\n  \n  We're powered by purpose-driven people like you who believe in being inclusive and creative, and value safety, innovation, integrity and community service. We are a Fortune 200 company, 19,000 colleagues strong serving more than 10 million customers at six energy companies - Atlantic City Electric (ACE), Baltimore Gas and Electric (BGE), Commonwealth Edison (ComEd), Delmarva Power & Light (DPL), PECO Energy Company (PECO), and Potomac Electric Power Company (Pepco).\n  \n  In our relentless pursuit of excellence, we elevate diverse voices, fresh perspectives and bold thinking. And since we know transforming the future of energy is hard work, we provide competitive compensation, incentives, excellent benefits and the opportunity to build a rewarding career.\n  \n  Are you in?\n  \n \n PRIMARY PURPOSE OF POSITION \n  Role is considered to be the Career Level. Perform detailed analysis and prepare comprehensive reports to support business operations. Conduct complex modeling, forecasting, trending, variance analysis, business case development, and other general financial and operational analysis. Monitor and coordinate activities to manage operational performance (e.g., safety, reliability, service level, productivity, etc.). Research and provide statistical financial and operational information. Track, maintain, analyze and provide current information on the operation's business scorecard or key performance indicators (KPIs). Develop, organize and present performance reports for both internal and external use. Develop and conduct short-term and long-range business planning. May develop, implement, evaluate or lead projects or initiatives (e.g., business plan or process improvement initiatives). Provide broad analytical and operational support to the business unit. Position may be required to work extended hours for coverage during storms or other energy delivery emergencies.   \n \n PRIMARY DUTIES AND ACCOUNTABILITIES \n \n  Develop and conduct short-term and long-range business planning. Conduct complex modeling, forecasting, trending, variance analysis, business case development, and other financial and operational analysis (e.g., operational effectiveness, budget, resource, workload, workforce and capacity utilization, competitor, etc.). \n  Provide statistical information to ensure the most effective utilization of personnel, equipment and/or materials for business operations. \n  Monitor and coordinate activities to manage operational performance (e.g., safety, reliability, service level, productivity, etc.) and make recommendations for improvement. \n  Track, maintain, analyze and provide current information on the operation's business scorecard or key performance indicators (KPIs). \n  Perform analysis of the operation's cost elements to ensure effective and efficient operations. \n  Prepare and analyze comprehensive and detailed reports. \n  Develop, organize and present detailed performance reports for both internal and external use. \n  May develop, implement, evaluate or lead projects or initiatives (e.g., business plan or process improvement initiatives). \n  Provide broad analytical and operational support to the business unit. \n \n \n  JOB SCOPE \n \n  This is an individual contributor position that works independently with only general direction, relying on knowledge skills and judgment acquired through education and relevant experience to plan and accomplish assigned tasks and goals. The position regularly mentors less experienced colleagues.  \n Position requires routine interaction with key internal and external stakeholders, executives, directors, managers and staff across the organization.  \n May require working extended hours and/or shift work to support business operations.  \n \n Qualifications   \n MINIMUM QUALIFICATIONS \n \n  Bachelor's degree in Business or Engineering AND 4-7 years of related business experience, or in lieu of degree, 6-9 years of relevant business experience is required. \n  Comprehensive understanding of applicable standards, methods, processes and practices, business fundamentals, and performance metrics in the specific functional area supported by this position (e.g., asset management, electric or gas operations, customer service, transmission and substation, distribution system operations, regulatory and external affairs, transmission operations and planning). \n  Advanced proficiency in standard software applications (e.g., Microsoft Word, Excel, Access, PowerPoint, Project), specialized business technologies and applications (e.g., workforce management, project management, PassPort, Brio, CIS, EPS, Business Objects, SAP planning, business modeling, forecasting, voice response unit, automatic call distributors, work management, outage management) to retrieve and analyze data in support of business needs. \n  Ability to provide in-depth analysis and apply managerial accounting concepts, customer intelligence gathering, forecasting, staff planning, scheduling, analytical and statistical problem solving, financial, accounting, business and budget analysis. \n  Demonstrated strong analytical skills for project evaluation including analysis of complex projects with economic, financial, risk and decision analysis. Proficient in business case development and ability to effectively present business cases to business unit leadership. \n  Customer driven with good oral and written communications skills and strong analytical, problem solving and project management skills. \n  Comprehensive knowledge of the practices, procedures and principles of performance analysis (trending, root cause and gap analysis) benchmarking and audit compliance. Ability to analyze organizational data and complex problems, interpret and recommend alternative courses of action, and implement intervention strategies to attain performance targets.  \n \n \n PREFERRED QUALIFICATIONS \n  MBA or relevant advanced degree.",
        "cleaned_desc": "  Bachelor's degree in Business or Engineering AND 4-7 years of related business experience, or in lieu of degree, 6-9 years of relevant business experience is required. \n  Comprehensive understanding of applicable standards, methods, processes and practices, business fundamentals, and performance metrics in the specific functional area supported by this position (e.g., asset management, electric or gas operations, customer service, transmission and substation, distribution system operations, regulatory and external affairs, transmission operations and planning). \n  Advanced proficiency in standard software applications (e.g., Microsoft Word, Excel, Access, PowerPoint, Project), specialized business technologies and applications (e.g., workforce management, project management, PassPort, Brio, CIS, EPS, Business Objects, SAP planning, business modeling, forecasting, voice response unit, automatic call distributors, work management, outage management) to retrieve and analyze data in support of business needs. \n  Ability to provide in-depth analysis and apply managerial accounting concepts, customer intelligence gathering, forecasting, staff planning, scheduling, analytical and statistical problem solving, financial, accounting, business and budget analysis. \n  Demonstrated strong analytical skills for project evaluation including analysis of complex projects with economic, financial, risk and decision analysis. Proficient in business case development and ability to effectively present business cases to business unit leadership. \n  Customer driven with good oral and written communications skills and strong analytical, problem solving and project management skills. \n  Comprehensive knowledge of the practices, procedures and principles of performance analysis (trending, root cause and gap analysis) benchmarking and audit compliance. Ability to analyze organizational data and complex problems, interpret and recommend alternative courses of action, and implement intervention strategies to attain performance targets.  \n \n ",
        "techs": [
            "microsoft word",
            "excel",
            "access",
            "powerpoint",
            "project",
            "workforce management",
            "project management",
            "passport",
            "brio",
            "cis",
            "eps",
            "business objects",
            "sap planning",
            "business modeling",
            "forecasting",
            "voice response unit",
            "automatic call distributors",
            "work management",
            "outage management"
        ],
        "cleaned_techs": [
            "microsoft",
            "excel",
            "access",
            "powerpoint",
            "project",
            "workforce management",
            "project management",
            "passport",
            "brio",
            "cis",
            "eps",
            "business objects",
            "sap planning",
            "business modeling",
            "forecasting",
            "voice response unit",
            "automatic call distributors",
            "work management",
            "outage management"
        ]
    },
    "8d51cdd671cde302": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 95000.0,
        "salary_max": 120000.0,
        "title": "Business Intelligence (BI) Analyst",
        "company": "Blue Circle Health Inc",
        "desc": "Description:\n  \n  The BI Analyst is a critical part of building a learning healthcare infrastructure at Blue Circle Health to improve how we serve people living with type 1 diabetes in need. This role will be designing and implementing Tableau Dashboards that allow individuals, teams, and the organization to monitor and assess the delivery of care to patients. \n  This is a full-time, home-based position, eligible for a full benefits package including paid holidays and time off, and competitive pay in a flexible, mission-based work environment. Candidates can live anywhere in the U.S. \n  Responsibilities & Capabilities \n \n  Develop and maintain data visualizations, reports, and dashboards using business intelligence tools such as Tableau or similar platforms. \n  Work with users and leadership to iteratively design, prototype, and build BI solutions, perform analyses, and ensure dashboards provide necessary interaction for teams to understand their data. \n  Conduct data analysis using various tools and techniques to identify trends, patterns, and insights that can drive strategic decision-making and improvements to operations. \n  Effectively communicate summaries, findings, insights, and recommendations to stakeholders through presentations, reports, and other mediums. \n  Identify opportunities for process improvements and operational efficiencies. \n  Support management team in effective use of reporting dashboards and support the training of their teams to utilize them. \n  Perform data quality assessments, ensuring that data is accurate, complete, and relevant for analysis purposes. \n  Collaborate with engineering teams to enhance data infrastructure, data governance, and data integration processes.. \n  Stay up-to-date with industry trends and best practices in business intelligence, data analytics, and data visualization techniques. \n  Ensure compliance with data privacy regulations and maintain confidentiality and security of sensitive and protected health information. \n  Requirements:\n  \n  Expected Experience \n \n  4+ years of experience as a Business Intelligence Analyst preferred. \n  Experience engaging directly with users to understand their workflows and their reporting objectives. \n  Experience working directly with healthcare users (clinicians and other care team members) is strongly preferred. \n  Experience publishing and maintaining dashboards in Tableau or other business intelligence platforms. \n  Production experience using SQL to query multidimensional data sets, data warehouses, and databases. \n  Proficiency managing and supporting users with Google Sheets or Excel for one off analyses and sharing with external stakeholders. \n  Previous analysis and reporting work in a healthcare setting. \n  Experience working in a fully remote environment across many time zones. \n \n  Additional Desired Experience \n \n  Data warehouse management - creating and maintaining user tables and views using SQL, documenting with ERDs and other diagrams. \n  Experience using AWS, in particular Amazon Redshift. \n  Advanced data analytics and machine learning such as using pandas, map-reduce, Apache Spark / Snowflake, AWS SageMaker, etc \n  Experience working with CSVs, JSON, and other data formats.",
        "cleaned_desc": "  Develop and maintain data visualizations, reports, and dashboards using business intelligence tools such as Tableau or similar platforms. \n  Work with users and leadership to iteratively design, prototype, and build BI solutions, perform analyses, and ensure dashboards provide necessary interaction for teams to understand their data. \n  Conduct data analysis using various tools and techniques to identify trends, patterns, and insights that can drive strategic decision-making and improvements to operations. \n  Effectively communicate summaries, findings, insights, and recommendations to stakeholders through presentations, reports, and other mediums. \n  Identify opportunities for process improvements and operational efficiencies. \n  Support management team in effective use of reporting dashboards and support the training of their teams to utilize them.    Perform data quality assessments, ensuring that data is accurate, complete, and relevant for analysis purposes. \n  Collaborate with engineering teams to enhance data infrastructure, data governance, and data integration processes.. \n  Stay up-to-date with industry trends and best practices in business intelligence, data analytics, and data visualization techniques. \n  Ensure compliance with data privacy regulations and maintain confidentiality and security of sensitive and protected health information. \n  Requirements:\n     Production experience using SQL to query multidimensional data sets, data warehouses, and databases. \n  Proficiency managing and supporting users with Google Sheets or Excel for one off analyses and sharing with external stakeholders. \n  Previous analysis and reporting work in a healthcare setting. \n  Experience working in a fully remote environment across many time zones. \n \n  Additional Desired Experience ",
        "techs": [
            "tableau",
            "sql",
            "google sheets",
            "excel"
        ],
        "cleaned_techs": [
            "tableau",
            "sql",
            "google sheets",
            "excel"
        ]
    },
    "a10025c256cee5fe": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 84055.32,
        "salary_max": 106432.77,
        "title": "Data Quality Analyst",
        "company": "Trianz",
        "desc": "About Trianz \n  Trianz is a digital transformation technology and services firm that helps clients transform their value proposition and business value chains. We deliver end-to-end digital transformation services from strategy and roadmaps to experience design, technology implementations, and managed services. With a multi-disciplinary, collaborative model, we help clients understand their competitive digital maturity, develop effective strategies, and transform using our IP-led platforms and technology services. We lead with data and IP that accelerates digital transformations, optimizes investments, and delivers measurable results. We have been rated #1 for 'transformational impact' and 'predictability of execution' by a Fortune 1000 client base- 5 years straight. \n  At Trianz, we offer you an open and learning-oriented culture essential to emerge as a leader. Completely focused on the Digital Evolution philosophy and phenomenon, we view delivering our value proposition consistently as a non-negotiable commitment. Our enablers include Intelligent Team Formations, a Client-Centric Approach, Predictability in Execution, and establishing a Unique Relationship Experience. A culture of innovation, encouraging our people to create, and belief in the importance of training and development set us apart.     Role:  Data Quality   Analyst   Employment Type:  Contract   for   6  months (can be extended depending on the scope of the  project)   Location:   Remote     End Client: Mass Mutual \n  Trianz is looking for a Data Quality Analyst. This role is a key member of the Investment Delivery Services (IDS) team responsible for delivering and maintaining automated data management controls supporting the investment data ecosystem. This role will utilize industry leading methodologies to document, test and deploy data management controls to the investment data stakeholders.  \n About the Role \n \n The data management analyst will work closely with investment data stakeholders including Investment Accounting, Investment Operations, Investment Reporting, Treasury, and IT. \n The goal of this position is to deliver automated data management controls to the various teams.  \n \n Job Responsibilities \n \n The analyst role will be responsible for documenting requirements, ensuring proper software development methodologies are followed, testing of developed products and helping with production support. \n Schedule and lead requirement gathering sessions that align stakeholder needs with application features/capabilities. \n Assist with design and documenting automated data management controls in Infogix/Precisely. \n Work with Infogix/Precisely development team to ensure requirements / design is met. \n Assist QA resources with Test Plan and Script details. \n Assisting with production support and issue resolution. \n Successfully deliver and maintain automated data management controls using Precisely Software.  \n \n Required Skills \n \n Bachelor\u2019s degree in finance, Business Administration, Information Management or comparable work experience. \n Ability to work in close collaboration with project/program managers, architects, developers, and testers to define and implement standards and best practices. \n Skilled in interacting between internal business partners, internal IT teams, and offshore partners. \n Ability to communicate clearly and to simplify complex topics for a wide range of audiences (both written and verbal). \n Ability to work through complex issues, identify themes, and develop solutions, in time-critical situations. \n Ability to quickly learn new technologies and platforms. \n Ability to adhere to project schedules and meet deadlines in the execution of job responsibilities. \n Critical thinking, problem solving, documenting requirements. \n Familiarity / understanding of Investment Products, Investment Back Office Operations, or Accounting. \n Experienced with SDLC and AGILE methodologies. \n Experienced with eliciting and documenting requirements and user stories. \n Experienced with process improvement. \n Self-starter with excellent time management. \n \n Trianz is growing at a faster pace than the industry for the last five years. Read through some of the key industry recognitions we have received for our innovative execution and strategic client initiatives  here . \n    Equal Employment Opportunity \n  Trianz is an Equal Opportunity Employer and does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law).",
        "cleaned_desc": " Critical thinking, problem solving, documenting requirements. \n Familiarity / understanding of Investment Products, Investment Back Office Operations, or Accounting. \n Experienced with SDLC and AGILE methodologies. \n Experienced with eliciting and documenting requirements and user stories. \n Experienced with process improvement. \n Self-starter with excellent time management. \n ",
        "techs": [
            "none"
        ],
        "cleaned_techs": []
    },
    "213eb16e850b8781": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 79007.16,
        "salary_max": 100040.66,
        "title": "Business Analyst",
        "company": "Acuity INC",
        "desc": "Overview: \n  \n   Acuity, Inc. has an opening for a Business Analyst to support a very visible government client. Are you a dynamic and creative individual who enjoys working in a collaborative environment for the development of sophisticated applications? Come check out Acuity!\n  \n \n \n  In this role, you will serve as the Business Analyst and serve as the voice of the customer and proactively represent stakeholders. Through the creation, prioritization, management and execution of the product backlog, the Business Analyst collaborates cross-functionally and focuses the development team on a clear vision of the product, its goals and delivery. The Business Analyst collaborates with the Product Owner, who has deep customer knowledge, an excellent ability to transform strategic intent into fully elaborated requirements in the form of user stories, feature specifications and acceptance criteria. The Business Analyst may also conducts some work as a Product Owner and drives and articulates product value delivery through the agile ceremonies including sprint planning, backlog refinement, daily scrum, demo and retrospectives seeking improvement through every Sprint.\n   Responsibilities: \n  \n   The Business Analyst is responsible for work directly with the Release Train Engineer and maintaining effective communications with the Scrum Master, development teams, and business units. The Business Analyst, is also responsible for gathering, analyzing, and documenting project technical and non-technical requirements via user stories; as well as developing Program Increment Plans, Roadmaps, Program Vision, and Content Presentations.\n  \n \n  Qualifications: \n  \n  Education/Skills Requirements: \n \n \n  Bachelor\u2019s degree required. \n  Experience as a business analyst, financial analyst or functional analyst is a plus. \n  Knowledge or understanding of AGILE LESS or SCALE. \n  A benefit would be having a working knowledge of SAFe methodology. \n  Preference to having experience working closely with developers and testers to ensure requirements and functional designs are translated accurately into working technical designs and that test plans and scripts serve customer needs and business scenarios. \n  Experience managing backlog refinements and organizing meetings by defining, grooming, and prioritizing user stories with Acceptance Criteria (AC) or \u201cdefinition of done\u201d (DOD). \n \n \n  Required Clearance: \n \n \n  Must be US Citizen and have the ability to obtain and maintain a federal government level clearance. \n  Must pass High Risk, Law Enforcement Sensitive background check. \n \n \n \n  About Acuity: \n \n \n   Acuity Inc. is a leading management and technology consulting firm that specializes in serving the federal government. Our innovative, collaborative and rewarding work environment has earned repeat honors from the Washington Business Journal\u2019s Best Places to Work and SmartCEO Corporate Culture awards.\n  \n \n \n  We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, disability status, protected veteran status or any other characteristic protected by law. \n \n \n \n  #CB\n  \n \n   #LI-PA1\n  \n \n   #LI-HYBRID",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "51e67bdce8f6d151": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 67159.19,
        "salary_max": 85038.5,
        "title": "Business Analyst",
        "company": "EMERGENT METHOD",
        "desc": "Emergent Talent, a division of Emergent Method, is seeking qualified candidates to fill an analyst position on a large-scale IT system implementation project. The selected individual will work with project sprint teams to support testing and validation processes for system changes and enhancements.  \n This is a full-time position that will support ongoing project operations in Baton Rouge, Louisiana.  Remote candidates will be considered and are encouraged to apply.   \n The ideal candidate is flexible, able to work in a fast-paced and rapidly changing environment, confident handling a wide range of tasks and utilizing problem-solving skills, and willing to embrace the challenges associated with complex system implementation projects. Additionally, candidates should have relevant technical backgrounds and experience with exceptional interpersonal and communication skills.  \n Key responsibilities include:  \n \n Working directly with user acceptance testing and quality assurance teams to understand the development and impact of new system functionality in an Agile environment \n  Troubleshooting and resolving issues, working cross-team to develop and implement solutions, and providing technical support and consultation \n  Investigating root causes of client and system issues and proposing effective solutions \n  Defining and planning testing strategies for the functionality that will be delivered during individual sprints \n  Conducting data set-up, mock file creation, and batch execution for test validation activities \n  Supporting the execution of scenarios for test cases including functional, regression, and smoke tests \n  Interpreting, creating, and maintaining technical documentation \n  Collaborating effectively with project team members \n  Providing regular support throughout the day for testing, as well as consistent status updates \n  Utilizing strong communication skills to present and communicate data, trends, insights, and recommendations \n \n \n  QUALIFICATIONS \n \n  Ideal candidates should possess the following knowledge, skills, and abilities:  \n \n A bachelor\u2019s degree in information sciences, computer science, management information systems, or a related technical field is preferred (experience that demonstrates commensurate experience will be considered in lieu of a degree) \n  Strong technical skills including SQL, understanding of databases, and web-based applications  \n Experience working in the system development life cycle; working in an Agile environment considered a plus \n  Ability to transform technical information into functional questions/solutions  \n Experience with Jira or similar tools \n  Strong analytical, problem solving, and interpersonal skills \n  Quick learner with the ability to pivot swiftly based on changing direction \n  Ability to multitask and juggle multiple requests and competing priorities \n  Self-motivated and deadline-oriented to ensure tasks are completed without supervision \n  Ability to be a team player, foster a cooperative environment with the project team, and adapt to the team  \n \n Preferred qualifications: \n \n  Understanding of Medicaid and other similar government programs  \n Experience in Agile development methodologies \n  Experience with integrated eligibility systems and enterprise applications  \n \n \n BACKGROUND \n \n  Emergent Method is a Louisiana-based management consulting firm focused on helping companies and organizations innovate, grow, improve their performance, and achieve their missions. \n  We do this by bringing people and ideas together to understand opportunities, let ideas emerge, and create multi-faceted solutions with results greater than the sum of their parts. We come from diverse backgrounds that lend to expertise in solving complex problems, implementing impactful change, helping our clients navigate shifting landscapes, and seizing opportunities \u2013 including those they had not seen before. From strategic planning and organizational design to project management and strategic communication, we are dedicated to helping our clients adapt, innovate, and grow \u2013 pushing systems, agencies, and industries to new heights, while developing strategic, systemic solutions and processes to support those goals. \n  We opened our doors in 2012 and have been recognized for growth and achievement over the years, including the Greater Baton Rouge Business Report and Junior Achievement's Company of the Year (2017), Consulting Magazine\u2019s fastest growing consulting firms (2018, 2019, 2020, 2022), Inc. Magazine\u2019s fastest-growing private companies in America (2018, 2019, 2020, 2021, 2022), LSU\u2019s 100 fastest growing alumni-owned businesses (2018, 2019, 2020, 2021, 2022), and the Louisiana Association of Business and Industry\u2019s Company of the Year (2019). \n  We are dedicated to seeking the same measures of continuous improvement for the clients we serve and supporting them in all actions of planning, growth, and emergence to the forefront of their industries. \n  To learn more about Emergent Method and our team, visit our website at emergentmethod.com. To learn more about Emergent Method and our team, visit our website at emergentmethod.com. \n  Emergent Method and associated entities are equal opportunity employers. We do not discriminate against employees or applicants for employment on any legally recognized basis or any protected class under federal, state, or local law. \n   \n 1yKOzhkpl7",
        "cleaned_desc": "Emergent Talent, a division of Emergent Method, is seeking qualified candidates to fill an analyst position on a large-scale IT system implementation project. The selected individual will work with project sprint teams to support testing and validation processes for system changes and enhancements.  \n This is a full-time position that will support ongoing project operations in Baton Rouge, Louisiana.  Remote candidates will be considered and are encouraged to apply.   \n The ideal candidate is flexible, able to work in a fast-paced and rapidly changing environment, confident handling a wide range of tasks and utilizing problem-solving skills, and willing to embrace the challenges associated with complex system implementation projects. Additionally, candidates should have relevant technical backgrounds and experience with exceptional interpersonal and communication skills.  \n Key responsibilities include:  \n \n Working directly with user acceptance testing and quality assurance teams to understand the development and impact of new system functionality in an Agile environment \n  Troubleshooting and resolving issues, working cross-team to develop and implement solutions, and providing technical support and consultation \n  Investigating root causes of client and system issues and proposing effective solutions \n  Defining and planning testing strategies for the functionality that will be delivered during individual sprints    Conducting data set-up, mock file creation, and batch execution for test validation activities \n  Supporting the execution of scenarios for test cases including functional, regression, and smoke tests \n  Interpreting, creating, and maintaining technical documentation \n  Collaborating effectively with project team members \n  Providing regular support throughout the day for testing, as well as consistent status updates \n  Utilizing strong communication skills to present and communicate data, trends, insights, and recommendations \n \n \n  QUALIFICATIONS   \n  Ideal candidates should possess the following knowledge, skills, and abilities:  \n \n A bachelor\u2019s degree in information sciences, computer science, management information systems, or a related technical field is preferred (experience that demonstrates commensurate experience will be considered in lieu of a degree) \n  Strong technical skills including SQL, understanding of databases, and web-based applications  \n Experience working in the system development life cycle; working in an Agile environment considered a plus \n  Ability to transform technical information into functional questions/solutions  \n Experience with Jira or similar tools \n  Strong analytical, problem solving, and interpersonal skills    Quick learner with the ability to pivot swiftly based on changing direction \n  Ability to multitask and juggle multiple requests and competing priorities \n  Self-motivated and deadline-oriented to ensure tasks are completed without supervision \n  Ability to be a team player, foster a cooperative environment with the project team, and adapt to the team  \n \n Preferred qualifications: \n \n  Understanding of Medicaid and other similar government programs  \n Experience in Agile development methodologies ",
        "techs": [
            "emergent talent",
            "emergent method",
            "analyst position",
            "it system implementation project",
            "project sprint teams",
            "testing and validation processes",
            "baton rouge",
            "louisiana",
            "remote candidates",
            "flexible",
            "fast-paced environment",
            "problem-solving skills",
            "complex system implementation projects",
            "technical backgrounds",
            "relevant technical experience",
            "interpersonal skills",
            "user acceptance testing",
            "quality assurance teams",
            "troubleshooting",
            "cross-team collaboration",
            "technical support",
            "investigation",
            "proposing solutions",
            "testing strategies",
            "data set-up",
            "mock file creation",
            "batch execution",
            "test validation activities",
            "functional",
            "regression",
            "smoke tests",
            "technical documentation",
            "project team members",
            "status updates",
            "communication skills",
            "bachelor\u2019s degree",
            "information sciences",
            "computer science",
            "management information systems",
            "related technical field",
            "sql",
            "databases",
            "web-based applications",
            "system development life cycle",
            "agile environment",
            "jira",
            "analytical skills",
            "quick learner",
            "multitasking",
            "self-motivated",
            "deadline-oriented",
            "team player",
            "cooperative environment",
            "medicaid",
            "government programs",
            "agile development methodologies"
        ],
        "cleaned_techs": [
            "emergent talent",
            "emergent method",
            "it system implementation project",
            "project sprint teams",
            "testing and validation processes",
            "baton rouge",
            "louisiana",
            "remote candidates",
            "flexible",
            "fast-paced environment",
            "complex system implementation projects",
            "technical backgrounds",
            "relevant technical experience",
            "user acceptance testing",
            "quality assurance teams",
            "troubleshooting",
            "cross-team collaboration",
            "technical support",
            "investigation",
            "proposing solutions",
            "testing strategies",
            "data set-up",
            "mock file creation",
            "batch execution",
            "test validation activities",
            "functional",
            "regression",
            "smoke tests",
            "project team members",
            "status updates",
            "information sciences",
            "computer science",
            "management information systems",
            "related technical field",
            "sql",
            "databases",
            "system development life cycle",
            "agile environment",
            "jira",
            "quick learner",
            "multitasking",
            "self-motivated",
            "deadline-oriented",
            "team player",
            "cooperative environment",
            "medicaid",
            "government programs",
            "agile development methodologies"
        ]
    },
    "e30c9a55a9e5ad6d": {
        "terms": [
            "data analyst"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Business Analyst (E-commerce)",
        "company": "Nagarro",
        "desc": "Full-time \n  Service Region: Eastern Europe \n \n \n \n \n \n  Company Description \n \n \n  We're Nagarro. \n  We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale \u2014 across all devices and digital mediums, and our people exist everywhere in the world (19,000+ experts across 35 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in! \n  By this point in your career, it is not just about the tech you know or how well you can code. It is about what more you want to do with that knowledge. Can you help your teammates proceed in the right direction? Can you tackle the challenges our clients face while always looking to take our solutions one step further to succeed at an even higher level? Yes? You may be ready to join us. \n \n \n \n \n \n  Job Description \n \n \n \n  Works on RFP/RFI/RFQ responses to understand business needs \n  Works on projects as Product Owner or Proxy Product Owner \n  Clearly communicates business needs of the client to development teams \n  Clearly communicates solutions to business needs to client \n  Creates and maintains business needs documentation \n  Advocates and uses Business Analysis techniques. Educates the client and teams if required \n  Equally comfortably works in Agile and Waterfall project setups \n  Participates in project governance responsible for requirements traceability \n  Occasionally travels to client locations for sales pitches \n \n \n \n \n \n \n  Qualifications \n \n \n \n  University degree or equivalent \n  Experience working on B2B and B2C E-Commerce solutions as Business Analyst \n  Domain knowledge in one or more of the following is an advantage: Retail, Automotive, Telecom, CPG \n  Excellent written and verbal English. \n  German language is an advantage \n  Strong soft skills and ability to listen is a must \n \n \n \n \n \n \n  Domain certifications are an advantage",
        "cleaned_desc": " \n  University degree or equivalent \n  Experience working on B2B and B2C E-Commerce solutions as Business Analyst \n  Domain knowledge in one or more of the following is an advantage: Retail, Automotive, Telecom, CPG \n  Excellent written and verbal English. \n  German language is an advantage \n  Strong soft skills and ability to listen is a must \n \n \n ",
        "techs": [
            "university degree or equivalent",
            "b2b and b2c e-commerce solutions",
            "business analyst",
            "retail",
            "automotive",
            "telecom",
            "cpg",
            "excellent written and verbal english",
            "german language",
            "strong soft skills"
        ],
        "cleaned_techs": [
            "b2b and b2c e-commerce solutions",
            "retail",
            "automotive",
            "telecom",
            "cpg",
            "excellent written and verbal english",
            "german language"
        ]
    },
    "3b809fadc8ef8d61": {
        "terms": [
            "data analyst"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Business Analyst",
        "company": "Nagarro",
        "desc": "Full-time \n  Service Region: Eastern Europe \n \n \n \n \n \n  Company Description \n \n \n  We're Nagarro.    We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale \u2014 across all devices and digital mediums, and our people exist everywhere in the world (19,000+ experts across 33 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!    By this point in your career, it is not just about the tech you know or how well you can code. It is about what more you want to do with that knowledge. Can you help your teammates proceed in the right direction? Can you tackle the challenges our clients face while always looking to take our solutions one step further to succeed at an even higher level? Yes? You may be ready to join us. \n \n \n \n \n \n  Job Description \n \n \n \n  Participate in the software design, development, and implementation of hybris applications with integration to various modules of SAP. \n  Design, develop, and maintain scalable and stable eCommerce solutions that meet business needs. \n  Transform business function requirements into technical program specs to code, test and debug programs. \n  Execute unit tests, systems, integration and acceptance tests and testing tools for functions of high complexity. \n  Translate development requirements and specifications into high quality, efficient solutions \n  Complete work in a timely and accurate manner while providing exceptional customer service \n  Work on specific area(s) of website functionality such as search, cart and checkout etc and developing features for those areas of responsibility \n  Ensure alignment of all critical systems with security and data privacy policies. \n  Work closely with internal team and external partners to ensure new developments align with roadmap and integrate seamlessly with other platform components \n  Identify and develop improvements for test coverage of new and existing code bases \n  Collaborate with project stakeholders to ensure all requirements are met. \n \n \n \n \n \n \n  Qualifications \n \n \n \n  Previous extensive experience as a Business Analyst within eCommerce business domain. \n  Good hands-on knowledge of Hybris Commerce Payments and Hybris Commerce Order Management or similar tools that are used in eCommerce projects. \n  Nice-to-have: experience with SAP cloud based eCommerce solutions (B2C, B2B) or good level of understanding of Hybris OOTB functionalities.",
        "cleaned_desc": "  Translate development requirements and specifications into high quality, efficient solutions \n  Complete work in a timely and accurate manner while providing exceptional customer service \n  Work on specific area(s) of website functionality such as search, cart and checkout etc and developing features for those areas of responsibility \n  Ensure alignment of all critical systems with security and data privacy policies. \n  Work closely with internal team and external partners to ensure new developments align with roadmap and integrate seamlessly with other platform components \n  Identify and develop improvements for test coverage of new and existing code bases \n  Collaborate with project stakeholders to ensure all requirements are met. \n ",
        "techs": [
            "translate development requirements",
            "specifications",
            "high quality",
            "efficient solutions",
            "complete work",
            "timely",
            "accurate manner",
            "exceptional customer service",
            "specific area(s)",
            "website functionality",
            "search",
            "cart",
            "checkout",
            "developing features",
            "responsibilities",
            "alignment",
            "critical systems",
            "security",
            "data privacy policies",
            "internal team",
            "external partners",
            "new developments",
            "roadmap",
            "integrate seamlessly",
            "platform components",
            "identify",
            "develop improvements",
            "test coverage",
            "new code bases",
            "collaborate",
            "project stakeholders",
            "requirements"
        ],
        "cleaned_techs": [
            "translate development requirements",
            "specifications",
            "high quality",
            "efficient solutions",
            "complete work",
            "timely",
            "accurate manner",
            "exceptional customer service",
            "specific area(s)",
            "website functionality",
            "search",
            "cart",
            "checkout",
            "developing features",
            "responsibilities",
            "alignment",
            "critical systems",
            "data privacy policies",
            "internal team",
            "external partners",
            "new developments",
            "roadmap",
            "integrate seamlessly",
            "platform components",
            "identify",
            "develop improvements",
            "test coverage",
            "new code bases",
            "collaborate",
            "project stakeholders",
            "requirements"
        ]
    },
    "eb5e647d282057fd": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 70.0,
        "salary_max": 70.0,
        "title": "Business Performance Analyst",
        "company": "Aptara",
        "desc": "Title: Business Performance Analyst \n Start: 23rd Oct \n Duration: 1 Year of Contract \n Location: Hybrid: expected 1-2 Days a week in San Ramon.CA \n Position Summary: The Business Performance Analyst, Expert\u2019s primary function will create monthly performance metrics and dashboards in support of Gas Operations and Gas Engineering. Facilitates monthly reporting of results. The Business Performance Analyst, Expert will implement and maintain custom data and analytic reporting using MS PowerBI, PowerApp and Excel to provide insight into and facilitate the day-to-day operations of Gas Operational Excellence. \n The successful candidate will work across functional teams and partner with peers and leaders on process change implementation and ongoing measurement and reporting. \n Responsibilities: \n \n Facilitates monthly and weekly metrics reporting related to operational performance \n Collects data to identify root cause of problems on performances and implements solutions for improvements \n Aligns improvement to performance shortfalls. Surveys and analyzes best practices for \n \n techniques, processes and implementation \n \n Determines data needs, collects data, selects from different analytical techniques to conduct root cause analysis, determine issues designs and maintains operational tools and reports \n Projects are moderately to highly complex and require selecting methods, approaches and tactics to resolve problems and obtain solutions \n Creates analysis that accurately describes the state of the business \n Creates complex and automated data routines and processes for reporting and data delivery ensuring accurate data transformation \n Design data analysis to achieve business objectives, process data, analyze data, write clear and concise data findings using power queries and power BI reporting \n Applies judgment solve problems of diverse scope \n Typically receives general instructions on new assignments or projects. Receives little \n \n instruction on recurring assignments \n \n May Assist in coaching and developing less experienced staff and end users \n Understands business process needs and develops solutions \n Work Execution and Operational support, ability to travel 3 to 5% of time \n \n Qualifications : \n \n 7 years relevant business experience \n BA/BS in Computer Science/Information or related field \n Expert excel skills (Power Pivots & Tables, V Look up, etc) \n Expert in Microsoft BI stack including PowerView and PowerPivot; PowerBI, DAX \n Expert in MS Excel/Access/PowerPoint \n Experience with metrics/dashboard creation, reporting, and presenting data \n Experience working with data stored across multiple databases/data sources \n MBA or other advanced degrees. \n Advanced statistical, modeling, and process improvement knowledge \n Advanced reporting knowledge and experience with multiple reporting tools \n Demonstrated experience in data visualization best practices using power BI \n Ability to work independently and part of a team \n Ability to keep pace and contribute to a high performing team \n Self-Starter \n Advanced problem-solving skills and data analysis skills \n \n Job Type: Contract \n Pay: $70.00 per hour \n Benefits: \n \n 401(k) \n \n Schedule: \n \n 8 hour shift \n \n Work Location: Remote",
        "cleaned_desc": " \n techniques, processes and implementation \n \n Determines data needs, collects data, selects from different analytical techniques to conduct root cause analysis, determine issues designs and maintains operational tools and reports \n Projects are moderately to highly complex and require selecting methods, approaches and tactics to resolve problems and obtain solutions \n Creates analysis that accurately describes the state of the business \n Creates complex and automated data routines and processes for reporting and data delivery ensuring accurate data transformation \n Design data analysis to achieve business objectives, process data, analyze data, write clear and concise data findings using power queries and power BI reporting \n Applies judgment solve problems of diverse scope \n Typically receives general instructions on new assignments or projects. Receives little \n   Expert in Microsoft BI stack including PowerView and PowerPivot; PowerBI, DAX \n Expert in MS Excel/Access/PowerPoint \n Experience with metrics/dashboard creation, reporting, and presenting data \n Experience working with data stored across multiple databases/data sources \n MBA or other advanced degrees. \n Advanced statistical, modeling, and process improvement knowledge \n Advanced reporting knowledge and experience with multiple reporting tools \n Demonstrated experience in data visualization best practices using power BI \n Ability to work independently and part of a team \n Ability to keep pace and contribute to a high performing team \n Self-Starter ",
        "techs": [
            "powerview",
            "powerpivot",
            "powerbi",
            "dax",
            "ms excel",
            "ms access",
            "ms powerpoint",
            "power queries",
            "power bi",
            "microsoft bi stack",
            "metrics/dashboard creation",
            "data visualization",
            "reporting tools"
        ],
        "cleaned_techs": [
            "powerview",
            "powerpivot",
            "powerbi",
            "dax",
            "excel",
            "ms access",
            "ms powerpoint",
            "power queries",
            "microsoft bi stack",
            "metrics/dashboard creation",
            "data visualization",
            "reporting tools"
        ]
    },
    "b61d394ca7bc1895": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 0.0,
        "salary_max": 125000.0,
        "title": "Marketing Data Analyst (BigQuery)",
        "company": "Source Select Group, LLC",
        "desc": "Marketing Data Analyst (BigQuery)  Location : REMOTE Type : Full Time \n PLEASE NOTE: VENDORS OR CANDIDATES THAT REQUIRE SPONSORSHIP NOW OR IN THE FUTURE WILL NOT BE CONSIDERED (NO 3rd Party Vendor applicants) \n Are you a data-driven problem solver with a passion for turning numbers into actionable insights?Are you ready to take your career in Business Intelligence (BI) to the next level? Look no further! We are seeking a talented and experienced  Marketing Data Analyst  to join our dynamic team and revolutionize the way we make decisions. \n Join us on this exciting journey as we unlock the power of data to unlock new opportunities and achieve our goals! \n With cutting-edge tools, a collaborative environment, and a focus on innovation, this is the perfect opportunity for a seasoned  Marketing Data Analyst  like you to thrive and make a meaningful impact. If this sounds like you, please apply! \n SUMMARY \n The  Marketing Data Analyst  will lead our marketing analytics efforts and will provide customer insights and proposals by analyzing data and monitoring relevant market conditions. This role will report on a wide range of metrics and KPI\u2019s relevant to the business, using a deep understanding of digital marketing data to translate complex data into actionable insights for our marketing teams. \n CORE FUNCTIONS \n \n Develop and maintain analytics reports and dashboards for marketing teams that will provide insights on marketing performance, user behavior, and campaign effectiveness. \n Analyze large datasets using tools such as BigQuery, Domo, Looker, and AWS Redshift to identify trends, patterns, and insights that can drive business decisions. \n Work closely with cross-functional teams to provide insights into user behavior and support the development of data-driven marketing strategies. \n Collaborate with the data engineering team to ensure that data is accurate, consistent, and available in a timely manner. \n Use statistical analysis techniques to identify opportunities for improvement in marketing performance. \n \n QUALIFICATIONS & REQUIREMENTS \n \n Bachelor's degree in Business, Marketing, Statistics, Computer Science, or equivalent experience required. \n 5+ years of experience in marketing analytics required, preferably in an e-commerce or digital marketing environment. \n Experience with Google Cloud, Bigquery, Segment, and Google Analytics 4. \n Experience working with cross-functional teams to develop data-driven marketing strategies. \n \n Skills, Abilities, and Knowledge \n \n Strong analytical and problem-solving skills, with a demonstrated ability to translate data into actionable insights. \n Strong knowledge of statistical analysis techniques, including regression, correlation, and cluster analysis. \n Excellent communication and presentation skills, with the ability to effectively communicate insights to both technical and non-technical stakeholders. \n Proven ability to manage multiple projects and priorities in a fast-paced environment. \n Ability to synthesize large amounts of information, identify issues, recommend solutions proactively, and apply problem-solving and analytical expertise. \n \n Job Type: Full-time \n Pay: Up to $125,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Experience level: \n \n 5 years \n \n Schedule: \n \n Monday to Friday \n \n Work Location: Remote",
        "cleaned_desc": " 5+ years of experience in marketing analytics required, preferably in an e-commerce or digital marketing environment. \n Experience with Google Cloud, Bigquery, Segment, and Google Analytics 4. \n Experience working with cross-functional teams to develop data-driven marketing strategies. \n \n Skills, Abilities, and Knowledge \n \n Strong analytical and problem-solving skills, with a demonstrated ability to translate data into actionable insights. \n Strong knowledge of statistical analysis techniques, including regression, correlation, and cluster analysis. \n Excellent communication and presentation skills, with the ability to effectively communicate insights to both technical and non-technical stakeholders. ",
        "techs": [
            "google cloud",
            "bigquery",
            "segment",
            "google analytics 4"
        ],
        "cleaned_techs": [
            "gcp",
            "bigquery",
            "segment",
            "google analytics 4"
        ]
    },
    "891e596182e0e8d9": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 82076.66,
        "salary_max": 103927.34,
        "title": "Data Analyst- Measurement & Analytics",
        "company": "Tential",
        "desc": "**NO C2C Arrangements at this time!!!!** \n \n  Our Big 4 Client is looking for a Data Analyst (Measurement & Analytics) professional to join their team remotely.\n  \n  Team: Measurement & Analytics Reporting Team \n   Location: Fully Remote\n   Duration: Contract- starts at 6 month, very likely to get extended\n  \n \n Overall Job Description: \n \n  The Measurement & Analytics reporting team partners closely with L&D stakeholders, crafting insightful operational reports, leadership dashboards, and reporting metrics. We excel in developing innovative solutions, automating manual tasks, and enhancing efficiency within their Learning & Development department. \n  \n \n Responsibilities: \n \n \n Collaborate closely with stakeholders to comprehend business needs; create and execute diverse reporting solutions, ensuring accuracy and relevance.  \n Manage extensive datasets, generating detailed reports for learning programs, monitoring learner progress, and evaluating learning impact.  \n Validate reports and data against source systems, guaranteeing quality and precision.  \n Implement data cleansing and update routines, utilizing standard data quality functions to maintain accuracy and leverage expertise in data and technology management.  \n \n \n Top 4 Skills:  \n \n \n \n Data Analytics Tool (Tableu, PowerBi, etc.) \n \n \n Stakeholder management \n \n \n Attention to detail  \n \n \n Time management and managing of projects simultaneously \n \n \n SQL \n \n \n \n Requirements: \n \n \n 3-5 years of experience in a Reporting, Data Analytics Role. \n Proficiency in Microsoft 365, Power BI, Alteryx, and Azure cloud-based tools.  \n Familiarity with Scrum or project management methodologies, including sprint planning, backlog grooming, and team collaboration.  \n Experience in coordinating cross-functional teams and managing project timelines  \n Proficiency in data analysis such as data visualization, data cleaning and preparation.  \n Strong knowledge of SQL such as data retrieval, data manipulation, performance tuning, Indexing, functions, stored procedures.  \n Experience working with large datasets and the ability to create and maintain complex reports.  \n Ability to validate reports and data sets against source systems to ensure accuracy and quality.  \n Familiarity with BI (Business Intelligence Tools) tools like Alteryx, Power BI, Power Automate, Microsoft Profisee, Microsoft Purview, Azure Data Factory, MicroStrategy Business Intelligence, or other similar business intelligence and data warehouse tools.  Familiarity with Human Capital Management (HCM) systems like Human Capital Management (HCM), Learning Management System (LMS), Master Data Management (MDM), Databricks, Data Lakes, Apache Spark \n  \n \n \n Skills Preferred  \n \n Advanced SQL Proficiency: Ability to analyze and manipulate data, create and troubleshoot tables, queries, functions, and stored procedures, as well as create reporting visualizations using tools like PowerBI or MicroStrategy.  \n Data Analysis in Alteryx and Excel: Proficiency in leveraging advanced features, creating, and manipulating data effectively using Alteryx and Excel.  \n Reporting and Data Validation: Ability to build insightful reporting dashboards, packages, and ad hoc reporting requests for stakeholders.  \n Data Validation: Skill in validating reports and datasets against source systems to ensure accuracy and quality.  \n Collaboration and Stakeholder Engagement: Ability to work closely with project managers and stakeholders to understand business requirements and translate them into effective data solutions.  \n Problem-solving: Strong troubleshooting skills to identify and resolve issues related to data manipulation, reporting, and visualization.  \n Attention to Detail: Meticulousness in data analysis, validation, and reporting to ensure precision and accuracy.  \n Learning and Adaptability: Willingness to learn new tools and techniques, staying updated with the latest advancements in data analytics and visualization technologies.  A bachelor's degree or equivalent experience is preferred. An undergraduate degree in Computer Science or a related field, with coursework in statistics, analysis, data modeling, etc., is preferable. \n  \n \n  #EP\n   #LI-RA\n   #LI-Remote\n   #remote\n   #Dice",
        "cleaned_desc": " Responsibilities: \n \n \n Collaborate closely with stakeholders to comprehend business needs; create and execute diverse reporting solutions, ensuring accuracy and relevance.  \n Manage extensive datasets, generating detailed reports for learning programs, monitoring learner progress, and evaluating learning impact.  \n Validate reports and data against source systems, guaranteeing quality and precision.  \n Implement data cleansing and update routines, utilizing standard data quality functions to maintain accuracy and leverage expertise in data and technology management.  \n \n \n Top 4 Skills:  \n \n \n \n Data Analytics Tool (Tableu, PowerBi, etc.)   \n Requirements: \n \n \n 3-5 years of experience in a Reporting, Data Analytics Role. \n Proficiency in Microsoft 365, Power BI, Alteryx, and Azure cloud-based tools.  \n Familiarity with Scrum or project management methodologies, including sprint planning, backlog grooming, and team collaboration.  \n Experience in coordinating cross-functional teams and managing project timelines  \n Proficiency in data analysis such as data visualization, data cleaning and preparation.  \n Strong knowledge of SQL such as data retrieval, data manipulation, performance tuning, Indexing, functions, stored procedures.  \n Experience working with large datasets and the ability to create and maintain complex reports.  \n Ability to validate reports and data sets against source systems to ensure accuracy and quality.  \n Familiarity with BI (Business Intelligence Tools) tools like Alteryx, Power BI, Power Automate, Microsoft Profisee, Microsoft Purview, Azure Data Factory, MicroStrategy Business Intelligence, or other similar business intelligence and data warehouse tools.  Familiarity with Human Capital Management (HCM) systems like Human Capital Management (HCM), Learning Management System (LMS), Master Data Management (MDM), Databricks, Data Lakes, Apache Spark \n    \n \n Skills Preferred  \n \n Advanced SQL Proficiency: Ability to analyze and manipulate data, create and troubleshoot tables, queries, functions, and stored procedures, as well as create reporting visualizations using tools like PowerBI or MicroStrategy.  \n Data Analysis in Alteryx and Excel: Proficiency in leveraging advanced features, creating, and manipulating data effectively using Alteryx and Excel.  \n Reporting and Data Validation: Ability to build insightful reporting dashboards, packages, and ad hoc reporting requests for stakeholders.  \n Data Validation: Skill in validating reports and datasets against source systems to ensure accuracy and quality.  \n Collaboration and Stakeholder Engagement: Ability to work closely with project managers and stakeholders to understand business requirements and translate them into effective data solutions.  \n Problem-solving: Strong troubleshooting skills to identify and resolve issues related to data manipulation, reporting, and visualization.  \n Attention to Detail: Meticulousness in data analysis, validation, and reporting to ensure precision and accuracy.  \n Learning and Adaptability: Willingness to learn new tools and techniques, staying updated with the latest advancements in data analytics and visualization technologies.  A bachelor's degree or equivalent experience is preferred. An undergraduate degree in Computer Science or a related field, with coursework in statistics, analysis, data modeling, etc., is preferable. \n  \n ",
        "techs": [
            "tableu",
            "powerbi",
            "microsoft 365",
            "power bi",
            "alteryx",
            "azure",
            "scrum",
            "sql",
            "power automate",
            "microsoft profisee",
            "microsoft purview",
            "azure data factory",
            "microstrategy business intelligence",
            "hcm",
            "lms",
            "mdm",
            "databricks",
            "data lakes",
            "apache spark",
            "alteryx",
            "excel"
        ],
        "cleaned_techs": [
            "tableu",
            "powerbi",
            "microsoft 365",
            "alteryx",
            "azure",
            "scrum",
            "sql",
            "power automate",
            "microsoft profisee",
            "microsoft purview",
            "microstrategy business intelligence",
            "hcm",
            "lms",
            "mdm",
            "databricks",
            "data lakes",
            "apache spark",
            "excel"
        ]
    },
    "af2cd171e94158b2": {
        "terms": [
            "data analyst"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Financial Analyst - Financial Planning & Analysis",
        "company": "MD Anderson Cancer Center",
        "desc": "The University of Texas MD Anderson Cancer Center in Houston is one of the world's most respected centers focused on cancer patient care, research, education and prevention. It was named the nation's No. 1 hospital for cancer care in U.S. News & World Report's 2023-24 rankings. It is one of the nation's original three comprehensive cancer centers designated by the National Cancer Institute. \n  \n \n SUMMARY  \n \n  The primary purpose of the Financial Analyst position is to provide support for the financial reporting, planning and analysis activities of the department. The analyst will be responsible for producing daily, monthly and quarterly reports. The analyst will seek to automate reporting functions and will be knowledgeable in various reporting solutions (PeopleSoft, Essbase, Smartview, OBIEE, etc.). \n  \n  This position requires independent thinking, independent production, independent problem-solving and initiative.\n  \n \n JOB SPECIFIC COMPETENCIES \n \n \n Budget Reporting Activities  \n  Assist with the development and maintenance of annual budget reports and other tasks as assigned. \n   Assist with development of the Enterprise budget and any supporting reports or documentation\n   Assist with development of the Regents Budget schedules and any supporting reports or documentation\n   Continually develop and implement methods to assess and increase utilization of financial reporting/data analysis and budgeting tools\n   Provide support and analysis for the institutional budgeting process\n   Maintain and continually refresh PeopleSoft, Hyperion Planning, Smart View and Essbase skills\n  \n \n Reporting, Data Analysis and Support  \n  Prepare and distribute regular financial reports to department heads and senior management\n   Extract and analyze data in a timely manner as requested by management\n   Ensure accuracy and timeliness of financial reporting, adhering to internal and external reporting requirements\n   Automate and integrate reporting where needed to improve efficiency, accuracy and timeliness\n   Document and update processes as required\n   Ad hoc reports as requested by management or end users.\n  \n \n Financial Reconciliation  \n  Perform advanced analysis, report and summarize financial and metric activities. \n   Leads investigatory analysis, planning, and correction of discrepancies.\n   Notify and work with Finance departments to resolve any reconciling items on a timely basis\n   Provides alerts to departmental management for pending items requiring remediation as needed.\n  \n \n Other \n \n  Assists with any on-going and special projects in support of the objectives of the department and division. \n  \n  Other duties as assigned.\n  \n \n Required Education: \n \n  Bachelor's degree in Business Administration or related business specialty. \n  \n \n Required Experience: \n \n  Two years business experience to include project management, data analysis or accounting. \n  \n \n Preferred Experience/Skills: \n \n  Two to five years of experience, financial reporting, data analysis, data sourcing, proficient in Excel, Smart View experience, time management skills, flexible, willing to learn, experience working in a fast environment. \n  \n \n Work location: \n \n  Prefer a candidate in the local Houston area. \n  \n  It is the policy of The University of Texas MD Anderson Cancer Center to provide equal employment opportunity without regard to race, color, religion, age, national origin, sex, gender, sexual orientation, gender identity/expression, disability, protected veteran status, genetic information, or any other basis protected by institutional policy or by federal, state or local laws unless such distinction is required by law. http://www.mdanderson.org/about-us/legal-and-policy/legal-statements/eeo-affirmative-action.html\n  \n  Additional Information \n  \n \n Requisition ID: 162682  \n Employment Status: Full-Time  \n Employee Status: Regular  \n Work Week: Days  \n Minimum Salary: US Dollar (USD) 66,000  \n Midpoint Salary: US Dollar (USD) 82,500  \n Maximum Salary : US Dollar (USD) 99,000  \n FLSA: exempt and not eligible for overtime pay  \n Fund Type: Hard  \n Work Location: Remote (within Texas only)  \n Pivotal Position: No  \n Referral Bonus Available?: No  \n Relocation Assistance Available?: No  \n Science Jobs: No  \n \n  #LI-Remote",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "5b839a3dff9ccc2f": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 65000.0,
        "salary_max": 110000.0,
        "title": "Data Reporting Analyst",
        "company": "Clinician Nexus",
        "desc": "ABOUT US AND ABOUT YOU \n  Clinician Nexus enables health care organizations to build thriving clinician teams with industry-leading technology products, workforce and compensation analytics, and automated workflow solutions. Backed by extensive technical expertise and industry-leading data, we deliver innovative approaches to help clients plan, educate, and engage their clinical workforce at every stage of the lifecycle. We are committed to providing our clients with outstanding guidance and support as they focus on shaping the future of health care. \n  The Data Reporting Analyst will assist with survey data collection, data validation, benchmark & report creation as well as assist with the analysis of various health care compensation surveys. Working with CIAI leadership, you will be responsible for assisting with initiatives focused on the standardization of technical processes and methodology enhancements. This role is expected to assist with complex data projects and deliverable creation. \n  PRIMARY ACCOUNTABILITIES \n \n  Conduct in-depth data cleaning for compensation and custom surveys utilizing in-house and statistical software. \n  With guidance, produce compensation survey reports utilizing Qualtrics. \n  Provide support to internal and external clients and survey participants regarding survey processes and analysis via telephone and email. \n  Provide quality assurance to compensation reports per formatting and analysis guidelines. \n  Delve into data to identify discrepancies, patterns, and insights. Concisely present findings and suggest solutions. \n  Assist senior members of the team with ad hoc survey reports, analyses, and projects. \n  Assists senior members of the team with training materials for new and existing processes. \n  Update tool and process documentation utilizing CIAI standards. \n  With guidance, produce flagship survey cuts reporting. \n  Identifies and defines both process and data improvements and assists with implementing enhancements. Ensures compliance with deliverable reporting requirements by performing quality data audits and analysis. \n  Participate in cross-functional efforts for standardization and optimization. \n  Provides input related to business requirements for survey tools and processes. \n  Program complex custom surveys utilizing Qualtrics software. \n \n \n  KNOWLEDGE, SKILLS AND ABILITIES \n \n  BA or BS in a related field or equivalent experience required. Emphasis in finance, economics, statistical research, research methodology or mathematics preferred. \n  A minimum of 3 years related experience in statistical analysis, survey, or data analytics. \n  Intermediate technical knowledge and experience working with Excel, Word & PowerPoint. \n  Qualtrics experience required. \n  Experience using advanced Qualtrics logic, all question types (select one, select all, matrix type, etc.), and tools such as Survey Flow and File Submission required. \n  Using Qualtrics, have knowledge of how to utilize advanced routing logic, advanced display logic, and how to implement advanced validation checks. \n  Understanding of the various distribution options within Qualtrics strongly preferred \n  Knowledge of JavaScript is a plus. \n  Basic programming experience in Alteryx a plus \n  Basic programming experience including R, SQL, and/or Python a plus. \n  Experience with Tableau or Power BI a plus. \n  Knowledge of the health care industry preferred. \n  AzureDevops experience a plus \n  Agile Project Management Methodology experience a plus \n  Intermediate experience interfacing with internal/external counterparts required. \n  Basic experience creating standard operating procedures and other business documentation. \n  Advanced experience supporting analytic projects. \n \n  REQUIRED BEHAVIORAL ATTRIBUTES \n \n  Advanced experience in creating, summarizing, and presenting ideas or analytical findings in a business-friendly and user-friendly language. \n  Self-starter with demonstrated ability to successfully assist with project deliverables in a very driven, fast-paced, and changing organization. \n  Recognized as an emerging individual contributor. \n  Able to demonstrate a high level of flexibility, adaptability, and the ability to work under tight deadlines or changing needs. \n  Strong organizational skills and ability to consistently manage time and execute tasks per established deadlines in a high-pressure environment. \n  Must be able to respond to survey participants in a quick and concise manner having a high degree of service. \n  Consistently performs with a high degree of critical thinking and attention to detail. \n  Collaborative, team-player, able to work in a virtual and matrixed team-based environment, and able to develop and maintain relationships. \n  Able to consistently manage multiple tasks simultaneously and maintain high-quality work. \n  Strong written and oral communication skills. \n \n  SALARY, BENEFITS AND PERKS \n \n      Reflected below is the base salary range offered for this position. Actual salaries may vary depending on factors including but not limited to academic achievements, skills, and experience. The range listed is just one component of the total rewards package offered to candidates.\n     \n \n \n \n  $65,000 - $110,000 annually \n  Competitive total rewards package \n  Medical and dental coverage at no premium cost for employees \n  401(k) and profit-sharing retirement plans \n  Flexible spending accounts \n  Generous paid time off (PTO) \n  Federal and company holidays \n  Gender-neutral parental leave \n  Bereavement and pet leave \n  Continuing education and professional accreditation sponsorship \n  Life and AD&D insurance \n  Short- and long-term disability \n  Employee assistance program \n  Mental health support program \n  Additional perks \n \n  Clinician Nexus  is an Equal Employment Opportunity/Affirmative Action employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law or marital status.",
        "cleaned_desc": "  Participate in cross-functional efforts for standardization and optimization. \n  Provides input related to business requirements for survey tools and processes. \n  Program complex custom surveys utilizing Qualtrics software. \n \n \n  KNOWLEDGE, SKILLS AND ABILITIES \n \n  BA or BS in a related field or equivalent experience required. Emphasis in finance, economics, statistical research, research methodology or mathematics preferred. \n  A minimum of 3 years related experience in statistical analysis, survey, or data analytics. \n  Intermediate technical knowledge and experience working with Excel, Word & PowerPoint. \n  Qualtrics experience required. \n  Experience using advanced Qualtrics logic, all question types (select one, select all, matrix type, etc.), and tools such as Survey Flow and File Submission required. \n  Using Qualtrics, have knowledge of how to utilize advanced routing logic, advanced display logic, and how to implement advanced validation checks. \n  Understanding of the various distribution options within Qualtrics strongly preferred \n  Knowledge of JavaScript is a plus.    Basic programming experience in Alteryx a plus \n  Basic programming experience including R, SQL, and/or Python a plus. \n  Experience with Tableau or Power BI a plus. \n  Knowledge of the health care industry preferred. \n  AzureDevops experience a plus \n  Agile Project Management Methodology experience a plus \n  Intermediate experience interfacing with internal/external counterparts required. \n  Basic experience creating standard operating procedures and other business documentation. \n  Advanced experience supporting analytic projects. \n \n  REQUIRED BEHAVIORAL ATTRIBUTES \n \n  Advanced experience in creating, summarizing, and presenting ideas or analytical findings in a business-friendly and user-friendly language. \n  Self-starter with demonstrated ability to successfully assist with project deliverables in a very driven, fast-paced, and changing organization. \n  Recognized as an emerging individual contributor. ",
        "techs": [
            "qualtrics software",
            "excel",
            "word",
            "powerpoint",
            "javascript",
            "alteryx",
            "r",
            "sql",
            "python",
            "tableau",
            "power bi",
            "azuredevops",
            "agile project management methodology"
        ],
        "cleaned_techs": [
            "qualtrics software",
            "excel",
            "word",
            "powerpoint",
            "javascript",
            "alteryx",
            "r",
            "sql",
            "python",
            "tableau",
            "powerbi",
            "azuredevops",
            "agile project management methodology"
        ]
    },
    "fb61918641015908": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 103421.734,
        "salary_max": 130954.97,
        "title": "Senior Business Analyst",
        "company": "CareMetx LLC",
        "desc": "From  Intake to Outcomes , CareMetx is dedicated to supporting the patient journey by providing hub services, innovative technology, and decision-making data to pharmaceutical, biotechnology, and medical device innovators. \n \n  Job Title:  Sr. Business Analyst \n \n  POSITION SUMMARY: \n \n  The Senior Business Analyst plays a critical role in understanding and assessing the impact of business changes and needs; analyzing and capturing requirements; communicating requirements to relevant stakeholders; and monitoring the build of the solution to ensure it meets the requirements. \n \n  The Senior Business Analyst must have a solid understanding of the client's existing business processes, the key drivers and measures of success for the business, and the short- and long-term direction of the business and related technologies. They must have a working knowledge of the business area that they support and should be equally aligned with the IT department. Through adaptable communication skills and the ability to translate between technical and business language, they work collaboratively, negotiating requirements across multiple groups. Strong leadership, relationship management, interpersonal, negotiation and communication skills are also required. \n  The Senior Business Analyst is the liaison between the operations and the IT organization. They collect, analyze, develop, document, communicate business requirements, and support user testing to achieve business goals. They will interact directly with clients, key stakeholders and IT development to ensure proposed processes and systems meet desired needs. They will support one or more moderately to highly complex business processes and serve as a team member within an Agile delivery team. \n \n  PRIMARY DUTIES AND RESPONSIBILITIES: \n \n  Act as senior level business analyst for highly visible program implementations or program design enhancements \n  Document and maintain business and functional requirements and ensure consistency \n  Provide consultative assistance to business users and stakeholders looking to streamline processes by analyzing and determining problem/opportunity/solution resolution \n  Elicit requirements from end users to solve a business problem and identify the desired results \n  Anticipate, quantify and resolve problems and issues with requirements. \n  Communicate with clients and stakeholders using data process models to clarify and validate requirements \n  Oversee implementation, coordinating tests and observing initiation of the system to validate performance and ensure all business requirements and their integrity are maintained. \n  Address issues and questions related to intended functionality of the system, as well as support the business in implementing the required changes to make effective use of the new system \n  Serve as the conduit between the customer community (internal and external customers) and the software development team through which requirements flow \n  Perform all responsibilities within the guidelines and IT policies and directives at or above our client's performance and evaluation standards. \n  Utilize time management skills and multi-tasking capabilities to handle multiple and parallel projects, deliverables while working both independently and collaboratively within a team. \n  Maintain user confidence and protect operations by keeping information confidential. \n  Occasional travel may be required. \n \n \n  EXPERIENCE AND EDUCATIONAL REQUIREMENTS: \n \n  Bachelor's degree in computer science, engineering or management information systems with a minimum of five (5) years of experience as a Business Analyst, Systems Engineer or equivalent. \n  Experience in the Pharmaceutical, Medical Device or Healthcare industry. \n \n \n  MINIMUM SKILLS, KNOWLEDGE AND ABILITY REQUIREMENTS: \n \n  Proven management and supervisory experience for the work of one or more Business Analysts \n  Demonstrated experience in multiple phases of the software development lifecycle and project management \n  Proven leadership skills with the ability to effectively and professionally represent the organization at the client site. \n  Ability to work autonomously with little oversight or direction \n  Critical thinker who can evaluate information gathered from multiple sources, reconcile conflicts, decompose high-level information into details, abstract up from low-level information to a general understanding, and distinguish user requests from the underlying true needs \n  Employ facilitation techniques in discussing requirements with clients and internal Agile delivery teams \n  Able to identify \u201cas-is\u201d state vs. \u201cto-be\u201d state using a variety of applicable tools and techniques depending on the project and its complexity including but not limited to use-case diagramming, ERD diagrams, data flows and business process workflows. \n  Experience in Agile Development methodologies \n  Management and maintenance of requirements traceability \n  Write logical, comprehensive, concise reports, meeting minutes, correspondence; communicate effectively orally and in writing. \n  Well-organized with the ability to manage multiple tasks at once in order to meet demanding deadlines. \n  Team player who can effectively work in a fast-paced environment. \n  Proficient in Microsoft Office (Excel, Access, PowerPoint, Word) \n  Proficient in Microsoft Visio, Project and SharePoint \n \n \n  Preferred Skills \n \n  Experience working in a startup atmosphere \n  Experience developing and customizing requirements, process, and design documentation \n  Experience working with Jira to to create work items and collaborate with Agile delivery teams to prioritize, plan, and execute work \n  Experience working with and presenting to executive level clients \n  Experience performing requirements, design, and testing activities \n  Change management experience to include internal/external system training, external communication development on behalf of the client, demo facilitation \n  Strong understanding of system architecture and a desire to participate in development strategy \n  Experience with service-oriented architecture and accompanying functional documentation \n \n \n CareMetx  considers equivalent combinations of experience and education for most jobs. All candidates who believe they possess equivalent experience and education are encouraged to apply. \n \n \n  At  CareMetx  we work hard, we believe in what we do and we want to be a company that does right by our employees. Our niche industry is an integral player in getting specialty products and devices to the patients who need them by managing reimbursements for those products, identifying alternative funding when insurers don\u2019t pay, and providing clinical services. \n \n  CareMetx  is an equal employment opportunity employer. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, color, sex, sexual orientation, gender identity, religion, disability, age, genetic information, veteran status, ancestry, or national or ethnic origin.",
        "cleaned_desc": " \n \n  EXPERIENCE AND EDUCATIONAL REQUIREMENTS: \n \n  Bachelor's degree in computer science, engineering or management information systems with a minimum of five (5) years of experience as a Business Analyst, Systems Engineer or equivalent. \n  Experience in the Pharmaceutical, Medical Device or Healthcare industry. \n \n \n  MINIMUM SKILLS, KNOWLEDGE AND ABILITY REQUIREMENTS: \n \n  Proven management and supervisory experience for the work of one or more Business Analysts \n  Demonstrated experience in multiple phases of the software development lifecycle and project management \n  Proven leadership skills with the ability to effectively and professionally represent the organization at the client site.    Ability to work autonomously with little oversight or direction \n  Critical thinker who can evaluate information gathered from multiple sources, reconcile conflicts, decompose high-level information into details, abstract up from low-level information to a general understanding, and distinguish user requests from the underlying true needs \n  Employ facilitation techniques in discussing requirements with clients and internal Agile delivery teams \n  Able to identify \u201cas-is\u201d state vs. \u201cto-be\u201d state using a variety of applicable tools and techniques depending on the project and its complexity including but not limited to use-case diagramming, ERD diagrams, data flows and business process workflows. \n  Experience in Agile Development methodologies \n  Management and maintenance of requirements traceability \n  Write logical, comprehensive, concise reports, meeting minutes, correspondence; communicate effectively orally and in writing. \n  Well-organized with the ability to manage multiple tasks at once in order to meet demanding deadlines. \n  Team player who can effectively work in a fast-paced environment. \n  Proficient in Microsoft Office (Excel, Access, PowerPoint, Word) \n  Proficient in Microsoft Visio, Project and SharePoint \n \n    Preferred Skills \n \n  Experience working in a startup atmosphere \n  Experience developing and customizing requirements, process, and design documentation \n  Experience working with Jira to to create work items and collaborate with Agile delivery teams to prioritize, plan, and execute work \n  Experience working with and presenting to executive level clients \n  Experience performing requirements, design, and testing activities \n  Change management experience to include internal/external system training, external communication development on behalf of the client, demo facilitation \n  Strong understanding of system architecture and a desire to participate in development strategy \n  Experience with service-oriented architecture and accompanying functional documentation \n \n \n CareMetx  considers equivalent combinations of experience and education for most jobs. All candidates who believe they possess equivalent experience and education are encouraged to apply. ",
        "techs": [
            "jira",
            "microsoft office (excel",
            "access",
            "powerpoint",
            "word)",
            "microsoft visio",
            "microsoft project",
            "sharepoint",
            "use-case diagramming",
            "erd diagrams",
            "data flows",
            "business process workflows",
            "agile development methodologies",
            "requirements traceability",
            "service-oriented architecture"
        ],
        "cleaned_techs": [
            "jira",
            "microsoft",
            "access",
            "powerpoint",
            "word)",
            "microsoft visio",
            "microsoft project",
            "sharepoint",
            "use-case diagramming",
            "erd diagrams",
            "data flows",
            "business process workflows",
            "agile development methodologies",
            "requirements traceability",
            "service-oriented architecture"
        ]
    },
    "ca71c94e9e63f427": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 54922.0,
        "salary_max": 107099.0,
        "title": "Medical Economics Analyst - REMOTE",
        "company": "Molina Healthcare",
        "desc": "JOB DESCRIPTION \n \n  Job Summary \n \n  Responsible for conducting analyses of insured medical populations with the goal of identifying opportunities to improve financial performance. Extracts, analyzes, and synthesizes data from various sources to identify risks and opportunities. \n \n  KNOWLEDGE/SKILLS/ABILITIES \n \n  Analyze claims and other data sources to identify early signs of trends or other issues related to medical care costs \n \n  Draw actionable conclusions based on analyses performed \n \n  Work closely with clinical, provider network and other personnel to design and perform studies related to the quantification of medical interventions \n \n  Work with business owners to track key performance indicators of medical interventions \n \n  Perform pro forma sensitivity analyses in order to estimate the expected financial value of proposed medical cost improvement initiatives \n \n  Extract and compile information from various systems to support executive decision-making \n \n  Ability to mine and manage information from large data sources. \n \n  JOB QUALIFICATIONS \n \n  Required Education \n \n  Bachelor's Degree in Mathematics, Statistics, or Economics \n \n  Required Experience \n \n  2-5 years of experience \n \n  Preferred Experience \n \n  Proficiency with Excel and SQL for retrieving specified information from datalake. \n \n \n Pay Range:  $54,922 - $107,099 \n \n \n \n Actual compensation may vary from posting based on geographic location, work experience, education and/or skill level. \n \n To all current Molina employees:  If you are interested in applying for this position, please apply through the intranet job listing. \n \n  Molina Healthcare offers a competitive benefits and compensation package. Molina Healthcare is an Equal Opportunity Employer (EOE) M/F/D/V.",
        "cleaned_desc": " \n  Required Experience \n \n  2-5 years of experience \n \n  Preferred Experience \n \n  Proficiency with Excel and SQL for retrieving specified information from datalake. \n ",
        "techs": [
            "excel",
            "sql"
        ],
        "cleaned_techs": [
            "excel",
            "sql"
        ]
    },
    "dc347d0fa6235d50": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 142900.0,
        "salary_max": 178625.0,
        "title": "Lead Business Analyst - Remote, US",
        "company": "Earnest Current Job Openings",
        "desc": "Earnest's mission is to make higher education accessible and affordable for everyone.  We empower past, present, and soon-to-be students to maximize their financial futures through thoughtful guidance and impactful products. \n  We build tools that help people feel in control of their financial future, including: \n \n Private student loans  - low rates, people-first service, and flexible payments. \n Student loan refinancing  - break free from high interest rates or monthly payments. \n Scholarships  - access to thousands of scholars to help students pay less. \n \n What makes an \"Earnie\" culture: \n \n Drivers  \u2013 Drivers are satisfied by making things happen, not coming along for the ride. They feel a strong sense of ownership for their projects and teams and demand high standards from themselves and others. \n Humility  \u2013 Humble team players check their egos and consider the team's needs above their own. They are self-aware of their strengths and opportunities for improvement. \n Growth Mindset  \u2013 People with a growth mindset approach challenges and failures as learning opportunities. They seek feedback to improve, give feedback to others, and genuinely want to perform well. \n \n Earnies are committed to helping students live their best lives, free from the stress of student debt. If you're as passionate as we are about our mission, read more below, and let's build something great together! \n  As the Lead Business Analyst, you will report to the Manager of Analytics and play a pivotal role in our organization, contributing to our growth and success by providing invaluable insights and analysis to drive strategy and to ensure we are successfully executing against our goals as a business. This role requires true analytical curiosity and the ability to solve open-ended problems. You are comfortable analyzing and distilling deep insights from large amounts of data to improve the performance of the business across various teams. You have excellent communication skills and have experience presenting complex analyses to leadership teams and cross-functional partners. \n  What you'll do: \n \n Ongoing Performance Insights \n \n Lead ongoing development of models, analyses, and dashboards against our business to drive visibility for our leadership team \n Deliver weekly executive business reports, translating data into actionable insights that drive strategic decision-making \n \n KPI Design and Interpretation \n \n Collaborate with cross-functional teams to design, implement, and interpret key performance metrics \n Perform root cause analysis to understand what's driving performance across key business metrics \n \n Strategic Analysis \n \n Support strategic initiatives from planning to execution, offering data-driven recommendations and tracking their success \n Provide analysis that contributes to the development of business plans, budgets, and forecasts to align with our growth objectives \n \n Cross-Functional Partnership \n \n Build strong relationships with cross-functional leaders in operations, strategy, finance, and general management teams \n Work collaboratively to constantly elevate deep analysis and insights, fostering a culture of data-driven decision-making \n \n Team Management and Coaching \n \n Coach analysts through their analysis, providing guidance and feedback to enhance their analytical skills \n Prioritize analytics work and initiatives, ensuring alignment with business objectives and resource optimization \n \n \n Your impact: \n \n Informed data decision-making at all levels of the organization, reducing the risk of making decisions based on intuition or incomplete information \n The ability for us to act quickly against your insights and recommendations to improve our overall business \n Our leadership team will have a constant pulse of what's working and not working across our different initiatives through reporting and insights up-leveled to them \n Deep understanding of performance against our key strategic initiatives to keep moving us forward as a business \n \n About you: \n \n Bachelor's degree (B.S.) in quantitative field such as Statistics, Mathematics, Economics, Business, or Finance or a combination of relevant education, experience and training \n 4+ years of relevant work experience in a data-centric role (e.g. analytics, business intelligence, strategy, management consulting, investment banking, etc.) \n 1+ years of people management or mentorship experience \n Excellent communications skills and an ability to drive storytelling and presentation up to executive-level leadership \n Experience manipulating and summarizing data using SQL \n Experience in using analytics tools such as Snowflake, Looker, DBT, and Amplitude \n \n Even better: \n \n Experience developing forecasting models \n Experience using in a statistical software such as Python or R \n \n Earnest believes in enabling our employees to live their best lives. We offer a variety of perks and competitive benefits, including: \n \n Health, Dental, & Vision benefits plus savings plans \n Work anywhere in the U.S. \n Mac computers + work from home stipend to set up your home office \n Monthly internet and phone reimbursement \n Employee Stock Purchase Plan \n RSUs \n 401(k) plan to help you save for retirement plus a company match \n Robust tuition reimbursement program \n $1,000 travel perk on each Earnie-versary to anywhere in the world \n Competitive days of annual PTO \n Annual \"my day\" \n Pet insurance! \n Competitive parental leave \n Plenty of Earnest swag, optional in person team gatherings, picnics, celebrations, and plenty of fun virtual events \n \n At Earnest, we are committed to building an environment where our employees feel included, valued, and heard. Our belief is that a strong commitment to diversity, inclusion, equity, and belonging enables us to move forward with our mission. We are dedicated to adding new perspectives to the team and encourage anyone to apply if your experience is close to what we are looking for. \n  Earnest provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, veteran status disability or genetics. Qualified applicants with criminal histories will be considered for the position in a manner consistent with the Fair Chance Ordinance. \n  #LI-NS1 \n  Base Salary Bands: \n \n \n    A little about our pay philosophy: We take pride in ensuring we are compensating our employees fairly and equitably. We are showcasing a range and actual starting pay may be based on several factors including but not limited to, market rate, the qualified pool of candidates, internal compensation, candidate location, and budgetary constraints. This number does not necessarily reflect your total compensation, but is a range for your base salary. \n   \n Pay Range  \n \n   $142,900\u2014$178,625 USD",
        "cleaned_desc": " 1+ years of people management or mentorship experience \n Excellent communications skills and an ability to drive storytelling and presentation up to executive-level leadership \n Experience manipulating and summarizing data using SQL \n Experience in using analytics tools such as Snowflake, Looker, DBT, and Amplitude \n \n Even better: \n \n Experience developing forecasting models \n Experience using in a statistical software such as Python or R \n \n Earnest believes in enabling our employees to live their best lives. We offer a variety of perks and competitive benefits, including: \n \n Health, Dental, & Vision benefits plus savings plans \n Work anywhere in the U.S. \n Mac computers + work from home stipend to set up your home office \n Monthly internet and phone reimbursement \n Employee Stock Purchase Plan \n RSUs ",
        "techs": [
            "sql",
            "snowflake",
            "looker",
            "dbt",
            "amplitude",
            "python",
            "r"
        ],
        "cleaned_techs": [
            "sql",
            "snowflake",
            "looker",
            "dbt",
            "amplitude",
            "python",
            "r"
        ]
    },
    "0140573cbccce354": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 91458.305,
        "salary_max": 115806.586,
        "title": "Remote Business Analyst \u2013 Project Management Office",
        "company": "University of Maryland Medical System",
        "desc": "Company Description\n   The University of Maryland Medical System is a 14-hospital system with academic, community and specialty medical services reaching every part of Maryland and beyond. UMMS is a national and regional referral center for trauma, cancer care, Neurocare, cardiac care, women\u2019s and children\u2019s health and physical rehabilitation. UMMS is the fourth largest private employer in the Baltimore metropolitan area and one of the top 20 employers in the state of Maryland. No organization will give you the clinical variety, the support, or the opportunities for professional growth that you\u2019ll enjoy as a member of our team. \n \n \n \n Job Description\n   General Summary  Under limited supervision, develops, builds, and participates in coordinating and supporting business analysis activities for moderate to complex initiatives. This role is responsible for developing the business analysis process, including business case development, requirements elicitation, project estimating and requirements validation. In addition, this role is responsible for mentoring staff which could include individuals from the PMO as well as operational entities.   \n Principal Responsibilities and Tasks  The following statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not to be construed as an exhaustive list of all job duties performed by personnel so classified. \n \n  Manages multiple business analysis needs for the Medical System by ingesting project intake requests, developing appropriate business cases/needs assessments, authoring request for proposals as needed, determining total cost of ownership by engaging all affected teams to develop a clear financial worksheet prior to submission to governance groups for review.   \n Develop/Implement and continuously improves business analysis processes and supporting tools, e.g. forms, templates, etc.in conjunction with IS&T PMO teams.   \n Assumes the role of a change leader, handling programs/portfolios and creating process methodologies, processes, tools and techniques.   \n Develop and mentor other PMO staff and provide coaching to all around BA related processes and activities.   Identify leading practices for business case development. \n       \n Participates in various phases of the project life cycle to ensure customer business and technical requirements are captured and validated, business benefits are defined and measurable, requirements are translated into project scope and actual project benefits are measured and documented. \n  Responsible for providing assistance to the project/business sponsor in the coordination and development of accurate and complete concept documentation for approved preliminary proposals. Provides unbiased and factual information including identifying alignment to strategic and operating objectives, defining high level business and technical requirements and scope. \n  Works with the PMO teams to continuously improve associated PMO processes and tools. \n  Responsible for packaging PMO requests and transferring approved business case/concept information to the responsible project manager. \n \n \n \n \n Qualifications\n   Education, Competencies and Experience \n \n  Bachelor of Science degree in Computer Science or Business Management. Master\u2019s degree in Computer Science or Business Management preferred. \n  7+ years total IT, project management and business analyst experience with at least: - Min 5 years spent developing and analyzing complex business requirements \n  PMI Certified Project Management Professional (PMP) preferred.   \n Certified Business Analysis Professional (CBAP) preferred.   \n Healthcare experience preferred   \n Mastery level proficiency/able to mentor staff utilizing advanced business analysis and project management tools/techniques in a highly complex environment.   \n |Experience and knowledge of IT, Business Analysis, Quality, and Project Management industry standards, best practices and frameworks is required.   May substitute an equivalent combination of education and experience \n  \n Knowledge, Skills and Abilities \n \n  Demonstrate ability to think strategically. Demonstrate sufficient understanding of some functions to structure and manage project work. Generate innovative and practical solutions to complex or unusual problems. \n  Support client relationship management and oversees IT project execution and reporting. Possesses excellent customer service skills. \n  Act as lead resource for project financials, developing total cost of ownership. Work with the team to make decisions considering the degree of impact on areas involved. Demonstrated ability to reach agreement and potential solutions through flexibility and compromise with peers, customer/client, and managers. \n  Proficiency in the use of personal computers and related peripheral equipment is required. Familiarity with Microsoft Office products and Microsoft Project is preferred. \n  Structure and develop project material to meet client expectations, deliver value and minimize potential obstacles and risks. Understand and manage stakeholder needs and build commitment to project solutions that deliver target business value. \n  Highly effective verbal and written communication skills are necessary in order to work with all levels within the organization, and produce clear and concise reports of relatively complicated issues. \n  Ability to communicate effectively to direct, influence and motivate large size groups of functional mid-management or project team members, maintaining a persuasive and credible presentation style at all levels of the organization. \n \n  Additional Information\n   All your information will be kept confidential according to EEO guidelines.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "87ab9c0e4a9af36f": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 83040.26,
        "salary_max": 105147.48,
        "title": "Power BI Analyst",
        "company": "BGIS",
        "desc": "BGIS is currently seeking a  BI Analyst  to join the team in  Pompano Beach, FL and surrounding areas  .  \n BGIS is a leading provider of integrated real estate management services, including facilities management, project management, energy & sustainability services, strategic workplace consulting, real estate services and capital planning. Its range of solutions, supported by efficient systems, processes, and people, enables it to create and optimize places that work for its clients. The team is dedicated to inspiring better business performance and helping clients focus on their core businesses.  \n The Strategic Information Manager is responsible providing business support in areas including but not limited to service delivery governance, service level performance measurement, management of account level change process, identifying and developing value creation opportunities for the client.  \n RESPONSIBILITIES  \n Governance  \n \n Participates in client meetings to discuss client requirements. Develops and recommends solutions for client requirements. Facilitates the execution of client requirements with the account or product line team and the client  \n Audits account and product line team\u2019s work execution to ensure compliance with factors including but not limited to internal processes and service delivery obligations  \n Collaborates with relevant stakeholders to ensure data integrity of operations data within applicable databases for assigned account  \n Identifies solutions to implement changes in internal processes and technologies for the assigned account. Collaborates with the account and product line team to implement changes  \n \n Service Level Performance Measurement  \n \n Collaborates with the client and account or product line leaders to identify and establish performance metrics, baseline levels and improvement targets  \n Implements and communicates performance metrics, expectations and requirements to account and product line team  \n Compiles, monitors, measures, analyzes and reports performance results  \n Conducts root cause analysis and identifies business issues contributing to performance gaps  \n Recommends solutions to remediate performance gaps and resolve business issues  \n Collaborates with account and product line leaders to develop action plans to remediate performance gaps for and resolving business issues  \n Monitors action plan for progress  \n \n Management of Account Level Change Process  \n \n Provides business support for account level changes to meet client requirements by identifying impact and process changes  \n Conducts analysis including but not limited to process analysis and service delivery impact  \n Collaborates with relevant stakeholders to amend masters services agreement  \n \n Value Analysis  \n \n Conducts value analysis, identifies, develops and recommends value creation and efficiency opportunities based on findings as well as understanding of clients\u2019 business strategy and needs  \n Creates and submits business cases for account and product line leader approval  \n Participates in client meetings with the account and product line leader to obtain approval of recommendation  \n Develops roadmaps to achieve client requirements for greater value creation. Develops and recommends options to support the implementation of roadmaps  \n \n Process Improvement & Strategic Initiatives  \n \n Supports process improvement efforts. Conducts business process audits and analysis to determine process effectiveness and to identify areas for improvement. Participates in the review, development and refinement of business processes. Leads the implementation, facilitates the adoption of processes and monitors for compliance for the assigned account  \n Supports strategic initiatives by conducting value analysis, recommending solutions and developing plans to achieve desired results. Monitors the execution of strategic initiative plans to ensure proper execution and achievement of desired outcomes  \n Supports innovation initiatives through activities including but not limited to maintaining current awareness of industry trends and practices, conducting analysis, utilizing out-of-box thinking to develop and recommend innovative solutions  \n Participates in Strategic Information Management Council to share best practices. Leverages and implements best practices within assigned account  \n \n Reporting and Analysis  \n \n Executes and delivers client reporting and analytical requirements as well as reporting and analysis for quarterly business reviews  \n Other duties as assigned  \n \n Required Education, Knowledge, and Abilities  \n \n University graduation in business administration  \n More than five years of strategic analysis, business analysis or continuous improvement work experience  \n Prior dashboard design and development experience with Power BI or similar Business Intelligence software  \n Exceptional business acumen  \n Expert level business and strategic analytical abilities  \n Creative, innovative, out-of-the-box thinker  \n Exceptional problem solving and solution identification and development abilities  \n Exceptional organizational, multi-tasking abilities  \n Expert level ability to influence and persuade without authority  \n Expert ability to establish strong partnerships with the client representatives at the senior management to executive management level  \n Exceptional client service orientation  \n Exceptional interpersonal skills  \n Exceptional verbal and written communication skills  \n Exceptional collaborative skills  \n Licenses and/or Professional Accreditation  \n Ability to travel 15% of the time  \n \n Visit us online at https://www.bgis.com/us/careers/ for more information.  \n Our company culture includes a robust mix of sound business practices and employee initiatives that promote personal and professional development, work/life balance, health and wellness and community involvement.  \n The Company is an equal opportunity employer. We believe every employee has the right to work in surroundings that are free from all forms of unlawful discrimination. We are committed to providing equal employment opportunity to all employees and applicants without regard to race, color, religion, gender, national origin, age, disability, ancestry, creed, marital status, sexual orientation, or Veteran or military status, genetic information or any other basis prohibited by local, state or federal law in the relevant jurisdiction. This policy applies to all terms and conditions of employment including, but not limited to employment, advancement, assignment, and training.  \n BGIS is committed to strengthening our diversity through recruiting and retaining minority and women professionals from all backgrounds. Our commitment is consistent with our recognition that it is the outstanding people within BGIS who have always been the source of our strength. We recognize that promoting diversity is an integral component of our continuing quest for organizational excellence.  \n This commitment to Equal Employment Opportunity is made equally as a social responsibility and as an economic and business necessity.  \n Anyone with questions or concerns regarding Equal Employment Opportunity should contact their direct supervisor or the Human Resources Department without fear of retaliation of any kind.  \n #LI-Remote  \n #LI-KBBGIS",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "88d5223ee7a36170": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 83040.26,
        "salary_max": 105147.48,
        "title": "Power BI Analyst",
        "company": "BGIS",
        "desc": "BGIS is currently seeking a  BI Analyst  to join the team in  Pompano Beach, FL and surrounding areas  .  \n BGIS is a leading provider of integrated real estate management services, including facilities management, project management, energy & sustainability services, strategic workplace consulting, real estate services and capital planning. Its range of solutions, supported by efficient systems, processes, and people, enables it to create and optimize places that work for its clients. The team is dedicated to inspiring better business performance and helping clients focus on their core businesses.  \n The Strategic Information Manager is responsible providing business support in areas including but not limited to service delivery governance, service level performance measurement, management of account level change process, identifying and developing value creation opportunities for the client.  \n RESPONSIBILITIES  \n Governance  \n \n Participates in client meetings to discuss client requirements. Develops and recommends solutions for client requirements. Facilitates the execution of client requirements with the account or product line team and the client  \n Audits account and product line team\u2019s work execution to ensure compliance with factors including but not limited to internal processes and service delivery obligations  \n Collaborates with relevant stakeholders to ensure data integrity of operations data within applicable databases for assigned account  \n Identifies solutions to implement changes in internal processes and technologies for the assigned account. Collaborates with the account and product line team to implement changes  \n \n Service Level Performance Measurement  \n \n Collaborates with the client and account or product line leaders to identify and establish performance metrics, baseline levels and improvement targets  \n Implements and communicates performance metrics, expectations and requirements to account and product line team  \n Compiles, monitors, measures, analyzes and reports performance results  \n Conducts root cause analysis and identifies business issues contributing to performance gaps  \n Recommends solutions to remediate performance gaps and resolve business issues  \n Collaborates with account and product line leaders to develop action plans to remediate performance gaps for and resolving business issues  \n Monitors action plan for progress  \n \n Management of Account Level Change Process  \n \n Provides business support for account level changes to meet client requirements by identifying impact and process changes  \n Conducts analysis including but not limited to process analysis and service delivery impact  \n Collaborates with relevant stakeholders to amend masters services agreement  \n \n Value Analysis  \n \n Conducts value analysis, identifies, develops and recommends value creation and efficiency opportunities based on findings as well as understanding of clients\u2019 business strategy and needs  \n Creates and submits business cases for account and product line leader approval  \n Participates in client meetings with the account and product line leader to obtain approval of recommendation  \n Develops roadmaps to achieve client requirements for greater value creation. Develops and recommends options to support the implementation of roadmaps  \n \n Process Improvement & Strategic Initiatives  \n \n Supports process improvement efforts. Conducts business process audits and analysis to determine process effectiveness and to identify areas for improvement. Participates in the review, development and refinement of business processes. Leads the implementation, facilitates the adoption of processes and monitors for compliance for the assigned account  \n Supports strategic initiatives by conducting value analysis, recommending solutions and developing plans to achieve desired results. Monitors the execution of strategic initiative plans to ensure proper execution and achievement of desired outcomes  \n Supports innovation initiatives through activities including but not limited to maintaining current awareness of industry trends and practices, conducting analysis, utilizing out-of-box thinking to develop and recommend innovative solutions  \n Participates in Strategic Information Management Council to share best practices. Leverages and implements best practices within assigned account  \n \n Reporting and Analysis  \n \n Executes and delivers client reporting and analytical requirements as well as reporting and analysis for quarterly business reviews  \n Other duties as assigned  \n \n Required Education, Knowledge, and Abilities  \n \n University graduation in business administration  \n More than five years of strategic analysis, business analysis or continuous improvement work experience  \n Prior dashboard design and development experience with Power BI or similar Business Intelligence software  \n Exceptional business acumen  \n Expert level business and strategic analytical abilities  \n Creative, innovative, out-of-the-box thinker  \n Exceptional problem solving and solution identification and development abilities  \n Exceptional organizational, multi-tasking abilities  \n Expert level ability to influence and persuade without authority  \n Expert ability to establish strong partnerships with the client representatives at the senior management to executive management level  \n Exceptional client service orientation  \n Exceptional interpersonal skills  \n Exceptional verbal and written communication skills  \n Exceptional collaborative skills  \n Licenses and/or Professional Accreditation  \n Ability to travel 15% of the time  \n \n Visit us online at https://www.bgis.com/us/careers/ for more information.  \n Our company culture includes a robust mix of sound business practices and employee initiatives that promote personal and professional development, work/life balance, health and wellness and community involvement.  \n The Company is an equal opportunity employer. We believe every employee has the right to work in surroundings that are free from all forms of unlawful discrimination. We are committed to providing equal employment opportunity to all employees and applicants without regard to race, color, religion, gender, national origin, age, disability, ancestry, creed, marital status, sexual orientation, or Veteran or military status, genetic information or any other basis prohibited by local, state or federal law in the relevant jurisdiction. This policy applies to all terms and conditions of employment including, but not limited to employment, advancement, assignment, and training.  \n BGIS is committed to strengthening our diversity through recruiting and retaining minority and women professionals from all backgrounds. Our commitment is consistent with our recognition that it is the outstanding people within BGIS who have always been the source of our strength. We recognize that promoting diversity is an integral component of our continuing quest for organizational excellence.  \n This commitment to Equal Employment Opportunity is made equally as a social responsibility and as an economic and business necessity.  \n Anyone with questions or concerns regarding Equal Employment Opportunity should contact their direct supervisor or the Human Resources Department without fear of retaliation of any kind.  \n #LI-Remote  \n #LI-KBBGIS",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "ee2df8c0730eea10": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 73133.555,
        "salary_max": 92603.38,
        "title": "Business Analyst 3 (Remote)",
        "company": "WTW",
        "desc": "Business Analyst 3 (Remote)\n   \n \n \n    New Jersey, United States\n   \n \n \n  October 16, 2023 \n \n \n \n \n The Role \n \n Learns and applies WTW best practices, standards, and tools \n Gathers requirements from client/vendors and consults them to develop the most effective solution based on their business needs and WTW\u2019s best practices \n Performs system configuration based on the requirements \n Translates complex client business requirements into functional and technical specification documents \n Executes test plans for system functionalities \n Creates test plans based on requirements \n Coaches and mentors junior business analysts and peer review their work \n Writes queries using SQL and analyzes results to reduce rework \n Follows the System Development Lifecycle. \n Understand the big picture and partners with shared services as needed to help with issue resolution \n \n The Requirements \n \n 2-4 years\u2019 experience as a Business Analyst \n Preferred understanding of the fundamentals of health and welfare and broader benefit plan design \n Intermediate SQL skills \n Experience with test plans creation, testing procedures, and test plan execution. \n Analytical, critical-thinking, and problem-solving skills \n Organizational and time management skills \n Demonstrates flexibility, accountability, and ability to deliver multiple projects with quality and excellence. \n College degree required. \n Ability to work extended hours as needed. \n \n Compensation and Benefits \n  Base salary range and benefits information for this position are being included in accordance with requirements of various state/local pay transparency legislation. Please note that salaries may vary for different individuals in the same role based on several factors, including but not limited to location of the role, individual competencies, education/professional certifications, qualifications/experience, performance in the role and potential for revenue generation (Producer roles only). \n  Compensation \n  The base salary compensation range being offered for this role is 82,000 \u2013 95,000 USD. \n  This role is also eligible for an annual short-term incentive bonus. \n \n \n  Company Benefits \n  WTW provides a competitive benefit package which includes the following (eligibility requirements apply): \n \n Health and Welfare Benefits:  Medical (including prescription coverage), Dental, Vision, Health Savings Account, Commuter Account, Health Care and Dependent Care Flexible Spending Accounts, Group Accident, Group Critical Illness, Life Insurance, AD&D, Group Legal, Identify Theft Protection, Wellbeing Program and Work/Life Resources (including Employee Assistance Program) \n Leave Benefits:  Paid Holidays, Annual Paid Time Off (includes paid state/local paid leave where required), Short-Term Disability, Long-Term Disability, Other Leaves (e.g., Bereavement, FMLA, ADA, Jury Duty, Military Leave, and Parental and Adoption Leave), Paid Time Off (Washington State only) \n Retirement Benefits:  Contributory Pension Plan and Savings Plan (401k). All Level 38 and more senior roles may also be eligible for non-qualified Deferred Compensation and Deferred Savings Plans. If Level 38 or more senior role is in Washington State, you must add the retirement benefits paragraph to the job description. \n \n At WTW, we trust you to know your work and the people, tools and environment you need to be successful. The majority of our colleagues work in a \u201dhybrid\u201d style, with a mix of remote, in-person and in-office interactions dependent on the needs of the team, role and clients. Our flexibility is rooted in trust and \u201chybrid\u201d is not a one-size-fits-all solution. \n  We understand flexibility is key to supporting an inclusive and diverse workforce and so we encourage requests for all types of flexible working as well as location-based arrangements. Please speak to your recruiter to discuss more.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "9c49301f7b3746e8": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 98975.914,
        "salary_max": 125325.57,
        "title": "Technical Business Analyst",
        "company": "Accolite",
        "desc": "Join a team of the top 1%. \n \n  We believe in attracting and retaining the brightest technical minds from diverse backgrounds and nationalities to come together and create an enriched pool of global talent. Recognized as one of North America's fastest-growing companies, we aim to attract people who are passionate about technology and creating a real impact while working in collaborative environment with the latest technologies. \n \n  Our mission is to solve our client's most complex digital challenges by engaging the brightest of technical minds. We are looking for candidates who thrive in an innovative and collaborative environment who love solving problems and having fun while doing it. \n \n \n Position Responsibilities include: \n \n \n Provide analysis, technical design and planning activities for business and technology initiatives: \n  o Determine and define project scope and objectives \n o Perform and document requirements gathering and analysis \n o Design high level technical solutions including frontends, databases and interfaces \n o Facilitate business process changes \n o Manage the product development team to create a strong end product. \n o Develop and implement test plans to ensure successful delivery \n o Schedule and lead meetings to identify and correct issues \n o Provide project updates on a consistent basis to various stakeholders about strategy, adjustments, and progress \n o Prepare solution for deployment including support staffing, documentation, maintenance procedures and training \n \n \n Small to medium size projects \n  o Predict resources needed to reach objectives and manage resources in an effective and efficient manner \n o Prepare project budget and track costs \n o Develop and manage a detailed project schedule and work plan \n \n \n Partner with cross-functional users to maintain and develop high quality solutions in a timely and cost-effective manner \n Ability to work/meet during flexible hours. This person will need to coordinate with international users across different time zones (USA, Europe and Asia). \n Collaborate with internal Technology resources to drive enterprise-wide strategies, procedures, and standards \n  Qualifications/Experience/Education \n \n \n Bachelor\u2019s Degree or higher in Information Science or related field is preferred. Equivalent combination of experience, education and training will be considered \n 7 or more years of experience as product owner or business analyst of software development project through the full life cycle required \n Product Owner and/or Certified Business Analysis Professional (CBAP) certification or equivalent is preferred. \n Knowledge of Insurance or Re-insurance domain preferred \n Knowledge of SQL and relational databases is preferred. \n Proven ability to complete projects according to outlined scope, budget, and timeline. \n Strong working knowledge of SDLC and Agile development principles and processes. \n Demonstrated ability to translate complex technical terminology, concepts and issues in terms understandable. to technical and non-technical management and resource staff. \n Ability to work collaboratively throughout the organization by establishing effective working relationships. \n Creative and innovative thinker. \n Strong analytical and problem-solving skills. \n Excellent verbal and written communication skills and interpersonal skills. \n  Great teams do amazing things, but truly diverse teams are the ones who can achieve extraordinary results, together. You can build your dream career at Accolite in a culture of learning, inclusion, and openness. We encourage applications from everyone regardless of race, religion, color, national origin, gender, sexual orientation, age, marital status, or disability status. \n \n  Is this you? If so, come join us to transform the future with the top tech talent to solve real-world business problems.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "1c3b40c7e0346caf": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 90.0,
        "salary_max": 91.0,
        "title": "Senior Technical Business Analyst",
        "company": "Asset Staffing",
        "desc": "Major financial firm seeks long term remote SENIOR TECHNICAL BUSINESS ANALYST for long term contract role: \n THIS IS NOT AN I.T. ROLE \n You will work with the business and technology teams on solutions from inception through ongoing maintenance. This role provides analysis support from ideation to implementation of new technology and processes, including workflow, operational process, systems and data flow design. This position will lead business and functional requirements gathering, analysis and documentation, including workflows, wireframes, user stories, acceptance criteria, data mapping, data flows, assistance with user testing and where applicable, processes and procedures documentation. \n Communicate effectively to peers and business partners to include strong questioning and listening skills and the ability to look beyond obvious answers to understand impacts. Interview business counterparts to effectively draw out and articulate the business need/problem/ opportunity to be addressed. Provide scaled options for implementation based on the business need. Gather and analyze requirements. Organize and document business needs to enable design and development, including elicitation and documentation of current state and future state workflows, user stories, acceptance criteria, data mapping and UI wireframes. Elicit and document processes and procedures, as well as prepare training and adoption materials. Lead and influence cross-functional teams Provide consultation to development team and product owner for business facing application enhancements. Create test cases, coordinate user acceptance testing and verify production completeness \n REQUIREMENTS \n Bachelor\u2019s degree required/Master\u2019s degree preferred \n A minimum of 8-10 years of experience in Business Analysis \n Strong experience in requirements gathering, documentation and workflow diagramming \n Experience leading and directing others to achieve measurable results \n Demonstrated ability to articulate customized solutions to meet business requirements Advanced experience working with Agile/Lean/Kanban software development processes Advanced skills using a variety of tools to document workflows and data mapping including. but not limited to, Excel, SQL and Visio Understanding of analysis and documentation goals throughout the project lifecycle Strong domain knowledge in finance/asset management Ability to successfully manage and coordinate simultaneous project deliverables across groups and teams Strong experience with SQL, Excel, Visio, Balsamiq, Jira, Tableau, Agile \n Job Types: Temporary, Contract \n Pay: $90.00 - $91.00 per hour \n Benefits: \n \n Health insurance \n \n Experience level: \n \n 10 years \n 11+ years \n 8 years \n 9 years \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Education: \n \n Bachelor's (Required) \n \n Experience: \n \n Business analysis: 8 years (Required) \n SQL: 6 years (Required) \n Visio: 6 years (Required) \n Jira: 6 years (Required) \n Tableau: 5 years (Required) \n Agile: 6 years (Required) \n Lean: 6 years (Required) \n Kanban: 5 years (Required) \n Balsamiq: 5 years (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": " Experience leading and directing others to achieve measurable results \n Demonstrated ability to articulate customized solutions to meet business requirements Advanced experience working with Agile/Lean/Kanban software development processes Advanced skills using a variety of tools to document workflows and data mapping including. but not limited to, Excel, SQL and Visio Understanding of analysis and documentation goals throughout the project lifecycle Strong domain knowledge in finance/asset management Ability to successfully manage and coordinate simultaneous project deliverables across groups and teams Strong experience with SQL, Excel, Visio, Balsamiq, Jira, Tableau, Agile \n Job Types: Temporary, Contract \n Pay: $90.00 - $91.00 per hour \n Benefits: \n \n Health insurance \n ",
        "techs": [
            "excel",
            "sql",
            "visio",
            "balsamiq",
            "jira",
            "tableau",
            "agile"
        ],
        "cleaned_techs": [
            "excel",
            "sql",
            "visio",
            "balsamiq",
            "jira",
            "tableau",
            "agile"
        ]
    },
    "490a43bc427e7c0c": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 34.0,
        "salary_max": 36.0,
        "title": "Clinical Systems Analyst",
        "company": "RevereIT",
        "desc": "Role: Clinical Informatics Systems Analyst \n Location: Abbott Park, IL - 60064 (Remote- 75% travel, with occasional weekend work, dependent on business need) \n Duration: 12 Months \n Shift Timings: 8:00am to 5:00pm \n Key Skills:  LIS, Middleware, Server and Networking, auto verification. Sound understanding of diagnostics laboratory workflows and related technologies \n Experience: \n \u00b7 2-3 years of experience Lim experience (good) but need LIS experience / Hospital setting/research background/ Middleware work experience, Instrumentation, auto verification/ lab workflow/ Hospital setting with Networking experiencing. \n \u00b7 Hands on experience implementing, configuring, training, and supporting one or more laboratory information systems mentioned above is highly desirable. \n Qualification: \n \u00b7 Education in Computer Science, Information Systems, Medical or Hospital Informatics or related disciplines \n \u00b7 1-3 years of experience preferred installing, configuring and/ or maintaining diagnostics laboratory informatics applications or relevant comparable experience. \n Special Skills & Knowledge: \n \u00b7 Sound understanding of diagnostics laboratory workflows and related technologies \n \u00b7 Demonstrated understanding of one or more products such as laboratory information systems, middleware and/or analyzer management systems, or inventory management systems used in diagnostics laboratories \n \u00b7 Hands on experience implementing, configuring, training and supporting one or more laboratory information systems mentioned above is highly desirable \n \u00b7 Able to execute assigned tasks satisfactorily with a high degree of commitment and quality, independently \n \u00b7 Technical proficiency with general business applications such as MS Office, MS Project/ SmartSheet.Responsibilities: - Collect and analyze data from various sources using data mining techniques - Design and develop systems to support data analysis and reporting - Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions - Perform data analysis to identify trends, patterns, and insights - Develop and maintain SQL queries and scripts for data extraction and manipulation - Conduct system design and testing activities to ensure data integrity and accuracy - Provide technical support and troubleshooting for system-related issues - Stay up-to-date with industry trends and advancements in data analysis techniques \n Qualifications: - Bachelor's degree in Computer Science, Information Systems, or a related field - Proven experience in data analysis, preferably in the healthcare or managed care industry - Strong knowledge of data mining techniques, SQL, and database management systems - Familiarity with Epic or other electronic health record systems is a plus - Experience in clinical trials or bioinformatics is highly desirable - Excellent analytical skills with the ability to interpret complex data sets - Strong problem-solving abilities and attention to detail - Effective communication skills to collaborate with stakeholders at all levels \n Note: This position requires strong technical skills in data analysis and system design. The ideal candidate should have a solid understanding of healthcare industry processes and terminology. \n Job Type: Contract \n Salary: $34.00 - $36.00 per hour \n Expected hours: 40 per week \n Schedule: \n \n Day shift \n Monday to Friday \n \n Experience: \n \n Server: 1 year (Required) \n Laboratory: 1 year (Required) \n Networking: 1 year (Required) \n Middleware: 1 year (Required) \n LIS: 1 year (Required) \n \n Work Location: Remote",
        "cleaned_desc": " \u00b7 Hands on experience implementing, configuring, training and supporting one or more laboratory information systems mentioned above is highly desirable \n \u00b7 Able to execute assigned tasks satisfactorily with a high degree of commitment and quality, independently \n \u00b7 Technical proficiency with general business applications such as MS Office, MS Project/ SmartSheet.Responsibilities: - Collect and analyze data from various sources using data mining techniques - Design and develop systems to support data analysis and reporting - Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions - Perform data analysis to identify trends, patterns, and insights - Develop and maintain SQL queries and scripts for data extraction and manipulation - Conduct system design and testing activities to ensure data integrity and accuracy - Provide technical support and troubleshooting for system-related issues - Stay up-to-date with industry trends and advancements in data analysis techniques \n Qualifications: - Bachelor's degree in Computer Science, Information Systems, or a related field - Proven experience in data analysis, preferably in the healthcare or managed care industry - Strong knowledge of data mining techniques, SQL, and database management systems - Familiarity with Epic or other electronic health record systems is a plus - Experience in clinical trials or bioinformatics is highly desirable - Excellent analytical skills with the ability to interpret complex data sets - Strong problem-solving abilities and attention to detail - Effective communication skills to collaborate with stakeholders at all levels \n Note: This position requires strong technical skills in data analysis and system design. The ideal candidate should have a solid understanding of healthcare industry processes and terminology. \n Job Type: Contract \n Salary: $34.00 - $36.00 per hour ",
        "techs": [
            "ms office",
            "ms project/smartsheet",
            "sql",
            "database management systems",
            "epic",
            "electronic health record systems",
            "data analysis techniques"
        ],
        "cleaned_techs": [
            "microsoft",
            "ms project/smartsheet",
            "sql",
            "database management systems",
            "epic",
            "electronic health record systems",
            "data analysis techniques"
        ]
    },
    "6b3665297474824d": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 79865.98,
        "salary_max": 101128.12,
        "title": "Senior Business Analyst",
        "company": "Telarus",
        "desc": "Heaquarters is located in Sandy, Utah however this position is remote and can be worked from anywhere in the U.S. \n \n \n  About Telarus \n  Telarus is a Technology Solutions Brokerage that holds contracts with the world's leading cloud voice, contact center, cybersecurity, mobility and IoT providers. We make it easier and more profitable for trusted advisors to sell and source UCaaS, Cloud, Mobility, IoT, Cybersecurity, and Contact Center solutions. For its part, Telarus has been named the top Technology Solutions Brokerage by the members of the Telecom Association in each of the past three years, the first and only company in the carrier channel to accomplish this feat. \n  Telarus was recently named to the Inc. 5000 for the seventh year in a row! Not only does this place us in an elite group of companies, but it also validates our strategy where we reinvest significant dollars back into the business, building software and hiring specialty practice leaders that make our technology advisors better, faster, and more efficient than they would be on their own or with another competitor. \n \n  About the Role \n  Are you a numbers expert who enjoys digging into data to pinpoint trends? Join Telarus as a Senior Business Analyst to translate data into actionable recommendations to Telarus leadership as our company continues to grow. You will be responsible for operationalizing the data and turning data into insights. \n \n Provide technical, tradecraft, and subject matter expertise as well as hands-on operational and analytic support to stakeholders. \n Focus is gathering data from various systems/sources to perform operational and performance analysis, trending, forecasting, and business impact. You\u2019ll review data programs and initiatives that support and enhance the Company\u00b4s commitment to customers. It\u2019ll be a highly collaborative team environment. \n    \n Partner with Dir. Data Analytics to ensure that the use of business intelligence applications enhances business operations decision-making capabilities. \n      \n Define data needs and execute solutions to enable teams to make better business decisions. \n \n Responsible for assisting in the development of business cases, operational process standardization, reporting automation and creation of operations executive dashboards via BI Tools. Successful candidates will possess strong analytical, forecasting, and automation skills with expertise in program benefit assessment. \n Serve as a liaison between business operations and the Finance/Sales organization, analyzing, reporting, and interpreting operational data to assist with informed decision making. \n      \n Investigate the underlying drivers for the company\u2019s performance and interact with management on a regular basis to deliver operational updates and assist operational leadership in managing budgets and forecasts. \n Drive the enhancement of operational tools and processes to enhance operating performance and maintain an efficient process to analyze and report on various trends. \n Access data from a variety of systems and present accurate, meaningful, and relevant information to diverse audiences. \n \n You understand the business objectives and work with various operations departments in designing and implementing improvement programs (I.e. knowledge bases, process optimization, etc.), all in accordance with supplied standards and best practices, and complete accurate and timely documentation. \n      \n Work with stakeholders to analyze operations and recommend solutions that align business functions with organizational goals. \n Compile, analyze, interpret and present complex data to propose new strategies for current and future business changes. \n Develop project improvement scopes by identifying phases, elements, personnel requirements and costs. \n Assess trends and develop actionable insights based on an analysis of external market dynamics and other data sources. \n \n   \n \n \n \n Required Qualifications \n \n \n \n  4+ years of experience in analysis, development and implementation of strategy and business processes at a department or enterprise level \n 2+ years of Business Analyst experience with domain expertise \n Prior consulting experience preferred (Big 4, MBB, etc) \n Expertise with business operations and management tools and frameworks \n Experience with data analysis tools and techniques. \n Knowledge of financial modeling. \n Ability to work independently in a collaborative, dynamic team environment. \n High degree of initiative and capacity to lead multiple priorities of significant scope in a fast-paced environment \n Problem solving mindset and solution-enabler; ability to identify problems, make decisions and drive innovative solutions \n Understanding of the Telarus business model and the telecommunications industry \n Technical skills: Proficient in Microsoft Office Suite (specifically Excel and PowerPoint) \n Education: Bachelor\u2019s degree in business or a related field. \n Skills: Expert in analyzing and interpreting data for multiple projects simultaneously; Effective prioritization, and organizational skills. \n \n \n \n \n Heaquarters is located in Sandy, Utah however this position is remote and can be worked from anywhere in the U.S.",
        "cleaned_desc": " \n \n Required Qualifications \n \n \n \n  4+ years of experience in analysis, development and implementation of strategy and business processes at a department or enterprise level \n 2+ years of Business Analyst experience with domain expertise \n Prior consulting experience preferred (Big 4, MBB, etc) \n Expertise with business operations and management tools and frameworks \n Experience with data analysis tools and techniques.   Knowledge of financial modeling. \n Ability to work independently in a collaborative, dynamic team environment. \n High degree of initiative and capacity to lead multiple priorities of significant scope in a fast-paced environment \n Problem solving mindset and solution-enabler; ability to identify problems, make decisions and drive innovative solutions \n Understanding of the Telarus business model and the telecommunications industry \n Technical skills: Proficient in Microsoft Office Suite (specifically Excel and PowerPoint) \n Education: Bachelor\u2019s degree in business or a related field. \n Skills: Expert in analyzing and interpreting data for multiple projects simultaneously; Effective prioritization, and organizational skills. \n \n \n ",
        "techs": [
            "microsoft office suite (specifically excel and powerpoint)"
        ],
        "cleaned_techs": [
            "microsoft"
        ]
    },
    "04f1060db5c62ba5": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 75157.0,
        "salary_max": 127767.0,
        "title": "Senior ServiceNow Business Analyst - Remote",
        "company": "ICF",
        "desc": "*We are open to supporting 100% remote work anywhere within the U.S.* \n  \n \n \n  ICF\u2019s IT Modernization division is a rapidly growing, entrepreneurial, technology-driven department, seeking a motivated Senior ServiceNow Business Analyst to support the upcoming needs of our customers.\n  \n \n \n \n \n  The Work\n  \n \n   The Senior ServiceNow Business Analyst serves as a translator between technical teams and the client\u2019s business community to collect, clarify, analyze, and translate business requirements into documentation and conceptual design from which applications and solutions are developed. This position is within the context of an Agile team employing the Scrum development framework. The Business Analyst defines detailed stories and epics, with applicable acceptance criteria in collaboration with product owners and ICF\u2019s technology team and facilitates meetings with client and internal technical and operational teams. This position will be working directly with stakeholders and must be able to communicate effectively via phone and web conferencing as many stakeholders work remotely.\n  \n \n \n \n \n  In addition to supporting all phases of the project, this position will also be responsible for authoring content and peer reviewing a wide array of documents, including functional, technical, training as well as marketing, and proposals.\n  \n \n \n   Based on your experiences and interests, we may ask you as a technology professional to support growth-related activities, including (but not limited to) RFI, RFP, prototypes, and oral presentations. Team members are also expected to uphold and maintain appropriate certifications necessary for their practice expertise.\n  \n \n \n \n \n  Minimum Job Requirements:\n  \n \n  5 or more years of experience as a Business Analyst in a fast-paced application development environment \n  3 or more years of recent experience working in an Agile development environment as a business analyst (i.e., Scrum, Kanban, etc.) \n  1 or more years of understanding of the full software development lifecycle (SDLC) including recent successful and demonstrated experience with Agile methodologies (Scrum) is expected. \n  1 or more years of experience with the ServiceNow Platform \n  3 or more years of experience facilitating requirements gathering, Joint Application Design (JAD) sessions, and capturing client requirements and feedback. \n  US Citizenship is required (required by the federal government for this position). \n  Must be able to obtain Public Trust clearance. \n  MUST RESIDE IN THE United States (U.S.) and the work MUST BE PERFORMED in the United States (U.S.), as this work is for a federal contract and laws do apply. \n \n \n \n \n \n  Desired Skills:\n  \n \n  Bachelor\u2019s Degree \n  Excellent oral and written communication skills  \n Solid understanding of various software development cycles (e.g., Agile, Waterfall, etc.); knowledge of requirements management, and configuration management methodologies, along with corresponding support tools, i.e., JIRA, etc. \n  Conceptual understanding of Object-Oriented enterprise software system development processes, methodologies, as well as major technologies (such as Java and .Net), and approaches (such as modularity and SOA) Skilled at analyzing existing system documentation to summarize existing system functionality as it relates to the project at hand. \n  Good understanding of basic system technologies as they relate to the project deliverables. \n  Experience with business process mapping and the use of project management software Ability to provide technical assistance and troubleshooting by effectively responding to inquiries. \n \n \n \n \n \n  #DMD\n  \n \n   #Indeed\n  \n \n   #LI-CC1\n  \n \n   #SENW22\n  \n \n   #YRU23\n  \n \n \n   Working at ICF\n  \n  ICF is a global advisory and technology services provider, but we\u2019re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.\n  \n \n   We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our \n   \n   EEO & AA policy\n   .\n  \n \n \n   Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email \n   \n   icfcareercenter@icf.com\n    and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: \n   \n   Know Your Rights\n    and \n   \n   Pay Transparency Statement.\n   \n \n \n \n  Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:\n   $75,157.00 - $127,767.00\n   Nationwide Remote Office (US99)",
        "cleaned_desc": " \n \n \n \n \n  Desired Skills:\n  \n \n  Bachelor\u2019s Degree \n  Excellent oral and written communication skills  \n Solid understanding of various software development cycles (e.g., Agile, Waterfall, etc.); knowledge of requirements management, and configuration management methodologies, along with corresponding support tools, i.e., JIRA, etc. \n  Conceptual understanding of Object-Oriented enterprise software system development processes, methodologies, as well as major technologies (such as Java and .Net), and approaches (such as modularity and SOA) Skilled at analyzing existing system documentation to summarize existing system functionality as it relates to the project at hand. \n  Good understanding of basic system technologies as they relate to the project deliverables. \n  Experience with business process mapping and the use of project management software Ability to provide technical assistance and troubleshooting by effectively responding to inquiries. \n \n \n \n \n \n  #DMD",
        "techs": [
            "jira"
        ],
        "cleaned_techs": [
            "jira"
        ]
    },
    "a57f976ef18b1baf": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 62321.977,
        "salary_max": 78913.51,
        "title": "Mortgage Business Analyst",
        "company": "Randolph-Brooks Federal Credit Union",
        "desc": "Job Description and Requirements\n  \n \n \n   We are currently seeking a talented Mortgage Business Analyst to join our growing team. This will be a work from home position. All applicants must reside within the State of Texas and have the capability of performing all of the work from their home in Texas.\n  \n \n \n   The Mortgage Business Analyst is an experienced professional who works directly with Business user groups and management to establish business and system processes to ensure efficient usage of systems and to participate in the implementation of new products, processes, and general project execution. The Mortgage Business Analyst will help define requirements, document business needs, work with technology to configure systems according to specifications, and test enhancements from a Business user perspective.\n  \n \n \n   Essential Functions and Responsibilities:\n  \n \n \n \n     Work closely with Information Technology, Legal & Compliance, and Credit Administration (should possess a good working knowledge around these areas).\n    \n \n \n     Conduct data analytics.\n    \n \n \n     Gather, define, and document complex business requirements and processes.\n    \n \n \n     Participate in project work and change management, offering Mortgage Lending subject matter expertise.\n    \n \n \n     Prepare, review, and present status reports and Business summary presentations to Senior Executives.\n    \n \n \n     Effectively communicate project information to business stakeholders and management.\n    \n \n \n     Create and/or manage project plans, supporting Project Managers.\n    \n \n \n     Facilitate cross-functional team meetings to define project requirements, update teams about the progress of projects, and resolve any issues that may arise.\n    \n \n \n     When appropriate work as a team leader, providing support and guidance for less experienced team members.\n    \n \n \n     Train and support business users during new system implementations or upgrades, as well as roll out new processes.\n    \n \n \n     Lead UAT efforts for LOS or other software releases as necessary.\n    \n \n \n     All other duties as assigned (note: essential functions and responsibilities may change or new ones may be assigned at any time with or without notice).\n    \n \n \n \n   Requirements:\n  \n \n \n \n     High School Diploma or GED\n    \n \n \n     3 - 5 years of mortgage lending experience\n    \n \n \n     Demonstrated success in leading and implementing projects in a cross functional team environment\n    \n \n \n     Superior interpersonal, conflict resolution, and negotiating skills that aid in ultimately defining solutions that meet the needs of all users involved in a given project\n    \n \n \n     Proficiency in using Outlook, Word, Excel, Visio, Project, Adobe Professional, and PowerPoint\n    \n \n \n     Demonstrated advanced analytical, problem solving, and documentation skills\n    \n \n \n \n   Preferred:\n  \n \n \n \n     BS degree in Information Systems, Business, or Finance\n    \n \n \n     Knowledge of SQL, Data Analytics Software, or other equivalent technologies\n    \n \n \n     Knowledge of advanced Excel and Access tools\n    \n \n \n     PMP or other project management certification\n    \n \n \n     Experience with ICE Encompass, ICE Encompass Product and Pricing Service (EPPS), and Velocify\n    \n \n \n     ICE Encompass Admin Certification\n    \n \n \n \n  All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, or other legally protected status.",
        "cleaned_desc": "     3 - 5 years of mortgage lending experience\n    \n \n \n     Demonstrated success in leading and implementing projects in a cross functional team environment\n    \n \n \n     Superior interpersonal, conflict resolution, and negotiating skills that aid in ultimately defining solutions that meet the needs of all users involved in a given project\n    \n \n \n     Proficiency in using Outlook, Word, Excel, Visio, Project, Adobe Professional, and PowerPoint\n    \n \n \n     Demonstrated advanced analytical, problem solving, and documentation skills\n    \n \n \n \n   Preferred:\n  \n \n   \n     BS degree in Information Systems, Business, or Finance\n    \n \n \n     Knowledge of SQL, Data Analytics Software, or other equivalent technologies\n    \n \n \n     Knowledge of advanced Excel and Access tools\n    \n \n \n     PMP or other project management certification\n    \n \n \n     Experience with ICE Encompass, ICE Encompass Product and Pricing Service (EPPS), and Velocify\n    \n \n \n     ICE Encompass Admin Certification\n    \n \n ",
        "techs": [
            "outlook",
            "word",
            "excel",
            "visio",
            "project",
            "adobe professional",
            "powerpoint",
            "sql",
            "data analytics software",
            "advanced excel",
            "access tools",
            "ice encompass",
            "ice encompass product and pricing service (epps)",
            "velocify",
            "ice encompass admin certification"
        ],
        "cleaned_techs": [
            "outlook",
            "word",
            "excel",
            "visio",
            "project",
            "adobe",
            "powerpoint",
            "sql",
            "data analytics software",
            "advanced excel",
            "access tools",
            "ice encompass",
            "ice encompass product and pricing service (epps)",
            "velocify",
            "ice encompass admin certification"
        ]
    },
    "4b1211d5b58b31b7": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 55600.0,
        "salary_max": 79400.0,
        "title": "SDLC Business Analyst",
        "company": "Gainwell Technologies LLC",
        "desc": "Great companies need great teams to propel their operations. Join the group that solves business challenges and enhances the way we work and grow. Working at Gainwell carries its rewards. You\u2019ll have an incredible opportunity to grow your career in a company that values your contributions and puts a premium on work flexibility, learning, and career development. \n \n \n \n \n  Summary \n \n \n  As a  SDLC Business Analyst  at Gainwell, you can contribute your skills as we harness the power of technology to help our clients improve the health and well-being of the members they serve \u2014 a community\u2019s most vulnerable. Connect your passion with purpose, teaming with people who thrive on finding innovative solutions to some of healthcare\u2019s biggest challenges. Here are the details on this position. \n \n \n \n \n  Your role in our mission \n \n \n  Gainwell empowers you show you\u2019re a pro and help clients deliver better health and human services outcomes using innovative technology and solutions. \n \n  Analyze, plan, design, document and make recommendations to improve business processes to support client\u2019s technological aspirations. \n  Willingness to learn technical aspects of the role in supporting the application. \n  Be a supportive bridge between clients, project managers and technical personnel to define, track and communicate business requirements and their expected impact by building basic conceptual data and process models \n  Create, execute or analyze basic test scenarios to verify that client requirements are built into system design \n  Ensure that \u201cintent of change\u201d is carried out through every project phase by participating in technical reviews and inspections \n \n \n \n \n \n  What we're looking for \n \n \n \n  Three or more years as Business Analyst with experience translating technical goals and requirements into real action for clients. \n  Knowledge of basic computer programming concepts such as configuration, development and batch processing \n  Knowledge in Software Development Life Cycle (SDLC) \n  Knowledge in Structure Query Language (SQL) is a plus but not required. \n  Strong eliciting and written Business requirements, good communicator, have attention to detail to deliver messages to business leaders, clients and technical personnel. \n  An influencer and team player who that motivates others to action and communicates key technical ideas in a digestible way. \n \n \n \n \n \n  What you should expect in this role \n \n \n \n  Fully remote opportunity from most US locations \n  Video cameras must be used during all interviews, as well as during the initial week of orientation. \n  #LI-NR1 \n \n \n \n \n \n  The pay range for this position is $55,600.00 - $79,400.00 per year, however, the base pay offered may vary depending on geographic region, internal equity, job-related knowledge, skills, and experience among other factors. Put your passion to work at Gainwell. You\u2019ll have the opportunity to grow your career in a company that values work flexibility, learning, and career development. All salaried, full-time candidates are eligible for our generous, flexible vacation policy, a 401(k) employer match, comprehensive health benefits, and educational assistance. We also have a variety of leadership and technical development academies to help build your skills and capabilities. \n \n  We believe nothing is impossible when you bring together people who care deeply about making healthcare work better for everyone. Build your career with Gainwell, an industry leader. You\u2019ll be joining a company where collaboration, innovation, and inclusion fuel our growth. Learn more about Gainwell at our company website and visit our Careers site for all available job role openings. \n \n  Gainwell Technologies is committed to a diverse, equitable, and inclusive workplace. We are proud to be an Equal Opportunity Employer, where all qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We celebrate diversity and are dedicated to creating an inclusive environment for all employees.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "34c09a91b2b9785b": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 75157.0,
        "salary_max": 127767.0,
        "title": "Senior Business Analyst (Remote)",
        "company": "ICF",
        "desc": "*This role can be 100% remote, sitting anywhere within the continental US. *\n  \n \n \n   ICF is a rapidly growing, entrepreneurial, multi-faceted consulting company, seeking a Senior Business Analyst to support our Department of Defense (DoD) Child and Youth Programs (CYP) project. ICF works with government representatives and users to build and enhance CYP systems. The DoD recognizes the importance of providing military and DoD-affiliated families with access to quality, affordable childcare programs. Access to childcare directly affects the efficiency, mission readiness, morale, and retention of DoD personnel worldwide.\n  \n \n \n   This Senior BA will participate in building and integrating web-based applications to support our DoD Military Family Readiness project. This Senior Business Analyst will work directly with customers and internal subject matter experts to understand the business goals and objectives of the project, and then translate those findings into actionable specifications for delivery. The Senior Business Analyst will work with technical leads to design new features and functionality based on customer need, collaborate with the development team to refine requirements, and manage solutions from design and development through production deployment. The Senior Business Analyst will produce technical documents detailing integration points and features for multiple applications. The ICF team performs custom software development to modernize and integrate siloed, legacy applications using modular design standards. The team works closely with clients and other contractors to ensure the performance and reliability of public-facing, mission-critical applications.\n  \n \n \n   What you\u2019ll be doing:\n  \n \n   Manage and support daily work requests for the project team by:\n  \n \n  Support all aspects of our IT delivery lifecycle including requirements, design, testing and delivery of web-based tools, though primary function will focus on requirements, design and implementation \n  Directing the work of other Business Analysts in support of the project manager\u2019s project schedule \n  Engaging with end-users to understand business needs to develop and document requirements \n  Developing technical documents in support of the integration of multiple applications and development of new application features \n  Communicating with multiple teams to ensure requirements are clear and well understood from a functional and technical perspective \n  Logging incoming requests/work in team tools (Jira/Confluence, Smartsheet or Trello) \n  Communicating with clients and the team about the status of work \n  Conducting demos or training necessary for the successful execution of the project \n \n \n   What you must have:\n  \n \n  Bachelor\u2019s Degree in related field \n  5+ years\u2019 experience working on web and/or software development teams \n  5+ years\u2019 experience conducting and leading interviews to gather and validate requirements \n  3+ years\u2019 experience in documenting requirements and specifications for web and/or software development teams \n  3+ years' experience with technical writing and developing technical requirements documents \n  US Citizenship is required (required by the federal government for this position). \n  Must be able to obtain Public Trust clearance. \n  MUST RESIDE IN THE United States (U.S.) and the work MUST BE PERFORMED in the United States (U.S.), as this work is for a federal contract and laws do apply \n \n \n   What we'd like you to have:\n  \n \n  Experience developing standard documentation, including requirement and design specifications, use cases, and other supporting system documentation \n  Experience with technical writing and developing technical requirements documents \n  Excellent oral and written communication skills, especially in a client interaction context and in facilitating large team meetings to drive delivery \n  Experience scoping work efforts into actionable pieces for delivery \n  Excellent analytical and problem-solving skills \n  Familiarity with Agile Scrum methods \n  Swift ability to break down tasks into Epics, Stories & Tasks \n  Ability to work independently and within one or more teams with excellent time management, organizational, and reporting skills \n \n \n   Why you\u2019ll love working here:\n  \n \n  Flexible work location \n  Generous vacation and retirement plans \n  Comprehensive health benefits \n  Diverse workforce that values equality and inclusion \n  Ongoing training and development opportunities \n  Friendly community with lots of social events \n  Participation in charity initiatives \n  Employee support program \n \n \n \n   #EET\n  \n \n   #LI-CC1\n  \n \n   #Indeed\n  \n \n \n   Working at ICF\n  \n  ICF is a global advisory and technology services provider, but we\u2019re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.\n  \n \n   We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our \n   \n   EEO & AA policy\n   .\n  \n \n \n   Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email \n   \n   icfcareercenter@icf.com\n    and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: \n   \n   Know Your Rights\n    and \n   \n   Pay Transparency Statement.\n   \n \n \n \n  Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:\n   $75,157.00 - $127,767.00\n   Nationwide Remote Office (US99)",
        "cleaned_desc": "  \n \n  Experience developing standard documentation, including requirement and design specifications, use cases, and other supporting system documentation \n  Experience with technical writing and developing technical requirements documents \n  Excellent oral and written communication skills, especially in a client interaction context and in facilitating large team meetings to drive delivery \n  Experience scoping work efforts into actionable pieces for delivery \n  Excellent analytical and problem-solving skills \n  Familiarity with Agile Scrum methods \n  Swift ability to break down tasks into Epics, Stories & Tasks \n  Ability to work independently and within one or more teams with excellent time management, organizational, and reporting skills \n \n \n   Why you\u2019ll love working here:\n  \n \n  Flexible work location \n  Generous vacation and retirement plans \n  Comprehensive health benefits \n  Diverse workforce that values equality and inclusion \n  Ongoing training and development opportunities \n  Friendly community with lots of social events ",
        "techs": [
            "agile scrum methods",
            "epics",
            "stories & tasks",
            "technical writing",
            "requirement and design specifications",
            "use cases",
            "documentation",
            "oral and written communication",
            "client interaction",
            "large team meetings",
            "scoping work efforts",
            "analytical skills",
            "problem-solving skills",
            "time management",
            "organizational skills",
            "reporting skills",
            "flexible work location",
            "vacation and retirement plans",
            "health benefits",
            "training and development opportunities",
            "social events."
        ],
        "cleaned_techs": [
            "agile scrum methods",
            "epics",
            "stories & tasks",
            "technical writing",
            "requirement and design specifications",
            "use cases",
            "oral and written communication",
            "client interaction",
            "large team meetings",
            "scoping work efforts",
            "time management",
            "flexible work location",
            "vacation and retirement plans",
            "health benefits",
            "training and development opportunities",
            "social events."
        ]
    },
    "08e20532b53cbcc5": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 75592.57,
        "salary_max": 95717.04,
        "title": "IT Business Analyst",
        "company": "Carenet Healthcare",
        "desc": "The  Business Analyst  will represent the Solution Development team as a subject matter expert. He/she will be responsible for assisting with analyzing the needs of our client and internal partners and translating those needs into requirements documentation, user stories and acceptance criteria. This individual has a high level of confidence and can negotiate at the highest level with people both face to face and remote. \n How you will help our team succeed: \n \n Understand the software development lifecycle \n Elicits, captures, analyzes and articulates business requirements into user stories with supporting documentation such as acceptance criteria, process flows and wireframes, all in-order to clearly document and communicate a set of functional requirements. \n Documents existing and To Be processed using process-flow diagrams and perform detailed gap analysis. \n Balances business requirements with technical feasibility and sustainability. \n Establishes and maintains agreements and understanding of requirements across all stages of the design, development and deployment. \n Participates in Integration Testing, User Acceptance Testing (UAT) and Functionality Testing as appropriate. \n Performs detailed analysis of data, uses case scenarios, systems, and integration points to identify potential gaps or areas of concern. \n Collaborates with the Solution Manager and team to translate customer/product needs into deliverable, realistic features and improvements, escalate issues and risks. \n Understands customer/product priorities and dependencies across multiple projects and advises the Solution Manager on the best way to approach and prioritize work to maximize value. \n Facilitates meetings, presentations and delivers demonstrations. \n \n What you will need to be successful: \n \n Bachelor's degree in Information Systems or Computer Science or related field from a four-year college or university with minimum eight(8) years of experience in IT including at least five (5) years in a Business Analyst role. \n Must have a broad working knowledge of Solution Development, IT process, tools, and techniques, as well as Production/Infrastructure Operations preferably in Microsoft technologies. \n Experience preferred in the areas of accountability for 24/7 call center technologies, reporting, remote workforce, database structures and concepts; including solid planning, prioritization, organizational skills and attention to detail. \n High knowledge of custom developed applications and packaged based applications. This position requires strong analytical skills and the ability to manage complex technical projects. Exceptional communication and negotiation skills are vital. \n Participation in Agile cadence such as Scrum standup meetings, backlog grooming sessions, sprint planning, and retrospective meetings preferred. \n \n Candidates must be willing to be interviewed at night. (No PH daytime interview schedules). \n Carenet Health delivers multi-dimensional value to healthcare organizations in areas such as revenue optimization, cost containment and consumer experience. Our clients choose us\u2014and stay with us for an average of seven years or more\u2014because of our clinical expertise and our experience creating meaningful connections that deliver impact and ROI. \n Our solutions include multi-channel consumer engagement programs that support quality and satisfaction performance metrics, as well as on-demand clinical engagement and telehealth services that improve care and lower costs. Intelligent contact strategies, empathy-focused interactions, high-touch navigation assistance and best-in-class partners are a few of the key factors in our success. \n Learn more at carenethealthcare.com \n Job Type: Full-time \n Benefits: \n \n Health insurance \n \n Experience level: \n \n 10 years \n 11+ years \n 4 years \n 5 years \n 6 years \n 7 years \n 8 years \n 9 years \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Application Question(s): \n \n What are your pay expectations? (Numbered answers only) \n \n Education: \n \n Bachelor's (Required) \n \n Experience: \n \n IT solutions business analyst: 3 years (Required) \n Business analysis: 5 years (Required) \n SDLC: 3 years (Required) \n \n Work Location: Remote \n Expected Start Date: 10/02/2023",
        "cleaned_desc": " Facilitates meetings, presentations and delivers demonstrations. \n \n What you will need to be successful: \n \n Bachelor's degree in Information Systems or Computer Science or related field from a four-year college or university with minimum eight(8) years of experience in IT including at least five (5) years in a Business Analyst role. \n Must have a broad working knowledge of Solution Development, IT process, tools, and techniques, as well as Production/Infrastructure Operations preferably in Microsoft technologies. \n Experience preferred in the areas of accountability for 24/7 call center technologies, reporting, remote workforce, database structures and concepts; including solid planning, prioritization, organizational skills and attention to detail. \n High knowledge of custom developed applications and packaged based applications. This position requires strong analytical skills and the ability to manage complex technical projects. Exceptional communication and negotiation skills are vital. \n Participation in Agile cadence such as Scrum standup meetings, backlog grooming sessions, sprint planning, and retrospective meetings preferred. \n \n Candidates must be willing to be interviewed at night. (No PH daytime interview schedules). \n Carenet Health delivers multi-dimensional value to healthcare organizations in areas such as revenue optimization, cost containment and consumer experience. Our clients choose us\u2014and stay with us for an average of seven years or more\u2014because of our clinical expertise and our experience creating meaningful connections that deliver impact and ROI. ",
        "techs": [
            "facilitates meetings",
            "presentations",
            "demonstrations",
            "microsoft technologies",
            "database structures",
            "custom developed applications",
            "packaged based applications",
            "agile cadence",
            "scrum standup meetings",
            "backlog grooming sessions",
            "sprint planning",
            "retrospective meetings."
        ],
        "cleaned_techs": [
            "facilitates meetings",
            "presentations",
            "demonstrations",
            "microsoft technologies",
            "database structures",
            "agile cadence",
            "scrum standup meetings",
            "backlog grooming sessions",
            "sprint planning",
            "retrospective meetings."
        ]
    },
    "54431f7d12f82105": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 95329.06,
        "salary_max": 120707.84,
        "title": "Salesforce Business Analyst",
        "company": "Bottomline Technologies",
        "desc": "Bottomline is at the forefront of digital transformation. We are a growing global market leader uniquely equipped to address the changing needs of how businesses pay and get paid. Our culture of  Working with and for each other  enables us to  delight our customers . We empower our teams to  think like owners  driving customer delight, helping them grow their business and win in their markets. \n  We are looking for a  Salesforce Business Analyst  to innovate, win, and grow with us! The successful candidate will play a crucial role in bridging the gap between the business needs and the solutions provided by our IT department and external vendors. You'll work closely with business stakeholders, Salesforce admins, the development team, consultants, and report into a Sr. Director of Enterprise Business Systems. \n  Position Summary: \n  Bottomline leverages multiple instances of Salesforce to manage various aspects of the lead-to-cash process, ranging from marketing, sales, CPQ (Configure, Price, Quote), onboarding, professional services, and support. As we embark on a multi-year project roadmap, we are seeking a world-class Business Analyst who can effectively translate business needs into requirements, gain domain expertise, and actively participate in the SDLC (Software Development Life Cycle) for our CRM projects. \n  To be successful in this role, you will have a passion for Salesforce, people, and processes. \n  How You'll Contribute: \n \n Gather and document business requirements from various stakeholders across the organization. \n Develop a deep understanding of the business processes and Salesforce instances across marketing, sales, CPQ, onboarding, professional services, and support. \n Actively participate in the Software Development Life Cycle (SDLC) from requirement gathering, analysis, design, testing, and implementation of CRM projects \n Collaborate with stakeholders, project managers, and technical teams to ensure the requirements are clear, complete, and understood by all parties. \n Develop user stories, use cases, and functional specifications as required. \n Analyze the impact of proposed solutions across the organization and develop use cases to explain/demonstrate business requirements/specifications to the IT team and other stakeholders. \n Participate in user acceptance testing (UAT) and ensure the requirements are met. \n Help in the development and delivery of end-user training. \n Provide post-implementation support and analysis. \n \n \n \n  What will make you successful : \n \n Minimum of 5 years of experience as a Business Analyst, with at least 3 years of experience working with Salesforce CRM \n Strong understanding of the lead-to-cash process \n Demonstrated experience in translating business needs into requirements and actively participating in the SDLC. \n Experience in creating user stories, use cases, and functional specifications. \n Strong analytical, problem-solving, and decision-making skills \n Excellent verbal and written communication skills \n Ability to work in a fast-paced environment and manage multiple priorities. \n Salesforce Administrator or Salesforce Business Analyst certification is a plus. \n \n You'll love Bottomline because in everything we do we seek to  delight our customers  and we are passionate about building a  company of which we can all be proud , and this starts with building amazing teams filled with team members that challenge you every day. \n  Start your #LifeAtBottomline",
        "cleaned_desc": " Experience in creating user stories, use cases, and functional specifications. \n Strong analytical, problem-solving, and decision-making skills \n Excellent verbal and written communication skills \n Ability to work in a fast-paced environment and manage multiple priorities. \n Salesforce Administrator or Salesforce Business Analyst certification is a plus. \n ",
        "techs": [
            "user stories",
            "use cases",
            "functional specifications",
            "salesforce administrator",
            "salesforce business analyst"
        ],
        "cleaned_techs": [
            "user stories",
            "use cases",
            "functional specifications",
            "salesforce administrator"
        ]
    },
    "91a7e2d813ca9797": {
        "terms": [
            "data analyst"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Operation Research System Analyst (ORSA)",
        "company": "Iron EagleX",
        "desc": "Overview: \n  \n  Iron EagleX is a veteran owned defense contracting company based in Tampa, FL. \n \n \n   It is our mission to provide solutions to the most challenging technical problems facing the Department of Defense while simultaneously making a positive impact on our employees and community.\n   Responsibilities: \n  \n  Job Description: \n \n \n \n \n \n Iron EagleX  is looking for an Operation Research System Analyst (ORSA) to join us, supporting a USSOCOM customer located at MacDill AFB. The J52 has responsibility for developing USSOCOM strategy, policy, and plans that providing key and critical insights through structured and analytic studies of SOF strategies, plans, and posture to inform USSOCOM senior leaders\u2019 strategic understanding and articulation of risk that drive decisions on forces, footprints, and agreements; planning guidance and implementation; and force development; developing the Global SOF Posture Plan (GSPP); serving as the link for USSOCOM on the Joint Force Posture Planning process and events; and providing key and critical insights through structured and analytic studies of SOF strategies, plans, and posture to inform USSOCOM senior leaders\u2019 strategic understanding and articulation of risk that drive decisions on forces, footprints, and agreements; planning guidance and implementation; and force development.\n  \n \n \n  Job Duties Include (but are not limited to): \n \n \n   Formulate and apply mathematical modeling and other optimizing methods to develop and interpret information that assists management with decision making, policy formulation, or other managerial functions. May collect and analyze data and develop decision support software, service, or products. May develop and supply optimal time, cost, or logistics networks for program evaluation, review, or implementation. Provide structured analysis and strategic assessment during the development of strategic planning, command strategy, posture, readiness, risk assessments, and operational return on investments that support evidence-based recommendations to and decisions by USSOCOM senior leaders. The Contractor shall perform tasks such as:\n  \n \n  Support assigned Operational Planning Teams (OPTs), Crisis Action Teams (CATs), and other boards, bureaus, centers, cells and working groups (B2C2WG) as needed. \n  Support the development of USSOCOM strategic documents by providing ORSA expertise, document preparation and coordination, data collection and analysis, and seminar/symposium, conference, and wargame support. \n  Provide technical expertise and capability for conducting ORSA projects, which are studies and analyses efforts using ORSA methods and tools in support of joint planning and contingency operations (refer to Joint Publications 3-0 and 5-0). \n  Execute high-visibility, urgent, and critical ORSA projects, requiring a variety of ORSA methods and tools. Such projects may be very large, extremely complex and of major importance to national security. \n  Apply operation research methods to identify and solve real-world problems for USSOCOM. Exercise sound military and analytic judgment in applying standard professional ORSA practices. Be creative and innovative in selecting and applying methods and tools to solve problems, enhance performance, or increase efficiency and effectiveness. \n  Use statistical analysis, simulations, stochastic and deterministic modeling, or other methods to analyze information and develop practical solutions to fit specific situations. \n  Perform analysis of program/project performance and design of experiments. \n  Understand and explain underlying, unique, and very difficult to define relationships that may require unconventional approaches or the application and adaptation of sophisticated analytical techniques producing original results. \n  Develop analytical approaches and supporting processes to address a wide variety of ambiguous, complex, compounding problems given only a skeletal framework or foundation for departure characterized by either their expansive breadth or depth where analytic precedents and guidelines often do not exist. \n  Design, develop and advocate for new analytic capabilities and technical improvements consistent with evolving requirements within the command (e.g., data mining and analysis, systems analysis, social analysis, wargaming, survey design and analysis). \n  Communicate with, prepare correspondence and presentations for, and advise senior leaders. \n  Analyze USSOCOM strategic documents alignment with the national-level strategic guidance and direction, including the Unified Command Plan, National Military Strategy, Defense Planning Scenarios, Contingency Planning Guidance, and other relevant national strategic documents. \n  Research and analyze various Department of Defense (DOD), Interagency, Service, and Combatant Command policies, strategies, and concepts associated with USSOCOM strategy and missions. \n  Develop an assessment plan and provide analysis for USSOCOM plans, to include the campaign plans, the Special Operations Forces Enterprise Plan, global campaign plans, and posture plans. \n  Provide analysis for strategic planning meetings, briefings, seminars, and strategy war games. \n  Conduct and prepare executive-level, research-based strategic level studies, assessments, and papers to analyze, assess, and synchronize command strategic documents and processes. \n  Develop and maintain a methodology and structure for implementing DOD and USSOCOM advancing analytics (ADVANA) and/or other data management systems (DMS); develop and maintain the processes and structure for incorporating ADVANA and/or other DMS to support USSOCOM J5 analysis and assessment efforts. \n  Prepare assessment products for 1- and 2-star Joint Planning Board, 3-star JCS led OPSDEPs and 4-star TANK. \n  Brief senior leaders on assessment and strategic questions. \n  Collaborate across USSOCOM enterprise on studies and research conducted for this PWS. \n  Assist on measuring performance, effectiveness, and overall objective ratings. \n  Conduct studies, research, assessments, and wargames that support USSOCOM decision making with evidence-based and data-driven analyses. \n  Qualifications: \n  \n  Required Skills & Experience: \n \n \n   Optional Specializations (One or More Areas Preferred): 5+ years in SOF; PSYOP; MILDEC; CWMD; CYBER; AT/FP; SNA; Special Programs; Interagency Experience, Intelligence, CCMD.\n  \n \n \n  Education & Certifications: \n \n \n  Civilian Education: MA/MS Preferred Disciplines: Social Sciences, Public Policy, Applied Statistics, Operations Research, Systems Analysis; OR Military Education: CGSC Equivalent or Higher (SAMS or equivalent; War College or equivalent; JPME II). \n \n \n  MS degree or higher in Operations Research and 10-15+ years operations research systems analyst (ORSA) experience. \n \n \n \n  Security Clearance: \n \n \n  An active Top Secret SCI security clearance is required. \n \n \n \n  Benefits: \n \n \n  National health, vision, and dental plans \n  20 days of PTO and 11 paid holidays \n  Life Insurance \n  Short and long term disability plans \n  401(K) retirement plan \n  Incentive and recognition programs \n  Relocation opportunities \n \n \n \n  Iron EagleX is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, among other things, or status as a qualified individual with disability.",
        "cleaned_desc": " \n  Job Duties Include (but are not limited to): \n \n \n   Formulate and apply mathematical modeling and other optimizing methods to develop and interpret information that assists management with decision making, policy formulation, or other managerial functions. May collect and analyze data and develop decision support software, service, or products. May develop and supply optimal time, cost, or logistics networks for program evaluation, review, or implementation. Provide structured analysis and strategic assessment during the development of strategic planning, command strategy, posture, readiness, risk assessments, and operational return on investments that support evidence-based recommendations to and decisions by USSOCOM senior leaders. The Contractor shall perform tasks such as:\n  \n \n  Support assigned Operational Planning Teams (OPTs), Crisis Action Teams (CATs), and other boards, bureaus, centers, cells and working groups (B2C2WG) as needed. \n  Support the development of USSOCOM strategic documents by providing ORSA expertise, document preparation and coordination, data collection and analysis, and seminar/symposium, conference, and wargame support. \n  Provide technical expertise and capability for conducting ORSA projects, which are studies and analyses efforts using ORSA methods and tools in support of joint planning and contingency operations (refer to Joint Publications 3-0 and 5-0). \n  Execute high-visibility, urgent, and critical ORSA projects, requiring a variety of ORSA methods and tools. Such projects may be very large, extremely complex and of major importance to national security. \n  Apply operation research methods to identify and solve real-world problems for USSOCOM. Exercise sound military and analytic judgment in applying standard professional ORSA practices. Be creative and innovative in selecting and applying methods and tools to solve problems, enhance performance, or increase efficiency and effectiveness. \n  Use statistical analysis, simulations, stochastic and deterministic modeling, or other methods to analyze information and develop practical solutions to fit specific situations. \n  Perform analysis of program/project performance and design of experiments. \n  Understand and explain underlying, unique, and very difficult to define relationships that may require unconventional approaches or the application and adaptation of sophisticated analytical techniques producing original results. \n  Develop analytical approaches and supporting processes to address a wide variety of ambiguous, complex, compounding problems given only a skeletal framework or foundation for departure characterized by either their expansive breadth or depth where analytic precedents and guidelines often do not exist. \n  Design, develop and advocate for new analytic capabilities and technical improvements consistent with evolving requirements within the command (e.g., data mining and analysis, systems analysis, social analysis, wargaming, survey design and analysis). ",
        "techs": [
            "mathematical modeling",
            "optimizing methods",
            "decision support software",
            "time networks",
            "cost networks",
            "logistics networks",
            "strategic assessment",
            "risk assessments",
            "operational return on investments",
            "operational planning teams (opts)",
            "crisis action teams (cats)",
            "joint publications 3-0 and 5-0",
            "statistical analysis",
            "stochastic modeling",
            "deterministic modeling",
            "simulations",
            "design of experiments",
            "sophisticated analytical techniques",
            "data mining and analysis",
            "systems analysis",
            "social analysis",
            "wargaming",
            "survey design and analysis"
        ],
        "cleaned_techs": [
            "mathematical modeling",
            "optimizing methods",
            "decision support software",
            "time networks",
            "cost networks",
            "logistics networks",
            "strategic assessment",
            "risk assessments",
            "operational return on investments",
            "operational planning teams (opts)",
            "crisis action teams (cats)",
            "joint publications 3-0 and 5-0",
            "statistical analysis",
            "stochastic modeling",
            "deterministic modeling",
            "simulations",
            "design of experiments",
            "sophisticated analytical techniques",
            "data mining and analysis",
            "systems analysis",
            "social analysis",
            "wargaming",
            "survey design and analysis"
        ]
    },
    "37be026780cb085e": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 79020.36,
        "salary_max": 100057.39,
        "title": "Business Analyst",
        "company": "Donegal Insurance Group",
        "desc": "Company Overview \n  Donegal Insurance Group  is an insurance holding company whose insurance subsidiaries offer personal and commercial property & casualty lines of insurance. \n  Headquartered in Lancaster County, Pennsylvania, along with four (4) regional offices located in:  Athens, GA, Grand Rapids, MI, Albuquerque, NM, and Glen Allen, VA , our steady growth and successes have allowed us to establish a culture of which we're proud. Check out our Glassdoor profile where our rating speaks for itself. \n  At Donegal, our values are founded on supporting the independent agency system, providing best-in-class service, and building relationships customers can trust. By joining the Donegal family, you would be joining a team of dedicated, hard-working employees, all with a common goal of providing peace of mind to our policyholders and being there when it matters most. \n  Job Summary \n  The  Business Analyst  collaborates with business users and the technical team to translate user requirements into system specifications, develops and coordinates user testing, and assists in the training and implementation of new systems and/or system enhancements.  \n The Business Analyst will act as a liaison between Commercial Lines, Programming and Product Development departments for their assigned Line of Business. Ideal candidate must possess commercial insurance knowledge with an emphasis on rates, underwriting, technology, and coverage forms.  \n Responsibilities and Duties \n \n  Act as the primary interface between the business unit and IT Development \n  Well versed in state specific laws, making sure the program is up to date on compliance with those laws \n  Gathers, analyzes and documents user needs and requirements and assists technical staff in translating the requirements into systems specifications \n  Assists with the project in facilitating and formulating a solutions path \n  Analyst will elicit actual needs of the stakeholders and align the needs of the business units with the technological capabilities \n  Participates in project status meetings including documenting and distributing meeting minutes \n  Coordinates and assists all aspects of implementation including user training as well as producing user procedures and documentation \n  Design and develop test cases, test scripts and validation processes based upon system requirements and functional specifications in an agile software development environment \n  Conducts and coordinates post-implementation testing and routine proactive follow-up with business units to ensure a quality product and high level of functionality is achieved \n  Provides daily support for Underwriting business unit including providing technical guidance, troubleshooting software issues, and working with the Underwriting Staff to ensure the business goals and needs are met for the business units \n  Assist in some project management responsibilities \n  Assist in the troubleshooting of technical and functional issues with various business initiatives \n \n  Qualifications and Skills \n \n  Bachelor\u2019s Degree in Risk Management, Insurance, Business Administration, Project Management or related field \n  Minimum five years\u2019 P&C insurance experience \n  Ability to write business rules pertaining to Donegal product \n  Ability to learn Donegal\u2019s systems by understanding the coverage and the entry fields when developing programs \n  Strong underwriting background on both the business and technical side \n  Strong verbal and written communication skills \n \n  Work Arrangement \n Donegal Insurance Group has a variety of positions that are either onsite, hybrid, or remote. With each department and position being different, the work arrangement for a specific position will be reviewed with candidates during a initial phone screening. For a fully remote position, the ideal candidate must live within our Donegal footprint (Current approved states for remote work are:  AL, AZ, CO, CT, DE, FL, GA, IA, IL, IN, MA, MD, MI, MN, MO, NC, ND, NE, NH, NJ, NM, NY, OH, PA, SC, SD, TN, TX, UT, VA, WI, and, WV . Please note, this list is subject to change without notice.) \n Benefits \n Donegal Insurance Group offers a comprehensive benefits package for all full-time, permanent positions. Learn more about our benefit offerings by visiting our Benefits page. \n Additional Information \n Donegal Insurance Group participates in E-Verify in the following states:  Alabama, Arizona, Florida, Georgia, Louisiana, Mississippi, Nebraska, North Carolina, South Carolina, Tennessee, and Utah . If you reside in one of the listed states, please review the \"Notice of E-Verify Participation\" and the \"Right to Work Poster\" on the links below: \n \n Notice of E-Verify Participation Poster (English and Spanish) \n Right to Work Poster (English and Spanish) \n \n \n xdC0wfAXoU",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "f356f41f4b9708f2": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 79250.56,
        "salary_max": 100348.875,
        "title": "BCBA / Behavior Analyst Remote",
        "company": "BURNETT THERAPEUTIC SERVICES INC",
        "desc": "Job description \n \n  The BCBA position will be held by an individual who has the training, education, and/or experience necessary to serve as the Qualified Autism Service Professional. This position will provide clinical supervision as well as oversight to the programs being implemented by the Supervisor and Behavior Interventionists. The BCBA is an individual who meets all off the following criteria: Provides behavioral health treatment, Develops and Implements Treatment Plans with the oversight and guidance of the Clinical Director, and has training and experience in providing services for individuals with developmental disabilities including pervasive developmental disorder, intellectual disability, autism or other delays.\n  \n \n Essential Job Functions / Key Performance Areas \n \n  To perform this job successfully, the Supervisor must be able to satisfactorily perform each essential function listed below:\n  \n \n \n Designing, developing and implementing a wide range of ABA programs for individuals with autism, developmental disabilities, and/or delays \n Collaborating with families and other professionals to tailor ABA programs to each client\u2019s specific needs \n Directly delivering individual and group learning experiences to assist each consumer served in obtaining his/her treatment goals and objectives \n Meet with, train, and supervise Supervisors and direct care staff (Behavior Interventionists) to provide guidance and review progress on goals, determine the effectiveness of intervention strategies and consult about the overall implementation of the treatment plan via telehealth \n Evaluate and update client progress reports \n Conduct assessments, write treatment goals, analyze data and write progress reports, under the direction and supervision of the BCBA/Licensed Clinical Supervisor \n Conduct comprehensive Functional Behavior Assessments utilizing standardized assessment tools including the VB-MAPP, ABBLS-R, Vineland-III, etc., under the direction and supervision of the BCBA / Licensed Clinical Supervisor \n Develop and oversee the implementation of behavioral plans for children and adults diagnosed with developmental disabilities including pervasive developmental disorder, intellectual disability, autism or other delays. \n Provide assistance in the development of adaptive skills in order for the individual to take a more positive role in the school, home & community setting \n Provide prescribed behavioral consultation to the family and / or school \n Supervise ABA programs to assist clients learn a variety of skills related to cooperation with daily routines and demands, coping with stress, play and social interactions, functional communication, attending to tasks, safety awareness and self-help \n Empower parents, caregivers and other support staff to influence clients within their natural routines so they can be more successful, independent and self-empowered\n  \n \n \n Establish and maintain positive and supportive relationships with clients, all family members and any other staff persons supporting the client\u2019s individualized needs \n Define target behaviors and write treatment goals with coordinating evidence-based programs, reinforcement and antecedent management strategies, protocols, interventions, etc., under the direction and supervision of the BCBA / Licensed Clinical Supervisor \n Model for and teach parents/care providers how to implement interventions and protocols utilized during sessions \n Model for and teach support staff how to implement intervention strategies and protocols across a variety of settings \n Provide appropriate data sheets that reflect client's goals/programs \n Set up and maintain ABA session schedule for support staff and clients and manage planned and unplanned adjustments or cancelations \n Meet with families/caregivers in their homes OR via telehealth on a weekly basis to review progress on goals, determine the effectiveness of intervention strategies and consult about the overall implementation of the treatment plan \n Job Types: Full-time / Remote / Hybrid Options also available\n  \n  Job Type: Full-time\n  \n  Benefits:\n  \n \n \n Dental insurance \n Health insurance \n Vision insurance \n Schedule:\n  \n \n \n Monday to Friday \n  Education:\n  \n \n \n Master's (Required) \n  License/Certification:\n  \n \n \n BCBA (Required) \n  Work Location: Remote\n  \n This is a remote position.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "728a4be9711a48f8": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 0.0,
        "salary_max": 43500.0,
        "title": "BUSINESS ANALYST",
        "company": "All Hands and Hearts Smart Response, Inc.",
        "desc": "Diversity is the acknowledgment, promotion and celebration of our differences and similarities. All racial identities, ethnicities, sexes, nationalities, gender identities and expressions, physical and mental abilities, sexual orientations, ages, spiritualities, points of view and socioeconomic statuses can fit in with us. We support individuals from around the world and serve communities affected by disasters with purpose. Response, recovery, resilience and renewal are why we exist. Respect for the value of diversity, inclusion and social justice drives us forward every day. Everyone is welcome. \n \n \n  Position Purpose \n  A versatile position within a dynamic nonprofit that provides  community-inspired, volunteer-powered disaster relief . The position is full-time and is responsible for tracking, managing and analyzing program finances as well as using data to analyze a variety of business challenges and opportunities. \n \n \n  Reporting Relationships \n \n Reports to  Chief Financial Officer \n \n \n Roles reporting to position  None \n \n \n Approximate number of reports  0 \n \n \n \n  Essential Functions \n \n Assisting in cross-functional data driven projects and enabling organizational collaboration \n \n \n Maintaining expense tracking procedures \n \n \n Reporting on program financials \n \n \n Assisting with monthly revenue reconciliation and reporting \n \n \n Managing program budgets \n \n \n Assisting in annual audit preparation \n \n \n We understand vaccination is the best way to protect our beneficiaries, community members, staff and volunteers, therefore we require all of our on program staff members to be fully vaccinated against COVID-19. To be considered for this position you must be fully vaccinated. Subject to applicable guidance, the term fully vaccinated currently refers to when an individual has received either a single-shot vaccine or both doses of a two-shot vaccine, and two weeks have elapsed since the final dose. \n \n \n Perform other duties as required \n \n \n \n  Position Requirements \n \n Education  Associate Degree in Finance or related field \n \n \n Related Experience  Mission driven and eager to learn, flexible with an interest in finance and analytics preferred. \n \n \n Special Competencies/Certifications  Good written and oral communication skills and confident working with Google Workspace, particularly Google Docs and Google Sheets; and Microsoft Office. Applicant needs to be well organized and have a flexible approach to managing multiple tasks and prioritizing them appropriately. The individual is able to manage a large workload and perform in a fast-paced environment. \n \n \n  Working Conditions and Physical Demands \n \n Home office environment \n \n \n Constantly operates computer \n \n \n Travel up to 25% of the time \n \n \n Living in communal environment while on program \n \n \n  Compensation & Benefits \n  - Compensation  Typical starting salary is $36,000 - $37,875 USD. Annual pay increases may be available, up to a salary maximum of $43,500 for U.S. based employees. \n \n Flexible working hours \n \n \n Unlimited PTO \n \n \n Opportunity to travel to program \n \n \n Medical, vision, and dental insurance plans (US Employees) \n \n \n Flex Spending Account (US Employees) \n \n \n 401K with safe harbor match for U.S. staff (US Employees) \n \n \n  All Hands and Hearts is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, Veteran status, age, or any other protected characteristic.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "16da07a4f6423dd8": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 100000.0,
        "salary_max": 120000.0,
        "title": "Sr. Analyst, Revenue Cycle Management",
        "company": "Teladoc Health",
        "desc": "Teladoc Health is a global, whole person care company made up of a diverse community of people dedicated to transforming the healthcare experience. As an employee, you\u2019re empowered to show up every day as your most authentic self and be a part of something bigger \u2013 thriving both personally and professionally. Together, let\u2019s empower people everywhere to live their healthiest lives.  \n \n Position Summary  \n This position reports to the Manager of Revenue Cycle Analytics and is responsible for providing analytical and systems support to identify and resolve issues affecting revenue streams and reimbursement for various Teladoc services. The Senior Analyst will act as a lead in developing and generating all data analytics pertaining to revenue cycle activities and key performance indicators (KPIs). This position will work closely with subject matter experts in their assigned area to support daily business functions, performance improvement initiatives, reconciliation, benchmarking, and reporting. This individual will build collaborative relationships with internal stakeholders to support payer contracts and agreements for long-term profitability.  \n \n Roles and responsibilities  \n \n Expertise in Revenue Cycle Management; including understanding of billing, denial management, claims resubmission, and accounts receivable processes  \n Expert analytical skills with the ability to run queries, compile data from various sources for evaluation  \n Present data analysis for departmental leadership in organized format to clearly convey trends and identify key areas for revenue optimization. Highlight corrective solutions to minimize delays and denials to advance performance and achieve targets.  \n Monitor reimbursement trends and identify opportunities  \n Understanding of details in conjunction with business requirements and payer guidelines, determine root cause of issues and create action plans and options for resolution of revenue stream issues  \n Strong interpersonal skills to effectively engage internal stakeholders and Payers to identify root cause, initiate and steer corrective actions, document progress and report outcomes  \n Strategic mindset to understand the implications of business decisions that impact reimbursement processes  \n Understanding of contract terms and the ability to determine key contractual requirements that affect revenue capture  \n Expert at prioritizing multiple, complex assignments and consistently following through to meet agreed upon timelines and revenue targets.  \n Perform claims audits as necessary to optimize revenue capture and identify issues for process improvement  \n Expertise in quality and data integrity through both manual querying and automated templates  \n Prioritizes requests and implements timely escalation to the appropriate teams for resolution and corrective actions.  \n Perform other special projects and duties as needed or assigned.  \n \n \n Requirements/Preferences  \n \n Bachelor\u2019s degree in business management or similar field required. Equivalent experience may be substituted.  \n 4+ years of progressive experience in health care revenue cycle management, finance/accounting and/or applicable fields  \n Revenue Cycle certifications in billing, or data analyst from a nationally accredited organization is preferred.  \n Possess a high degree of both technical and analytics aptitude with a background that supports an ability to summarize, analyze and report business data  \n Demonstrated ability to effectively communicate (written and verbal) business processes, objectives and trends  \n Willingness to adopt and master software tools/platforms  \n Experience using data from multiple sources in reporting including Excel, Cognos, PowerBI, Metabase, Looker, Tableau and others.  \n Experience using patient accounting software such as NextGen, Altera  \n Critical thinking and business judgement to identify issues, alternative solutions and approaches to drive accurate conclusions to complex problems  \n \n \n The base salary range for this position is $100,000.00-120,000.00  .  In addition to a base salary, this position is eligible for a performance bonus and benefits (subject to eligibility requirements) listed here: Teladoc Health Benefits 2023 . Total compensation is based on several factors including, but not limited to, type of position, location, education level, work experience, and certifications. This information is applicable for all full-time positions.  \n \n \n \n \n \n \n \n \n \n \n \n \n Why Join Teladoc Health?   \n   A New Category in Healthcare:  Teladoc Health is transforming the healthcare experience and empowering people everywhere to live healthier lives.      Our Work Truly Matters  : Recognized as the world leader in whole-person virtual care, Teladoc Health uses proprietary health signals and personalized interactions to drive better health outcomes across the full continuum of care, at every stage in a person\u2019s health journey.      Make an Impact:  In more than 175 countries and ranked Best in KLAS for Virtual Care Platforms in 2020, Teladoc Health leverages more than a decade of expertise and data-driven insights to meet the growing virtual care needs of consumers and healthcare professionals.      Focus on PEOPLE:  Teladoc Health has been recognized as a top employer by numerous media and professional organizations. Talented, passionate individuals make the difference, in this fast-moving, collaborative, and inspiring environment.      Diversity and Inclusion:  At Teladoc Health we believe that personal and professional diversity is the key to innovation. We hire based solely on your strengths and qualifications, and the way in which those strengths can directly contribute to your success in your new position.      Growth and Innovation:  We\u2019ve already made healthcare yet remain on the threshold of very big things. Come grow with us and support our mission to make a tangible difference in the lives of our Members.   \n \n As an Equal Opportunity Employer, we never have and never will discriminate against any job candidate or employee due to age, race, religion, color, ethnicity, national origin, gender, gender identity/expression, sexual orientation, membership in an employee organization, medical condition, family history, genetic information, veteran status, marital status, parental status or pregnancy.  \n \n Teladoc Health respects your privacy and is committed to maintaining the confidentiality and security of your personal information. In furtherance of your employment relationship with Teladoc Health, we collect personal information responsibly and in accordance with applicable data privacy laws, including but not limited to, the California Consumer Privacy Act (CCPA). Personal information is defined as: Any information or set of information relating to you, including (a) all information that identifies you or could reasonably be used to identify you, and (b) all information that any applicable law treats as personal information. Teladoc Health\u2019s Notice of Privacy Practices for U.S. Employees\u2019 Personal information is available at this link.",
        "cleaned_desc": " \n \n Requirements/Preferences  \n \n Bachelor\u2019s degree in business management or similar field required. Equivalent experience may be substituted.  \n 4+ years of progressive experience in health care revenue cycle management, finance/accounting and/or applicable fields  \n Revenue Cycle certifications in billing, or data analyst from a nationally accredited organization is preferred.  \n Possess a high degree of both technical and analytics aptitude with a background that supports an ability to summarize, analyze and report business data  \n Demonstrated ability to effectively communicate (written and verbal) business processes, objectives and trends  \n Willingness to adopt and master software tools/platforms  ",
        "techs": [
            "none"
        ],
        "cleaned_techs": []
    },
    "e712d518d1018cc9": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 81000.0,
        "salary_max": 94000.0,
        "title": "BCBA Behavior Analyst - ABA Therapist (FT)",
        "company": "Delta-T Grp.",
        "desc": "Job Details: \n \n \n  BCBA - Center/Hybrid - Competitive Pay \n  Full Job Description: BCBA ABA Therapy Clinic \n  Total First Year Compensation $81,000 to $94,000 \n  Job Type: Full-Time Center, Hybrid \n  Pay: $81,000 to $94,000 (First Year) \n \n \n   Kaleidoscope ABA is a private agency looking to hire a center-based BCBA position to work part of your week remotely\n  \n \n \n  Position Overview: \n \n \n  The Hybrid BCBA will deliver direct services to clients as well as manage and supervise a small caseload of 4-6 clients. \n  The Clinical Manager and Operations Manager at Kaleidoscope ABA will provide you with administrative support, scheduling help, and general assistance with the Hybrid BCBA's caseload. \n \n \n  What do we offer Full-Time Clinicians: \n \n \n  HYBRID Remote Work Schedule. \n  Full-Time Mon-Friday schedule - NO weekends or late nights. \n  Attractive Benefits Plan! \n  Guaranteed salary regardless of client cancellations. \n  Up to $6000 annual incentive bonus (paid monthly). \n  Small caseload. \n  Laptop provided. \n  Medical, Dental, and Vision Insurance. \n  8 Paid Holidays + 16 PTO Days Yr 2, 11 Yr 1. \n  CEU stipend. \n  Voluntary Benefits - STD, LTD, etc. \n  401K, 401K match. \n \n \n  Responsibilities: \n \n \n  Conduct assessments and reassessments for clients. \n  Develop individual goals and objectives to be included in client Treatment Plans. \n  Develop written guidelines for behavioral interventions, teaching plans, and programs. \n  Collect data for each goal/objective during each direct session. \n  Record data into company software and the individual's confidential file. \n  Provide training in behavioral interventions and applied behavior analysis to families and staff. \n  Analyze data collected to determine program effectiveness. \n  Supervise and coach Behavior Therapists. \n \n \n  Supplemental Pay: \n \n \n  Monthly incentive. \n  Sign-on bonus $3000. \n \n \n  Benefits: \n \n \n  Medical Insurance \n  Dental Insurance \n  Vision Insurance \n  Life Insurance \n  Paid Holidays (8 days) \n  PTO 16 days -Yr 2, (11 - Yr 1) \n  Voluntary STD, LTD, Accident, Cancer \n  401K \n  401K Match 6% \n  CEU Stipend \n \n \n  EXPERIENCE & EDUCATION REQUIRED: \n \n \n  Possess a minimum of a master's degree or national equivalent with a major in psychology, or special education. \n  Applied Behavior Analysis or a related field of study is preferred. \n  Obtained certification as a Board-Certified BCBA as verified through the Certification Board. \n  Active Arizona License. \n  A minimum of 1 year of experience in working with children, adolescents, and/or adults with various special needs. \n  Knowledge of appropriate behavioral intervention strategies, earning theories and instructional methods, ethics, laws, and regulations of acceptable behavior interventions. \n  Proficient in technology, such as Office 365, Microsoft Word, Excel, PowerPoint, and ABA software platforms. \n  Use a computer and behavioral software to prepare documents and maintain client records. \n  Work independently and make decisions within the framework of established guidelines. \n  Supervise clients during treatment sessions. \n  Ability to bend, kneel, crouch, and spend time on their feet. \n  Ability to lift items to 50 lbs. on occasion. \n \n \n  ABOUT US: \n \n \n   Kaleidoscope ABA provides center-based, community, and home-based Applied Behavioral Analysis (ABA) Therapy services to children and young adults. We use evidence-based, best-practice models to support and provide effective treatment to individuals with Autism Spectrum Disorder (ASD) as well as other behavioral and developmental disabilities. Our team partners with family members to create individualized ABA treatment plans designed to meet the individual needs of each person we serve.\n   \n \n \n  Call, email, or apply: \n \n \n   Kandace Robinson, 215-278-8372, krobinson@kfamilysolutions.org\n    https://www.kfsaba.org\n    https://www.kfsaba.org/eeo-statement/ (Kaleidoscope Family Solutions ABA, Inc. is an EEO employer.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "7aefffab8c1ce80d": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 87616.055,
        "salary_max": 110941.445,
        "title": "SOX Compliance Analyst",
        "company": "Realign LLC",
        "desc": "Job Type: Contract \n Job Category: IT \n Job Description \n \n Job Title: SOX Compliance Analyst (Remote) \n \n  Job Summary: \n  We are seeking a skilled and detail-oriented SOX Compliance Analyst to join our team remotely. As a SOX Compliance Analyst, you will be responsible for ensuring that the company's operations comply with Sarbanes-Oxley Act (SOX) requirements and maintaining a robust internal control environment. Your role will involve conducting risk assessments, developing and implementing compliance strategies, and performing regular audits to identify any deficiencies.  \n \n Responsibilities and Duties: \n \n Develop and implement comprehensive SOX compliance programs and control frameworks. \n Conduct risk assessments to identify potential areas of non-compliance. \n Perform regular audits to evaluate internal controls and identify control deficiencies. \n Analyze and interpret relevant regulations and apply them to the organization's operations. \n Collaborate with cross-functional teams to design and implement effective internal controls. \n Stay updated with any changes in SOX regulations and communicate the impact to the relevant stakeholders. \n Advise management on compliance matters and provide recommendations for improvement. \n Prepare detailed reports documenting audit findings and control deficiencies. \n Assist in the development and execution of SOX training programs for employees. \n Monitor the progress of remediation efforts for identified weaknesses or deficiencies. \n Assist in external audits and liaise with external auditors, if required. \n Maintain documentation of all compliance-related activities and ensure their accuracy and completeness. \n Participate in continuous improvement initiatives to enhance the effectiveness of compliance programs. \n Keep abreast of industry best practices and emerging trends in SOX compliance. \n \n \n  Qualifications and Skills: \n \n Bachelor's degree in Accounting, Finance, or a related field. \n Proven work experience in SOX compliance, internal audit, or a related role. \n In-depth knowledge of the Sarbanes-Oxley Act and relevant regulations. \n Strong analytical and problem-solving skills. \n Exceptional attention to detail. \n Excellent verbal and written communication skills. \n Ability to work independently and remotely while maintaining a high level of productivity. \n Proficient in using Microsoft Office Suite and other relevant software. \n Professional certifications such as Certified Internal Auditor (CIA) or Certified Public Accountant (CPA) are preferred. \n \n \n  #SOXComplianceAnalyst #RemoteJob #ComplianceJobs #FinanceJobs #Audit #InternalControls #RiskAssessment #FinancialCompliance #USJobs #FinanceCareers #RemoteWork #ComplianceManagement #SOXRegulations #Auditing #AccountingJobs \n \n Required Skills  Data Analyst",
        "cleaned_desc": " Strong analytical and problem-solving skills. \n Exceptional attention to detail. \n Excellent verbal and written communication skills. \n Ability to work independently and remotely while maintaining a high level of productivity. \n Proficient in using Microsoft Office Suite and other relevant software. \n Professional certifications such as Certified Internal Auditor (CIA) or Certified Public Accountant (CPA) are preferred. \n \n ",
        "techs": [
            "microsoft office suite"
        ],
        "cleaned_techs": [
            "microsoft"
        ]
    },
    "c6a81af01879925f": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 90940.0,
        "salary_max": 154598.0,
        "title": "Lead ServiceNow Business Analyst - Remote",
        "company": "ICF",
        "desc": "*We are open to supporting 100% remote work anywhere within the U.S.*\n  \n \n \n   ICF\u2019s Digital Modernization Division is a rapidly growing, entrepreneurial, technology department, seeking a Senior ServiceNow Business Analyst to support upcoming needs with our federal customers. Our Digital Modernization Division is an information technology and management consulting department that offers integrated, strategic solutions to its public and private-sector clients. ICF has the expertise, agility, and commitment to design, build, and operate high-performance IT engines to support all aspects of our client\u2019s business.\n  \n \n \n   Job Responsibilities\n  \n \n   We are seeking a Lead ServiceNow Business Analyst - HRSD to assist our customer\u2019s business needs with HR system integration to the ServiceNow platform. This role will be a key member of the development Team, implementing and supporting the ServiceNow HRSD platform, associated processes and external system integration. You will be responsible for developing a strong understanding of the business needs of the stakeholders and how the ServiceNow application will support those needs. This analyst role will partner with development and IT partners to build, document, and test ServiceNow HRSD. You will also provide support to end-users in UAT by training users on new functionality and presenting updates to end-users.\n  \n \n \n   The Lead ServiceNow Business Analyst serves as a translator between technical teams and the client\u2019s business community to collect, clarify, analyze and translate business requirements into documentation and conceptual design from which applications and solutions are developed. This position is within the context of an Agile team employing a Scrum development framework. The Business Analyst defines detailed stories and epics, with applicable acceptance criteria in collaboration with product owners and ICF\u2019s technology team, and facilitates meetings with client and internal technical and operational teams.\n  \n \n   This position will be working directly with stakeholders, serving in both Business Analyst and Project Manager roles, and must be able to communicate effectively via phone and web conferencing as many of the stakeholders work remotely.\n  \n \n \n   In addition to supporting all phases of the project, this position will also be responsible for authoring content and peer-reviewing a wide array of documents, including functional, technical, training as well as marketing, and proposals.\n  \n \n \n   Minimum Job Requirements:\n  \n \n  Bachelor\u2019s Degree \n  5+ years of experience as a Business Analyst in a fast-paced application development environment \n  5+ years recent experience working in an Agile development environment as a business analyst (i.e., Scrum, Kanban, etc.) \n  2+ years of experience with the ServiceNow Platform \n  5+ years of experience facilitating requirements gathering, Joint Application Design (JAD) sessions, capturing client requirements and feedback \n  US Citizenship required due to federal contract requirements \n  Must be able to obtain Public Trust clearance. \n \n \n \n   Desired Skills:\n  \n \n  Solid understanding of various software development cycles (e.g., Agile, Waterfall, etc.); knowledge of requirements management, configuration management methodologies, along with corresponding support tools, i.e., JIRA, etc. \n  Conceptual understanding of Object-Oriented enterprise software system development processes, methodologies, as well as major technologies (such as Java and .Net) and approaches (such as modularity and SOA) \n  Skilled at analyzing existing system documentation to summarize existing system functionality as it relates to the project at hand \n  Good understanding of basic system technologies as they relate to the project deliverables \n  Ability to maintain project plans, resourcing schedules, and forecasted activities \n  Experience with business process mapping and the use of project management software \n  Ability to provide technical assistance and troubleshooting by effectively responding to inquiries \n  Experience thriving in ambiguous software development environments \n  Ability to work well under constantly changing deadlines and priorities \n  Experience with ServiceNow, Appian, or similar BPM software \n  Excellent oral and written communication skills \n \n \n \n   Working at ICF\n  \n  ICF is a global advisory and technology services provider, but we\u2019re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.\n  \n \n   We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our \n   \n   EEO & AA policy\n   .\n  \n \n \n   Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email \n   \n   icfcareercenter@icf.com\n    and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: \n   \n   Know Your Rights\n    and \n   \n   Pay Transparency Statement.\n   \n \n \n \n  Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:\n   $90,940.00 - $154,598.00\n   Nationwide Remote Office (US99)",
        "cleaned_desc": "  2+ years of experience with the ServiceNow Platform \n  5+ years of experience facilitating requirements gathering, Joint Application Design (JAD) sessions, capturing client requirements and feedback \n  US Citizenship required due to federal contract requirements \n  Must be able to obtain Public Trust clearance. \n \n \n \n   Desired Skills:\n  \n \n  Solid understanding of various software development cycles (e.g., Agile, Waterfall, etc.); knowledge of requirements management, configuration management methodologies, along with corresponding support tools, i.e., JIRA, etc. \n  Conceptual understanding of Object-Oriented enterprise software system development processes, methodologies, as well as major technologies (such as Java and .Net) and approaches (such as modularity and SOA) \n  Skilled at analyzing existing system documentation to summarize existing system functionality as it relates to the project at hand \n  Good understanding of basic system technologies as they relate to the project deliverables \n  Ability to maintain project plans, resourcing schedules, and forecasted activities \n  Experience with business process mapping and the use of project management software ",
        "techs": [
            "servicenow platform",
            "jira",
            "java",
            ".net",
            "soa"
        ],
        "cleaned_techs": [
            "servicenow platform",
            "jira",
            "java",
            ".net",
            "soa"
        ]
    },
    "98bcf7b9e0e0cb4c": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 85000.0,
        "salary_max": 97000.0,
        "title": "Senior Business Analyst - Remote",
        "company": "ASRC Federal Holding Company",
        "desc": "Job Title:  Business Analyst \n \n \n Location:  Denver, CO \n \n  Job Description \n \n  ASRC Federal Vistronix (ASRC Federal) is actively seeking a Senior Business Analyst to join our Denver-based team and support defining business requirements for software to support for our federal customer. The successful candidate will have a solid understanding of Business Analysis principles and tools, possess excellent written and oral communication skills, be able to create and edit technical documents, and be able to define requirements for government systems and processes. Most of the work will be performed virtually, but some work at the government site in Denver can be expected. \n \n  ASRC Federal is a technical, professional services company providing state-of-the-art solutions to government and commercial clients. Our services include custom-engineered solutions that integrate with the latest technology, resulting in advanced information technology systems; business and management consulting services to assess client; and strategic and tactical program expertise to support continuity and provide comprehensive oversight for mission-critical initiatives. \n \n  We partner with government and commercial agencies that require the development of systems, such as communication systems, asset management, network deployment and engineering services, power and energy management solutions, portal applications, command and control, and GIS to operate more efficiently and profitably. \n \n  As a leading IT consulting and strategic outsourcing leader, we are always looking for exceptionally bright and motivated people to join our team. We are thought leaders in our market space \u2013 providing comprehensive solutions to our clients, throughout the enterprise. ASRC Federal staff members enjoy a collaborative working environment and recognize our staff member\u2019s contributions to the team\u2019s success as well as individual professional accomplishments. We offer competitive salaries and a comprehensive employee benefits package. If you are looking for an opportunity to use your skills in innovative ways, in an environment that promotes freethinking, presents positive challenges, and makes a real impact \u2013 ASRC Federal is the place for you! \n \n  Responsibilities \n \n  The successful candidate will work with business sponsors and other relevant stakeholders to define and document the business requirements for complex government software systems. The ability to work with both technical and non-technical audiences is very important, so having a solid understanding of the software development process is key. Principal responsibilities will include but are not limited to: \n \n \n \n Performing business analysis to support the project managers in defining project scope and objectives. \n Facilitating meetings with subject matter experts from various levels of the business to organize work and priorities. \n Reviewing and analyzing existing business requirements and processes. \n Re-engineering business processes to meet new business requirements. \n Eliciting information and functional specifications and design criteria from product owners and business sponsors through oral and written communication. \n Develop User Stories for business process specifications to support the evolution of the business and applications. \n Document the specifications of design principles, processes, models, and standards that address business requirements and goals. \n Create and edit technical documentation such as user guides, UAT guides, CSOM, Concept of Operations, Business Case, and Alternatives Analysis style deliverables. \n Participating in Scrum and Sprint Ceremony meetings and processes. \n Developing system and business requirements based on customer interviews and workshops. \n Support customer meetings through facilitation, developing agendas, tracking action items, and/or taking meeting minutes. \n Perform other duties as assigned \n Some limited domestic travel may be required. \n  Education and Experience \n \n \n \n A Bachelor\u2019s Degree and a minimum 5 years\u2019 experience working as a Business Analyst in an enterprise environment OR a Master\u2019s Degree in an IT related field and at least 2 years of experience working as a Business Analyst. \n  The successful candidate is subject to a U.S. Federal Government background investigation and must be able to meet the requirements to hold a position of public trust. \n \n  ASRC Federal is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. \n \n  We invest in the lives of our employees, both in and out of the workplace, by providing competitive pay and benefits packages. This position is offering a pay range of $85,000 - $97,000 depending on experience, seniority, geographic locations, and other factors permitted by law. Benefits offered may include health care, dental, vision, life insurance; 401(k); education assistance; paid time off including PTO, holidays, and any other paid leave required by law. \n \n  Education and Experience \n \n \n \n A Bachelor\u2019s Degree and a minimum 5 years\u2019 experience working as a Business Analyst in an enterprise environment OR a Master\u2019s Degree in an IT related field and at least 2 years of experience working as a Business Analyst \n  Required Skills and Competencies \n \n \n \n Strong technical writing skills, with the ability to concisely describe requirements and detailed functional requirements. **Candidates will be required to provide a writing sample. \n Good interpersonal skills. \n Logical and systematic analytical and logical thinking with the ability to communicate complex processes or ideas in a simple straight forward manner. \n Professional written and verbal communication skills to effectively convey advanced analytical and technical topics. \n Ability to analyze, develop, and test requirements to optimize and automate system administration and business process activities. \n Experience working with a wide range of personnel, including business sponsors, product owners, project managers, development teams, and end users at all levels of the organization. \n Experience developing business process models and other graphical depictions of business and IT functionality. \n Comfortable dealing with ambiguity and adapting to changes in a matrixed environment. \n Expert working knowledge with MS Office (Office 365), SharePoint, and Microsoft Visio. \n Experience with Agile and SAFe methodologies. \n  Desired Skills \n \n \n \n Working knowledge of Atlassian, JIRA, and Confluence. \n Familiarity with formalized requirements management frameworks (including requirements traceability) such as IEEE and Service Oriented Architecture (SOA) concepts. \n Knowledge of GIS and a general understanding of ESRI applications and tools. \n Certification in ITIL. \n Solid understanding of Agile and SAFe development methodologies. \n Prior government work experience a plus. \n  The successful candidate is subject to a U.S. Federal Government background investigation and must be able to meet the requirements to hold a position of public trust. \n \n  Equal Opportunity Employer Minorities/Women/Protected Veterans/Disabled",
        "cleaned_desc": "  Education and Experience \n \n \n \n A Bachelor\u2019s Degree and a minimum 5 years\u2019 experience working as a Business Analyst in an enterprise environment OR a Master\u2019s Degree in an IT related field and at least 2 years of experience working as a Business Analyst \n  Required Skills and Competencies \n \n \n \n Strong technical writing skills, with the ability to concisely describe requirements and detailed functional requirements. **Candidates will be required to provide a writing sample. \n Good interpersonal skills. \n Logical and systematic analytical and logical thinking with the ability to communicate complex processes or ideas in a simple straight forward manner. \n Professional written and verbal communication skills to effectively convey advanced analytical and technical topics. \n Ability to analyze, develop, and test requirements to optimize and automate system administration and business process activities. \n Experience working with a wide range of personnel, including business sponsors, product owners, project managers, development teams, and end users at all levels of the organization.   Experience developing business process models and other graphical depictions of business and IT functionality. \n Comfortable dealing with ambiguity and adapting to changes in a matrixed environment. \n Expert working knowledge with MS Office (Office 365), SharePoint, and Microsoft Visio. \n Experience with Agile and SAFe methodologies. \n  Desired Skills \n \n \n \n Working knowledge of Atlassian, JIRA, and Confluence. \n Familiarity with formalized requirements management frameworks (including requirements traceability) such as IEEE and Service Oriented Architecture (SOA) concepts. \n Knowledge of GIS and a general understanding of ESRI applications and tools. \n Certification in ITIL. \n Solid understanding of Agile and SAFe development methodologies. \n Prior government work experience a plus. \n  The successful candidate is subject to a U.S. Federal Government background investigation and must be able to meet the requirements to hold a position of public trust. ",
        "techs": [
            "jira",
            "confluence",
            "ms office (office 365)",
            "sharepoint",
            "microsoft visio",
            "atlassian",
            "ieee",
            "service oriented architecture (soa)",
            "gis",
            "esri applications",
            "itil"
        ],
        "cleaned_techs": [
            "jira",
            "confluence",
            "microsoft",
            "sharepoint",
            "microsoft visio",
            "atlassian",
            "ieee",
            "service oriented architecture (soa)",
            "gis",
            "itil"
        ]
    },
    "46c581bef66e2389": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 43.27,
        "salary_max": 43.27,
        "title": "Business Intelligence Analyst III (Contract Talent)",
        "company": "Robert Half",
        "desc": "Robert Half is seeking a Business Intelligence Analyst III to be part of the Financial Data and Analytics (FD&A) Team. FD&A team has a centralized ownership of our Finance & Accounting reporting infrastructure including Data Governance, Operational Analytics, Finance & Accounting Reporting, Analytics and Tools & Technology. Our team works with all sub-teams in Corporate Accounting globally to ensure our data definitions and accounting and finance glossaries are accurate and up to date. We participate in the Business Analytics Council and collaborate with D&A teams throughout the organization. We also produce batch reporting for Finance & Accounting and maintain the reporting tools and technology SAP Business Objects, Domo, Workday. We provide Data Analytics support by bringing data and dashboard solutions to key decision-makers. \n  Specific responsibilities include: \n \n  Manage BI Tools/Systems \n \n  Develop architecture/implementation plans for Business Intelligence projects. \n  Create/Manage/maintain business security model, ensuring that access to data systems is granted to authorized individuals only. Create/modify user roles. Provide Security changes as needed. \n  Provide system-level support and expertise. \n  Validate Installation, patching, & configuration of BI environment, ensuring that customer base can effectively use the tool \n  Install and configure Business Object software on multiple servers. \n  Experience with Business Objects upgrades, including UNV/UNX conversion and troubleshooting performance degradation. \n  Develop standards and guidelines for the effective and efficient delivery of enterprise BI, including operational reporting, dashboarding, and business analytics across the spectrum of descriptive to prescriptive \n  Monitor BI application, ensuring that services are running and performing optimally, and that production certified content is running as planned \n  Perform technical and advisory functions in the development of BI system architecture and the creation of semantic models for reporting, dashboards, delivery and scheduling. \n  Migrate developed content to various environments and maintain order in production environment (organization, scheduling, etc.) with a documented process and attention to detail. \n  Experience creating report schedules and CMC calendar maintenance. \n  Ensure by working with IT that BI systems are kept current with patches and updates, and inform the analyst community accordingly \n  Maintain data source connectivity through ODBC/JDBC configurations \n  Publication/Distribution Configuration Updates \n  Review universe and report designs and recommend solutions for performance optimization \n  Understanding of AWS Security group console and security vulnerability software. \n \n \n  Strategy / Business Development/Consulting / Support \n \n  Partner with field/business/IT teams to understand data/reporting needs. \n  Test designs and coordinate content migration to production following thorough testing and UAT; Influence the schedule and timing accordingly. \n  Develop the strategy for working effectively with other business units to build and iterate on semantic model designs. \n  Troubleshoot existing universes/reports/dashboards and optimize performance through analyzing the design, generated SQL, and assessing database performance. \n  Use experience and expertise in data sources to recommend database level enhancements, for consideration in Data Strategy discussions. \n  Institute processes for effectively managing change for BI Analysts and promoting content to various environments. \n  Effectively anticipate how business users will interact with semantic model and build accordingly for maximum user adoption. \n  Communicate updates to customers and managers regularly, informing of progress and/or resetting expectations accordingly. \n  Develop the strategy for effectively managing the BI environment for multiple customers across field, business and IT verticals. \n  Envision new ways to utilize and promote BI technology across the company. \n  Develop automated reporting and dashboard solutions for customers using BI intermediate/advanced tools. \n  Play a central role in Robert Half\u2019s Analyst Community to guide standards, best practices, and strong data governance within our cloud BI platforms. \n \n \n  Business Intelligence Configuration, Support and Data Modelling \n \n  Analyze new/existing data sources and determine most efficient table relationships (evaluating primary keys and indexes) and model in semantic modeling tool (Universe Designer) for ease of use by business users \n  Integrate data structure and business rules into common semantic model, allowing business users to pull data from a common definition \n  Partner with business data owners to define metrics and maintain data definitions on-going \n  Implement and maintain data governance employing Master Data Management methods \n  Regularly evaluate semantic models for improvement \n  Determine appropriate user cases for compatible query mode versus dynamic query mode \n  Evaluate opportunities for data improvements, recommending summary tables, materialized views, cubes, or separate data marts to data services partners \n  Assist in the ongoing development of technical best practices for data movement, data quality, data cleansing, data security and privacy. \n \n \n  Subject Matter Expertise (SME) / Training / Mentoring \n \n  Advanced knowledge of BI architecture and tool sets \n  Provide as-needed development support and mentoring for field/business/analysts \n  Prepare data from cloud data sources (Salesforce, Workday, AWS, etc.) and in-network data warehouses using cloud ETL. \n  Organize an Analyst Community and hold regular forums to discuss successes and challenges, and inform of new product enhancements. \n \n \n  Advanced Analytics \n \n  Understanding of new and advanced analytics including various tools SAP Business Objects Lumera, Tableau, Domo, Workday Prism Analytics, Cognos and PowerBI \n  Assist in designing the dashboard solutions including KPIs, drill-down capabilities for customers using BI advanced analytic tools. \n  Support customers in using new technologies and analyze data discrepancies between source systems and BI dashboards. \n \n  Qualifications: \n \n  Bachelor\u2019s degree in business or computer science, with focus in business analytics; or equivalent relevant experience. Master\u2019s degree a plus \n  5-10 years\u2019 experience working with large data sets from complex reporting platforms, including Oracle, and SQL Server \n  5-10 years using business intelligence development tools to solve complex business problems (e.g. SAP Business Objects, Cognos, OBIEE, MicroStrategy, Domo) \n  3 years Business Objects Universe/Designer experience required, including experience building complicated business semantic models, using both optimized (star schema) and non-optimized (transactional) data sources; using conformed dimensions, dimensional modeling, dynamic data functions (date/time), and embedding data definitions \n  3 years SAP Business Objects Administration (CMC proficient) experience required , including user/security management \n  5-10 years proficiency in writing and interpreting SQL \n  Solid understanding of business report development process and SDLC \n  Proficient in standard business applications, (e.g. Excel, Access, Visio, PowerPoint) \n  Demonstrated subject matter expertise in several functional business areas \n  Solid understanding of business processes, strategies, and key revenue/margin drivers \n  Knowledge of financial accounting concepts and SOX controls is a plus \n  Ability to interact and communicate with IT, business users of varying levels of expertise and 3rd party vendors. \n  Ability to communicate complex technical information to less-technical users \n  Ability to communicate in-depth business processes to technical resources \n  Ability to create very complex process flow diagrams or flowcharts that demonstrate business or system process flow \n  Ability to gather requirements effectively; document requirements and confirm observations with business owners. \n  Ability to perform fit/gap analysis based on requirements. \n  Ability to execute on business/technical requirements and build multi-purpose interactive dashboard solutions \n  Ability to create detailed and complex test plans for medium-sized initiatives. \n  Ability to prioritize job responsibilities \n  Demonstrated customer service abilities \n  Demonstrated problem solving skills \n  Knowledge of business systems software \n  Ability to write very complex queries and reports \n  Ability to develop and manage multiple enterprise-wide project plans, meeting established deliverables and timelines \n  Ability to execute based upon directions from senior team members \n  Ability to provide guidance, mentoring, and day-to-day support to junior personnel. \n  Ability to conduct training to users and peers \n  Ability to create thorough and complex documentation \n  Ability to facilitate, conduct meetings, gather information and present status \n  Ability to design business solutions for new processes and new business practices. \n  Adaptable and demonstrates good judgment \n \n \n  The typical pay rate for this position is $43.27/hour to $63.46/hour. The rate is negotiable depending upon experience and location. \n \n  Benefits are available to contract/temporary professionals, including medical, vision, and dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit roberthalf.gobenefits.net for more information. \n \n  Robert Half Inc. is an Equal Opportunity Employer. M/F/Disability/Veteran \n \n  As part of Robert Half\u2019s Corporate Services facility employment process, any offer of employment is contingent upon successful completion of a background check. \n \n  Robert Half is committed to being an equal employment employer offering opportunities to all job seekers, including individuals with disabilities. If you believe you need a reasonable accommodation in order to search for a job opening or to apply for a position, please contact us by sending an email to HRSolutions@roberthalf.com or call 1.855.744.6947 for assistance. \n \n  In your email please include the following: \n \n  The specific accommodation requested to complete the employment application. \n  The location(s) (city, state) to which you would like to apply.",
        "cleaned_desc": "Robert Half is seeking a Business Intelligence Analyst III to be part of the Financial Data and Analytics (FD&A) Team. FD&A team has a centralized ownership of our Finance & Accounting reporting infrastructure including Data Governance, Operational Analytics, Finance & Accounting Reporting, Analytics and Tools & Technology. Our team works with all sub-teams in Corporate Accounting globally to ensure our data definitions and accounting and finance glossaries are accurate and up to date. We participate in the Business Analytics Council and collaborate with D&A teams throughout the organization. We also produce batch reporting for Finance & Accounting and maintain the reporting tools and technology SAP Business Objects, Domo, Workday. We provide Data Analytics support by bringing data and dashboard solutions to key decision-makers. \n  Specific responsibilities include: \n \n  Manage BI Tools/Systems \n \n  Develop architecture/implementation plans for Business Intelligence projects. \n  Create/Manage/maintain business security model, ensuring that access to data systems is granted to authorized individuals only. Create/modify user roles. Provide Security changes as needed. \n  Provide system-level support and expertise. \n  Validate Installation, patching, & configuration of BI environment, ensuring that customer base can effectively use the tool \n  Install and configure Business Object software on multiple servers. \n  Experience with Business Objects upgrades, including UNV/UNX conversion and troubleshooting performance degradation. \n  Develop standards and guidelines for the effective and efficient delivery of enterprise BI, including operational reporting, dashboarding, and business analytics across the spectrum of descriptive to prescriptive \n  Monitor BI application, ensuring that services are running and performing optimally, and that production certified content is running as planned \n  Perform technical and advisory functions in the development of BI system architecture and the creation of semantic models for reporting, dashboards, delivery and scheduling. \n  Migrate developed content to various environments and maintain order in production environment (organization, scheduling, etc.) with a documented process and attention to detail. \n  Experience creating report schedules and CMC calendar maintenance. \n  Ensure by working with IT that BI systems are kept current with patches and updates, and inform the analyst community accordingly \n  Maintain data source connectivity through ODBC/JDBC configurations \n  Publication/Distribution Configuration Updates \n  Review universe and report designs and recommend solutions for performance optimization \n  Understanding of AWS Security group console and security vulnerability software. \n   \n  Strategy / Business Development/Consulting / Support \n \n  Partner with field/business/IT teams to understand data/reporting needs. \n  Test designs and coordinate content migration to production following thorough testing and UAT; Influence the schedule and timing accordingly. \n  Develop the strategy for working effectively with other business units to build and iterate on semantic model designs. \n  Troubleshoot existing universes/reports/dashboards and optimize performance through analyzing the design, generated SQL, and assessing database performance. \n  Use experience and expertise in data sources to recommend database level enhancements, for consideration in Data Strategy discussions. \n  Institute processes for effectively managing change for BI Analysts and promoting content to various environments. \n  Effectively anticipate how business users will interact with semantic model and build accordingly for maximum user adoption. \n  Communicate updates to customers and managers regularly, informing of progress and/or resetting expectations accordingly. \n  Develop the strategy for effectively managing the BI environment for multiple customers across field, business and IT verticals. \n  Envision new ways to utilize and promote BI technology across the company. \n  Develop automated reporting and dashboard solutions for customers using BI intermediate/advanced tools. \n  Play a central role in Robert Half\u2019s Analyst Community to guide standards, best practices, and strong data governance within our cloud BI platforms. \n \n \n  Business Intelligence Configuration, Support and Data Modelling \n \n  Analyze new/existing data sources and determine most efficient table relationships (evaluating primary keys and indexes) and model in semantic modeling tool (Universe Designer) for ease of use by business users \n  Integrate data structure and business rules into common semantic model, allowing business users to pull data from a common definition \n  Partner with business data owners to define metrics and maintain data definitions on-going    Implement and maintain data governance employing Master Data Management methods \n  Regularly evaluate semantic models for improvement \n  Determine appropriate user cases for compatible query mode versus dynamic query mode \n  Evaluate opportunities for data improvements, recommending summary tables, materialized views, cubes, or separate data marts to data services partners \n  Assist in the ongoing development of technical best practices for data movement, data quality, data cleansing, data security and privacy. \n \n \n  Subject Matter Expertise (SME) / Training / Mentoring \n \n  Advanced knowledge of BI architecture and tool sets \n  Provide as-needed development support and mentoring for field/business/analysts \n  Prepare data from cloud data sources (Salesforce, Workday, AWS, etc.) and in-network data warehouses using cloud ETL. \n  Organize an Analyst Community and hold regular forums to discuss successes and challenges, and inform of new product enhancements. \n \n \n  Advanced Analytics \n \n  Understanding of new and advanced analytics including various tools SAP Business Objects Lumera, Tableau, Domo, Workday Prism Analytics, Cognos and PowerBI \n  Assist in designing the dashboard solutions including KPIs, drill-down capabilities for customers using BI advanced analytic tools. \n  Support customers in using new technologies and analyze data discrepancies between source systems and BI dashboards. \n \n  Qualifications:   \n  Bachelor\u2019s degree in business or computer science, with focus in business analytics; or equivalent relevant experience. Master\u2019s degree a plus \n  5-10 years\u2019 experience working with large data sets from complex reporting platforms, including Oracle, and SQL Server \n  5-10 years using business intelligence development tools to solve complex business problems (e.g. SAP Business Objects, Cognos, OBIEE, MicroStrategy, Domo) \n  3 years Business Objects Universe/Designer experience required, including experience building complicated business semantic models, using both optimized (star schema) and non-optimized (transactional) data sources; using conformed dimensions, dimensional modeling, dynamic data functions (date/time), and embedding data definitions \n  3 years SAP Business Objects Administration (CMC proficient) experience required , including user/security management \n  5-10 years proficiency in writing and interpreting SQL \n  Solid understanding of business report development process and SDLC \n  Proficient in standard business applications, (e.g. Excel, Access, Visio, PowerPoint) \n  Demonstrated subject matter expertise in several functional business areas \n  Solid understanding of business processes, strategies, and key revenue/margin drivers \n  Knowledge of financial accounting concepts and SOX controls is a plus \n  Ability to interact and communicate with IT, business users of varying levels of expertise and 3rd party vendors. \n  Ability to communicate complex technical information to less-technical users \n  Ability to communicate in-depth business processes to technical resources \n  Ability to create very complex process flow diagrams or flowcharts that demonstrate business or system process flow \n  Ability to gather requirements effectively; document requirements and confirm observations with business owners. \n  Ability to perform fit/gap analysis based on requirements. \n  Ability to execute on business/technical requirements and build multi-purpose interactive dashboard solutions \n  Ability to create detailed and complex test plans for medium-sized initiatives. \n  Ability to prioritize job responsibilities \n  Demonstrated customer service abilities ",
        "techs": [
            "sap business objects",
            "domo",
            "workday",
            "universe designer",
            "oracle",
            "sql server",
            "cognos",
            "obiee",
            "microstrategy",
            "salesforce",
            "tableau",
            "powerbi"
        ],
        "cleaned_techs": [
            "sap business objects",
            "domo",
            "workday",
            "universe designer",
            "oracle",
            "sql",
            "cognos",
            "obiee",
            "microstrategy",
            "salesforce",
            "tableau",
            "powerbi"
        ]
    },
    "dd30b860f322574e": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 79250.56,
        "salary_max": 100348.875,
        "title": "BCBA / Behavior Analyst Remote",
        "company": "BURNETT THERAPEUTIC SERVICES INC",
        "desc": "Job description \n \n  The BCBA position will be held by an individual who has the training, education, and/or experience necessary to serve as the Qualified Autism Service Professional. This position will provide clinical supervision as well as oversight to the programs being implemented by the Supervisor and Behavior Interventionists. The BCBA is an individual who meets all off the following criteria: Provides behavioral health treatment, Develops and Implements Treatment Plans with the oversight and guidance of the Clinical Director, and has training and experience in providing services for individuals with developmental disabilities including pervasive developmental disorder, intellectual disability, autism or other delays.\n  \n \n Essential Job Functions / Key Performance Areas \n \n  To perform this job successfully, the Supervisor must be able to satisfactorily perform each essential function listed below:\n  \n \n \n Designing, developing and implementing a wide range of ABA programs for individuals with autism, developmental disabilities, and/or delays \n Collaborating with families and other professionals to tailor ABA programs to each client\u2019s specific needs \n Directly delivering individual and group learning experiences to assist each consumer served in obtaining his/her treatment goals and objectives \n Meet with, train, and supervise Supervisors and direct care staff (Behavior Interventionists) to provide guidance and review progress on goals, determine the effectiveness of intervention strategies and consult about the overall implementation of the treatment plan via telehealth \n Evaluate and update client progress reports \n Conduct assessments, write treatment goals, analyze data and write progress reports, under the direction and supervision of the BCBA/Licensed Clinical Supervisor \n Conduct comprehensive Functional Behavior Assessments utilizing standardized assessment tools including the VB-MAPP, ABBLS-R, Vineland-III, etc., under the direction and supervision of the BCBA / Licensed Clinical Supervisor \n Develop and oversee the implementation of behavioral plans for children and adults diagnosed with developmental disabilities including pervasive developmental disorder, intellectual disability, autism or other delays. \n Provide assistance in the development of adaptive skills in order for the individual to take a more positive role in the school, home & community setting \n Provide prescribed behavioral consultation to the family and / or school \n Supervise ABA programs to assist clients learn a variety of skills related to cooperation with daily routines and demands, coping with stress, play and social interactions, functional communication, attending to tasks, safety awareness and self-help \n Empower parents, caregivers and other support staff to influence clients within their natural routines so they can be more successful, independent and self-empowered\n  \n \n \n Establish and maintain positive and supportive relationships with clients, all family members and any other staff persons supporting the client\u2019s individualized needs \n Define target behaviors and write treatment goals with coordinating evidence-based programs, reinforcement and antecedent management strategies, protocols, interventions, etc., under the direction and supervision of the BCBA / Licensed Clinical Supervisor \n Model for and teach parents/care providers how to implement interventions and protocols utilized during sessions \n Model for and teach support staff how to implement intervention strategies and protocols across a variety of settings \n Provide appropriate data sheets that reflect client's goals/programs \n Set up and maintain ABA session schedule for support staff and clients and manage planned and unplanned adjustments or cancelations \n Meet with families/caregivers in their homes OR via telehealth on a weekly basis to review progress on goals, determine the effectiveness of intervention strategies and consult about the overall implementation of the treatment plan \n Job Types: Full-time / Remote / Hybrid Options also available\n  \n  Job Type: Full-time\n  \n  Benefits:\n  \n \n \n Dental insurance \n Health insurance \n Vision insurance \n Schedule:\n  \n \n \n Monday to Friday \n  Education:\n  \n \n \n Master's (Required) \n  License/Certification:\n  \n \n \n BCBA (Required) \n  Work Location: Remote\n  \n This is a remote position.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "33b12128aef51d42": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 98147.36,
        "salary_max": 124276.42,
        "title": "Sr. Business Analyst *REMOTE*",
        "company": "Professional Solutions Delivered, LLC",
        "desc": "Positions are REMOTE Work.  Applicants must have the ability to provide their own work environment, internet access, and be able to attend online meetings daily.\n         Professional Solutions Delivered, LLC (ProSoDel) is a total solutions provider for government and commercial customers in the areas of Program Management, Logistics and Information Technology Support Services. We are currently seeking a  Senior Business  Analyst  to join our team of professionals in support of the Department of Veterans Affairs. \n  Essential Duties & Job Functions: \n \n  In this role, you will manage the delivery of stakeholder and business partners initiatives on the ServiceNow platform. \n  This position will interact with various levels of management, internal service teams, and third-party vendors in order to support customers within the Department of Veteran's Affairs. \n  You will identify and solve for core business problems by turning high-level business requirements into clear, well-defined User Stories and Acceptance Criteria. \n  As needed, you will perform gap analysis of current and/or desired processes to ServiceNow\u2019s OOB functionality and conduct \u201cwalkthroughs\u201d of proposed solution to align stakeholder expectations. \n  You will collaborate with ServiceNow product owner, process owners, platform architect and engineering leads, and PMO to develop and maintain ServiceNow product roadmaps that aligns with the organization\u2019s strategic objectives while balancing platform technical manageability and business needs. \n  You will support project team in creation of non-project management, pre-release artifacts (i.e. process flow diagrams, design mockup, UAT test plan and scripts, etc.). You will plan and develop user enablement content (i.e. operating procedures, how-to documents, training materials, feature release note, etc.). \n  You will define, setup, and supervise project benefits realization through regular reports and dashboards. \n  You will conduct periodic business review with team members to communicate realized value, identify improvement opportunities, and strengthen long term partnership. \n  You will serve as a liaison between the internal customer and development team by promptly respond to inquiries and effectively communicate ServiceNow PPM program status to stakeholders and management. \n \n  Abilities: \n \n  Possess written and oral communications skills, to include public speaking. \n  To develop strong remote team and client relationships \n  Possess excellent supervision/management, analytical, and organizational skills \n  Demonstrate ability to manage multiple projects on tight deadlines and to prioritize appropriately. \n  Demonstrate the ability to work collaboratively to achieve objectives. \n  Possess a solid track record of delivering excellent customer service. \n \n  Job Requirements (Education, Experience, Professional Associations) \n \n  United States Citizen \n  Education:\n          \n  Bachelor's degree with 6 years\u2019 experience in relevant field, or \n  Associate's degree with 10 years' experience in relevant field, or \n  High school diploma with 16 years' experience in relevant field. \n \n \n \n  Clearance: T2 \n  5+ Years of total IT experience as a Business Analyst \n  4+ years of experience eliciting requirements and conducting business process reviews/documentation while applying detailed knowledge of ServiceNow to design an optimal solution (maybe non-required) \n  Experience implementing systems using Agile methodology \n  Experience working with US Federal Government customers \n \n  Non-Required Qualifications \n \n  Certified System Administrator (CSA) or other ServiceNow certifications is highly desired \n  Experience with the ServiceNow Project & Portfolio Management (PPM) application \n  Experience with the Department of Veteran's Affairs \n  Certified Associate in Project Management (CAPM), Project Management Professional (PMP), Agile SAFe, and/or SCRUM Master certifications",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "e4cd82f0cac4b30f": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 97557.664,
        "salary_max": 123529.74,
        "title": "Sr. Business Analyst *REMOTE*",
        "company": "Professional Solutions Delivered, LLC",
        "desc": "Positions are REMOTE Work.  Applicants must have the ability to provide their own work environment, internet access, and be able to attend online meetings daily.\n         Professional Solutions Delivered, LLC (ProSoDel) is a total solutions provider for government and commercial customers in the areas of Program Management, Logistics and Information Technology Support Services. We are currently seeking a  Senior Business  Analyst  to join our team of professionals in support of the Department of Veterans Affairs. \n  Essential Duties & Job Functions: \n \n  In this role, you will manage the delivery of stakeholder and business partners initiatives on the ServiceNow platform. \n  This position will interact with various levels of management, internal service teams, and third-party vendors in order to support customers within the Department of Veteran's Affairs. \n  You will identify and solve for core business problems by turning high-level business requirements into clear, well-defined User Stories and Acceptance Criteria. \n  As needed, you will perform gap analysis of current and/or desired processes to ServiceNow\u2019s OOB functionality and conduct \u201cwalkthroughs\u201d of proposed solution to align stakeholder expectations. \n  You will collaborate with ServiceNow product owner, process owners, platform architect and engineering leads, and PMO to develop and maintain ServiceNow product roadmaps that aligns with the organization\u2019s strategic objectives while balancing platform technical manageability and business needs. \n  You will support project team in creation of non-project management, pre-release artifacts (i.e. process flow diagrams, design mockup, UAT test plan and scripts, etc.). You will plan and develop user enablement content (i.e. operating procedures, how-to documents, training materials, feature release note, etc.). \n  You will define, setup, and supervise project benefits realization through regular reports and dashboards. \n  You will conduct periodic business review with team members to communicate realized value, identify improvement opportunities, and strengthen long term partnership. \n  You will serve as a liaison between the internal customer and development team by promptly respond to inquiries and effectively communicate ServiceNow PPM program status to stakeholders and management. \n \n  Abilities: \n \n  Possess written and oral communications skills, to include public speaking. \n  To develop strong remote team and client relationships \n  Possess excellent supervision/management, analytical, and organizational skills \n  Demonstrate ability to manage multiple projects on tight deadlines and to prioritize appropriately. \n  Demonstrate the ability to work collaboratively to achieve objectives. \n  Possess a solid track record of delivering excellent customer service. \n \n  Job Requirements (Education, Experience, Professional Associations) \n \n  United States Citizen \n  Education:\n          \n  Bachelor's degree with 6 years\u2019 experience in relevant field, or \n  Associate's degree with 10 years' experience in relevant field, or \n  High school diploma with 16 years' experience in relevant field. \n \n \n \n  Clearance: T2 \n  5+ Years of total IT experience as a Business Analyst \n  4+ years of experience eliciting requirements and conducting business process reviews/documentation while applying detailed knowledge of ServiceNow to design an optimal solution (maybe non-required) \n  Experience implementing systems using Agile methodology \n  Experience working with US Federal Government customers \n \n  Non-Required Qualifications \n \n  Certified System Administrator (CSA) or other ServiceNow certifications is highly desired \n  Experience with the ServiceNow Project & Portfolio Management (PPM) application \n  Experience with the Department of Veteran's Affairs \n  Certified Associate in Project Management (CAPM), Project Management Professional (PMP), Agile SAFe, and/or SCRUM Master certifications",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "32e4f79073bb0cb5": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 81000.0,
        "salary_max": 94000.0,
        "title": "BCBA - Behavior Analyst - ABA Therapist (FT)",
        "company": "Delta-T Grp.",
        "desc": "Job Details: \n \n \n  Position: BCBA - Center/Hybrid - Competitive Pay. \n  Full Job Description: BCBA ABA Therapy \n  Total First Year Compensation $81,000 to $94,000. \n  Job Type:  Full-Time Center, Hybrid. \n  Pay:  $81,000 to $94,000 (First Year). \n \n \n   Kaleidoscope ABA is a private agency looking to hire a center-based BCBA position to work part of your week remotely.\n   \n \n Position Overview: \n \n \n  The Hybrid BCBA will deliver direct services to clients as well as manage and supervise a small caseload of 4-6 clients. \n  The Clinical Manager and Operations Manager at Kaleidoscope ABA will provide you with administrative support, scheduling help, and general assistance with the Hybrid BCBA's caseload. \n \n \n  What do we offer Full-Time Clinicians: \n \n \n  HYBRID Remote Work Schedule. \n  Full-Time Mon-Friday schedule - NO weekends or late nights. \n  Attractive Benefits Plan! \n  Guaranteed salary regardless of client cancellations. \n  Up to $6000 annual incentive bonus (paid monthly). \n  Small caseload. \n  Laptop provided. \n  Medical, Dental, and Vision Insurance. \n  8 Paid Holidays + 16 PTO Days Yr 2, 11 Yr 1. \n  CEU stipend. \n  Voluntary Benefits - STD, LTD, etc. \n  401K, 401K match. \n \n \n  Responsibilities: \n \n \n  Conduct assessments and reassessments for clients. \n  Develop individual goals and objectives to be included in client Treatment Plans. \n  Develop written guidelines for behavioral interventions, teaching plans, and programs. \n  Collect data for each goal/objective during each direct session. \n  Record data into company software and the individual's confidential file. \n  Provide training in behavioral interventions and applied behavior analysis to families and staff. \n  Analyze data collected to determine program effectiveness. \n  Supervise and coach Behavior Therapists. \n \n \n  Supplemental Pay: \n \n \n  Monthly incentive. \n  Sign-on bonus of $3000. \n \n \n  Benefits: \n \n \n  Medical Insurance \n  Dental Insurance \n  Vision Insurance \n  Life Insurance \n  Paid Holidays (8 days) \n  PTO 16 days -Yr 2, (11 - Yr 1) \n  Voluntary STD, LTD, Accident, Cancer \n  401K \n  401K Match 6% \n  CEU Stipend \n \n \n  EXPERIENCE & EDUCATION REQUIRED: \n \n \n  Possess a minimum of a master's degree or national equivalent with a major in psychology, or special education. \n  Applied Behavior Analysis or a related field of study is preferred. \n  Obtained certification as a Board-Certified BCBA as verified through the Certification Board. \n  Active Arizona License. \n  A minimum of 1 year of experience in working with children, adolescents, and/or adults with various special needs. \n  Knowledge of appropriate behavioral intervention strategies, earning theories and instructional methods, ethics, laws, and regulations of acceptable behavior interventions. \n  Proficient in technology, such as Office 365, Microsoft Word, Excel, PowerPoint, and ABA software platforms. \n  Use a computer and behavioral software to prepare documents and maintain client records. \n  Work independently and make decisions within the framework of established guidelines. \n  Supervise clients during treatment sessions. \n  Ability to bend, kneel, crouch, and spend time on their feet. \n  Ability to lift items to 50 lbs. on occasion. \n \n \n  ABOUT US: \n \n \n   Kaleidoscope ABA provides center-based, community, and home-based Applied Behavioral Analysis (ABA) Therapy services to children and young adults. We use evidence-based, best-practice models to support and provide effective treatment to individuals with Autism Spectrum Disorder (ASD) as well as other behavioral and developmental disabilities. Our team partners with family members to create individualized ABA treatment plans designed to meet the individual needs of each person we serve.\n   \n \n \n  Call, email, or apply: \n \n \n   Kandace Robinson, 215-278-8372, krobinson@kfamilysolutions.org\n    https://www.kfsaba.org\n    https://www.kfsaba.org/eeo-statement/ (Kaleidoscope Family Solutions ABA, Inc. is an EEO employer.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "5521cd6e42694ea2": {
        "terms": [
            "data analyst"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Care Insights Data Analyst - REMOTE",
        "company": "Huron Consulting Group Inc.",
        "desc": "The Opportunity\n  \n \n   Huron helps its clients drive growth, enhance performance and sustain leadership in the markets they serve. We help healthcare organizations build innovation capabilities and accelerate key growth initiatives, enabling organizations to own the future, instead of being disrupted by it. Together, we empower clients to create sustainable growth, optimize internal processes and deliver better consumer outcomes.\n   \n  Health systems, hospitals and medical clinics are under immense pressure to improve clinical outcomes and reduce the cost of providing patient care. Investing in new partnerships, clinical services and technology is not enough to create meaningful and substantive change. To succeed long-term, healthcare organizations must empower leaders, clinicians, employees, affiliates and communities to build cultures that foster innovation to achieve the best outcomes for patients.\n   \n  Joining the Huron team means you\u2019ll help our clients evolve and adapt to the rapidly changing healthcare environment and optimize existing business operations, improve clinical outcomes, create a more consumer-centric healthcare experience, and drive physician, patient and employee engagement across the enterprise.\n   \n  Join our team as the expert you are now and create your future.\n  \n \n   Position Summary\n   The Care Insights team at Huron Consulting Group works with Hospitals, Health Systems, and consulting teams to analyze market growth, network integrity patterns, and disease prevalence.\n  \n  The Care Insights Data Analyst will work alongside managers and associates to query a 38 billion record claims data set and provide meaningful analytics to end users. These users\u2019 range in level of \u201cdata comfortability\u201d and deliverables will vary from executive level power points to large parquet file data dumps to IT users.\n  \n  In this role, the Analyst must be comfortable with SQL to read and query a table, Python \u2013 specifically Pandas to work with files that are too big for excel and transform data, Excel, and Power Point. The Analyst should be comfortable presenting data discoveries to end user clients both internal and external.\n  \n  Let\u2019s get to work \u2013 together.\n  \n   Qualifications\n  \n \n \n   Required:\n  \n \n \n \n     U.S. work authorization is required.\n    \n \n \n     B.S. / B.A. degree with a focus in in computer science, information systems, engineering, or other quantitative or scientific disciplines and/or or equivalent work experience.\n    \n \n \n     A minimum of 1-3 years of professional work experience as a Business Analyst and/ or similar type of role.\n    \n \n \n     Expert level data visualization skills.\n    \n \n \n     Strong collaboration skills and ability to work across multiple team\u2019s w/ a variety of expertise and various levels.\n    \n \n \n     Communicates well with a wide range of audiences (technical and non-technical, staff and managerial), and in a wide range of settings (e.g., one-on-one, small group, in person, conference calls and via email) and across multiple time zones.\n    \n \n \n     Excellent interpersonal skills; highly invested in developing strong relationships both with customers and colleagues.\n    \n \n \n     Comfortable working with a wide range of stakeholders and functional teams including engineering and product, sales and marketing, delivery, and customer success.\n    \n \n \n     Ability to multi-task effectively in a fast-paced environment; and reprioritize and meet deadlines as new tasks arise.\n    \n \n \n     This position can be Full Time remote and located anywhere within the U.S.\n    \n \n \n     Potential to travel up to 10% annually.\n    \n \n \n \n   Preferred:\n  \n \n \n \n     A minimum of 1 -3 years of domain expertise in Clinical Operations/ Care Delivery, Population Health, Quality Based Healthcare, or related field.\n    \n \n \n     Passion for making healthcare better for patients and more insightful for providers working to improve healthcare outcomes.\n    \n \n \n     Comfortable with data driven analytics and products as well as healthcare quality datasets and metrics.\n    \n \n \n     Familiarity within healthcare providers (i.e. health/hospital systems and healthcare data).\n    \n \n \n     Custom visualization programming experience (D3, R, matplotlib, Plotly, etc.)\n    \n \n \n     Strong interpersonal, verbal, and written communication skills to deliver your message to varied audiences (e.g., client stakeholders, delivery teams, etc.).\n    \n \n \n     Strong analytical and logical problem-solving approach.\n    \n \n \n     Highly self-motivated and able to work independently as well as in a team environment.\n    \n \n \n     Well organized, skilled in time management and strong attention to detail.\n    \n \n \n \n   The estimated base salary range for this job is $65,000 - $90,000. The range represents a good faith estimate of the range that Huron reasonably expects to pay for this job at the time of the job posting. The actual salary paid to an individual will vary based on multiple factors, including but not limited to specific skills or certifications, years of experience, market changes, and required travel. This job is also eligible to participate in Huron\u2019s annual incentive compensation program, which reflects Huron\u2019s pay for performance philosophy. Inclusive of annual incentive compensation opportunity, the total estimated compensation range for this job is $70,200 - $100,800. The job is also eligible to participate in Huron\u2019s benefit plans which include medical, dental and vision coverage and other wellness programs. The salary range information provided is in accordance with applicable state and local laws regarding salary transparency that are currently in effect and may be implemented in the future.\n  \n \n \n   #LI-JY1\n  \n \n   #LI-Remote\n  \n \n   Posting Category\n   Healthcare\n  \n   Opportunity Type\n   Regular\n  \n   Country\n   United States of America",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "fc955ba6794578f4": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 92948.59,
        "salary_max": 117693.625,
        "title": "Business Analyst",
        "company": "Sprezzatura Management Consulting",
        "desc": "Position Description: \n  Sprezzatura is seeking an experienced Business Analyst to join our team and play a pivotal role in supporting the Customer Experience Datawarehouse initiative. The ideal candidate should possess a strong problem-solving attitude, with a proven ability to develop precise requirements and user stories that align seamlessly with our business goals and objectives. This individual will exercise sound judgment, uphold discretion, and exhibit exceptional written and verbal communication skills. In addition, strong administrative and organizational capabilities are essential, along with a track record of delivering projects on schedule and achieving desired outcomes. If you're ready to make a meaningful impact on a dynamic project, we invite you to apply and be part of our team's success. \n \n  Roles/Responsibilities: \n \n Requirements Gathering and Analysis: Collaborate with stakeholders, including business users, subject matter experts, and technical teams, to elicit, analyze, and document functional and non-functional requirements for projects and systems. Conduct interviews, workshops, and other techniques to gather comprehensive and accurate requirements. \n Requirements Documentation: Create clear, concise, and comprehensive requirements documents, including user stories, use cases, process flows, and functional specifications. Ensure that requirements are well-defined, unambiguous, and aligned with business goals and objectives. \n Requirements Validation and Verification: Review and validate requirements with stakeholders to ensure accuracy, completeness, and alignment with their needs. Facilitate requirements workshops and meetings to resolve conflicts, clarify ambiguities, and gain consensus on requirements. \n JIRA Experience including the creation and management of JIRA tickets through a development lifecycle.  \n Data Analysis: Collect, organize, synthesize, and analyze data; summarize findings; develop conclusions and recommendations. \n Change Management: Assess and manage requirements changes throughout the project lifecycle, ensuring that changes are properly evaluated, documented, and communicated to stakeholders. Collaborate with project teams to assess the impact of changes on project scope, schedule, and resources. \n Stakeholder Engagement: Engage with stakeholders to understand their needs, gather feedback, and provide regular updates on requirements-related activities. Build and maintain strong relationships with stakeholders to foster collaboration and ensure their requirements are accurately captured and addressed. \n Process Improvement: Continuously assess and improve requirements management processes, frameworks, and tools to enhance efficiency and effectiveness. Identify opportunities for automation, standardization, and best practices implementation to streamline requirements gathering, analysis, and documentation. \n Administrative Support: Coordinate, schedule, and support meetings by providing agendas, and meeting minutes. \n Participates in continuous improvement activities by identifying and analyzing the effectiveness and efficiency of existing processes and developing strategies for improvements. \n Use the knowledge and perspective gained through project work to identify the best path forward. \n Business development and proposal support as required. \n \n \n Qualifications and Skills: \n \n Bachelor's degree in a relevant field, such as Business Analysis, Computer Science, Information Management, Information Technology, or a related discipline. \n Minimum of 5 years of experience as a Requirements Analyst or in a similar role, demonstrating expertise in requirements gathering, analysis, and documentation. \n Strong knowledge and practical experience with requirements elicitation techniques, such as interviews, workshops, and prototyping. \n Proven expertise in cloud computing platforms such as AWS, Azure, or Google Cloud. \n Demonstrated experience working with cloud-based data warehousing solutions (e.g., Amazon Redshift, Snowflake, Azure Synapse Analytics). \n Proficiency in API integration. \n Strong grasp of data extraction, transformation, and loading (ETL) processes in a cloud environment. \n Proficiency in requirements management tools, such as JIRA, Confluence, or similar software. \n Excellent analytical and problem-solving skills, with the ability to interpret and prioritize requirements based on business value and project constraints. \n Strong written and verbal communication skills, with the ability to effectively communicate complex information to technical and non-technical stakeholders. \n Detail-oriented and results-driven, with a commitment to delivering high-quality requirements documentation within deadlines. \n Ability to work independently and collaboratively within a team environment, managing multiple priorities and adapting to changing project needs. \n Professional certifications in business analysis, such as CBAP or CCBA, are desirable. \n Familiarity with agile or iterative development methodologies is a plus. \n \n \n \n  Transitioning military and/or Veterans with relevant experience are invited to apply. Sprezzatura is an equal opportunity employer. Sprezzatura offers benefits including healthcare, 401K, vacation, and paid sick leave. \n \n \n  Company Description \n  Sprezzatura Management Consulting, LLC (www.sprezzmc.com) is a Washington, DC-area Service-Disabled Veteran-Owned Small Business (SDVOSB) that enables government transformation by supplying insight and leadership at the intersection of people, processes, and technology. We apply knowledge, project, and life-cycle management best practices to catalyze change.",
        "cleaned_desc": " Proven expertise in cloud computing platforms such as AWS, Azure, or Google Cloud. \n Demonstrated experience working with cloud-based data warehousing solutions (e.g., Amazon Redshift, Snowflake, Azure Synapse Analytics). \n Proficiency in API integration. \n Strong grasp of data extraction, transformation, and loading (ETL) processes in a cloud environment. \n Proficiency in requirements management tools, such as JIRA, Confluence, or similar software. \n Excellent analytical and problem-solving skills, with the ability to interpret and prioritize requirements based on business value and project constraints. \n Strong written and verbal communication skills, with the ability to effectively communicate complex information to technical and non-technical stakeholders. \n Detail-oriented and results-driven, with a commitment to delivering high-quality requirements documentation within deadlines. ",
        "techs": [
            "aws",
            "azure",
            "google cloud",
            "amazon redshift",
            "snowflake",
            "azure synapse analytics",
            "api integration",
            "jira",
            "confluence"
        ],
        "cleaned_techs": [
            "aws",
            "azure",
            "gcp",
            "snowflake",
            "api integration",
            "jira",
            "confluence"
        ]
    },
    "3d8a230da7b30db2": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 45.0,
        "salary_max": 65.0,
        "title": "Sr. Data Warehouse Reporting Analyst",
        "company": "The Judge Group",
        "desc": "Our client is currently seeking a Sr. Data Warehouse Reporting Analyst \n   This role will be remote for candidates residing in Southern California only.\n   TOP SKILL: Focus on Power BI. \n  The Data Warehouse Reporting Analyst Sr. collaborates with the Information Technology Services (ITS) and Data Warehouse (DW) teams to develop and support organization-wide reporting. The incumbent participates in gathering and documenting report requirements and is responsible for designing, building and administering reporting solutions for requesting departments. Assistance will be provided to departmental Business/Data Analysts in developing reports using the available Business Intelligence (BI) tools. The incumbent will develop ad-hoc reports for business users as applicable and participates in the design and development of the reporting infrastructure using Structured Query Language (SQL) Server technologies and BI reporting tools. The incumbent is responsible for administering report security and deployment within ITS and data security policies as well as future environment upgrades. \n This job will have the following responsibilities: \n \n \n \n Participates in a mission-driven culture of high-quality performance, with a member focus on customer service, consistency, dignity and accountability.   Assists the team in carrying out department responsibilities and collaborates with others to support short- and long-term goals/priorities for the department.   Leads the development of business and functional requirements for reports.   Designs, develops, tests and implements reports/dashboards and Extraction, Transformation and Loading (ETL) to fulfill business requirements as needed.   Serves as a subject matter expert across the organization and assists users with reporting (including ad-hoc reporting) and analysis activities and tools.   Supports user training, testing, documentation and the roll-out of reports and end-user reporting tools.   Administers Business Intelligence security, deployment, upgrades and maintenance.   Maintains knowledge of business processes and application design and understand cross-functional business relationships and their use of information. \n \n \n Qualifications & Requirements: \n \n   5 years of experience with general relational databases and reporting development required.   5 years of experience with BI data warehousing reporting analysis, design, testing and administration required.   5 years of experience developing Microsoft SQL Server stored procedures and using reporting tools such as SQL Server Reporting Services (SSRS), Tableau, Power BI, MicroStrategy, Business Objects and/or Cognos for developing reports required.   An equivalent combination of education and experience sufficient to successfully perform the essential duties of the position such as those listed above is also qualifying.     Preferred Qualifications   Bachelor?s degree in Information Technology or related field.   1 year of experience in developing reports in a health care related business, preferably a health plan.   Experience with SSRS, Tableau and/or Power BI strongly.",
        "cleaned_desc": "   TOP SKILL: Focus on Power BI. \n  The Data Warehouse Reporting Analyst Sr. collaborates with the Information Technology Services (ITS) and Data Warehouse (DW) teams to develop and support organization-wide reporting. The incumbent participates in gathering and documenting report requirements and is responsible for designing, building and administering reporting solutions for requesting departments. Assistance will be provided to departmental Business/Data Analysts in developing reports using the available Business Intelligence (BI) tools. The incumbent will develop ad-hoc reports for business users as applicable and participates in the design and development of the reporting infrastructure using Structured Query Language (SQL) Server technologies and BI reporting tools. The incumbent is responsible for administering report security and deployment within ITS and data security policies as well as future environment upgrades.   \n   5 years of experience with general relational databases and reporting development required.   5 years of experience with BI data warehousing reporting analysis, design, testing and administration required.   5 years of experience developing Microsoft SQL Server stored procedures and using reporting tools such as SQL Server Reporting Services (SSRS), Tableau, Power BI, MicroStrategy, Business Objects and/or Cognos for developing reports required.   An equivalent combination of education and experience sufficient to successfully perform the essential duties of the position such as those listed above is also qualifying.     Preferred Qualifications   Bachelor?s degree in Information Technology or related field.   1 year of experience in developing reports in a health care related business, preferably a health plan.   Experience with SSRS, Tableau and/or Power BI strongly.",
        "techs": [
            "power bi",
            "sql server reporting services (ssrs)",
            "tableau",
            "microstrategy",
            "business objects",
            "cognos"
        ],
        "cleaned_techs": [
            "powerbi",
            "sql",
            "tableau",
            "microstrategy",
            "business objects",
            "cognos"
        ]
    },
    "694b7e07fb7c4429": {
        "terms": [
            "data analyst"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Accounting Analyst",
        "company": "JCPenney",
        "desc": "Accounting Analyst - General Ledger Accounting Analyst -  Payables & Recoveries \n J.C. Penney Company \n Salt Lake City, UT \n With varying degrees of supervision, develops, implements, and operates reporting and analytical activities within assigned functions. This role will also support the effective processing and accumulation of financial data and its timely presentation to operating management. \n Primary Responsibilities: \n \n Performs assigned reporting and analytical activities to provide timely delivery of financial information to appropriate Company management that accurately measures financial results and enhances decision making. \n Develops, implements, and conducts projects and analysis, as assigned, to ensure that proper and cost-effective controls are included in systems and processes. \n Develops and refines techniques that support reporting standards, controls, and measurements for management's use. \n Collects and compiles data and presents oral and written reports to support development, implementation, or evaluation of analytical and accounting-related programs and procedures. Identifies areas in current systems and procedures that should be reviewed for improvement. \n Develops or supervises the development of proper written documentation for all finance, analytical and reporting-related processes in assigned functions. \n Assists in review of account reconciliations including research and analysis on reconciling items. \n \n Core Competencies & Accomplishments: \n \n Demonstrates strong interpersonal skills. \n Advanced Analytical, Decision Making, and Conceptual Skills \n Thorough knowledge of procedures, systems, operations, standards, and reports that apply to financial reporting. \n Experienced user of SQL query tools \n Cross-functional enterprise understanding and the ability to lead cross-functional projects \n Demonstrates ability to lead multiple processes \n Experienced with data mining and independently able to find and analyze data to provide recommendations to management. \n Capable of managing workload and prioritizing multiple responsibilities in a dynamic work environment. \n Work independently without supervision. \n Quickly learn and understand systems. \n Ability to collaborate with multiple departments and drive results \n \n Experience/Education : \n \n Bachelor's degree in Accounting or business related major OR 5 years of progressive reporting and analytical experience with exposure to multiple aspects of these functions. \n Familiarity with Data Mining and Lean Tools, as well as Microsoft Office products, Oracle R12, AssureNET, and Reconnect software tools preferred \n Lean, Six Sigma certification a plus \n \n What you get: \n We offer a competitive benefits package including medical/dental/vision, term life insurance, paid vacation/holidays, 401(k) Savings Plan with company match, and an associate discount on JCPenney merchandise. \n About JCPenney: \n JCPenney is the shopping destination for diverse, working American families. With inclusivity at its core, the Company's product assortment meets customers' everyday needs and helps them commemorate every special occasion with style, quality and value. JCPenney offers a broad portfolio of fashion, apparel, home, beauty and jewelry from national and private brands and provides personal services including salon, portrait and optical. The Company and its 50,000 associates worldwide serve customers where, when and how they want to shop - from jcp.com to more than 650 stores in the U.S. and Puerto Rico. In 2022, JCPenney celebrates 120 years as an iconic American brand by continuing its legacy of connecting with customers through shopping and community engagement. Please visit JCPenney's Newsroom to learn more and follow JCPenney on Facebook, Instagram, Twitter and LinkedIn. \n For more opportunities to join our team please visit our careers page. \n Job Title:  Accounting Analyst General Ledger Location:  Salt Lake City, UT, United States- Job ID:  1113128 \n J.C. Penney Company Inc. Plano, Texas \n Job Type: Full-time \n Benefits: \n \n 401(k) \n 401(k) matching \n Dental insurance \n Disability insurance \n Employee assistance program \n Employee discount \n Flexible schedule \n Flexible spending account \n Health insurance \n Life insurance \n Paid time off \n Professional development assistance \n Tuition reimbursement \n Vision insurance \n Work from home \n \n Physical setting: \n \n Office \n \n Schedule: \n \n 8 hour shift \n Day shift \n Monday to Friday \n \n Work Location: Remote",
        "cleaned_desc": " Core Competencies & Accomplishments: \n \n Demonstrates strong interpersonal skills. \n Advanced Analytical, Decision Making, and Conceptual Skills \n Thorough knowledge of procedures, systems, operations, standards, and reports that apply to financial reporting. \n Experienced user of SQL query tools \n Cross-functional enterprise understanding and the ability to lead cross-functional projects \n Demonstrates ability to lead multiple processes \n Experienced with data mining and independently able to find and analyze data to provide recommendations to management. \n Capable of managing workload and prioritizing multiple responsibilities in a dynamic work environment. \n Work independently without supervision. \n Quickly learn and understand systems. \n Ability to collaborate with multiple departments and drive results ",
        "techs": [
            "sql query tools",
            "data mining"
        ],
        "cleaned_techs": [
            "sql",
            "data mining"
        ]
    },
    "7df120899386e00d": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 100000.0,
        "salary_max": 135000.0,
        "title": "API Designer / APi Technical Business Analyst API Design FTE",
        "company": "Robert Half",
        "desc": "IMMEDIATE HIRE!!! \n APi Designer (Highly technical API Designer without Coding ) (Direct Hire PERMANENT) \n ** 100% REMOTE NOW \n ** YOU NEED TO BE ABLE TO WORK EASTERN TIME ZONE HOURS / EASTERN HOURS !!! \n APi Designer \u2013 THIS IS NOT A DEVELOPER POSITION \n \n \n Looking for a Senior level resource / Experienced APi Designer \n  Tired of feeling like your work doesn't make an impact? FUN Team & COOL project work ! ! Our clients offers on their Dev team - the perks of a friendly, family type of environment - great benefits, bonus plan, growth, but has the appeal of being able to offer someone the opportunity to come in and make a lasting imprint with their work and ideas! \n As a APi Designer you will work with all aspects of the SDLC. Our client is looking for a Mid level Agile 5+ years API Designer using \u2013 Open 3.0, YAML, working with RESTful APi\u2019s. Experience working in an Agile team. \n Have all of the benefits of a completely casual environment, fast paced business where you can see the impact of your work! Our client is looking for a APi Designer (PERM Direct Hire Position) to add to their team. If you are looking for an opportunity to do MORE than just requirements gathering, than check out this opportunity! Work CLOSELY with the Global Development teams and across the US. You will get the opportunity to also grow and develop your product management and project management skills in leading projects, running meetings, and directing project resources! \n \n \n What You Will Need: \n \n \n YOU WILL BE DESIGNING API\u2019S NOT CODING THEM \n 5 years of APi experience \u2013 Designing APi\u2019s and APi EndPoints \n Working with RESTful APi\u2019s \n Designing APi End Points and Specifications for Web, API\u2019s, & Mobile applications. \n 5+ years designing RESTful APIs Using OpenAPI 3.0 OR Swagger. OpenAPI 3.0 or 2.0 Experience \n Must be able to write and comprehend GraphQL schemas \n YAML \u2013 Need to have YAML experience & RAML \n Anypoint Platform experience \n Defining User Stories \n Query Parameters and APi Components \n  This is a APi Designer Direct Hire position with a base salary range up to 125K plus bonus - depending on experience. For immediate and confidential consideration on this APi Designer opportunity, it is best to contact me directly, Carrie Danger. You can ONE CLICK APPLY AT on our Robert Half Website and Specifically Apply to this posting. For immediate and confidential consideration on this APi Designer Direct Hire position or other permanent IT opportunities in the Des Moines or the Iowa area, please call Carrie Danger, SVP Permanent Placement at My Direct Office #: 515-259-6087, or e-mail resume confidentially to Carrie Danger, email address : - Please find my direct email address on my LinkedIN page. Please be assured that your resume will not be submitted to any client companies without your direct permission. Our fees are paid by our client employers, never any fees to you, our candidates. Simply curious? Just inquire for more details, & definitely feel free to email me directly for details and just inquire for More Information! \n ! \n \n \n What You Will Need to Have : \n \n \n YOU WILL BE DESIGNING API\u2019S NOT CODING THEM \n 5 years of APi experience \u2013 Designing APi\u2019s and APi EndPoints \n Designing APi End Points and Specifications for Web, API\u2019s, & Mobile applications. \n 5+ years designing RESTful APIs Using OpenAPI 3.0 OR Swagger. OpenAPI 3.0 or 2.0 Experience \n Anypoint Platform experience \n Must be able to write and comprehend GraphQL schemas \n YAML \u2013 Need to have YAML experience \n Defining User Stories \n Query Parameters and APi Components \n Agile experience \n PERMANENT Direct Hire Position \n No Third Party Inquiries Please. No sponsorship provided! \n  Technology Doesn't Change the World, People Do. \u00ae \n \n  Robert Half is the world\u2019s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles. \n \n  Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go. Download the Robert Half app and get 1-tap apply, notifications of AI-matched jobs, and much more. \n \n  All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit roberthalf.gobenefits.net for more information. \n \n  \u00a9 2023 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking \u201cApply Now,\u201d you\u2019re agreeing to Robert Half\u2019s Terms of Use .",
        "cleaned_desc": " What You Will Need to Have : \n \n \n YOU WILL BE DESIGNING API\u2019S NOT CODING THEM \n 5 years of APi experience \u2013 Designing APi\u2019s and APi EndPoints \n Designing APi End Points and Specifications for Web, API\u2019s, & Mobile applications. \n 5+ years designing RESTful APIs Using OpenAPI 3.0 OR Swagger. OpenAPI 3.0 or 2.0 Experience \n Anypoint Platform experience \n Must be able to write and comprehend GraphQL schemas \n YAML \u2013 Need to have YAML experience ",
        "techs": [
            "openapi 3.0",
            "swagger",
            "anypoint platform",
            "graphql schemas",
            "yaml"
        ],
        "cleaned_techs": [
            "openapi 3.0",
            "swagger",
            "anypoint platform",
            "graphql schemas",
            "yaml"
        ]
    },
    "bcd4a1e8fb0415b8": {
        "terms": [
            "data analyst"
        ],
        "salary_min": 89894.7,
        "salary_max": 113826.72,
        "title": "Business Analyst",
        "company": "Solutions By Design II",
        "desc": "SBD is seeking a Mid-to- Sr Level Business Analyst to join an exciting project to assist with the Requirements, Analysis & Design (RAD) and documentation of the As-Is System Architecture for the Defense Contract Management Agency. The team is responsible for providing support in updating the functional requirements documentation, system architecture and system design artifacts of the current system. \n  Responsibilities: \n \n  Business and system requirements gathering and analysis \n  System Design specification and documentation  \n Process mapping and workflow documentation \n  Architecture and analysis diagramming \n  Developing end to end business and systems process flows within the actuarial model process \n  Develop documentation deliverables  \n Preparing documentation of actuarial models in accordance with internal policies to ensure proper readability and usability amongst stakeholders \n  Perform documentation quality control \n  Facilitate and manage stakeholder meetings \n \n  Minimum Qualifications: \n \n  Bachelor's Degree in Business, Information Technology or related field or 4 years of comparable experience without a degree \n  Must have an existing DoD Public Trust or Secret clearance or have completed a full background investigation within the past 5 years \n  3 years of experience with Business and system requirements gathering and analysis \n  3 years of experience with System Design specification and documentation  \n 3 years of experience with Process mapping and workflow documentation \n  3 years of experience with Architecture and analysis diagramming  3 years of experience with Developing end to end business and systems process flows within the actuarial model process \n  \n \n Preferred Qualifications: \n \n  5 years of experience with Business and system requirements gathering and analysis \n  5 years of experience with System Design specification and documentation  \n 5 years of experience with Process mapping and workflow documentation \n  5 years of experience with Architecture and analysis diagramming \n  5 years of experience with Developing end to end business and systems process flows within the \n  Client management \n  General IT system architecture knowledge or experience \n  Knowledge of IT systems and applications \n  Experience in gathering requirements for system design and development \n  Strong documentation and writing skills \n  Strong requirements gathering and identification skills \n  Strong organization skills \n  Fast Learner  \n Strong analytical skills \n  Self-Motivated \n  Strong written and verbal communication skills \n  Understands and has experience with federal government contracts",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "77f1038f4ded6818": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 40.34,
        "salary_max": 86.7,
        "title": "senior AWS Data Engineer",
        "company": "FACEBOOK APP",
        "desc": "AWS data engineers have a wide range of responsibilities, which can include: \n Creating data models that can be used to extract information from various sources and store it in a usable format Maintaining the integrity of data by designing backup and recovery procedures Identifying opportunities to improve performance by improving database structure or indexing methods Conducting research to identify new technologies that can be applied to current projects Analyzing data to find patterns or insights that can be used to develop strategies or make business decisions Developing new applications using existing data sets to create new products or improve existing services Maintaining existing applications by updating existing code or adding new features to meet new requirements Designing and implementing security measures to protect data from unauthorized access or misuse Recommending infrastructure changes to improve storage capacity or performance \n Job Types: Contract, Full-time \n Salary: $40.34 - $86.70 per hour \n Experience level: \n \n 10 years \n 11+ years \n 7 years \n 8 years \n 9 years \n \n Experience: \n \n AWS Data Engineer: 10 years (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "713aaea093a693fd": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 130000.0,
        "salary_max": 160000.0,
        "title": "Data Engineer",
        "company": "Nascent",
        "desc": "About Nascent\u2026 \n Nascent is a team of builders who back early-stage web3 founders creating products and primitives for an open financial world. Founded in 2020, we've invested in 50+ early-stage teams (https://www.nascent.xyz/portfolio) that we believe have the potential to create substantive change, expand boundaries and find new horizons. Building from a base of permanent capital, we also deploy a sizable liquid portfolio utilizing a range of strategies that ensure we are among the most active users of the open financial system we are helping to build. The fluid structure that enables our team to build, use, and invest in the future of crypto makes Nascent both an ideal early-stage partner and long-term ally. \n The   Opportunity \n As a Data Engineer at Nascent, you'll play a critical role in building and maintaining our data infrastructure and pipelines across our liquid and investing activities. This includes designing and building ETLs, optimizing research and transaction infrastructure, and implementing APIs for data querying and access. Reporting directly to the Data Engineering Lead you'll support the strategy and execution of data acquisition to enable internal research and externally facing data projects. You'll serve as a key partner across the organization, delivering internal data projects such as dashboarding and visualizations to improve investing performance.  \n The ideal candidate will have experience in designing and building data pipelines and APIs, with a strong foundation in data modeling, database design, and query optimization. Think data engineer with a software engineering lens, you can design and build software beyond data pipelines (you know the difference). You'll have a passion for working with large datasets and a desire to leverage data to drive business insights and support decision-making. As a fast-moving crypto-native firm, we have a lot of activities and both ingest and create a lot of data\u2014working closely with our Data Engineering Lead you will execute high impact work across data-related activities for the firm. This is a full time fully remote position with preference for EST working hours.  \n About you \n \n You thrive in less structured environments and are at your best when driving and delivering results with the freedom to build and execute your own plan. \n  You are as excited by starting a project as completing, maintaining, and continually optimizing it. \n  You find the plethora of opportunities to leverage data to drive value for the firm exciting versus overwhelming \n \n Required Experience \n \n Building and managing data pipelines on bare metal outside of pure cloud infrastructure (AWS, Azure, GCP, etc.) \n  Hands-on experience from beginning to end of a shipped working product (i.e., you're an experienced builder) \n  Substantive experience building and maintaining enterprise data systems, ETL frameworks, & pipelines \n  Excellent Python and SQL skills \n  Familiarity with APIs (REST, Websockets, etc) \n  Experience beyond testing (e.g. quality processes, verification & validation) \n  Experience in configuring and managing cloud infrastructure (preferred AWS) \n \n Our Team & Culture \n At Nascent, we are an interdisciplinary team of investors, builders & creators, capable of achieving more together than we can as individuals. We offer the opportunity to contribute to building the future global economic system with a world-class team and culture that pairs the freedom to explore, experiment & play with a competitive drive to win. We invest in our people by providing the autonomy to build, coupled with accountability & honest feedback to help learn, grow, perform & win. We're a fully distributed team that understands the value of in-person time\u2014we host two team retreats per year and encourage team members to come together for more frequent in-person work. \n Principles that drive our team & work \n \n Build for the long term \n  Align incentives \n  Be nimble \n  Compete to win \n  Explore, experiment, play \n  Always be building \n  Give and embrace real feedback \n \n What We Offer \n At Nascent, we offer a competitive total compensation package heavily weighted toward bonus, ensuring that when we perform at our best and the firm wins we all win. \n \n The opportunity to learn, experiment and build in an entrepreneurial environment \n  Remote and distributed working environment \n  Comprehensive health benefits package including dental, vision, and life \n  Generous paid parental leave & supported return to work \n  Home Office, coworking space and wellness stipend \n  Retirement plan matching contributions \n  Open vacation policy as well as flexible work hours and location \n  Access to our internal performance coaching, technical experts and support for continuing your skill development and growth \n  Team activities and bi-annual in-person team retreats \n \n We are an equal opportunity employer and celebrate diversity and differences of perspectives. We do not discriminate on the basis of any status, inclusive of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",
        "cleaned_desc": "About Nascent\u2026 \n Nascent is a team of builders who back early-stage web3 founders creating products and primitives for an open financial world. Founded in 2020, we've invested in 50+ early-stage teams (https://www.nascent.xyz/portfolio) that we believe have the potential to create substantive change, expand boundaries and find new horizons. Building from a base of permanent capital, we also deploy a sizable liquid portfolio utilizing a range of strategies that ensure we are among the most active users of the open financial system we are helping to build. The fluid structure that enables our team to build, use, and invest in the future of crypto makes Nascent both an ideal early-stage partner and long-term ally. \n The   Opportunity \n As a Data Engineer at Nascent, you'll play a critical role in building and maintaining our data infrastructure and pipelines across our liquid and investing activities. This includes designing and building ETLs, optimizing research and transaction infrastructure, and implementing APIs for data querying and access. Reporting directly to the Data Engineering Lead you'll support the strategy and execution of data acquisition to enable internal research and externally facing data projects. You'll serve as a key partner across the organization, delivering internal data projects such as dashboarding and visualizations to improve investing performance.  \n The ideal candidate will have experience in designing and building data pipelines and APIs, with a strong foundation in data modeling, database design, and query optimization. Think data engineer with a software engineering lens, you can design and build software beyond data pipelines (you know the difference). You'll have a passion for working with large datasets and a desire to leverage data to drive business insights and support decision-making. As a fast-moving crypto-native firm, we have a lot of activities and both ingest and create a lot of data\u2014working closely with our Data Engineering Lead you will execute high impact work across data-related activities for the firm. This is a full time fully remote position with preference for EST working hours.  \n About you \n \n You thrive in less structured environments and are at your best when driving and delivering results with the freedom to build and execute your own plan. \n  You are as excited by starting a project as completing, maintaining, and continually optimizing it.    You find the plethora of opportunities to leverage data to drive value for the firm exciting versus overwhelming \n \n Required Experience \n \n Building and managing data pipelines on bare metal outside of pure cloud infrastructure (AWS, Azure, GCP, etc.) \n  Hands-on experience from beginning to end of a shipped working product (i.e., you're an experienced builder) \n  Substantive experience building and maintaining enterprise data systems, ETL frameworks, & pipelines \n  Excellent Python and SQL skills \n  Familiarity with APIs (REST, Websockets, etc) ",
        "techs": [
            "data infrastructure",
            "pipelines",
            "etls",
            "research and transaction infrastructure",
            "apis",
            "data acquisition",
            "data modeling",
            "database design",
            "query optimization",
            "large datasets",
            "business insights",
            "decision-making",
            "data pipelines",
            "software engineering",
            "data-related activities",
            "bare metal",
            "aws",
            "azure",
            "gcp",
            "enterprise data systems",
            "etl frameworks",
            "python",
            "sql",
            "apis (rest",
            "websockets)"
        ],
        "cleaned_techs": [
            "data infrastructure",
            "pipelines",
            "etls",
            "research and transaction infrastructure",
            "apis",
            "data acquisition",
            "database design",
            "query optimization",
            "large datasets",
            "business insights",
            "decision-making",
            "data pipelines",
            "software engineering",
            "data-related activities",
            "bare metal",
            "aws",
            "azure",
            "gcp",
            "enterprise data systems",
            "etl frameworks",
            "python",
            "sql",
            "apis (rest",
            "websockets)"
        ]
    },
    "e65490aaaff7f716": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 100000.0,
        "salary_max": 140000.0,
        "title": "Data Engineer 2",
        "company": "Stratagen",
        "desc": "Stratagen is currently seeking a highly skilled Data Engineer 2 to join our team. In this role, you will play a key part in implementing essential changes to the legacy National Crime Information Center (NCIC) system in the FBI\u2019s Criminal Justice Information Services (CJIS) Division . These changes are critical for enabling the agile development of the NCIC 3rd Generation (N3G) system. As a Data Engineer, you will have diverse responsibilities encompassing data profiling, data design, data management, and the generation of test data. \n Required: \n \n B.A. or B.S. from an accredited institution \n A minimum of twelve (12) years of data management, database administration or equivalent experience \n A minimum of eight (8) years of data engineering, data analysis, or equivalent experience \n Active Top Secret Clearance \n \n Preferred; however, not required: \n \n Prior work experience with the Federal Government \n Prior work experience with CJIS systems and associated persistent and/or non- persistent data \n Prior work experience with either Amazon web Services (AwS) or Azure \n Familiarity with Agile development \n Prior work experience with the Scaled Agile Framework (SAFe) \n Experience building data visualizations and reports \n Experience working with IT and business stakeholders in designing the data architecture for a system \n Experience synthesizing and analyzing extremely large data sets \n Familiarity with optimization models \n \n Job Type: Full-time \n Pay: $100,000.00 - $140,000.00 per year \n Benefits: \n \n Dental insurance \n Health insurance \n Health savings account \n Life insurance \n Paid time off \n Retirement plan \n Vision insurance \n \n Experience level: \n \n 11+ years \n \n Schedule: \n \n Monday to Friday \n \n Security clearance: \n \n Top Secret (Required) \n \n Work Location: Remote",
        "cleaned_desc": "Stratagen is currently seeking a highly skilled Data Engineer 2 to join our team. In this role, you will play a key part in implementing essential changes to the legacy National Crime Information Center (NCIC) system in the FBI\u2019s Criminal Justice Information Services (CJIS) Division . These changes are critical for enabling the agile development of the NCIC 3rd Generation (N3G) system. As a Data Engineer, you will have diverse responsibilities encompassing data profiling, data design, data management, and the generation of test data. \n Required: \n \n B.A. or B.S. from an accredited institution \n A minimum of twelve (12) years of data management, database administration or equivalent experience \n A minimum of eight (8) years of data engineering, data analysis, or equivalent experience \n Active Top Secret Clearance \n   Preferred; however, not required: \n \n Prior work experience with the Federal Government \n Prior work experience with CJIS systems and associated persistent and/or non- persistent data \n Prior work experience with either Amazon web Services (AwS) or Azure \n Familiarity with Agile development \n Prior work experience with the Scaled Agile Framework (SAFe) \n Experience building data visualizations and reports ",
        "techs": [
            "stratagen",
            "data engineer 2",
            "national crime information center (ncic)",
            "fbi\u2019s criminal justice information services (cjis) division",
            "ncic 3rd generation (n3g)",
            "data profiling",
            "data design",
            "data management",
            "test data",
            "b.a.",
            "b.s.",
            "accredited institution",
            "data management",
            "database administration",
            "data engineering",
            "data analysis",
            "top secret clearance",
            "federal government",
            "cjis systems",
            "amazon web services (aws)",
            "azure",
            "agile development",
            "scaled agile framework (safe)",
            "data visualizations",
            "reports."
        ],
        "cleaned_techs": [
            "stratagen",
            "data engineer 2",
            "national crime information center (ncic)",
            "fbi\u2019s criminal justice information services (cjis) division",
            "ncic 3rd generation (n3g)",
            "data design",
            "data management",
            "test data",
            "b.a.",
            "b.s.",
            "accredited institution",
            "database administration",
            "top secret clearance",
            "federal government",
            "cjis systems",
            "aws",
            "azure",
            "agile development",
            "scaled agile framework (safe)",
            "data visualizations",
            "reports."
        ]
    },
    "d78c21e67324852a": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 94369.61,
        "salary_max": 119492.95,
        "title": "Data Engineer",
        "company": "Data Ideology",
        "desc": "Data Engineer - REMOTE \n \n  Data Ideology \n  At DI, we provide Data & Analytics expertise to drive measurable business outcomes, often solving complex business problems for our clients. Our data analytics advisory services enable our customers to transform data into insights by driving a culture of empowerment and ownership of results. Our team consists of highly motivated individuals who are passionate about learning, understanding, collaborating, and who are intellectually curious. For more information about Data Ideology visit www.dataideology.com . \n \n  Data Engineer - Full-time (FT) \n  We are looking for a Data Engineer to join our growing team. Data Engineer will leverage their business and technical knowledge to develop production ready data models by integrating multiple sources of data while also working with business and technical teams to understand business strategy and objectives, gather information, and ensure business requirements are being fulfilled throughout the entire data & analytics lifecycle. \n \n  Key Responsibilities \n \n  To perform in this position successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. Other duties may be assigned to meet business needs. \n \n  Ability to collect and understand business requirements and translate those requirements into an actionable data warehouse plan. \n  Knowledge of multi-dimensional and tabular design patterns and ability to identify solutions that leverage these modeling techniques. \n  Ability to work within the SDLC framework in multiple environments and understand the complexities and dependencies of the data warehouse build within those constraints. \n  Ability to define and implement best practices across database design and ETL. \n  Ability to direct the work of others, including but not limited to directing ETL development demonstrating an understand key concepts of ETL/ELT including best practices for optimization and scheduling. \n \n \n  Supervisory Responsibilities: None \n \n  Qualifications \n  Education and Experience: \n \n  Proven understanding of data warehousing, Data Architecture, and BI. \n  Experience with data pipelines and architecture/engineering. \n  Knowledge of modern apps and data platforms. \n  Cloud based project implementation. \n  SnowflakeDB experience a plus. \n \n  Knowledge, Skills and Abilities: \n \n  BI/Data Warehousing (3+ years) \n  Cloud platforms (1+ years) \n  ETL (3+ years) \n  SQL/ SSIS (3+ years) \n \n  Work Environment: \n \n  Remote work from home. \n  Hours of work and days are generally Monday through Friday. Specific business hours will depend on client needs. \n \n  Physical Demands: \n \n  Must be able to remain in a stationary position 50% of the time. \n  The person in this position must occasionally move about inside the office to access file cabinets, library stacks, office machinery, etc. \n  Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and printer. \n  The person in this position frequently communicates with clients and coworkers. Must be able to exchange accurate information in these situations. \n \n  Benefits: \n \n  Unlimited Discretionary Time Off Policy \n  Insurance (medical, dental, vision) for employees \n  100% company paid - short and long-term disability insurance for employees \n  100% company paid - life insurance and AD&D insurance for employees \n  100% company paid \u2013 employee assistance program \n  Retirement plans with company match \n  Training and Certification Reimbursement annually \n  Performance-based incentive program \n  Commission incentive program \n  Profit Sharing Plan \n  Referral Bonuses \n \n \n  Data Ideology is an EEO Employer \n   \n QqCwurEzXO",
        "cleaned_desc": "  Knowledge of multi-dimensional and tabular design patterns and ability to identify solutions that leverage these modeling techniques. \n  Ability to work within the SDLC framework in multiple environments and understand the complexities and dependencies of the data warehouse build within those constraints. \n  Ability to define and implement best practices across database design and ETL. \n  Ability to direct the work of others, including but not limited to directing ETL development demonstrating an understand key concepts of ETL/ELT including best practices for optimization and scheduling. \n \n \n  Supervisory Responsibilities: None \n \n  Qualifications \n  Education and Experience: \n \n  Proven understanding of data warehousing, Data Architecture, and BI. \n  Experience with data pipelines and architecture/engineering.    Knowledge of modern apps and data platforms. \n  Cloud based project implementation. \n  SnowflakeDB experience a plus. \n \n  Knowledge, Skills and Abilities: \n \n  BI/Data Warehousing (3+ years) \n  Cloud platforms (1+ years) \n  ETL (3+ years) \n  SQL/ SSIS (3+ years) \n \n  Work Environment: \n ",
        "techs": [
            "multi-dimensional design patterns",
            "tabular design patterns",
            "sdlc framework",
            "data warehouse build",
            "database design",
            "etl",
            "best practices",
            "etl/elt optimization and scheduling",
            "data warehousing",
            "data architecture",
            "bi",
            "data pipelines",
            "architecture/engineering",
            "modern apps and data platforms",
            "cloud based project implementation",
            "snowflakedb",
            "bi/data warehousing",
            "cloud platforms",
            "etl",
            "sql/ssis"
        ],
        "cleaned_techs": [
            "multi-dimensional design patterns",
            "tabular design patterns",
            "sdlc framework",
            "data warehouse build",
            "database design",
            "etl",
            "etl/elt optimization and scheduling",
            "data warehousing",
            "data architecture",
            "bi",
            "data pipelines",
            "architecture/engineering",
            "modern apps and data platforms",
            "cloud based project implementation",
            "snowflakedb",
            "bi/data warehousing",
            "cloud platforms",
            "sql"
        ]
    },
    "e3a2b78dd10fb62c": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 140000.0,
        "salary_max": 175000.0,
        "title": "Lead Data Engineer",
        "company": "Canyon Associates",
        "desc": "Role and Responsibilities \n Must be a hands on Data Engineer with lead/management experience. Oversee 3 and design, create, test, deploy and support SQL code. Monitor database systems and daily ETL processes. Looking for 5+ years prior experience (hands-on) \n AWS, AZURE Cloud, Azure Databricks, Azure SQL Database, Data Structure, Power BI, Snowflake, Relational databases \n Job Type: Full-time \n Pay: $140,000.00 - $175,000.00 per year \n Benefits: \n \n 401(k) \n 401(k) matching \n Dental insurance \n Employee discount \n Flexible spending account \n Health insurance \n Health savings account \n Paid time off \n Parental leave \n Vision insurance \n \n Schedule: \n \n Monday to Friday \n \n People with a criminal record are encouraged to apply \n Education: \n \n Bachelor's (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "e2317b3154109084": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 97500.0,
        "salary_max": 176250.0,
        "title": "Data Lakehouse Engineer",
        "company": "Leidos",
        "desc": "Description   \n Leidos has an immediate opening for a Senior Systems Engineer supporting development of a data Lakehouse at HHS in Washington DC. This position is an excellent opportunity to interface directly with government personnel to help design and guide technical solutions and business processes for critical public health research and response mission spaces. \n \n  Primary Responsibilities \n \n  Work directly with the customer to capture and formalize \u2018Baseline\u2019 and \u2018To Be\u2019 business user stories, use cases, requirements, system architectures, and data schema. \n  Track and communicate solution and system development, production and maintenance activities. \n  Design and execute test plans for solution and system testing. \n  Help support the identification and considerations for alternative application solutions to meet business customer needs. \n  Work with the broader development, including SQL developers and Azure engineers. \n  Consider security and infrastructure implications of requested changes. \n \n \n  Basic Qualifications \n \n  Bachelor\u2019s Degree in Systems Engineering, Computer Science, Information Technology, Business Science, or related field required \n  8+ years\u2019 of combined experience in system engineering, software development or business analysis required. \n  Candidate must demonstrate a willing initiative to solicit customer requirements and translate them into formalized structured technical products. Candidate must have strong verbal and written communication skills. Concise writing and ability to communicate technical content to broad audiences are critical candidate abilities. \n  Candidate must be a US Citizen and be able to obtain and maintain a high-risk public trust clearance. \n  Must have experience or familiarity with: executing Agile, Scrum, and Kanban methodologies. Soliciting and documenting use cases and designs for system requirements (User Stories, Use Cases, Requirements, Specifications, Data Schema, Business Process Workflows). Use of system management tools such as Azure DevOps, Jira, Redmine, or similar system. Developing and executing testing and acceptance plans. Tracking bugs, issues and resolutions. Performing data analysis and reporting. Knowledge of government system security policies (ATO process) Ability to flexibly pivot to varying needs of the project while maintaining situational awareness \n \n \n  Preferred Qualifications \n \n  The preferred candidate will possess broader Systems Engineering knowledge and can readily execute low-level engineering tasks as well as high-level technical project management tasks. \n  The preferred candidate will have a MS in Systems Engineering or a related discipline. \n  Preferred candidates will have experience or familiarity with: \n  Knowledge of the Microsoft Azure ecosystem \n  Experience with Data Lakes and preferably Data Lakehouses \n  Experience working with federal IT systems \n  Experience working with SQL developers (or knowledge of SQL) \n  Prioritizing and communicating requirements and system development activities. \n  Communicating impact of requirement changes on active development activities. \n  Executing and presenting trade space of alternatives with cost benefit analysis. \n  Supporting Authority to Operate (ATO) activities and other production system processes. \n  Experience in developing advanced data visualizations. \n \n \n  Pay Range:  Pay Range $97,500.00 - $176,250.00\n  \n  The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law. \n  #Remote",
        "cleaned_desc": "  8+ years\u2019 of combined experience in system engineering, software development or business analysis required. \n  Candidate must demonstrate a willing initiative to solicit customer requirements and translate them into formalized structured technical products. Candidate must have strong verbal and written communication skills. Concise writing and ability to communicate technical content to broad audiences are critical candidate abilities. \n  Candidate must be a US Citizen and be able to obtain and maintain a high-risk public trust clearance. \n  Must have experience or familiarity with: executing Agile, Scrum, and Kanban methodologies. Soliciting and documenting use cases and designs for system requirements (User Stories, Use Cases, Requirements, Specifications, Data Schema, Business Process Workflows). Use of system management tools such as Azure DevOps, Jira, Redmine, or similar system. Developing and executing testing and acceptance plans. Tracking bugs, issues and resolutions. Performing data analysis and reporting. Knowledge of government system security policies (ATO process) Ability to flexibly pivot to varying needs of the project while maintaining situational awareness \n \n \n  Preferred Qualifications \n    The preferred candidate will possess broader Systems Engineering knowledge and can readily execute low-level engineering tasks as well as high-level technical project management tasks. \n  The preferred candidate will have a MS in Systems Engineering or a related discipline. \n  Preferred candidates will have experience or familiarity with: \n  Knowledge of the Microsoft Azure ecosystem \n  Experience with Data Lakes and preferably Data Lakehouses \n  Experience working with federal IT systems \n  Experience working with SQL developers (or knowledge of SQL) \n  Prioritizing and communicating requirements and system development activities. ",
        "techs": [
            "system engineering",
            "software development",
            "business analysis",
            "agile",
            "scrum",
            "kanban",
            "azure devops",
            "jira",
            "redmine",
            "executing testing",
            "acceptance plans",
            "tracking bugs",
            "data analysis",
            "reporting",
            "ato process",
            "systems engineering",
            "ms in systems engineering",
            "microsoft azure ecosystem",
            "data lakes",
            "data lakehouses",
            "federal it systems",
            "sql developers",
            "prioritizing requirements",
            "system development activities"
        ],
        "cleaned_techs": [
            "system engineering",
            "software development",
            "business analysis",
            "agile",
            "scrum",
            "kanban",
            "azure",
            "jira",
            "redmine",
            "executing testing",
            "acceptance plans",
            "tracking bugs",
            "reporting",
            "ato process",
            "systems engineering",
            "ms in systems engineering",
            "microsoft azure ecosystem",
            "data lakes",
            "data lakehouses",
            "federal it systems",
            "sql",
            "prioritizing requirements",
            "system development activities"
        ]
    },
    "daada383998fad4a": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 100000.0,
        "salary_max": 100000.0,
        "title": "Data Engineer",
        "company": "Comfort Keepers",
        "desc": "Job  Summary: \n  Comfort Keepers is seeking a skilled and experienced Data Engineer to join our data team. As a Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure and pipelines. You will work closely with cross-functional teams to ensure the availability and accessibility of high-quality data for analysis and reporting purposes. The ideal candidate is passionate about data, governance, has strong programming skills, is a creative problem solver and is proficient in building efficient and scalable data solutions. \n  Be part of a transformational experience as we meet the challenges of today\u2019s business landscape and lay the foundation for future growth. \n  Location: Remote-U.S. or Irvine, CA  Expected Salary Range: $100k \n  Responsibilities: \n \n  Build Azure data factory pipeline, Azure Data Lake, Datawarehouse, Power BI Reports and dashboards. \n  Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse. \n  Optimize and fine-tune data processes to ensure high performance and reliability. \n  Implement data validation, testing, and quality assurance processes to maintain data accuracy and consistency. \n  Work with large and complex datasets, both structured and unstructured, and employ appropriate data storage solutions. \n  Develop and maintain documentation for data pipelines, processes, and data dictionaries. \n  Monitor and troubleshoot data pipeline issues, resolving them in a timely manner to minimize downtime. \n  Collaborate with data scientists, analysts, agency partners and other stakeholders to understand data requirements and ensure data integrity, accuracy, and availability. \n  Additional responsibilities will be supporting applications and projects as appropriate. \n \n \n  Qualifications: \n \n  Microsoft SQL Server Database 2019 and above, including Azure SQL \n  Azure Data Factory, Azure Data Lake, Azure Devops , Azure Data warehouse \n  Experience with Repository and version control, GitHub \n  Microsoft Power BI and Power Query with DAX and M Language \n \n \n \n \n \n  Azure Analysis service\u2014Tabular Cube \n  Amazon Athena and Redshift \n  Development tools (Visual Studio) \n  Designing and implementing data models, and data lake solutions \n  Working with data scientists and digital teams to meet strategic data needs through project management tools like Microsoft Teams, JIRA, are desired \n  Strong problem-solving skills and attention to detail. \n  Excellent communication and teamwork skills. \n \n  Preferred: \n \n  C# Experience \n  Python, and/or R applications and languages while managing work using software version control like GitHub and/or Dev Ops \n \n \n  Work Environment: Remote \n  An Equal Opportunity and Affirmative Action employer, Comfort Keepers considers applicants for all positions without regard to race, color, religion, creed, gender, national origin, age, disability, marital or veteran status, or any legally protected status. We will make reasonable accommodations for qualified individuals with known disabilities unless doing so would result in an undue hardship. \n   \n Xwc2axfoet",
        "cleaned_desc": "Job  Summary: \n  Comfort Keepers is seeking a skilled and experienced Data Engineer to join our data team. As a Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure and pipelines. You will work closely with cross-functional teams to ensure the availability and accessibility of high-quality data for analysis and reporting purposes. The ideal candidate is passionate about data, governance, has strong programming skills, is a creative problem solver and is proficient in building efficient and scalable data solutions. \n  Be part of a transformational experience as we meet the challenges of today\u2019s business landscape and lay the foundation for future growth. \n  Location: Remote-U.S. or Irvine, CA  Expected Salary Range: $100k \n  Responsibilities: \n \n  Build Azure data factory pipeline, Azure Data Lake, Datawarehouse, Power BI Reports and dashboards. \n  Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse. \n  Optimize and fine-tune data processes to ensure high performance and reliability.    Implement data validation, testing, and quality assurance processes to maintain data accuracy and consistency. \n  Work with large and complex datasets, both structured and unstructured, and employ appropriate data storage solutions. \n  Develop and maintain documentation for data pipelines, processes, and data dictionaries. \n  Monitor and troubleshoot data pipeline issues, resolving them in a timely manner to minimize downtime. \n  Collaborate with data scientists, analysts, agency partners and other stakeholders to understand data requirements and ensure data integrity, accuracy, and availability. \n  Additional responsibilities will be supporting applications and projects as appropriate. \n \n \n  Qualifications:   \n  Microsoft SQL Server Database 2019 and above, including Azure SQL \n  Azure Data Factory, Azure Data Lake, Azure Devops , Azure Data warehouse \n  Experience with Repository and version control, GitHub \n  Microsoft Power BI and Power Query with DAX and M Language \n \n \n \n   \n  Azure Analysis service\u2014Tabular Cube \n  Amazon Athena and Redshift \n  Development tools (Visual Studio) \n  Designing and implementing data models, and data lake solutions \n  Working with data scientists and digital teams to meet strategic data needs through project management tools like Microsoft Teams, JIRA, are desired \n  Strong problem-solving skills and attention to detail. \n  Excellent communication and teamwork skills. \n ",
        "techs": [
            "azure data factory",
            "azure data lake",
            "azure devops",
            "azure data warehouse",
            "microsoft power bi",
            "power query",
            "dax",
            "m language",
            "azure analysis service - tabular cube",
            "amazon athena",
            "redshift",
            "visual studio",
            "microsoft teams",
            "jira"
        ],
        "cleaned_techs": [
            "azure",
            "powerbi",
            "power query",
            "dax",
            "m language",
            "aws",
            "redshift",
            "visual studio",
            "microsoft teams",
            "jira"
        ]
    },
    "58c1eaec310670eb": {
        "terms": [
            "data engineer",
            "machine learning engineer"
        ],
        "salary_min": 98308.07,
        "salary_max": 124479.914,
        "title": "Data Engineer",
        "company": "BusPatrol",
        "desc": "Overview: \n  \n  Location:  Remote (must be based out of US or Canada)\n  \n \n \n  Travel:  Less than 5%\n  \n \n \n  Manages Others:  No\n  \n \n \n  Education:  Bachelor\u2019s Degree in a technical field\n  \n \n \n  Experience:  3+ years of relevant experience in Data engineering, database engineering, business intelligence or business analytics\n  \n \n \n  THE OPPORTUNITY: \n \n \n   The Data Engineer at BusPatrol will help to build and maintain our data infrastructure to support reporting, analytics, and data science. The right candidate will have strong data architecture, ETL, and SQL skills, and a proven track record partnering with both the business and data team colleagues to construct a framework which delivers on all our needs. The candidate will also have strong operational skills to drive efficiency and speed, expertise in building repeatable data engineering processes, strong project management skills, and a vision for how to deliver data products.\n   Responsibilities: \n  \n Create and orchestrate data pipelines in state-of-the-art AWS environment (Redshift, EC2, EMR, S3, Lambda, etc. with AWS Glue) \n  Manage all aspects of the data and analytics system from ingestion to ETL to aggregate tables for analytics and reporting needs \n  Design and implement internal data pipeline jobs / process improvements using various modern techniques: automating manual processes, optimizing data delivery, re-designing infrastructure for scalability, etc. \n  Write scripts to schedule data ingestion and syncing. Evaluate, lead and form backend logic to create data marts from requirements for the purposes of self-serving stakeholders \n  Assist data architects to build the working framework to create metrics as code and optimize data search and retrieval \n  Troubleshoot data jobs / processes in production to fix data quality bugs or pipeline performance issues \n  Build and assemble large, complex data sets with multi-dimensional relationships that meet both functional and non-functional requirements from business stakeholders \n  Build integrations between systems \n  Qualifications: \n  \n Bachelor's degree or higher in a quantitative/technical field (e.g., Computer Science, Statistics, Engineering) or equivalent experience \n  3+ years of relevant experience in Data engineering, database engineering, business intelligence or business analytics \n  1+ years of SQL knowledge for various reporting and transformation needs (Redshift, MySQL, PostgreSQL, Snowflake, Databricks) \n  1+ years of experience in core languages such as Python (experience building classes preferred) \n  1+ years of experience with schema design and dimensional data modeling \n  Extensive experience with AWS (Redshift, EC2, EMR, S3, Lambda, etc. with AWS Glue) \n  An interest in understanding the business and its strategy, not just the data architecture, and how our work contributes to meeting business goals \n  Experience with Data Lake architectures, and with combining structured and unstructured data into unified representations. \n  Experience with API design and development of RESTful web services \n  Analytical mindset with the ability to structure and process qualitative data and draw insightful conclusions. \n  Data science and machine learning experience a plus \n  GIS experience a plus \n  Data visualization experience with tools like Tableau a plus \n  Experience with JIRA preferred \n  Must be an intellectually curious self-starter and motivated to continually learn. \n  BusPatrol Value Proposition: \n  \n  WHAT WE OFFER \n \n \n   BusPatrol employees get:\n  \n \n  A competitive salary and benefits package \n  Comprehensive personal time off, including volunteering and birthday days off \n  An opportunity to help build a company dedicated to children\u2019s safety \n  The chance to join an innovative and dedicated team, focused on leading edge technology \n  The occasion to participate in BusPatrol\u2019s culture of safety, learning, and teamwork \n \n \n   BusPatrol\u2019s school bus safety programs are violator-funded, meaning that those who break the law pay for the technology that protects children. We build solid partnerships in the communities in which we operate which, coupled with our innovative business model, leads to sustainable efforts to change driver behaviors.\n  \n \n  HOW WE WORK \n \n \n   On our mission to make the journey to and from school safer for children, the way we work together and with our partners is built on foundational cultural pillars.\n  \n \n  SAFETY  Safety is our focus, for the children we protect and for each other. We follow the letter and spirit of occupational safety law, relentlessly employ safety best practices, and foster learning and development on our worksites. We are safe to be ourselves and to make mistakes, and we create safe environments for our teams.  \n CONNECTION  We build strong relationships and teams in support of our mission. We promote and provide opportunities for employees to grow together. \n  EXCELLENCE  We commit to innovation and quality work in support of our mission and each other. The children we safeguard are at the forefront of our decisions and actions and we excel on their behalf. \n  IMPACT  We measure success by fulfilling our mission and keeping the company strong. We invest our time and energy in the actions that deliver results for students and for their communities. \n \n \n \n  We are looking for a valued member of the BusPatrol team to assist us in our quest to improve children\u2019s safety. This is an important role for us and a great opportunity for the right candidate. Our environment is inclusive, diverse, ignited, built on integrity, and deeply committed.\n  \n \n \n  The US salary range for this position is provided in this posting. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your Talent Partner can share more about the specific salary range for your preferred location and skill level during the hiring process. \n \n \n \n  Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, and/or commission (if applicable) or benefits.",
        "cleaned_desc": "  Experience:  3+ years of relevant experience in Data engineering, database engineering, business intelligence or business analytics\n  \n \n \n  THE OPPORTUNITY: \n \n \n   The Data Engineer at BusPatrol will help to build and maintain our data infrastructure to support reporting, analytics, and data science. The right candidate will have strong data architecture, ETL, and SQL skills, and a proven track record partnering with both the business and data team colleagues to construct a framework which delivers on all our needs. The candidate will also have strong operational skills to drive efficiency and speed, expertise in building repeatable data engineering processes, strong project management skills, and a vision for how to deliver data products.\n   Responsibilities: \n  \n Create and orchestrate data pipelines in state-of-the-art AWS environment (Redshift, EC2, EMR, S3, Lambda, etc. with AWS Glue) \n  Manage all aspects of the data and analytics system from ingestion to ETL to aggregate tables for analytics and reporting needs \n  Design and implement internal data pipeline jobs / process improvements using various modern techniques: automating manual processes, optimizing data delivery, re-designing infrastructure for scalability, etc. \n  Write scripts to schedule data ingestion and syncing. Evaluate, lead and form backend logic to create data marts from requirements for the purposes of self-serving stakeholders \n  Assist data architects to build the working framework to create metrics as code and optimize data search and retrieval \n  Troubleshoot data jobs / processes in production to fix data quality bugs or pipeline performance issues \n  Build and assemble large, complex data sets with multi-dimensional relationships that meet both functional and non-functional requirements from business stakeholders \n  Build integrations between systems    Qualifications: \n  \n Bachelor's degree or higher in a quantitative/technical field (e.g., Computer Science, Statistics, Engineering) or equivalent experience \n  3+ years of relevant experience in Data engineering, database engineering, business intelligence or business analytics \n  1+ years of SQL knowledge for various reporting and transformation needs (Redshift, MySQL, PostgreSQL, Snowflake, Databricks) \n  1+ years of experience in core languages such as Python (experience building classes preferred) \n  1+ years of experience with schema design and dimensional data modeling \n  Extensive experience with AWS (Redshift, EC2, EMR, S3, Lambda, etc. with AWS Glue) \n  An interest in understanding the business and its strategy, not just the data architecture, and how our work contributes to meeting business goals \n  Experience with Data Lake architectures, and with combining structured and unstructured data into unified representations. \n  Experience with API design and development of RESTful web services \n  Analytical mindset with the ability to structure and process qualitative data and draw insightful conclusions. \n  Data science and machine learning experience a plus \n  GIS experience a plus \n  Data visualization experience with tools like Tableau a plus \n  Experience with JIRA preferred \n  Must be an intellectually curious self-starter and motivated to continually learn. \n  BusPatrol Value Proposition: ",
        "techs": [
            "redshift",
            "ec2",
            "emr",
            "s3",
            "lambda",
            "aws glue",
            "mysql",
            "postgresql",
            "snowflake",
            "databricks",
            "python",
            "aws",
            "data lake",
            "api",
            "restful web services",
            "tableau",
            "jira"
        ],
        "cleaned_techs": [
            "redshift",
            "ec2",
            "emr",
            "s3",
            "lambda",
            "aws",
            "mysql",
            "postgresql",
            "snowflake",
            "databricks",
            "python",
            "data lake",
            "api",
            "restful web services",
            "tableau",
            "jira"
        ]
    },
    "0452074200507968": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 110425.055,
        "salary_max": 139822.72,
        "title": "Sr Data Engineer GCP",
        "company": "Microagility",
        "desc": "We are seeking a highly skilled and experienced Senior Data Engineer. The successful candidate will play a crucial role in data operations, leveraging their expertise in Google Cloud Platform (GCP) services, programming languages, and data engineering best practices. \n Key Responsibilities: \n \u00b7 Experience with GCP- Big Query, Data proc, cloud function, Composer. GCS buckets \n \u00b7 Strong Experience with Python, SQL, Pyspark \n \u00b7 Experience working with Git, Jenkins and CI/CD pipeline \n \u00b7 Prior Experience migrating code to GCP \n \u00b7 Work on Multiple project under tight deadlines. \n Key Requirements: \n \u00b7 7+ years of proven experience as a Data Engineer, with a strong record of accomplishment of working on data extraction, transformation, and loading processes. \n \u00b7 Proficiency in Python, SQL and GCP is essential. \n \u00b7 GCP data engineering certification \n \u00b7 Experience working migrating data science models. \n Education: \n \n Bachelor\u2019s Degree in Computer Science or related field \n \n Why Work with Us \n Competitive compensation. \n Opportunity to work on innovative projects for high-profile clients. \n A collaborative, inclusive, and supportive work environment. \n Opportunities for professional growth and development. \n Work with a team of passionate, talented, and creative individuals. \n Job Type: Contract \n Schedule: \n \n 8 hour shift \n \n Application Question(s): \n \n Do you have GCP certification? \n \n Experience: \n \n GCP Data Engineer: 7 years (Preferred) \n SQL, Python: 7 years (Preferred) \n CI/CD, Jenkis: 7 years (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": "We are seeking a highly skilled and experienced Senior Data Engineer. The successful candidate will play a crucial role in data operations, leveraging their expertise in Google Cloud Platform (GCP) services, programming languages, and data engineering best practices. \n Key Responsibilities: \n \u00b7 Experience with GCP- Big Query, Data proc, cloud function, Composer. GCS buckets \n \u00b7 Strong Experience with Python, SQL, Pyspark \n \u00b7 Experience working with Git, Jenkins and CI/CD pipeline \n \u00b7 Prior Experience migrating code to GCP \n \u00b7 Work on Multiple project under tight deadlines.   Key Requirements: \n \u00b7 7+ years of proven experience as a Data Engineer, with a strong record of accomplishment of working on data extraction, transformation, and loading processes. \n \u00b7 Proficiency in Python, SQL and GCP is essential. \n \u00b7 GCP data engineering certification \n \u00b7 Experience working migrating data science models. \n Education: \n ",
        "techs": [
            "google cloud platform (gcp) services",
            "big query",
            "data proc",
            "cloud function",
            "composer",
            "gcs buckets",
            "python",
            "sql",
            "pyspark",
            "git",
            "jenkins",
            "ci/cd pipeline",
            "gcp data engineering certification"
        ],
        "cleaned_techs": [
            "gcp",
            "big query",
            "data proc",
            "cloud function",
            "composer",
            "gcs buckets",
            "python",
            "sql",
            "pyspark",
            "git",
            "jenkins",
            "ci/cd pipeline"
        ]
    },
    "f32a0eb4fc36bab8": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 100000.0,
        "salary_max": 115000.0,
        "title": "Senior Engineer BOI Insights (Healthcare Data Analytics)",
        "company": "Agilon Health",
        "desc": "Agilon health is transforming healthcare by empowering community-based physicians with the resources and expertise they need to innovate the payment and delivery of care for seniors. \n The agilon health Total Care Model is powered by our purpose-built platform and frees physicians from the constraints of the traditional fee-for-service reimbursement model, all enabled through a growing national network of like-minded physician partners. With agilon health, physicians are able to practice team-based, coordinated care to serve the individual needs of their senior patients and to transition to a sustainable and predictable, long-term business model. \n As you might imagine, analytics and insights are the heart of how we support our physician partners and is our special sauce. We have a lot of analytics programs in place already, but we need more! From generating insights related to our risk adjustment programs to helping analyze health plan attribution, health plan and membership trends, building our next generation Cube or analytics surrounding our brand-new data lake, there's a lot of data to process and actionable insights to present, as well as analytics infrastructure to build. You will be executing some of this work individually, and others in tight partnership with other critical teams such as Finance, Clinical Analytics, and Medical Economics. \n There\u2019s much more data we can be leveraging and analyzing to generate and test our hypotheses to help improve patient outcomes and reduce medical waste. Come join the team and help make a direct impact on our senior members\u2019 lives! \n More about this role: \n - Be part of an agile team working collaboratively with Agilon leadership and many different cross-functional teams, including UX, Product, Technology, Operations, and Clinician teams - Leverage your analytical thinking to explore our ever-growing datasets to test hypotheses and bring life to your own insights - Generate insights that help improve patient outcomes and/or reduce medical waste - Continuously learn and share your new-found knowledge with others \n Desired Traits: \n 1. Experience: A minimum of 4 years of experience performing data analysis and generating intuitive dashboards. \n 2. Healthcare domain expertise: \n a. Exposure to the healthcare industry & domain is preferred, with specific knowledge in either risk adjustment, clinical analytics, clinical quality, or health economics. b. Familiarity with healthcare data models. \n 3. Education: A bachelor's degree or equivalent education is required. \n 4. Technical skills: \n a. should have strong proficiency in SQL, and expertise in Snowflake would be a plus. b. Data modeling skills are also important. c. Familiarity with data visualization tools such as Sigma, Tableau, Power BI, or Excel is necessary to create intuitive dashboards. d. Some experience with coding in Python or other programming languages is nice to have. \n 5. Analytical and problem-solving skills: Strong analytical and problem-solving abilities are essential for effectively analyzing data and providing meaningful insights. 6. Business Acumen: should be able to translate business needs into data analysis requirements, understanding the goals and objectives of the organization. 7. Curiosity and exploration: Should have an intellectual curiosity about data and the ability to go beyond immediate problems. Balancing deadlines and exploring new opportunities for analysis and insights is important. 8. Ownership and user focus: Takes ownership over the insights the team generates, and the processes used to develop those analyses, keeping our users' needs in clear focus 9. Startup environment readiness: Comfortable in a startup environment and ready to \u201croll up your sleeves!\u201d 10. Communication skills: Excellent communication skills are necessary to effectively convey complex findings to both technical and non-technical stakeholders. 11. Nimble learner: Should be eager to learn and adapt quickly to new technologies, methodologies, and industry trends. 12. Enthusiasm and drive: A proactive and enthusiastic approach to getting things done is valued in a candidate. \n Job Type: Full-time \n Pay: $100,000.00 - $115,000.00 per year \n Benefits: \n \n 401(k) \n 401(k) matching \n Dental insurance \n Employee assistance program \n Flexible spending account \n Health insurance \n Health savings account \n Life insurance \n Paid time off \n Parental leave \n Professional development assistance \n Vision insurance \n \n Compensation package: \n \n Yearly pay \n \n Experience level: \n \n 4 years \n \n Schedule: \n \n Monday to Friday \n \n Work Location: Remote",
        "cleaned_desc": " 2. Healthcare domain expertise: \n a. Exposure to the healthcare industry & domain is preferred, with specific knowledge in either risk adjustment, clinical analytics, clinical quality, or health economics. b. Familiarity with healthcare data models. \n 3. Education: A bachelor's degree or equivalent education is required. \n 4. Technical skills: \n a. should have strong proficiency in SQL, and expertise in Snowflake would be a plus. b. Data modeling skills are also important. c. Familiarity with data visualization tools such as Sigma, Tableau, Power BI, or Excel is necessary to create intuitive dashboards. d. Some experience with coding in Python or other programming languages is nice to have. \n 5. Analytical and problem-solving skills: Strong analytical and problem-solving abilities are essential for effectively analyzing data and providing meaningful insights. 6. Business Acumen: should be able to translate business needs into data analysis requirements, understanding the goals and objectives of the organization. 7. Curiosity and exploration: Should have an intellectual curiosity about data and the ability to go beyond immediate problems. Balancing deadlines and exploring new opportunities for analysis and insights is important. 8. Ownership and user focus: Takes ownership over the insights the team generates, and the processes used to develop those analyses, keeping our users' needs in clear focus 9. Startup environment readiness: Comfortable in a startup environment and ready to \u201croll up your sleeves!\u201d 10. Communication skills: Excellent communication skills are necessary to effectively convey complex findings to both technical and non-technical stakeholders. 11. Nimble learner: Should be eager to learn and adapt quickly to new technologies, methodologies, and industry trends. 12. Enthusiasm and drive: A proactive and enthusiastic approach to getting things done is valued in a candidate. \n Job Type: Full-time \n Pay: $100,000.00 - $115,000.00 per year ",
        "techs": [
            "healthcare domain expertise",
            "risk adjustment",
            "clinical analytics",
            "clinical quality",
            "health economics",
            "healthcare data models",
            "sql",
            "snowflake",
            "data modeling",
            "sigma",
            "tableau",
            "power bi",
            "excel",
            "python",
            "analytical skills",
            "problem-solving skills",
            "business acumen",
            "curiosity",
            "exploration",
            "ownership",
            "user focus",
            "startup environment readiness",
            "communication skills",
            "nimble learner",
            "enthusiasm",
            "drive"
        ],
        "cleaned_techs": [
            "healthcare domain expertise",
            "risk adjustment",
            "clinical analytics",
            "clinical quality",
            "health economics",
            "healthcare data models",
            "sql",
            "snowflake",
            "sigma",
            "tableau",
            "powerbi",
            "excel",
            "python",
            "business acumen",
            "curiosity",
            "ownership",
            "user focus",
            "startup environment readiness",
            "nimble learner",
            "enthusiasm",
            "drive"
        ]
    },
    "ee49bcbd9ddeff3c": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 84114.66,
        "salary_max": 106507.89,
        "title": "Data Engineer",
        "company": "CareQuest Institute for Oral Health Inc",
        "desc": "JOB SUMMARY: \n  The Data Engineer will lead activities in expanding and optimizing our data pipeline architecture as well as optimizing the data flow. Design and build out data models used for analytical reporting and data science needs. Support data analysts and data scientists on projects and will ensure data are consistently formatted and accurate for ongoing work. Manage the operational aspects of data platform solutions, including administering accessing controls, performance tuning, and automation of tasks. Develop and implement standard operating procedures associated with data engineering in line with organizational goals. Perform routine and ad hoc data extractions, create and deliver reports in a timely manner, and other analysis summaries as requested. Additional responsibilities include facilitating secure transfer of prepared datasets to external and internal partners and assisting in configuration and ongoing maintenance of Azure Cloud environment. \n \n  PRIMARY JOB RESPONSIBILITIES: \n \n  Setup and configure data warehouse of dental and medical claims, practice management systems, electronic health records data, and other structured and unstructured health data. \n  Implement Data Warehouse development methodologies. \n  Use the following tools and platforms: Azure Data Factory, Azure Storage Account, Azure DevOps, Azure Storage Explorer, Databricks, and Snowflake. \n  Ingest Data from various third-party data sources. \n  Develop and implement standard operating procedures to ensure compliance with licensing, legal, and ethical requirements. \n  Recommend the best practices for management, monitoring & optimization of data. \n  Ensure that data pipelines and data stores are high-performing, efficient, organized, and reliable, given a specific set of business requirements and constraints. \n  Process and analyze extracts of data from practice management system such as Dentrix, EagleSoft, Curve Dental, EPIC, Ace Dental. \n  Prepare routine and ad hoc data extractions, reports, and analysis summaries. \n  Facilitate secure storage and transfer of prepared datasets to external and internal partners. \n  Research and design Electronic Data Interchange processes. \n  Serve as technical resource for all department staff. \n  Lead technical implementation projects. \n  Develop and maintain database and dashboard applications for monitoring measures by CareQuest Institute staff. \n  Handle technical application development projects as assigned. \n  Perform new and existing set-up and maintenance processes. \n \n \n  Other duties as needed or required. \n \n \n  JOB QUALIFICATIONS: \n  Required: \n \n  Bachelor\u2019s degree in computer science or a related information technology field. \n  1-3 years prior related technical and business experience required. \n  1-3 years of experience with SQL programming. \n  1-2 years of experience with cloud database technologies, such as Azure, AWS, Google Cloud, Databricks, or Snowflake. \n  Proficiency in the following technical applications/programs necessary.\n      \n  Relational Databases (SQL Server preferred) \n  Knowledge of one or more data science languages/programs (Python or R preferred) \n \n \n  Preferred: \n \n  Experience with medical or dental claims data, electronic health records, and/or dental practice management software preferred. \n \n \n  Experience using data analytics and/or report development tools like PowerBI and Tableau. \n  Hands-on expertise with SQL, Python, Azure data services, and Snowflake experience building out a complex ETL/ELT pipeline. \n  Experience implementing security and compliance requirements and working with different data modeling techniques. \n  Experience designing and implementing data warehouse methodologies. \n \n \n  PHYSICAL DEMANDS: \n \n  Incumbent must be able to communicate effectively. \n  Manual dexterity and sitting is required in carrying out position own position responsibilities (i.e., use of personal computer). \n  Ability to travel or move about within and outside serviced facilities required. \n  Incumbent works primarily in either a private or shared office environment. \n \n \n   The specific statements shown in each section of this description are not intended to be all-inclusive. They represent typical elements and criteria necessary to successfully perform this position. \n  ** In accordance with CareQuest Institute for Oral Health\u2019s Compliance Plan, all employees must conduct CareQuest Institute for Oral Health business and activities in accordance with applicable laws, regulations, professional standards and ethical standards and report potential compliance or ethical issues to CareQuest Institute for Oral Health\u2019s designated Compliance Officer. ** \n  CareQuest Institute for Oral Health\u2019s Affirmative Action Program affirms our commitment to make reasonable accommodation for known physical or mental limitation of otherwise-qualified individuals with disabilities or special disabled veterans, unless the accommodation would impose an undue hardship on the operation of our business and activities. Please see Human Resources for additional information regarding this program.",
        "cleaned_desc": "JOB SUMMARY: \n  The Data Engineer will lead activities in expanding and optimizing our data pipeline architecture as well as optimizing the data flow. Design and build out data models used for analytical reporting and data science needs. Support data analysts and data scientists on projects and will ensure data are consistently formatted and accurate for ongoing work. Manage the operational aspects of data platform solutions, including administering accessing controls, performance tuning, and automation of tasks. Develop and implement standard operating procedures associated with data engineering in line with organizational goals. Perform routine and ad hoc data extractions, create and deliver reports in a timely manner, and other analysis summaries as requested. Additional responsibilities include facilitating secure transfer of prepared datasets to external and internal partners and assisting in configuration and ongoing maintenance of Azure Cloud environment. \n \n  PRIMARY JOB RESPONSIBILITIES: \n \n  Setup and configure data warehouse of dental and medical claims, practice management systems, electronic health records data, and other structured and unstructured health data. \n  Implement Data Warehouse development methodologies. \n  Use the following tools and platforms: Azure Data Factory, Azure Storage Account, Azure DevOps, Azure Storage Explorer, Databricks, and Snowflake. \n  Ingest Data from various third-party data sources. \n  Develop and implement standard operating procedures to ensure compliance with licensing, legal, and ethical requirements. \n  Recommend the best practices for management, monitoring & optimization of data. \n  Ensure that data pipelines and data stores are high-performing, efficient, organized, and reliable, given a specific set of business requirements and constraints.   \n \n  JOB QUALIFICATIONS: \n  Required: \n \n  Bachelor\u2019s degree in computer science or a related information technology field. \n  1-3 years prior related technical and business experience required. \n  1-3 years of experience with SQL programming. \n  1-2 years of experience with cloud database technologies, such as Azure, AWS, Google Cloud, Databricks, or Snowflake. \n  Proficiency in the following technical applications/programs necessary.\n      \n  Relational Databases (SQL Server preferred)    Knowledge of one or more data science languages/programs (Python or R preferred) \n \n \n  Preferred: \n \n  Experience with medical or dental claims data, electronic health records, and/or dental practice management software preferred. \n \n \n  Experience using data analytics and/or report development tools like PowerBI and Tableau. \n  Hands-on expertise with SQL, Python, Azure data services, and Snowflake experience building out a complex ETL/ELT pipeline. \n  Experience implementing security and compliance requirements and working with different data modeling techniques. \n  Experience designing and implementing data warehouse methodologies. ",
        "techs": [
            "azure data factory",
            "azure storage account",
            "azure devops",
            "azure storage explorer",
            "databricks",
            "snowflake",
            "sql server",
            "python",
            "r",
            "powerbi",
            "tableau"
        ],
        "cleaned_techs": [
            "azure",
            "databricks",
            "snowflake",
            "sql",
            "python",
            "r",
            "powerbi",
            "tableau"
        ]
    },
    "a500c31bd71fbb3a": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 55.0,
        "salary_max": 64.0,
        "title": "Data Engineer 2 (CHICAGO/HYBRID) - Freelance",
        "company": "Braintrust",
        "desc": "ABOUT US : \n  Braintrust is a user-owned talent network that connects top-tier professionals with the world's leading enterprises. We prioritize transparency, eliminating middlemen and high markups, ensuring job-seekers are matched swiftly to innovative roles while clients benefit from unparalleled efficiency and quality. \n  ABOUT THE HIRING PROCESS: \n  The hiring process for this role involves completing your Braintrust profile, applying directly to the role on Braintrust, and undergoing a one-time screening to ensure you meet our vetted talent specifications. After this, the hiring team will contact you directly if they believe you are a suitable match. \n  Our process isn't for everyone, that's intentional. If you believe that you are a top candidate for this job, please join our network to give yourself the opportunity to work with top companies. \n \n \n \n \n JOB TYPE : Freelance/ Contract Position (no agencies/C2C - see notes below) \n LOCATION : Hybrid - Chicago, IL \n SALARY RANGE : $55 - $64 /hr \n ESTIMATED DURATION : 40hr/week - Long term \n EXPERIENCE : 3-5 years \n BRAINTRUST JOB ID:  9746 \n \n THE OPPORTUNITY \n \n \n \n \n Requirements \n \n \n ***MUST BE LOCATED IN CHICAGO ARE AND ABLE TO GO TO THE OFFICE 3 DAYS A WEEK*** \n \n \n  The main function of a data engineer is to ensure that the data sets of an organization are supported by an architecture that supports the organization in achieving its strategic goal. A typical data engineer is responsible for setting enterprise standards for databases, data integration, and the means to get to the data. \n \n \n  Job Responsibilities: \n \n Develop and test data pipelines with ETL and store data in Snowflake, debug errors and make necessary modifications. \n Modify existing databases and tables and work with data engineers and analysts to make changes. \n Write code/scripts to validate data between different databases and work on designing new data pipelines. \n \n Skills: \n \n Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. \n Ability to work independently and manage one's time. \n Good knowledge of data modeling. \n Expertise with SQL, Python and Snowflake \n Basic knowledge of computer software, such as Visual Basic, Oracle, etc. \n \n Education/Experience: \n \n Associate's degree in computer programming or a relevant field required. Bachelor's/Master degree preferred. \n 2-4 years experience required \n \n \n \n \n \n \n \n What you'll be working on \n \n \n Position's Contributions to Work Group: \n \n The main function of a data engineer is to ensure that the data sets of an organization are supported by an architecture that supports the organization in achieving its strategic goal. \n A typical data engineer is responsible for setting enterprise standards for databases, data integration, and the means to get to the data. \n This position is working with a team responsible for managing Customer Value Agreements (CVA) reporting process and providing insights to business partners to increasing sales \n \n Typical task breakdown: \n \n Develop and test Airflow DAGs, debug errors and make necessary modifications during our migration from SAS to Airflow and Snowflake. \n Maintaining and enhancing tables in Snowflake \n Provide support to business partners to answer questions regarding CVA metrics. \n Working on migrating Airflow DAGs to AWS \n \n Interaction with team: \n \n Working with colleagues in Chicago, Michigan, and Geneva \n Will be interacting with business stakeholders to explain the CVA logic and metrics impact. \n \n Technical Skills  (Required) \n \n 2+ years exp in Python \n 2+ years exp with SQL \n 2+ years exp in Snowflake \n \n (Desired) \n \n Project management exp specifically Agile methodology \n Airflow experience \n AWS experience \n Data analytics \n Good understanding of Software Development Life Cycle \n Tableau, Power BI experience \n \n Soft Skills  (Required) \n \n Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. \n Ability to work independently and manage one's time. Critical thinking skills a must. \n Able to work well in a team in a collaborative workspace. \n Being proactive and take initiative. \n Able to work off hours when required to support production issues. \n \n (Desired) \n \n Being able to create connection with people outside the team and business partners. \n Organized \n \n \n \n \n \n \n \n \n \n \n Notes: \n  Our employers all have varying legal and geographic requirements for their roles, they trust Braintrust to find them the talent that meet their unique specifications. For that reason, this role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we'd welcome your application. \n  Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.",
        "cleaned_desc": " \n ***MUST BE LOCATED IN CHICAGO ARE AND ABLE TO GO TO THE OFFICE 3 DAYS A WEEK*** \n \n \n  The main function of a data engineer is to ensure that the data sets of an organization are supported by an architecture that supports the organization in achieving its strategic goal. A typical data engineer is responsible for setting enterprise standards for databases, data integration, and the means to get to the data. \n \n \n  Job Responsibilities: \n \n Develop and test data pipelines with ETL and store data in Snowflake, debug errors and make necessary modifications. \n Modify existing databases and tables and work with data engineers and analysts to make changes. \n Write code/scripts to validate data between different databases and work on designing new data pipelines. \n \n Skills: \n \n Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. \n Ability to work independently and manage one's time. \n Good knowledge of data modeling. \n Expertise with SQL, Python and Snowflake \n Basic knowledge of computer software, such as Visual Basic, Oracle, etc. \n \n Education/Experience: \n   Working on migrating Airflow DAGs to AWS \n \n Interaction with team: \n \n Working with colleagues in Chicago, Michigan, and Geneva \n Will be interacting with business stakeholders to explain the CVA logic and metrics impact. \n \n Technical Skills  (Required) \n \n 2+ years exp in Python \n 2+ years exp with SQL \n 2+ years exp in Snowflake \n \n (Desired) \n \n Project management exp specifically Agile methodology \n Airflow experience \n AWS experience \n Data analytics \n Good understanding of Software Development Life Cycle \n Tableau, Power BI experience \n \n Soft Skills  (Required) ",
        "techs": [
            "etl",
            "snowflake",
            "sql",
            "python",
            "airflow",
            "aws",
            "tableau",
            "power bi"
        ],
        "cleaned_techs": [
            "etl",
            "snowflake",
            "sql",
            "python",
            "airflow",
            "aws",
            "tableau",
            "powerbi"
        ]
    },
    "8c543ed004aa1d2c": {
        "terms": [
            "data engineer",
            "machine learning engineer"
        ],
        "salary_min": 94420.0,
        "salary_max": 136665.0,
        "title": "Sr. Lead Data Engineer",
        "company": "Lumen",
        "desc": "About Lumen \n  Lumen is a global technology leader, digitally connecting people, data and applications \u2013 quickly, securely, and effortlessly. Together, we are building a culture and company from the people up \u2013 committed to teamwork, trust and transparency. People power progress. We\u2019re looking for top-tier talent and offer the flexibility you need to thrive and deliver lasting impact. Join us as we digitally connect the world and shape the future. \n \n \n \n  The Role \n \n \n  Are you interested in serving as an integral part of our digital marketing team developing new tools and capabilities that will continue to advance Lumen\u2019s reputation as a technology leader? \n  In this role, you will be taking the lead in partnering closely with marketing, operations, and data science teams to utilize terabytes of data and translate them into actionable insights to create a competitive advantage for marketing/sales initiatives to win market share. Furthermore, you will be accountable for building and operationalizing critical components and tools to ensure the data coming in and going out are of the highest data quality and integrity. In the end state, the data solutions built and operated by you would rival the absolute best in the industry in engineering, operations, and usability excellence. \n \n \n \n \n \n  The Main Responsibilities \n \n \n  You are a great fit for this position if you: \n \n  Have strong passion in building reliable, accurate datasets that power end-to-end user scenarios and used in business decisions. \n  Highly believe that data is an asset that must be accessible to and applicable by every function in a modern business to guide and assess growth investments. \n  Are passionate about Data Engineering and believe that it is one of a kind emerging discipline and career path that you want to grow in. \n  Enjoy building and operating data services at terabyte and higher scales to enable data driven organizations. \n \n \n \n \n \n  What We Look For in a Candidate \n \n \n  Qualifications: \n \n  Experience building data models and performing complex queries using SQL \n  Experience performance tuning large datasets \n  Experience building large data pipelines and/or web services \n  Strong programming skills with Python and other scripting languages \n  8+ years of Business Intelligence or software development experience using industry technologies \n  4+ years of experience in building integration with upstream and downstream systems with REST APIs \n  Excellent problem solving, critical thinking, and communication skills \n  Ability to communicate effectively with technical and business teams, drive issues to closure \n  Strong understanding of data engineering and data stewardship roles in an organization \n  Ability to foster an innovative and inclusive team-oriented work environment. You\u2019ll play an active role in counselling and mentoring junior team members across the organization by providing structured and on-the-job feedback. \n  Ability to work on multiple priorities that span different areas of focus. You should feel comfortable talking to business stakeholders, analysts, data scientists and developers. \n  An entrepreneurial spirit who is excited by ambiguity, operates autonomously and can make informed decisions on the fly, grounded in a digital transformation point of view for marketing/sales initiatives. \n  BA/BS in Computer Science, Engineering or related technical field such as Statistics or Data Science \n \n \n  Other Qualifications: \n \n  Strong familiarity with big data and Hadoop ecosystem of tools is highly valuable \n  Knowledge of Lumen data, systems and processes including paid media, financial and inventory systems highly desirable \n  Experience with ETL and data integration development using multiple tools, including Informatica, Microsoft SSIS or other technologies \n  Experience with cloud data platforms is helpful \n  Combined IT and Marketing background \n  Machine Learning, Data Science, and statistical modeling experience are highly valued \n \n \n \n \n \n  Compensation \n \n \n  The starting salary for this role differs based on the employee's primary work location. Employees typically do not start at the top of the range, though compensation depends on each individual's qualifications. \n  Location Based Pay Ranges \n  $94420 - $118028  in these states: AR, ID, KY, LA, ME, MS, NE, SC, and SD.   $99390 - $124230  in these states: AZ, FL, GA, IN, IA, KS, MO, MT, NM, ND, OH, OK, PA, TN, UT, VT, WV, WI, and WY.   $104360 - $130448  in these states: CO, HI, MI, MN, NV, NH, NC, OR, and RI.   $109330 - $136665  in these states: AK, CA, CT, DE, DC, IL, MD, MA, NJ, NY, TX, VA, and WA. \n  As with the pay range variety that's based on the region of a country, specific offers are determined by various factors such as experience, education, skills, certifications and other business needs. \n \n \n \n \n  What to Expect Next \n \n \n \n \n  Requisition #: 331443 \n  Background Screening \n  If you are selected for a position, there will be a background screen, which may include checks for criminal records and/or motor vehicle reports and/or drug screening, depending on the position requirements. For more information on these checks, please refer to the Post Offer section of our FAQ page. Job-related concerns identified during the background screening may disqualify you from the new position or your current role. Background results will be evaluated on a case-by-case basis. \n  Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. \n  Equal Employment Opportunities \n  We are committed to providing equal employment opportunities to all persons regardless of race, color, ancestry, citizenship, national origin, religion, veteran status, disability, genetic characteristic or information, age, gender, sexual orientation, gender identity, gender expression, marital status, family status, pregnancy, or other legally protected status (collectively, \u201cprotected statuses\u201d). We do not tolerate unlawful discrimination in any employment decisions, including recruiting, hiring, compensation, promotion, benefits, discipline, termination, job assignments or training. \n  Disclaimer \n  The job responsibilities described above indicate the general nature and level of work performed by employees within this classification. It is not intended to include a comprehensive inventory of all duties and responsibilities for this job. Job duties and responsibilities are subject to change based on evolving business needs and conditions. \n \n  Salary Range \n \n  Salary Min :  \n 94420 \n \n \n  Salary Max :  \n 136665 \n \n \n  This information reflects the anticipated base salary range for this position based on current national data. Minimums and maximums may vary based on location. Individual pay is based on skills, experience and other relevant factors.  \n This position is eligible for either short-term incentives or sales compensation. Director and VP positions also are eligible for long-term incentive. To learn more about our bonus structure, you can view additional information here. We're able to answer any additional questions you may have as you move through the selection process. \n  As part of our comprehensive benefits package, Lumen offers a broad range of Health, Life, Voluntary Lifestyle and other benefits and perks that enhance your physical, mental, emotional and financial wellbeing. You can learn more by clicking here. \n  Note: For union-represented postings, wage rates and ranges are governed by applicable collective bargaining agreement provisions.",
        "cleaned_desc": "  Have strong passion in building reliable, accurate datasets that power end-to-end user scenarios and used in business decisions. \n  Highly believe that data is an asset that must be accessible to and applicable by every function in a modern business to guide and assess growth investments. \n  Are passionate about Data Engineering and believe that it is one of a kind emerging discipline and career path that you want to grow in. \n  Enjoy building and operating data services at terabyte and higher scales to enable data driven organizations. \n \n \n \n \n \n  What We Look For in a Candidate \n \n \n  Qualifications: \n \n  Experience building data models and performing complex queries using SQL \n  Experience performance tuning large datasets \n  Experience building large data pipelines and/or web services \n  Strong programming skills with Python and other scripting languages \n  8+ years of Business Intelligence or software development experience using industry technologies \n  4+ years of experience in building integration with upstream and downstream systems with REST APIs    Excellent problem solving, critical thinking, and communication skills \n  Ability to communicate effectively with technical and business teams, drive issues to closure \n  Strong understanding of data engineering and data stewardship roles in an organization \n  Ability to foster an innovative and inclusive team-oriented work environment. You\u2019ll play an active role in counselling and mentoring junior team members across the organization by providing structured and on-the-job feedback. \n  Ability to work on multiple priorities that span different areas of focus. You should feel comfortable talking to business stakeholders, analysts, data scientists and developers. \n  An entrepreneurial spirit who is excited by ambiguity, operates autonomously and can make informed decisions on the fly, grounded in a digital transformation point of view for marketing/sales initiatives. \n  BA/BS in Computer Science, Engineering or related technical field such as Statistics or Data Science \n \n \n  Other Qualifications: \n \n  Strong familiarity with big data and Hadoop ecosystem of tools is highly valuable \n  Knowledge of Lumen data, systems and processes including paid media, financial and inventory systems highly desirable \n  Experience with ETL and data integration development using multiple tools, including Informatica, Microsoft SSIS or other technologies \n  Experience with cloud data platforms is helpful \n  Combined IT and Marketing background \n  Machine Learning, Data Science, and statistical modeling experience are highly valued \n \n \n ",
        "techs": [
            "sql",
            "python",
            "rest apis",
            "hadoop ecosystem",
            "lumen data",
            "informatica",
            "microsoft ssis",
            "cloud data platforms",
            "machine learning",
            "data science",
            "statistical modeling"
        ],
        "cleaned_techs": [
            "sql",
            "python",
            "rest apis",
            "hadoop ecosystem",
            "lumen data",
            "informatica",
            "microsoft ssis",
            "cloud data platforms",
            "data science",
            "statistical modeling"
        ]
    },
    "ebf3b0433986571a": {
        "terms": [
            "data engineer",
            "mlops"
        ],
        "salary_min": 128242.22,
        "salary_max": 162383.22,
        "title": "Senior Data Engineer",
        "company": "FutureFit AI",
        "desc": "FutureFit AI is looking for a contract Senior Data Engineer to join our team. We have a culture of high trust, high impact, high velocity and a gritty determination to win. If you are passionate about development, like to have fun, do it right, and get things done we would love to hear from you! \n  An important note: Data shows that men on average apply for a role if they meet 3/10 requirements while women often only do so if it\u2019s 10/10. We work hard to be clear and specific about what our roles include and demand and encourage you to apply if you see a strong (but not necessarily perfect) fit between you and the opportunity. \n \n  About You: \n \n  Technology - You are a passionate data engineer, eager to learn, solve problems and share your knowledge with others \n  Data - Strong architecture and design around various data paradigms (document stores, data lakes/warehouses and relational) and databases (Mongo, Elastic, Postgres, Redshift, etc.) \n  Data Query - Experience writing highly performant data queries for large/complex data sets across relational and NoSQL databases \n  Data Pipelines - Experience in architecture and implementation of ETL processes, streaming data, data process orchestration (Airflow) \n  BI - Adept at connecting BI platforms (Looker) to data warehouses to provide critical insights via dashboards \n  MLOps - Experience with model versioning, deployment, monitoring and serving models via REST APIs \n  Understanding - You are eager to understand the business problem and collaborate with product to build a solution that meets both technology and business requirements \n  Communication - You can share your perspective clearly and constructively, you are comfortable challenging ideas and collaborating to find the best approach \n  Grit - You like to solve challenging problems, are not afraid of the unknown, eager to learn and take a systematic approach to solving problems \n  AI - You are interested in integrating advances in AI into FutureFit AI products and the development process to drive improvements in velocity, quality and engineer experience \n  Values - You are a good person who shares our values of excellence, transparency, and humanity \n \n \n  The Product\u2019s Technology Stack: \n \n  Front End - React, TypeScript, Tailwind, Storybook \n  Back End - Node.js, Nest.js, Python, Flask, GraphQL, Postgres, MongoDB, RedShift, Airflow \n  AWS - CloudFront, Cognito, ECS, RDS, S3, SQS, Elasticsearch, VPC, Lambda \n \n \n  Location: \n  Most days we work from home but everyone comes to the office at least once a week for face to face collaboration and team building. The office is located at 325 Front St West (a short walk from Union Station). \n \n  Our Company: \n  In looking at a job posting, it\u2019s often hard to get a basic picture of the company profile (size, stage, structure, etc.) which is why we are sharing it with you upfront. This helps you quickly decide and helps us focus any time we spend together on going beyond the basics: \n  Funding : We have raised one round of funding led by JP Morgan which fueled a significant growth trajectory for us and we have a safe financial runway to execute against. \n  Problem Domain:  Future of Work / Workforce Development - important that the problem domain interests you even if you haven\u2019t worked in the space before. \n  Customers : We partner with workforce development agencies, government agencies, and employers/enterprises. \n  Structure : We are organized around the following key departments: Growth, Customer Success, Product, Engineering, Data, People & Culture, Finance & Operations. \n  Team : We are a team of 30-50 across US and Canada with main hubs in New York and Toronto. This role will be based in Toronto.     Core Principles : Be Curious, Drive to Outcomes, Raise the Bar, Speed Matters, Own It, Put We Over Me \n \n  About FutureFit AI \n  At FutureFit AI, we\u2019re on a mission to unlock pathways between talent and opportunity using the power of AI. We focus on personalized, AI-powered career guidance for job seekers, emphasizing skills over extensive resumes, and partner with workforce development partners, governments, and employers to level access to opportunity. \n \n  FutureFit AI is a growing, venture-backed company focused on using technology to improve the lives and outcomes for people going through career transitions. We\u2019re a small, driven team, united by our commitment to the job seekers and workforce ecosystems we serve. We're not just building a company; we're shaping the future of work. \n \n  We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment. Please contact us to request an accommodation. \n   FutureFit AI All rights reserved, we are proud to be an equal opportunity workplace. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate on the basis of race, religion, color, gender identity, sexual orientation, age, disability, veteran status, or other applicable legally protected characteristics. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.",
        "cleaned_desc": "  Data Pipelines - Experience in architecture and implementation of ETL processes, streaming data, data process orchestration (Airflow) \n  BI - Adept at connecting BI platforms (Looker) to data warehouses to provide critical insights via dashboards \n  MLOps - Experience with model versioning, deployment, monitoring and serving models via REST APIs \n  Understanding - You are eager to understand the business problem and collaborate with product to build a solution that meets both technology and business requirements \n  Communication - You can share your perspective clearly and constructively, you are comfortable challenging ideas and collaborating to find the best approach \n  Grit - You like to solve challenging problems, are not afraid of the unknown, eager to learn and take a systematic approach to solving problems \n  AI - You are interested in integrating advances in AI into FutureFit AI products and the development process to drive improvements in velocity, quality and engineer experience \n  Values - You are a good person who shares our values of excellence, transparency, and humanity ",
        "techs": [
            "data pipelines",
            "etl processes",
            "streaming data",
            "data process orchestration (airflow)",
            "bi platforms (looker)",
            "data warehouses",
            "dashboards",
            "mlops",
            "model versioning",
            "deployment",
            "monitoring",
            "rest apis",
            "understanding",
            "communication",
            "grit",
            "ai",
            "futurefit ai products",
            "values."
        ],
        "cleaned_techs": [
            "data pipelines",
            "etl processes",
            "streaming data",
            "data process orchestration (airflow)",
            "bi platforms (looker)",
            "data warehouses",
            "dashboards",
            "mlops",
            "model versioning",
            "deployment",
            "monitoring",
            "rest apis",
            "understanding",
            "communication",
            "grit",
            "ai",
            "futurefit ai products",
            "values."
        ]
    },
    "f269202ada9ad1b0": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 90000.0,
        "salary_max": 100000.0,
        "title": "Data Engineer (Remote)",
        "company": "Cortica",
        "desc": "Cortica is looking for a \n   Data Engineer  to join its growing multi-disciplinary team!\n   \n \n \n   We are a rapidly growing healthcare company pioneering a unique, multispecialty approach to the treatment of pediatric neurodevelopmental differences. We serve a variety of patient populations including individuals with autism spectrum disorder, developmental delays, sensory processing disorder, articulation challenges, ADHD, anxiety, cerebral palsy, down syndrome, learning disorders, and more. Our care model, driven by insights and technologies from emerging neuroscience, brings together clinicians from a range of disciplines to design and deliver comprehensive care to children. Because the heart of Cortica's mission is direct patient care, the skill and compassion of our staff are crucial to achieving extraordinary experiences and outcomes for the families we serve.\n   \n \n \n   We believe a great culture attracts great people. We also believe that great culture and great people working together can change the world. Our hopes and dreams are bold and ambitious. Our mission is to design and deliver life-changing care \u2013 one child, one family, one community at a time. Ultimately, we envision a world that cultivates the full potential of every child. At Cortica, every team member in every role is instrumental in bringing our mission and vision to life.\n   \n \n \n   Cortica is an equal opportunity employer. Cortica celebrates diversity and fosters an inclusive environment, seeking ideas and opinions from everyone on the team. We safeguard equal rights and respect for all individuals, regardless of race, color, religion, sex, national origin, age, disability, creed, genetic information, sexual orientation, gender identity or expression, ancestry, veteran status or other applicable, legally protected characteristics. All Cortica employment decisions including but not limited to hiring are made based on an individual\u2019s qualifications and ability to successfully execute the job responsibilities.\n   \n \n \n  What will you do? \n \n \n \n  Design data solutions for the organization leveraging, AWS, Azure, Salesforce, Mulesoft, and multiple APIs. \n  Work closely with financial data analysts to build complex data models and data pipelines from health and business operations data utilizing Python, Airflow, MS SQL, Snowflake, and AWS. \n  Work on the data pipeline platform to enable Analytics and Data Science team members to build reliable data models and supporting pipelines quickly and efficiently. \n  Data modeling using Kimball methodology to support financial and operational analytics. \n  Consistently evolve data model & data schema based on business and engineering requirements. \n  Deliver data that will drive business decisions and improve quality of care. \n  Assemble large, complex data sets that meet functional / non-functional business requirements from a variety of data sources to the AWS Data Lake. \n  Support analytics teams with their development in Power BI. \n  Work with IT team to ensure data is secure and programming processes support security of Cortica\u2019s data. \n  Pitch in with IT colleagues in rolling out new systems and technology initiatives to the enterprise. \n  Work with stakeholders and various teams to assist with data-related technical issues and support their data infrastructure needs. \n \n \n  What\u2019s required? \n \n \n \n  Bachelor's degree \n  1-2 years\u2019 experience working with Data Warehouse ETL and building out data pipelines. \n  Excellent communication skills to collaborate with partners in IT, data science, and Finance \n  Experience with Python programming for the purpose of ETL. \n  Experience working with SQL. \n  Experience with Cloud data such as AWS or Azure big data offerings. \n  An understanding and experience building Power BI data models and reports. \n  A passion for data and data warehousing. \n  Curiosity to learn, test, and deploy new data technologies. \n \n \n  What makes an outstanding candidate? \n \n \n \n  Knowledge and experience working with financial data and modeling data into standardized financial data models. \n  A background in Kimball Modeling. \n  Experience building and optimizing \u2018big data\u2019 data pipelines, architectures, and data sets. \n  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \n  Strong Python scripting skills. \n  Experience working with or experience supporting users for Power BI/Tableau/Qlikview/Cognos analytics technologies. \n  Familiarity and experience with the Presto, Airflow, and the AWS ecosystem. \n \n \n  Your Compensation & Benefits \n \n \n \n   Cortica cares deeply about the well-being of each team member! The culture and experience we create for our employees are of the utmost importance to us. We offer a wide range of benefits. Among the benefits offered by the company are medical, dental, and vision insurance, 401Ks with company matching and rapid vesting, paid holidays and wellness days, life insurance, disability insurance options, tuition reimbursements for professional development and continuing education, and referral bonuses.\n   \n \n \n   The base pay range for this opening is \n   $90,000  to \n   $100,000 annually.  According to your skill level, relevant experience, education level, and location, you will receive compensation that fits appropriately within the range.\n  \n \n \n \n About Cortica: \n \n \n Cortica is a leading provider of advanced neurological therapies for children with autism and other developmental differences. We offer in-home, in-clinic, and telehealth services that empower families to achieve long-lasting, transformative results.",
        "cleaned_desc": " \n  What will you do? \n \n \n \n  Design data solutions for the organization leveraging, AWS, Azure, Salesforce, Mulesoft, and multiple APIs. \n  Work closely with financial data analysts to build complex data models and data pipelines from health and business operations data utilizing Python, Airflow, MS SQL, Snowflake, and AWS. \n  Work on the data pipeline platform to enable Analytics and Data Science team members to build reliable data models and supporting pipelines quickly and efficiently. \n  Data modeling using Kimball methodology to support financial and operational analytics. \n  Consistently evolve data model & data schema based on business and engineering requirements. \n  Deliver data that will drive business decisions and improve quality of care. \n  Assemble large, complex data sets that meet functional / non-functional business requirements from a variety of data sources to the AWS Data Lake. \n  Support analytics teams with their development in Power BI. \n  Work with IT team to ensure data is secure and programming processes support security of Cortica\u2019s data. \n  Pitch in with IT colleagues in rolling out new systems and technology initiatives to the enterprise. \n  Work with stakeholders and various teams to assist with data-related technical issues and support their data infrastructure needs.   \n \n  What\u2019s required? \n \n \n \n  Bachelor's degree \n  1-2 years\u2019 experience working with Data Warehouse ETL and building out data pipelines. \n  Excellent communication skills to collaborate with partners in IT, data science, and Finance \n  Experience with Python programming for the purpose of ETL. \n  Experience working with SQL. \n  Experience with Cloud data such as AWS or Azure big data offerings. \n  An understanding and experience building Power BI data models and reports. \n  A passion for data and data warehousing. \n  Curiosity to learn, test, and deploy new data technologies. \n   \n  What makes an outstanding candidate? \n \n \n \n  Knowledge and experience working with financial data and modeling data into standardized financial data models. \n  A background in Kimball Modeling. \n  Experience building and optimizing \u2018big data\u2019 data pipelines, architectures, and data sets. \n  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \n  Strong Python scripting skills. \n  Experience working with or experience supporting users for Power BI/Tableau/Qlikview/Cognos analytics technologies. \n  Familiarity and experience with the Presto, Airflow, and the AWS ecosystem. \n \n \n  Your Compensation & Benefits \n ",
        "techs": [
            "aws",
            "azure",
            "salesforce",
            "mulesoft",
            "python",
            "airflow",
            "ms sql",
            "snowflake",
            "kimball methodology",
            "power bi",
            "cortica's data",
            "etl",
            "cloud data (aws or azure)",
            "financial data modeling",
            "root cause analysis",
            "presto",
            "tableau",
            "qlikview",
            "cognos",
            "aws ecosystem"
        ],
        "cleaned_techs": [
            "aws",
            "azure",
            "salesforce",
            "mulesoft",
            "python",
            "airflow",
            "ms sql",
            "snowflake",
            "kimball methodology",
            "powerbi",
            "cortica's data",
            "etl",
            "cloud data (aws or azure)",
            "financial data modeling",
            "root cause analysis",
            "presto",
            "tableau",
            "qlikview",
            "cognos"
        ]
    },
    "f5038dbe42215e84": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 100512.195,
        "salary_max": 127270.836,
        "title": "Data Integration Engineer",
        "company": "Sprezzatura Management Consulting",
        "desc": "Position Description: \n  Sprezzatura is seeking a highly skilled and experienced Data Integration Engineer to support our federal client. In this position, you will play a crucial role in designing the integration of data from multiple sources into target data products and repositories. This role is responsible for designing, implementing, and maintaining data integration solutions that create a seamless flow of data between various systems, databases, and applications, contributing to the overall efficiency and effectiveness of the client's data infrastructure. \n \n   \n Responsibilities: \n \n Data Collection and Analysis : Collect and analyze data related to process performance, productivity, and customer feedback. Identify key performance indicators (KPIs) to measure process effectiveness and be able to derive and communicate impacts from recommended improvements. \n Process Documentation and Standardization : Document recommended processes and data flow changes in the form of workflows, standard operating procedures (SOPs), and process maps. Ensure clear and comprehensive documentation to guide process implementation and facilitate knowledge transfer.  \n Data Design : Collaborate with the project team to design, structure, and maintain data obtained from multiple sources and diverse data types in varying formats.  \n Data Pipeline:  Design and implement data engineering pipelines using cloud tools and frameworks Ensure proper integration of data from multiple sources into a target data repository. \n Process Analysis and Assessment:  Conduct a comprehensive analysis of existing systems and their associated data within the customer domain. Identify areas of inefficiency that are candidates for automation improvement, to create opportunities to enhance effectiveness and customer satisfaction. \n \n \n \n  Qualifications: \n \n Bachelor\u2019s degree in computer science, Information Technology, or a related field (or equivalent work experience). \n Must have 5 years of proven experience as a Junior Technologist, Data Engineer, Integration or Interface Analyst, or Technical Business Analyst, preferably in a large-scale enterprise environment serving federal clients.  \n Preferred experience supporting Veterans Affairs, and Veterans Benefits Administration (VBA) including VBA Backend systems. \n Ability to conduct in-depth data analysis by developing and running various queries against large-scale databases, using scripting languages such as SQL (Postgres, Oracle, or T-SQL) R, or Python.  \n Excellent problem-solving and analytical skills with the ability to understand complex enterprise architectures and data structures. \n Effective communication and interpersonal skills to collaborate with internal teams and stakeholders. \n \n \n \n  Transitioning military and/or Veterans with relevant experience are invited to apply. Sprezzatura is an equal-opportunity employer. Sprezzatura offers benefits including healthcare, 401K, vacation, and paid sick leave. \n \n  Company Description \n  Sprezzatura Management Consulting, LLC (www.sprezzmc.com) is a Washington, DC-area Service-Disabled Veteran-Owned Small Business (SDVOSB) that enables government transformation by supplying insight and leadership at the intersection of people, processes, and technology. We apply knowledge, project, and life-cycle management best practices to catalyze change.",
        "cleaned_desc": " \n Data Collection and Analysis : Collect and analyze data related to process performance, productivity, and customer feedback. Identify key performance indicators (KPIs) to measure process effectiveness and be able to derive and communicate impacts from recommended improvements. \n Process Documentation and Standardization : Document recommended processes and data flow changes in the form of workflows, standard operating procedures (SOPs), and process maps. Ensure clear and comprehensive documentation to guide process implementation and facilitate knowledge transfer.  \n Data Design : Collaborate with the project team to design, structure, and maintain data obtained from multiple sources and diverse data types in varying formats.  \n Data Pipeline:  Design and implement data engineering pipelines using cloud tools and frameworks Ensure proper integration of data from multiple sources into a target data repository.   \n Bachelor\u2019s degree in computer science, Information Technology, or a related field (or equivalent work experience). \n Must have 5 years of proven experience as a Junior Technologist, Data Engineer, Integration or Interface Analyst, or Technical Business Analyst, preferably in a large-scale enterprise environment serving federal clients.  \n Preferred experience supporting Veterans Affairs, and Veterans Benefits Administration (VBA) including VBA Backend systems. \n Ability to conduct in-depth data analysis by developing and running various queries against large-scale databases, using scripting languages such as SQL (Postgres, Oracle, or T-SQL) R, or Python.    Excellent problem-solving and analytical skills with the ability to understand complex enterprise architectures and data structures. \n Effective communication and interpersonal skills to collaborate with internal teams and stakeholders. \n \n \n ",
        "techs": [
            "data collection and analysis",
            "process documentation and standardization",
            "data design",
            "data pipeline",
            "cloud tools and frameworks",
            "bachelor\u2019s degree in computer science",
            "information technology",
            "or a related field",
            "5 years of proven experience as a junior technologist",
            "data engineer",
            "integration or interface analyst",
            "or technical business analyst",
            "experience supporting veterans affairs and veterans benefits administration (vba)",
            "ability to conduct in-depth data analysis using sql",
            "r",
            "or python",
            "problem-solving and analytical skills",
            "effective communication and interpersonal skills"
        ],
        "cleaned_techs": [
            "data collection and analysis",
            "data design",
            "data pipeline",
            "cloud tools and frameworks",
            "information technology",
            "or a related field",
            "5 years of proven experience as a junior technologist",
            "data engineer",
            "experience supporting veterans affairs and veterans benefits administration (vba)",
            "r",
            "or python"
        ]
    },
    "438a072845b56b63": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 90000.0,
        "salary_max": 115000.0,
        "title": "Software Engineer - Archiving and Data Services",
        "company": "Internet Archive",
        "desc": "Interested in a mission-driven job ensuring perpetual open access to information for a global audience? Enjoy helping scale the use of services and products critical to hundreds of national and international non-profits, libraries, universities, cultural heritage institutions, and mission-driven organizations? If so, the Internet Archive is seeking a Software Engineer for our Archiving & Data Services team. \n  Internet Archive (IA) is a non-profit digital library, top 200 website at archive.org, and an archive of over 99 petabytes of digital information running in many self-owned and operated data centers. Internet Archive also provides mission-aligned services to thousands of organizations working collaboratively to advance our shared goal of \u201cUniversal Access to All Knowledge.\u201d The Archiving & Data Services group provides a suite of paid, SaaS, and free products, as well as community programs, focused on the archiving, management, analysis, and accessibility of digital information. Its services are used by over 1,500 organizations around the world. \n  We are looking for a motivated, detail-oriented Software Engineer to join our team. The role will focus on Archive-It (archive-it.org), our platform for building, sharing, and preserving web archive collections. This position offers the opportunity to work with a range of technologies and gain deep knowledge about web crawling, archival replay, and large-scale distributed systems. Our services work with petabytes of archived data and facilitate the discovery and use of large-scale digital collections. The Software Engineer will have the unique opportunity to build things that further open access to information and advance the public good. \n  Key Responsibilities: \n \n  Collaborate with team members to understand user needs, design new features, support web crawling and preservation, and improve the performance and reliability of Archive-It and other department products. \n \n \n     Implement, test, and maintain software across our stack (Python, Elasticsearch, Postgres, Temporal, HTML/CSS/JS/TS).\n    \n \n \n     Develop, monitor, and maintain the Archive-It partner application, where web crawls are configured, scheduled, and reported.\n    \n \n \n     Improve a distributed system orchestrating web crawls and post-processing them for long term preservation, indexing for retrieval, deduplication, and reporting.\n    \n \n \n     Participate in code reviews to ensure the quality and stability of our software and diffusion of knowledge across the team.\n    \n  Document architecture, software, and features for internal and external users. \n \n  Qualification and Skills: \n \n  Degree in Computer Science or a related field, or equivalent experience, strongly preferred. \n \n \n     Proficiency in Python, with familiarity in Postgres, Elasticsearch, and HTML/CSS/JS preferred.\n    \n \n \n     A strong understanding of web services and distributed systems.\n    \n \n \n     Excellent problem-solving skills, attention to detail, and ability to work both independently and collaboratively.\n    \n \n \n     Experience with web crawling, Django, workflow systems (e.g. Temporal, Airflow), distributed databases (e.g. Cassandra, Scylla), Hadoop, and Ansible are a plus\n    \n \n \n     GitLab, GitHub, Sentry, Grafana, JIRA, are other tools we use.\n    \n \n \n     Our independently operated data centers run Ubuntu Linux VMs and our department runs everything from the VM up, so Linux experience is preferred.\n    \n  An interest in the Internet Archive\u2019s mission to provide Universal Access to All Knowledge is expected. \n \n  Job Details: \n  Remote applicants preferred. We have headquarters in San Francisco and Vancouver and candidates in those locations will have the option for hybrid remote/in-office arrangements. Candidates will need to have some time overlap with primarily North America (and largely Pacific Time) based colleagues. Compensation and title will be commensurate with experience and the role is open to candidates of varying seniority with a general, but negotiable, salary range of $90,000 to $115,000 based on living in the San Francisco, CA region. Compensation may be adjusted based on the geographic location of the finalist. \n  Benefits & Perks: \n  The Internet Archive is a remote first workplace and provides a comprehensive benefits package including; PTO, paid holidays, and medical benefits. Depending on where you live, we also provide these additional benefits; dental, vision, health savings accounts, flex spending accounts, commuter benefits, short term disability, long term disability and retirement programs. \n  At the Internet Archive, we believe we do our best work when our employees bring together diverse ideas. Members of all groups under represented in the tech industry and library world are strongly encouraged to apply. We are proud to be an equal opportunity workplace and are committed to equal employment opportunity regardless of race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or any other characteristic protected by applicable federal, state or local law.",
        "cleaned_desc": "  Document architecture, software, and features for internal and external users. \n \n  Qualification and Skills: \n \n  Degree in Computer Science or a related field, or equivalent experience, strongly preferred. \n \n \n     Proficiency in Python, with familiarity in Postgres, Elasticsearch, and HTML/CSS/JS preferred.\n    \n \n       A strong understanding of web services and distributed systems.\n    \n \n \n     Excellent problem-solving skills, attention to detail, and ability to work both independently and collaboratively.\n    \n \n \n     Experience with web crawling, Django, workflow systems (e.g. Temporal, Airflow), distributed databases (e.g. Cassandra, Scylla), Hadoop, and Ansible are a plus\n    \n ",
        "techs": [
            "postgres",
            "elasticsearch",
            "html/css/js",
            "python",
            "web services",
            "distributed systems",
            "web crawling",
            "django",
            "workflow systems (e.g. temporal",
            "airflow)",
            "distributed databases (e.g. cassandra",
            "scylla)",
            "hadoop",
            "ansible"
        ],
        "cleaned_techs": [
            "postgres",
            "elasticsearch",
            "html/css/js",
            "python",
            "web services",
            "distributed systems",
            "web crawling",
            "django",
            "workflow systems (e.g. temporal",
            "airflow)",
            "distributed databases (e.g. cassandra",
            "scylla)",
            "hadoop",
            "ansible"
        ]
    },
    "9d7ec8cc793571d8": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 90382.05,
        "salary_max": 114443.81,
        "title": "Sr Data Engineer - Optum Behavioral Health - Remote",
        "company": "Optum",
        "desc": "For those who want to invent the future of health care, here's your opportunity. We're going beyond basic care to health programs integrated across the entire continuum of care. Join us and help people live healthier lives while doing  your life's best  work.( sm ) \n \n \n  Join Optum Behavioral Health Solutions (OBHS) as a Senior Data Engineer. OBHS is one of the largest behavioral health networks in the country with more than 300,000 providers serving 37 million individuals. OBHS supports and engages people with mental health conditions or substance use disorders through our performance-tiered behavioral health network. \n \n \n \n \n You\u2019ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges. \n \n \n \n   \n \n \n Primary Responsibilities: \n \n \n \n Gather report requirements from customers, clarifying using business terms and translating to technical terms \n Create SSIS Packages or Power Automate flows for ETL to ensure data flows to a data warehouse \n Write Microsoft SQL Server queries, stored procedures, tables, and other objects to properly create and manage various datasets from the data warehouse \n Use Microsoft Visual Studio to Create, Run Test, and Deploy SQL Server Reporting Services reports for delivery to our customers \n Collaborate with System Admins, DBAs, and Web Developers to understand systems/processes necessary for quality reporting \n Digitally document using Microsoft SharePoint to collaborate with teams and customers \n Design future ETL Loads from outside vendors into our data warehouse \n \n \n \n \n \n \n \n You\u2019ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in. \n \n \n Required Qualifications: \n \n \n \n 5+ years of experience in Microsoft SQL Server Integration Services (SSIS): creating packages, scheduling, and configuring \n 5+ years of experience in Microsoft SQL server: Database Design, performance monitoring, Managing Queries \n 5+ years of experience with Microsoft Visual Studio \n 3+ years of experience working with non-technical customers for specification gathering \n 3+ years of experience managing Windows Servers: installing software, configuring server, security setup, managing services \n 2+ years of experience in Microsoft SQL Server Reporting Services (SSRS): creating reports, designing, and maintaining \n Familiarity with Power BI \n Willing or ability to accommodate Pacific Standard Time (PST) meetings and partial work schedule \n \n \n \n \n \n \n \n Preferred Qualifications: \n \n \n \n Bachelor\u2019s or higher degree in Computer Science or related field \n experience with Cloud Databases \n Experience with Azure DevOps \n Experience with GitHub \n Experience with Snowflake \n \n \n \n \n \n \n \n Careers at  OptumCare .  We're on a mission to change the face of health care. As the largest health and wellness business in the US, we help 58 million people navigate the health care system, finance their health care needs and achieve their health and well-being goals. Fortunately, we have a team of the best and brightest minds on the planet to make it happen. Together we're creating the most innovative ideas and comprehensive strategies to help heal the health care system and create a brighter future for us all. Join us and learn why there is no better place to do  your life's best  work.( sm ) \n \n \n \n   \n \n \n OptumCare is committed to creating an environment where physicians focus on what they do best: care for their patients. To do so, OptumCare provides administrative and business support services to both owned and affiliated medical practices which are part of OptumCare. Each medical practice part and their physician employees have complete authority with regards to all medical decision-making and patient care. OptumCare\u2019s support services do not interfere with or control the practice of medicine by the medical practices or any of their physicians. \n \n \n \n   \n \n California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only:  The salary range for California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island or Washington residents is $85,000 to $167,300. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you\u2019ll find a far-reaching choice of benefits and incentives.\n   \n \n \n \n   \n \n \n \n All employees working remotely will be required to adhere to UnitedHealth Group\u2019s Telecommuter Policy \n \n \n \n \n   \n \n \n At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone\u2013of every race, gender, sexuality, age,  location  and income\u2013deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized  groups  and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering  equitable  care that addresses health disparities and improves health outcomes \u2014 an enterprise priority reflected in our mission . \n \n \n \n \n \n \n \n Diversity creates a healthier atmosphere:  OptumCare  is an Equal Employment Opportunity/Affirmative Action employers and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law. \n \n \n \n   \n \n \n OptumCare  is a drug-free workplace. Candidates  are required to  pass a drug test before beginning employment",
        "cleaned_desc": " Collaborate with System Admins, DBAs, and Web Developers to understand systems/processes necessary for quality reporting \n Digitally document using Microsoft SharePoint to collaborate with teams and customers \n Design future ETL Loads from outside vendors into our data warehouse \n \n \n \n \n \n \n \n You\u2019ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in. \n \n \n Required Qualifications: \n \n \n \n 5+ years of experience in Microsoft SQL Server Integration Services (SSIS): creating packages, scheduling, and configuring \n 5+ years of experience in Microsoft SQL server: Database Design, performance monitoring, Managing Queries \n 5+ years of experience with Microsoft Visual Studio \n 3+ years of experience working with non-technical customers for specification gathering \n 3+ years of experience managing Windows Servers: installing software, configuring server, security setup, managing services \n 2+ years of experience in Microsoft SQL Server Reporting Services (SSRS): creating reports, designing, and maintaining   Familiarity with Power BI \n Willing or ability to accommodate Pacific Standard Time (PST) meetings and partial work schedule \n \n \n \n \n \n \n \n Preferred Qualifications: \n \n \n \n Bachelor\u2019s or higher degree in Computer Science or related field \n experience with Cloud Databases \n Experience with Azure DevOps \n Experience with GitHub \n Experience with Snowflake \n \n \n \n \n ",
        "techs": [
            "microsoft sql server integration services (ssis)",
            "microsoft sql server",
            "microsoft visual studio",
            "microsoft sql server reporting services (ssrs)",
            "power bi",
            "sharepoint",
            "azure devops",
            "github",
            "snowflake"
        ],
        "cleaned_techs": [
            "microsoft sql server integration services (ssis)",
            "microsoft sql server",
            "microsoft visual studio",
            "microsoft sql server reporting services (ssrs)",
            "powerbi",
            "sharepoint",
            "azure",
            "github",
            "snowflake"
        ]
    },
    "50026a89501b8477": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 100299.875,
        "salary_max": 127001.984,
        "title": "Data Engineer - Spark (Remote within US)",
        "company": "Corning",
        "desc": "Date:  Oct 16, 2023  \n Location:  Charlotte, NC, US, 28216  \n Company:  Corning  \n \n Requisition Number: 61854 \n \n \n Corning is vital to progress \u2013 in the industries we help shape and in the world we share. \n \n \n We invent life-changing technologies using materials science. Our scientific and manufacturing expertise, boundless curiosity, and commitment to purposeful invention place us at the center of the way the world interacts, works, learns, and lives. \n \n \n Our sustained investment in research, development, and invention means we\u2019re always ready to solve the toughest challenges alongside our customers. \n \n \n The global Information Technology (IT) Function is leading efforts to align IT and Business Strategy, leverage IT investments, and optimize end to end business processes and associated information integration technologies. Through these efforts, IT helps to improve the competitive position of Corning's businesses through IT enabled processes. \n \n \n IT also delivers Information Technology applications, infrastructure, and project services in a cost efficient manner to Corning worldwide. \n \n \n Purpose of the Position: \n \n \n The Data Engineer (Spark Engineer) will work with our core platform development team as well as domain experts, application developers, controls engineers and data scientists. \n \n \n Their primary responsibility will be to develop reliable and instrumented data ingestion pipelines that land inbound data from multiple process and operational data stores throughout the company to on-premise and cloud-based data lakes. \n \n \n These pipelines will require data validation and data profiling automation along with version control and CI/CD to ensure ongoing resiliency and maintainability of the inbound data flows supporting our advanced analytics projects. \n \n \n Day to Day Responsibilities \n \n As a Data Engineer for our advanced analytics platforms, your main responsibilities will be: \n     \n Design, test, deploy and maintain production big-data ingestion pipelines using established frameworks, patterns of practice, agile software development and CI/CD practices \n Work with cross-organizational data source teams to define data ingestion requirements for structured, unstructured and semi-structured data, pilot their implementation, ensure the data source teams accept the resulting landed data as valid \n Define and implement automated validation and profiling capabilities needed to ensure reliable data delivery, using agile software development and CI/CD practices \n Work with data source teams, domain experts and data scientists to define data cleansing and data enrichment requirements for landed data \n Implement data cleansing and enrichment code using established patterns of practice \n Work with data source teams, domain experts and data scientists to validate landed, cleansed and enriched data, using agile software development and CI/CD practices, while ensuring that the final datasets are directly usable by them without additional processing effort \n Participate in code reviews and technical information sharing with your team members and the broader software engineering community at Corning \n Stay up to date with industry standards and technological advancements that will improve the quality, productivity and performance of your work. \n Provide support in a DevOps environment to monitor tokens, jobs and overall system performance. \n \n \n \n Education and Experience:   \n \n Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline \n Prior experience in big data engineering roles, developing and maintaining ETL and ELT pipelines for data warehousing, on-premise and cloud datalake environments \n Expert level fluency in SQL to write and debug complex queries \n 3 years of demonstrated proficiency in Python or Spark applied to large scale data sets \n Strong understanding of workflow management platforms/ orchestration tools (Airflow or similar) \n 3 years of experience developing batch, micro-batch and streaming ingestion pipelines using high-level Apache Spark APIs (pySpark, SparkSQL, SparkR or Scala) \n Obsession for service observability, instrumentation, monitoring and alerting \n Strong, firsthand technical familiarity with Databricks, S3, parquet and Delta Lake architecture, technologies and tools \n Understanding of the Data Lifecycle Management process to collect, access, use, store, transfer, delete data \n Understanding of relational databases (e.g., MySQL, PostgreSQL), NoSQL databases (e.g., key-value stores like Redis, DynamoDB, RocksDB) \n Expert level proficiency with agile software development & continuous integration + continuous deployment methodologies along with supporting tools such as Git (Gitlab), Jira, Terraform, New Relic \n Strong, firsthand familiarity with notebook environments including JupyterHub \n DevOps experience with AWS platform services, including AWS S3 & EC2, Data Migration Services (DMS), RDS, EMR, RedShift, Lambda, DynamoDB, CloudWatch, CloudTrail \n Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy \n \n \n   \n Preferred Qualifications \n \n Prior full-stack app development experience (front-end, back-end, microservices) \n Familiarity with the following tools and technology practices: Oracle, Microsoft SQL Server \n Established enterprise ETL and integration tools including Informatica, Talend, dbt \n Established opensource data integration and DAG tools  \n Reporting and analysis tools including PowerBI, Tableau, SAS JMP \n \n \n \n This Position Does Not Allow for Immigration Sponsorship \n \n \n Salary Range:  The range for this position is $105,000 - $160,000. Starting pay for the successful applicant is dependent on a variety of job-related factors, including but not limited to geographic location, market demands, experience, training, and education. The benefits available for this position include medical, dental, vision, 401(k) plan, pension plan, life insurance coverage, disability benefits, and PTO. \n \n \n Corning Puts YOU First! \n \n   We are committed to supporting your health, financial, career development, and life goals as you grow professionally and personally to achieve your highest potential. All benefits begin as soon as you start your career at Corning.\n    \n \n \n Our monetary peer-to-peer recognition program is tied to our Values and celebrates you and your colleagues\u2019 contributions. \n Health and well-being benefits include medical, dental, vision, paid parental leave, mental health/substance use, fitness, and disease management programs. \n Financial benefits include a 401(k) savings plan with company matching contributions and a 100% company-paid pension benefit that grows steadily throughout your career. \n Companywide bonus and attractive short- and long-term compensation programs are available based on your role and responsibilities. \n Professional development programs help you grow and achieve your career goals. \n \n \n \n \n We prohibit discrimination on the basis of race, color, gender, age, religion, national origin, sexual orientation, gender identity or expression, disability, veteran status or any other legally protected status. \n \n \n We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation. \n \n   Nearest Major Market:  Charlotte",
        "cleaned_desc": " \n Purpose of the Position: \n \n \n The Data Engineer (Spark Engineer) will work with our core platform development team as well as domain experts, application developers, controls engineers and data scientists. \n \n \n Their primary responsibility will be to develop reliable and instrumented data ingestion pipelines that land inbound data from multiple process and operational data stores throughout the company to on-premise and cloud-based data lakes. \n \n \n These pipelines will require data validation and data profiling automation along with version control and CI/CD to ensure ongoing resiliency and maintainability of the inbound data flows supporting our advanced analytics projects. \n \n \n Day to Day Responsibilities \n \n As a Data Engineer for our advanced analytics platforms, your main responsibilities will be: \n     \n Design, test, deploy and maintain production big-data ingestion pipelines using established frameworks, patterns of practice, agile software development and CI/CD practices \n Work with cross-organizational data source teams to define data ingestion requirements for structured, unstructured and semi-structured data, pilot their implementation, ensure the data source teams accept the resulting landed data as valid \n Define and implement automated validation and profiling capabilities needed to ensure reliable data delivery, using agile software development and CI/CD practices \n Work with data source teams, domain experts and data scientists to define data cleansing and data enrichment requirements for landed data   Implement data cleansing and enrichment code using established patterns of practice \n Work with data source teams, domain experts and data scientists to validate landed, cleansed and enriched data, using agile software development and CI/CD practices, while ensuring that the final datasets are directly usable by them without additional processing effort \n Participate in code reviews and technical information sharing with your team members and the broader software engineering community at Corning \n Stay up to date with industry standards and technological advancements that will improve the quality, productivity and performance of your work. \n Provide support in a DevOps environment to monitor tokens, jobs and overall system performance. \n \n \n \n Education and Experience:   \n \n Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline \n Prior experience in big data engineering roles, developing and maintaining ETL and ELT pipelines for data warehousing, on-premise and cloud datalake environments \n Expert level fluency in SQL to write and debug complex queries \n 3 years of demonstrated proficiency in Python or Spark applied to large scale data sets \n Strong understanding of workflow management platforms/ orchestration tools (Airflow or similar) \n 3 years of experience developing batch, micro-batch and streaming ingestion pipelines using high-level Apache Spark APIs (pySpark, SparkSQL, SparkR or Scala) \n Obsession for service observability, instrumentation, monitoring and alerting \n Strong, firsthand technical familiarity with Databricks, S3, parquet and Delta Lake architecture, technologies and tools \n Understanding of the Data Lifecycle Management process to collect, access, use, store, transfer, delete data \n Understanding of relational databases (e.g., MySQL, PostgreSQL), NoSQL databases (e.g., key-value stores like Redis, DynamoDB, RocksDB) \n Expert level proficiency with agile software development & continuous integration + continuous deployment methodologies along with supporting tools such as Git (Gitlab), Jira, Terraform, New Relic ",
        "techs": [
            "spark",
            "etl",
            "elt",
            "sql",
            "python",
            "airflow",
            "apache spark",
            "pyspark",
            "sparksql",
            "sparkr",
            "scala",
            "databricks",
            "s3",
            "parquet",
            "delta lake",
            "mysql",
            "postgresql",
            "redis",
            "dynamodb",
            "rocksdb",
            "git",
            "gitlab",
            "jira",
            "terraform",
            "new relic"
        ],
        "cleaned_techs": [
            "spark",
            "etl",
            "elt",
            "sql",
            "python",
            "airflow",
            "apache spark",
            "pyspark",
            "sparksql",
            "sparkr",
            "scala",
            "databricks",
            "s3",
            "parquet",
            "delta lake",
            "mysql",
            "postgresql",
            "redis",
            "dynamodb",
            "rocksdb",
            "git",
            "gitlab",
            "jira",
            "terraform",
            "new relic"
        ]
    },
    "b15ec8b380fddd07": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 113000.0,
        "salary_max": 210000.0,
        "title": "Senior Data Engineer",
        "company": "Verizon",
        "desc": "When you join Verizon \n  Verizon is one of the world\u2019s leading providers of technology and communications services, transforming the way we connect around the world. We\u2019re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together\u2014lifting up our communities and striving to make an impact to move the world forward. If you\u2019re fueled by purpose, and powered by persistence, explore a career with us. Here, you\u2019ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife. \n \n  What you\u2019ll be doing... \n  As a part of our Big Data Product team, the Data Solutions Engineer will be responsible for developing and validating Big data products and applications which runs on the large Hadoop cluster and Cloud. The qualified engineer will be developing and testing ETL process, Migrating different applications to cloud , developing Data validation tools used for performing quality assessments and measurements on different data sets that feed data & AI products. \n  Building big data and batch/real-time analytical solutions that leverage emerging technologies. \n \n  Performing data migration and conversion activities on different applications and platforms. \n  Designing, developing and testing of data ingestion pipelines, perform end to end automation of ETL process for various datasets that are being ingested into the big data platform. \n  Performing data profiling/analysis, discovery, analysis, suitability and coverage of data, and identifying the various data types, formats, and data quality issues which exist within a given data source. \n  Developing transformation logic, interfaces and reports as needed to meet project requirements. \n  Participating in discussion for technical architecture, data modeling and ETL standards, collaborate with Product Managers, Architects and Senior Developers to establish the physical application framework (e.g. libraries, modules, execution environments). \n  Improving and performance-tuning the optimization of data pipelines. \n  Developing unit and integrated automated test suites to validate end to end data pipeline flow, data transformation rules, and data integrity. \n  Developing tools to measure the data quality and visualize the anomaly pattern in source and processed data. \n  Integrating automated processes into continuous integration workflows. \n  Contributing to data quality assurance standards and procedures. \n \n \n  What we\u2019re looking for... \n  You\u2019re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems. You thrive in a fast-paced, innovative environment working as a phenomenal teammate to drive the best results and business outcomes. \n \n  You'll need to have: \n \n  Bachelor's degree or four or more years of work experience. \n  Four or more years of relevant work experience. \n  Programming experience in Scala, Java or Python \n  Experience in data engineering, preferably in Google Cloud Platform with BigQuery \n  Hands-on experience in designing, building, testing and deploying data pipelines in Teradata and Hadoop platform with experience in, HDFS, Hive, Spark, Streaming, HBase, Kafka, Oozie etc. \n  Good organizational skills and strong written and verbal communication skills. \n \n \n  Even better if you have one or more of the following: \n \n  Bachelor\u2019s degree in Computer Science or equivalent education/training \n  Five or more years of Software development and Testing experience. \n  Experience with development and automated testing in a CI/CD environment. \n  Hands on experience in dashboard development using Looker/Tableau \n  Knowledge of GIT/Jenkins and pipeline automation is a must. \n  Experience with developing and testing real-time data-processing and Analytics Application Systems. \n  Strong knowledge in SQL development on Database and/or BI/DW \n  Strong knowledge in shell scripting Experience in Web Services - API development and testing. \n  A solid understanding of common software development practices and tools. \n  Strong analytical skills with a methodical approach to problem solving applied to the Big Data/AI domain. \n  Experience with incident management tools (Opsgenie, PagerDuty etc) is a plus \n \n \n  If Verizon and this role sound like a fit for you, we encourage you to apply even if you don\u2019t meet every \u201ceven better\u201d qualification listed above. \n \n  This role is eligible to be considered for the Department of Defense SkillBridge Program. \n \n \n \n \n \n \n \n \n  Where you\u2019ll be working \n \n \n \n \n \n \n  In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n  \n  Scheduled Weekly Hours  40\n  \n  Equal Employment Opportunity \n  We\u2019re proud to be an equal opportunity employer - and celebrate our employees\u2019 differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more. \n  Our benefits are designed to help you move forward in your career, and in areas of your life outside of Verizon. From health and wellness benefits, short term incentives, 401(k) Savings Plan, stock incentive programs, paid time off, parental leave, adoption assistance and tuition assistance, plus other incentives, we\u2019ve got you covered with our award-winning total rewards package. For part-timers, your coverage will vary as you may be eligible for some of these benefits depending on your individual circumstances.\n   If you are hired into a California, Colorado, Connecticut, Nevada, New York, Rhode Island or Washington work location, the compensation range for this position is between $113,000.00 and $210,000.00 annually based on a full-time schedule. The salary will vary depending on your location and confirmed job-related skills and experience. This is an incentive based position with the potential to earn more. For part time roles, your compensation will be adjusted to reflect your hours.",
        "cleaned_desc": "When you join Verizon \n  Verizon is one of the world\u2019s leading providers of technology and communications services, transforming the way we connect around the world. We\u2019re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together\u2014lifting up our communities and striving to make an impact to move the world forward. If you\u2019re fueled by purpose, and powered by persistence, explore a career with us. Here, you\u2019ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife. \n \n  What you\u2019ll be doing... \n  As a part of our Big Data Product team, the Data Solutions Engineer will be responsible for developing and validating Big data products and applications which runs on the large Hadoop cluster and Cloud. The qualified engineer will be developing and testing ETL process, Migrating different applications to cloud , developing Data validation tools used for performing quality assessments and measurements on different data sets that feed data & AI products. \n  Building big data and batch/real-time analytical solutions that leverage emerging technologies. \n \n  Performing data migration and conversion activities on different applications and platforms. \n  Designing, developing and testing of data ingestion pipelines, perform end to end automation of ETL process for various datasets that are being ingested into the big data platform. \n  Performing data profiling/analysis, discovery, analysis, suitability and coverage of data, and identifying the various data types, formats, and data quality issues which exist within a given data source. \n  Developing transformation logic, interfaces and reports as needed to meet project requirements. \n  Participating in discussion for technical architecture, data modeling and ETL standards, collaborate with Product Managers, Architects and Senior Developers to establish the physical application framework (e.g. libraries, modules, execution environments). \n  Improving and performance-tuning the optimization of data pipelines. \n  Developing unit and integrated automated test suites to validate end to end data pipeline flow, data transformation rules, and data integrity.    Developing tools to measure the data quality and visualize the anomaly pattern in source and processed data. \n  Integrating automated processes into continuous integration workflows. \n  Contributing to data quality assurance standards and procedures. \n \n \n  What we\u2019re looking for... \n  You\u2019re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solve business problems. You thrive in a fast-paced, innovative environment working as a phenomenal teammate to drive the best results and business outcomes. \n \n  You'll need to have: \n \n  Bachelor's degree or four or more years of work experience. \n  Four or more years of relevant work experience. \n  Programming experience in Scala, Java or Python \n  Experience in data engineering, preferably in Google Cloud Platform with BigQuery    Hands-on experience in designing, building, testing and deploying data pipelines in Teradata and Hadoop platform with experience in, HDFS, Hive, Spark, Streaming, HBase, Kafka, Oozie etc. \n  Good organizational skills and strong written and verbal communication skills. \n \n \n  Even better if you have one or more of the following: \n \n  Bachelor\u2019s degree in Computer Science or equivalent education/training \n  Five or more years of Software development and Testing experience. \n  Experience with development and automated testing in a CI/CD environment. \n  Hands on experience in dashboard development using Looker/Tableau \n  Knowledge of GIT/Jenkins and pipeline automation is a must. \n  Experience with developing and testing real-time data-processing and Analytics Application Systems. \n  Strong knowledge in SQL development on Database and/or BI/DW \n  Strong knowledge in shell scripting Experience in Web Services - API development and testing. ",
        "techs": [
            "hadoop",
            "cloud",
            "etl",
            "data validation",
            "data ingestion",
            "data profiling",
            "data transformation",
            "data pipelines",
            "data integrity",
            "continuous integration",
            "scala",
            "java",
            "python",
            "google cloud platform",
            "bigquery",
            "teradata",
            "hdfs",
            "hive",
            "spark",
            "streaming",
            "hbase",
            "kafka",
            "oozie",
            "looker",
            "tableau",
            "git",
            "jenkins",
            "dashboard development",
            "sql",
            "shell scripting",
            "web services api development"
        ],
        "cleaned_techs": [
            "hadoop",
            "cloud",
            "etl",
            "data validation",
            "data ingestion",
            "data transformation",
            "data pipelines",
            "data integrity",
            "continuous integration",
            "scala",
            "java",
            "python",
            "gcp",
            "bigquery",
            "teradata",
            "hdfs",
            "hive",
            "spark",
            "streaming",
            "hbase",
            "kafka",
            "oozie",
            "looker",
            "tableau",
            "git",
            "jenkins",
            "dashboard development",
            "sql",
            "shell scripting",
            "web services api development"
        ]
    },
    "6492b9f12e5769ae": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 103000.0,
        "salary_max": 158000.0,
        "title": "Data Center Site Operations Engineer - Professional Services",
        "company": "Arista Networks",
        "desc": "Company Description\n   Arista Networks is an industry leader in Cognitive Cloud Networking for mission critical data center and campus environments. Our award winning open source platforms deliver ultra low latency, high availability, automated analytics and secure network solutions. \n  Our culture is one that is founded on our core key values which resonate across all of our employee and include respect, integrity, teamwork, innovation, passion, trust and quality. \n \n \n \n Job Description\n   The Data Center Site Operations Engineer will help customers successfully plan and optimize their Data Center physical space, mechanical, cabling and other site infrastructure to receive Arista equipment and operate it in the highest performing and most efficient manner. \n  Arista Professional Services team helps customers throughout the entire technology lifecycle from design to implementation, migration and ongoing operation, to ensure a seamless transition from initial exposure to being able to operate and realize tangible business outcomes from their network infrastructure. \n  In this role, you would function as the subject matter expert in this area of the Arista PS team, interfacing with mechanical engineers in the Arista product team to understand detailed requirements, translate these into best practices for field installation, then take a customer facing role in training and advisory. \n  Responsibilities will be to provide expert consulting to customers including site design assistance, proactive site prep. for installation, demonstration of best practices, creating training material and delivering training to customers and partiers, site surveys and oversight of physical installation and commissioning. Areas of expertise should include: \n  Site requirements for all Arista Data Center products including: \n \n  Equipment installation logistics including floorspace and rack preparation, moving and lifting \n  Proper handling of all components from receiving, storage, unboxing and assembly \n  Size, Weight and Rack mounting considerations for safe operation and serviceability \n  Airflow and cooling requirements \n  Power requirements including power resiliency \n  Cable management considerations \n  Copper and Fiber cable types, connector types, pluggable modules \n \n  Understand overall DC design and optimization, including: \n \n  Floorplan and rack layouts \n  Power distribution and resiliency \n  Cooling strategies including liquid cooling \n  Cable layout strategies for different DC network topologies \n  Equipment density and cable length considerations \n \n \n \n \n Qualifications\n  \n \n  An engineering, IT or technology focused degree. \n  Mechanical Engineering degree a plus. \n  10+ years of industry experience designing, constructing and overseeing operations in complex datacenter environments. \n  Experience with the handling, installation and servicing of large scale IT equipment in the DC. \n  Experience creating technical drawings, documenting methodologies and training DC staff to execute operations. \n  Excellent written and verbal skills and ability to effectively communicate with all Arista, partner and customer levels. \n  Prior experience with DC site operations as well as with customer facing services or support will be a plus \n  This position is remote with a potential for 50% travel globally. \n \n  Compensation Information: \n  The new hire base pay for this role has a pay range of $103,000 to $158,000 across the US, within California, the base pay range for this role is $124,000 to 158,000. \n  Arista offers different pay ranges based on work location, so that we can offer consistent and  competitive pay appropriate to the market. The actual base pay offered will be based on a wide range of factors, including skills, qualifications, relevant experience, and work location.  The pay range provided reflects base pay only and in addition certain roles may also be eligible for discretionary Arista bonuses and equity. Employees in Sales roles are eligible to participate in Arista\u2019s Sales Incentive Plan, which pays commissions calculated as a percentage of eligible sales. US-based employees are also entitled to benefits including medical, dental, vision, wellbeing, tax savings and income protection. The recruiting team can share more details during the hiring process specific to the role and location. \n  Additional Information\n   All your information will be kept confidential according to EEO guidelines.",
        "cleaned_desc": "  Mechanical Engineering degree a plus. \n  10+ years of industry experience designing, constructing and overseeing operations in complex datacenter environments. \n  Experience with the handling, installation and servicing of large scale IT equipment in the DC. \n  Experience creating technical drawings, documenting methodologies and training DC staff to execute operations. \n  Excellent written and verbal skills and ability to effectively communicate with all Arista, partner and customer levels. \n  Prior experience with DC site operations as well as with customer facing services or support will be a plus \n  This position is remote with a potential for 50% travel globally. \n \n  Compensation Information: ",
        "techs": [
            "mechanical engineering degree",
            "technical drawings",
            "industry experience",
            "complex datacenter environments",
            "it equipment",
            "dc staff training",
            "written and verbal skills",
            "arista",
            "customer facing services",
            "support",
            "remote position",
            "potential for 50% travel."
        ],
        "cleaned_techs": [
            "technical drawings",
            "industry experience",
            "complex datacenter environments",
            "it equipment",
            "dc staff training",
            "arista",
            "customer facing services",
            "support",
            "remote position",
            "potential for 50% travel."
        ]
    },
    "17557d019e22347b": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 97663.74,
        "salary_max": 123664.055,
        "title": "Data Engineer 3 (Mid-Level, Remote)",
        "company": "UNCOMN",
        "desc": "Here at \n   UN \n COMN , our mission is to empower systems thinkers to create elegant solutions to complex problems \u2013 to improve the systems that improve our communities. Our team members apply their natural curiosity and grit to discover elegant solutions for our clients\u2019 most complex organizational, logistics, process, data, and technical challenges, with the overall goal of building great businesses that contribute to great communities.\n  \n \n \n \n   We\u2019re an award-winning firm, one of the country\u2019s fastest-growing and\u2014more importantly\u2014a consistent \u2018\n   Top Workplace \u2019 as evaluated by our own employees. We are a values-driven organization (see the \n   Core Values  section of our website) and we\u2019re looking for new Uncommon Geniuses to join our growing team, so if you\u2019re a person who likes to solve problems, fix things, build things, tweak things, or otherwise show creative flair, you might just be an \"\n   UN \n COMN  Genius\" and we encourage you to check out the specifics of this position below!\n  \n \n \n \n  UN \n COMN  is seeking a \n   Data Engineer 3  to:\n  \n \n  Meet with client stakeholders to gather requirements for data engineering tasks \n  Collaborate with cross-functional team to design loading and transformation processes within a big data environment \n  Analyze raw source data to determine contents and meaning and apply to client requirements \n  Design and build database objects for a large supply chain ecosystem \n  Design and build processes for loading and transforming data \n  Analyze and optimize processes for performance \n  Identify, troubleshoot, and fix bugs in data environment \n \n \n  5+ years of data engineering in AWS with a focus on ETL \n    \n 5+ years PL/pgSQL \n  5+ years Python \n  3+ years AWS Glue \n   \n 5+ years data analysis \n  3+ years data management in relational databases \n  3+ years gathering requirements for data engineering projects \n  Ability to build processes that support data transformation, workload management, data structures, dependency, and metadata \n  Ability to build and optimize data sets, big data pipelines, and architectures \n  Fluency with AWS CLI \n  Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions \n  Excellent analytic skills associated with working on unstructured datasets \n  Proactive approach to problem-solving \n  Natural curiosity and desire to learn customer data and business processes \n  Must be eligible to obtain a Secret clearance granted by the US Government as needed \n \n \n \n  Preferred \n \n  3+ years of data management using AWS Postgres RDS \n  Experience working with supply chain data \n  Experience working with EDI \n \n \n  Flexible PTO effective on day 1* \n  7 Paid Holidays & up to 3 Floating Holidays* \n  Eligible for Health Benefits on day 1* \n  401K Safe Harbor Match Program* \n  Training and Education Assistance* \n \n Must be a full-time employee \n \n \n \n \n \n  Don\u2019t meet every single requirement? We\u2019re dedicated to building an uncommon, inclusive, and authentic workplace, so if you\u2019re excited about this role but your experience doesn\u2019t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles. \n \n \n \n  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.",
        "cleaned_desc": " \n \n  UN \n COMN  is seeking a \n   Data Engineer 3  to:\n  \n \n  Meet with client stakeholders to gather requirements for data engineering tasks \n  Collaborate with cross-functional team to design loading and transformation processes within a big data environment \n  Analyze raw source data to determine contents and meaning and apply to client requirements \n  Design and build database objects for a large supply chain ecosystem \n  Design and build processes for loading and transforming data \n  Analyze and optimize processes for performance \n  Identify, troubleshoot, and fix bugs in data environment   \n \n  5+ years of data engineering in AWS with a focus on ETL \n    \n 5+ years PL/pgSQL \n  5+ years Python \n  3+ years AWS Glue \n   \n 5+ years data analysis \n  3+ years data management in relational databases \n  3+ years gathering requirements for data engineering projects \n  Ability to build processes that support data transformation, workload management, data structures, dependency, and metadata \n  Ability to build and optimize data sets, big data pipelines, and architectures \n  Fluency with AWS CLI    Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions \n  Excellent analytic skills associated with working on unstructured datasets \n  Proactive approach to problem-solving \n  Natural curiosity and desire to learn customer data and business processes \n  Must be eligible to obtain a Secret clearance granted by the US Government as needed \n \n \n \n  Preferred \n \n  3+ years of data management using AWS Postgres RDS \n  Experience working with supply chain data \n  Experience working with EDI \n ",
        "techs": [
            "pl/pgsql",
            "python",
            "aws glue",
            "aws cli",
            "aws postgres rds",
            "edi"
        ],
        "cleaned_techs": [
            "pl/pgsql",
            "python",
            "aws",
            "edi"
        ]
    },
    "b59619a131d6251a": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 172100.0,
        "salary_max": 258200.0,
        "title": "Principal Software Engineer, Data Platform - Remote",
        "company": "Clari",
        "desc": "Clari\u2019s Revenue platform gives forecasting accuracy and visibility from the sales rep to the board room on revenue performance - helping them spot revenue leak to answer if they will meet, beat, or miss their sales goals. With insights like this, no wonder leading companies worldwide, including Okta, Adobe, Workday, and Zoom use Clari to drive revenue accuracy and precision. We never get tired of our customers singing our praises because it fuels us to help them continue to achieve remarkable. The next generation of revenue excellence is here\u2026are you ready to achieve remarkable with us?\n  \n \n \n  About the Team \n \n \n   The Engineering team at Clari is an Agile shop that practices Scrum across all of our teams. We layer in coordination practices such as Big Room Planning to stay aligned to Clari\u2019s KPIs quarterly across sites and teams. If you love working in an Agile environment that values collaboration and continuous improvement then we can\u2019t wait to meet you.\n  \n \n \n  About the Role \n \n \n   We are looking for a talented Principal Software Engineer to join our Query Manager team. Query Manager is a part of Clari\u2019s Data Platform team, and is the interface that allows application and API developers to easily and efficiently retrieve data across hundreds of databases and billions of rows of data that comprise our ever-evolving Data Platform.\n  \n \n \n  You will work with remarkable colleagues to architect, build and optimize the query layer to derive exceptional performance from our data warehouse built on top of AWS Aurora Postgres. You will collaborate closely with the product management, architecture, application and infrastructure teams to build the data services that power our best-in-class enterprise product suite. Most of Clari\u2019s application and API queries are processed through the query manager layer. The products you build are used and loved by many of the most well-known companies in the world. Don\u2019t believe us? Hear what our customers have to say\n   \n \n \n \n \n  Come join this fluid, dynamic, and growing team to learn, teach, and make a big, measurable impact every day. We work in an open, collaborative environment and seek exceptional developers who enjoy problem-solving and straying outside their routine.\n   \n \n \n \n \n This is a fully remote opportunity and can be worked from any location in the United States. \n \n Responsibilities \n \n  Design and evolve the architecture for the query layer that powers Clari\u2019s product suite and platform \n  Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion \n  Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers \n  Write scalable, robust, and fully tested software for deployment in mission-critical production environments \n  Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization \n  Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent \n \n  Qualifications \n \n  10+ years of software development experience using Java or similar object-oriented languages for backend development \n  Deep expertise with relational database skills and concepts \n  Experience having led multiple projects from inception through deployment, maintenance, and support \n  Experience with Postgres and non-relational databases like MongoDB \n  Experience with AWS \n  Experience building scalable systems and architectures \n  Experience with database performance tuning \n \n  Perks and Benefits @ Clari  \n \n Remote-first with opportunities to work and celebrate in person \n  Medical, dental, vision, short & long-term disability, Life insurance, and EAP \n  Mental health support provided by Modern Health \n  Pre-IPO stock options \n  Well-being and professional development funds \n  Retirement 401(k) plan \n  100% paid parental leave, plus fertility and family planning support provided by Maven \n  Discretionary paid time off, monthly \u2018take a break\u2019 days, and Focus Fridays \n  Focus on culture: Charitable giving match, plus in-person and virtual events \n \n \n \n  It is Clari\u2019s intent to pay all Clarians competitive wages and salaries that are motivational, fair, and equitable. The goal of Clari\u2019s compensation program is to be transparent, attract potential employees, meet the needs of all current employees and encourage employees to stay and grow at Clari.\n  \n \n \n  Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to specific work location, skill set, depth of experience, education and certifications.\n  \n \n \n  The salary range for this position is $172,100 to $258,200. The compensation package for this position also includes stock options and company-paid benefits, including well-being and professional development stipends.\n  \n \n   #BI-Remote #LI-Remote\n  \n \n \n  You\u2019ll often hear our CEO talk about being remarkable. To Clari, remarkable means many things. We believe in providing interesting and meaningful work in a nurturing and inclusive environment. One that is free from discrimination for everyone without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, gender identity, or veteran status. Efforts have to be recognized. Voices have to be heard. And work-life balance has to be baked into the very fiber of the company. We are honored to be recognized by Inc. Magazine and Bay Area News Group as a best place to work for several years running. We\u2019d love to have you join us on our journey to remarkable!\n  \n \n \n  If you feel you don\u2019t meet 100% of the qualifications outlined above, we want you to apply! Clari believes in hiring people, not just skills. If you are passionate about learning and excited about what we are doing, then we want to hear from you. \n \n \n \n  Clari focuses on culture add, not culture fit. One of our values is One with Customers, and we know we can serve them better when we involve as many different perspectives as possible. Our team is made stronger by what makes you unique, so we hope you\u2019ll bring your whole self to the job.",
        "cleaned_desc": "  Design and evolve the architecture for the query layer that powers Clari\u2019s product suite and platform \n  Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion \n  Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers \n  Write scalable, robust, and fully tested software for deployment in mission-critical production environments \n  Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization \n  Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent \n \n  Qualifications \n \n  10+ years of software development experience using Java or similar object-oriented languages for backend development \n  Deep expertise with relational database skills and concepts \n  Experience having led multiple projects from inception through deployment, maintenance, and support \n  Experience with Postgres and non-relational databases like MongoDB \n  Experience with AWS \n  Experience building scalable systems and architectures \n  Experience with database performance tuning \n ",
        "techs": [
            "java",
            "relational database",
            "postgres",
            "mongodb",
            "aws"
        ],
        "cleaned_techs": [
            "java",
            "relational database",
            "postgres",
            "mongodb",
            "aws"
        ]
    },
    "c46b694c5764c340": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 191300.0,
        "salary_max": 286900.0,
        "title": "Principal Software Engineer, Data Platform - Remote",
        "company": "Clari",
        "desc": "Clari\u2019s Revenue platform gives forecasting accuracy and visibility from the sales rep to the board room on revenue performance - helping them spot revenue leak to answer if they will meet, beat, or miss their sales goals. With insights like this, no wonder leading companies worldwide, including Okta, Adobe, Workday, and Zoom use Clari to drive revenue accuracy and precision. We never get tired of our customers singing our praises because it fuels us to help them continue to achieve remarkable. The next generation of revenue excellence is here\u2026are you ready to achieve remarkable with us?\n  \n \n \n  About the Team \n \n \n   The Engineering team at Clari is an Agile shop that practices Scrum across all of our teams. We layer in coordination practices such as Big Room Planning to stay aligned to Clari\u2019s KPIs quarterly across sites and teams. If you love working in an Agile environment that values collaboration and continuous improvement then we can\u2019t wait to meet you.\n  \n \n \n  About the Role \n \n \n   We are looking for a talented Principal Software Engineer to join our Query Manager team. Query Manager is a part of Clari\u2019s Data Platform team, and is the interface that allows application and API developers to easily and efficiently retrieve data across hundreds of databases and billions of rows of data that comprise our ever-evolving Data Platform.\n  \n \n \n  You will work with remarkable colleagues to architect, build and optimize the query layer to derive exceptional performance from our data warehouse built on top of AWS Aurora Postgres. You will collaborate closely with the product management, architecture, application and infrastructure teams to build the data services that power our best-in-class enterprise product suite. Most of Clari\u2019s application and API queries are processed through the query manager layer. The products you build are used and loved by many of the most well-known companies in the world. Don\u2019t believe us? Hear what our customers have to say\n   \n \n \n \n \n  Come join this fluid, dynamic, and growing team to learn, teach, and make a big, measurable impact every day. We work in an open, collaborative environment and seek exceptional developers who enjoy problem-solving and straying outside their routine.\n   \n \n \n \n \n This is a fully remote opportunity and can be worked from any location in the United States. \n \n Responsibilities \n \n  Design and evolve the architecture for the query layer that powers Clari\u2019s product suite and platform \n  Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion \n  Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers \n  Write scalable, robust, and fully tested software for deployment in mission-critical production environments \n  Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization \n  Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent \n \n  Qualifications \n \n  10+ years of software development experience using Java or similar object-oriented languages for backend development \n  Deep expertise with relational database skills and concepts \n  Experience having led multiple projects from inception through deployment, maintenance, and support \n  Experience with Postgres and non-relational databases like MongoDB \n  Experience with AWS \n  Experience building scalable systems and architectures \n  Experience with database performance tuning \n \n  Perks and Benefits @ Clari  \n \n Remote-first with opportunities to work and celebrate in person \n  Medical, dental, vision, short & long-term disability, Life insurance, and EAP \n  Mental health support provided by Modern Health \n  Pre-IPO stock options \n  Well-being and professional development funds \n  Retirement 401(k) plan \n  100% paid parental leave, plus fertility and family planning support provided by Maven \n  Discretionary paid time off, monthly \u2018take a break\u2019 days, and Focus Fridays \n  Focus on culture: Charitable giving match, plus in-person and virtual events \n \n \n \n  It is Clari\u2019s intent to pay all Clarians competitive wages and salaries that are motivational, fair, and equitable. The goal of Clari\u2019s compensation program is to be transparent, attract potential employees, meet the needs of all current employees and encourage employees to stay and grow at Clari.\n  \n \n \n  Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to specific work location, skill set, depth of experience, education and certifications.\n  \n \n \n  The salary range for this position is $191,300 to $286,900. The compensation package for this position also includes stock options and company-paid benefits, including well-being and professional development stipends.\n  \n \n   #BI-Remote #LI-Remote\n  \n \n \n  You\u2019ll often hear our CEO talk about being remarkable. To Clari, remarkable means many things. We believe in providing interesting and meaningful work in a nurturing and inclusive environment. One that is free from discrimination for everyone without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, gender identity, or veteran status. Efforts have to be recognized. Voices have to be heard. And work-life balance has to be baked into the very fiber of the company. We are honored to be recognized by Inc. Magazine and Bay Area News Group as a best place to work for several years running. We\u2019d love to have you join us on our journey to remarkable!\n  \n \n \n  If you feel you don\u2019t meet 100% of the qualifications outlined above, we want you to apply! Clari believes in hiring people, not just skills. If you are passionate about learning and excited about what we are doing, then we want to hear from you. \n \n \n \n  Clari focuses on culture add, not culture fit. One of our values is One with Customers, and we know we can serve them better when we involve as many different perspectives as possible. Our team is made stronger by what makes you unique, so we hope you\u2019ll bring your whole self to the job.",
        "cleaned_desc": "  Design and evolve the architecture for the query layer that powers Clari\u2019s product suite and platform \n  Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion \n  Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers \n  Write scalable, robust, and fully tested software for deployment in mission-critical production environments \n  Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization \n  Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent \n \n  Qualifications \n \n  10+ years of software development experience using Java or similar object-oriented languages for backend development \n  Deep expertise with relational database skills and concepts \n  Experience having led multiple projects from inception through deployment, maintenance, and support \n  Experience with Postgres and non-relational databases like MongoDB \n  Experience with AWS \n  Experience building scalable systems and architectures \n  Experience with database performance tuning \n ",
        "techs": [
            "clari",
            "postgres",
            "mongodb",
            "aws"
        ],
        "cleaned_techs": [
            "clari",
            "postgres",
            "mongodb",
            "aws"
        ]
    },
    "4bab43da4e7ef4d4": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 191300.0,
        "salary_max": 286900.0,
        "title": "Principal Software Engineer, Data Platform - Remote",
        "company": "Clari",
        "desc": "Clari\u2019s Revenue platform gives forecasting accuracy and visibility from the sales rep to the board room on revenue performance - helping them spot revenue leak to answer if they will meet, beat, or miss their sales goals. With insights like this, no wonder leading companies worldwide, including Okta, Adobe, Workday, and Zoom use Clari to drive revenue accuracy and precision. We never get tired of our customers singing our praises because it fuels us to help them continue to achieve remarkable. The next generation of revenue excellence is here\u2026are you ready to achieve remarkable with us?\n  \n \n \n  About the Team \n \n \n   The Engineering team at Clari is an Agile shop that practices Scrum across all of our teams. We layer in coordination practices such as Big Room Planning to stay aligned to Clari\u2019s KPIs quarterly across sites and teams. If you love working in an Agile environment that values collaboration and continuous improvement then we can\u2019t wait to meet you.\n  \n \n \n  About the Role \n \n \n   We are looking for a talented Principal Software Engineer to join our Query Manager team. Query Manager is a part of Clari\u2019s Data Platform team, and is the interface that allows application and API developers to easily and efficiently retrieve data across hundreds of databases and billions of rows of data that comprise our ever-evolving Data Platform.\n  \n \n \n  You will work with remarkable colleagues to architect, build and optimize the query layer to derive exceptional performance from our data warehouse built on top of AWS Aurora Postgres. You will collaborate closely with the product management, architecture, application and infrastructure teams to build the data services that power our best-in-class enterprise product suite. Most of Clari\u2019s application and API queries are processed through the query manager layer. The products you build are used and loved by many of the most well-known companies in the world. Don\u2019t believe us? Hear what our customers have to say\n   \n \n \n \n \n  Come join this fluid, dynamic, and growing team to learn, teach, and make a big, measurable impact every day. We work in an open, collaborative environment and seek exceptional developers who enjoy problem-solving and straying outside their routine.\n   \n \n \n \n \n This is a fully remote opportunity and can be worked from any location in the United States. \n \n Responsibilities \n \n  Design and evolve the architecture for the query layer that powers Clari\u2019s product suite and platform \n  Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion \n  Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers \n  Write scalable, robust, and fully tested software for deployment in mission-critical production environments \n  Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization \n  Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent \n \n  Qualifications \n \n  10+ years of software development experience using Java or similar object-oriented languages for backend development \n  Deep expertise with relational database skills and concepts \n  Experience having led multiple projects from inception through deployment, maintenance, and support \n  Experience with Postgres and non-relational databases like MongoDB \n  Experience with AWS \n  Experience building scalable systems and architectures \n  Experience with database performance tuning \n \n  Perks and Benefits @ Clari  \n \n Remote-first with opportunities to work and celebrate in person \n  Medical, dental, vision, short & long-term disability, Life insurance, and EAP \n  Mental health support provided by Modern Health \n  Pre-IPO stock options \n  Well-being and professional development funds \n  Retirement 401(k) plan \n  100% paid parental leave, plus fertility and family planning support provided by Maven \n  Discretionary paid time off, monthly \u2018take a break\u2019 days, and Focus Fridays \n  Focus on culture: Charitable giving match, plus in-person and virtual events \n \n \n \n  It is Clari\u2019s intent to pay all Clarians competitive wages and salaries that are motivational, fair, and equitable. The goal of Clari\u2019s compensation program is to be transparent, attract potential employees, meet the needs of all current employees and encourage employees to stay and grow at Clari.\n  \n \n \n  Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to specific work location, skill set, depth of experience, education and certifications.\n  \n \n \n  The salary range for this position is $191,300 to $286,900. The compensation package for this position also includes stock options and company-paid benefits, including well-being and professional development stipends.\n  \n \n   #BI-Remote #LI-Remote\n  \n \n \n  You\u2019ll often hear our CEO talk about being remarkable. To Clari, remarkable means many things. We believe in providing interesting and meaningful work in a nurturing and inclusive environment. One that is free from discrimination for everyone without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, gender identity, or veteran status. Efforts have to be recognized. Voices have to be heard. And work-life balance has to be baked into the very fiber of the company. We are honored to be recognized by Inc. Magazine and Bay Area News Group as a best place to work for several years running. We\u2019d love to have you join us on our journey to remarkable!\n  \n \n \n  If you feel you don\u2019t meet 100% of the qualifications outlined above, we want you to apply! Clari believes in hiring people, not just skills. If you are passionate about learning and excited about what we are doing, then we want to hear from you. \n \n \n \n  Clari focuses on culture add, not culture fit. One of our values is One with Customers, and we know we can serve them better when we involve as many different perspectives as possible. Our team is made stronger by what makes you unique, so we hope you\u2019ll bring your whole self to the job.",
        "cleaned_desc": "  Design and evolve the architecture for the query layer that powers Clari\u2019s product suite and platform \n  Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion \n  Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers \n  Write scalable, robust, and fully tested software for deployment in mission-critical production environments \n  Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization \n  Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent \n \n  Qualifications \n \n  10+ years of software development experience using Java or similar object-oriented languages for backend development \n  Deep expertise with relational database skills and concepts \n  Experience having led multiple projects from inception through deployment, maintenance, and support \n  Experience with Postgres and non-relational databases like MongoDB \n  Experience with AWS \n  Experience building scalable systems and architectures \n  Experience with database performance tuning \n ",
        "techs": [
            "java",
            "postgres",
            "mongodb",
            "aws"
        ],
        "cleaned_techs": [
            "java",
            "postgres",
            "mongodb",
            "aws"
        ]
    },
    "05780574df4dc884": {
        "terms": [
            "data engineer",
            "machine learning engineer"
        ],
        "salary_min": 94420.0,
        "salary_max": 136665.0,
        "title": "Sr. Lead Data Engineer",
        "company": "Lumen",
        "desc": "About Lumen \n  Lumen is a global technology leader, digitally connecting people, data and applications \u2013 quickly, securely, and effortlessly. Together, we are building a culture and company from the people up \u2013 committed to teamwork, trust and transparency. People power progress. We\u2019re looking for top-tier talent and offer the flexibility you need to thrive and deliver lasting impact. Join us as we digitally connect the world and shape the future. \n \n \n \n  The Role \n \n \n  Are you interested in serving as an integral part of our digital marketing team developing new tools and capabilities that will continue to advance Lumen\u2019s reputation as a technology leader? \n  In this role, you will be taking the lead in partnering closely with marketing, operations, and data science teams to utilize terabytes of data and translate them into actionable insights to create a competitive advantage for marketing/sales initiatives to win market share. Furthermore, you will be accountable for building and operationalizing critical components and tools to ensure the data coming in and going out are of the highest data quality and integrity. In the end state, the data solutions built and operated by you would rival the absolute best in the industry in engineering, operations, and usability excellence. \n \n \n \n \n \n  The Main Responsibilities \n \n \n  You are a great fit for this position if you: \n \n  Have strong passion in building reliable, accurate datasets that power end-to-end user scenarios and used in business decisions. \n  Highly believe that data is an asset that must be accessible to and applicable by every function in a modern business to guide and assess growth investments. \n  Are passionate about Data Engineering and believe that it is one of a kind emerging discipline and career path that you want to grow in. \n  Enjoy building and operating data services at terabyte and higher scales to enable data driven organizations. \n \n \n \n \n \n  What We Look For in a Candidate \n \n \n  Qualifications: \n \n  Experience building data models and performing complex queries using SQL \n  Experience performance tuning large datasets \n  Experience building large data pipelines and/or web services \n  Strong programming skills with Python and other scripting languages \n  8+ years of Business Intelligence or software development experience using industry technologies \n  4+ years of experience in building integration with upstream and downstream systems with REST APIs \n  Excellent problem solving, critical thinking, and communication skills \n  Ability to communicate effectively with technical and business teams, drive issues to closure \n  Strong understanding of data engineering and data stewardship roles in an organization \n  Ability to foster an innovative and inclusive team-oriented work environment. You\u2019ll play an active role in counselling and mentoring junior team members across the organization by providing structured and on-the-job feedback. \n  Ability to work on multiple priorities that span different areas of focus. You should feel comfortable talking to business stakeholders, analysts, data scientists and developers. \n  An entrepreneurial spirit who is excited by ambiguity, operates autonomously and can make informed decisions on the fly, grounded in a digital transformation point of view for marketing/sales initiatives. \n  BA/BS in Computer Science, Engineering or related technical field such as Statistics or Data Science \n \n \n  Other Qualifications: \n \n  Strong familiarity with big data and Hadoop ecosystem of tools is highly valuable \n  Knowledge of Lumen data, systems and processes including paid media, financial and inventory systems highly desirable \n  Experience with ETL and data integration development using multiple tools, including Informatica, Microsoft SSIS or other technologies \n  Experience with cloud data platforms is helpful \n  Combined IT and Marketing background \n  Machine Learning, Data Science, and statistical modeling experience are highly valued \n \n \n \n \n \n  Compensation \n \n \n  The starting salary for this role differs based on the employee's primary work location. Employees typically do not start at the top of the range, though compensation depends on each individual's qualifications. \n  Location Based Pay Ranges \n  $94420 - $118028  in these states: AR, ID, KY, LA, ME, MS, NE, SC, and SD.   $99390 - $124230  in these states: AZ, FL, GA, IN, IA, KS, MO, MT, NM, ND, OH, OK, PA, TN, UT, VT, WV, WI, and WY.   $104360 - $130448  in these states: CO, HI, MI, MN, NV, NH, NC, OR, and RI.   $109330 - $136665  in these states: AK, CA, CT, DE, DC, IL, MD, MA, NJ, NY, TX, VA, and WA. \n  As with the pay range variety that's based on the region of a country, specific offers are determined by various factors such as experience, education, skills, certifications and other business needs. \n \n \n \n \n  What to Expect Next \n \n \n \n \n  Requisition #: 331443 \n  Background Screening \n  If you are selected for a position, there will be a background screen, which may include checks for criminal records and/or motor vehicle reports and/or drug screening, depending on the position requirements. For more information on these checks, please refer to the Post Offer section of our FAQ page. Job-related concerns identified during the background screening may disqualify you from the new position or your current role. Background results will be evaluated on a case-by-case basis. \n  Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. \n  Equal Employment Opportunities \n  We are committed to providing equal employment opportunities to all persons regardless of race, color, ancestry, citizenship, national origin, religion, veteran status, disability, genetic characteristic or information, age, gender, sexual orientation, gender identity, gender expression, marital status, family status, pregnancy, or other legally protected status (collectively, \u201cprotected statuses\u201d). We do not tolerate unlawful discrimination in any employment decisions, including recruiting, hiring, compensation, promotion, benefits, discipline, termination, job assignments or training. \n  Disclaimer \n  The job responsibilities described above indicate the general nature and level of work performed by employees within this classification. It is not intended to include a comprehensive inventory of all duties and responsibilities for this job. Job duties and responsibilities are subject to change based on evolving business needs and conditions. \n \n  Salary Range \n \n  Salary Min :  \n 94420 \n \n \n  Salary Max :  \n 136665 \n \n \n  This information reflects the anticipated base salary range for this position based on current national data. Minimums and maximums may vary based on location. Individual pay is based on skills, experience and other relevant factors.  \n This position is eligible for either short-term incentives or sales compensation. Director and VP positions also are eligible for long-term incentive. To learn more about our bonus structure, you can view additional information here. We're able to answer any additional questions you may have as you move through the selection process. \n  As part of our comprehensive benefits package, Lumen offers a broad range of Health, Life, Voluntary Lifestyle and other benefits and perks that enhance your physical, mental, emotional and financial wellbeing. You can learn more by clicking here. \n  Note: For union-represented postings, wage rates and ranges are governed by applicable collective bargaining agreement provisions.",
        "cleaned_desc": "  Have strong passion in building reliable, accurate datasets that power end-to-end user scenarios and used in business decisions. \n  Highly believe that data is an asset that must be accessible to and applicable by every function in a modern business to guide and assess growth investments. \n  Are passionate about Data Engineering and believe that it is one of a kind emerging discipline and career path that you want to grow in. \n  Enjoy building and operating data services at terabyte and higher scales to enable data driven organizations. \n \n \n \n \n \n  What We Look For in a Candidate \n \n \n  Qualifications: \n \n  Experience building data models and performing complex queries using SQL \n  Experience performance tuning large datasets \n  Experience building large data pipelines and/or web services \n  Strong programming skills with Python and other scripting languages \n  8+ years of Business Intelligence or software development experience using industry technologies \n  4+ years of experience in building integration with upstream and downstream systems with REST APIs    Excellent problem solving, critical thinking, and communication skills \n  Ability to communicate effectively with technical and business teams, drive issues to closure \n  Strong understanding of data engineering and data stewardship roles in an organization \n  Ability to foster an innovative and inclusive team-oriented work environment. You\u2019ll play an active role in counselling and mentoring junior team members across the organization by providing structured and on-the-job feedback. \n  Ability to work on multiple priorities that span different areas of focus. You should feel comfortable talking to business stakeholders, analysts, data scientists and developers. \n  An entrepreneurial spirit who is excited by ambiguity, operates autonomously and can make informed decisions on the fly, grounded in a digital transformation point of view for marketing/sales initiatives. \n  BA/BS in Computer Science, Engineering or related technical field such as Statistics or Data Science \n \n \n  Other Qualifications: \n \n  Strong familiarity with big data and Hadoop ecosystem of tools is highly valuable \n  Knowledge of Lumen data, systems and processes including paid media, financial and inventory systems highly desirable \n  Experience with ETL and data integration development using multiple tools, including Informatica, Microsoft SSIS or other technologies \n  Experience with cloud data platforms is helpful \n  Combined IT and Marketing background \n  Machine Learning, Data Science, and statistical modeling experience are highly valued \n \n \n ",
        "techs": [
            "sql",
            "python",
            "rest apis",
            "hadoop ecosystem",
            "lumen data",
            "informatica",
            "microsoft ssis",
            "cloud data platforms",
            "machine learning",
            "data science",
            "statistical modeling"
        ],
        "cleaned_techs": [
            "sql",
            "python",
            "rest apis",
            "hadoop ecosystem",
            "lumen data",
            "informatica",
            "microsoft ssis",
            "cloud data platforms",
            "data science",
            "statistical modeling"
        ]
    },
    "416b51eb49b3f599": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 191300.0,
        "salary_max": 286900.0,
        "title": "Principal Software Engineer, Data Platform - Remote",
        "company": "Clari",
        "desc": "Clari\u2019s Revenue platform gives forecasting accuracy and visibility from the sales rep to the board room on revenue performance - helping them spot revenue leak to answer if they will meet, beat, or miss their sales goals. With insights like this, no wonder leading companies worldwide, including Okta, Adobe, Workday, and Zoom use Clari to drive revenue accuracy and precision. We never get tired of our customers singing our praises because it fuels us to help them continue to achieve remarkable. The next generation of revenue excellence is here\u2026are you ready to achieve remarkable with us?\n  \n \n \n  About the Team \n \n \n   The Engineering team at Clari is an Agile shop that practices Scrum across all of our teams. We layer in coordination practices such as Big Room Planning to stay aligned to Clari\u2019s KPIs quarterly across sites and teams. If you love working in an Agile environment that values collaboration and continuous improvement then we can\u2019t wait to meet you.\n  \n \n \n  About the Role \n \n \n   We are looking for a talented Principal Software Engineer to join our Query Manager team. Query Manager is a part of Clari\u2019s Data Platform team, and is the interface that allows application and API developers to easily and efficiently retrieve data across hundreds of databases and billions of rows of data that comprise our ever-evolving Data Platform.\n  \n \n \n  You will work with remarkable colleagues to architect, build and optimize the query layer to derive exceptional performance from our data warehouse built on top of AWS Aurora Postgres. You will collaborate closely with the product management, architecture, application and infrastructure teams to build the data services that power our best-in-class enterprise product suite. Most of Clari\u2019s application and API queries are processed through the query manager layer. The products you build are used and loved by many of the most well-known companies in the world. Don\u2019t believe us? Hear what our customers have to say\n   \n \n \n \n \n  Come join this fluid, dynamic, and growing team to learn, teach, and make a big, measurable impact every day. We work in an open, collaborative environment and seek exceptional developers who enjoy problem-solving and straying outside their routine.\n   \n \n \n \n \n This is a fully remote opportunity and can be worked from any location in the United States. \n \n Responsibilities \n \n  Design and evolve the architecture for the query layer that powers Clari\u2019s product suite and platform \n  Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion \n  Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers \n  Write scalable, robust, and fully tested software for deployment in mission-critical production environments \n  Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization \n  Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent \n \n  Qualifications \n \n  10+ years of software development experience using Java or similar object-oriented languages for backend development \n  Deep expertise with relational database skills and concepts \n  Experience having led multiple projects from inception through deployment, maintenance, and support \n  Experience with Postgres and non-relational databases like MongoDB \n  Experience with AWS \n  Experience building scalable systems and architectures \n  Experience with database performance tuning \n \n  Perks and Benefits @ Clari  \n \n Remote-first with opportunities to work and celebrate in person \n  Medical, dental, vision, short & long-term disability, Life insurance, and EAP \n  Mental health support provided by Modern Health \n  Pre-IPO stock options \n  Well-being and professional development funds \n  Retirement 401(k) plan \n  100% paid parental leave, plus fertility and family planning support provided by Maven \n  Discretionary paid time off, monthly \u2018take a break\u2019 days, and Focus Fridays \n  Focus on culture: Charitable giving match, plus in-person and virtual events \n \n \n \n  It is Clari\u2019s intent to pay all Clarians competitive wages and salaries that are motivational, fair, and equitable. The goal of Clari\u2019s compensation program is to be transparent, attract potential employees, meet the needs of all current employees and encourage employees to stay and grow at Clari.\n  \n \n \n  Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to specific work location, skill set, depth of experience, education and certifications.\n  \n \n \n  The salary range for this position is $191,300 to $286,900. The compensation package for this position also includes stock options and company-paid benefits, including well-being and professional development stipends.\n  \n \n   #BI-Remote #LI-Remote\n  \n \n \n  You\u2019ll often hear our CEO talk about being remarkable. To Clari, remarkable means many things. We believe in providing interesting and meaningful work in a nurturing and inclusive environment. One that is free from discrimination for everyone without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, gender identity, or veteran status. Efforts have to be recognized. Voices have to be heard. And work-life balance has to be baked into the very fiber of the company. We are honored to be recognized by Inc. Magazine and Bay Area News Group as a best place to work for several years running. We\u2019d love to have you join us on our journey to remarkable!\n  \n \n \n  If you feel you don\u2019t meet 100% of the qualifications outlined above, we want you to apply! Clari believes in hiring people, not just skills. If you are passionate about learning and excited about what we are doing, then we want to hear from you. \n \n \n \n  Clari focuses on culture add, not culture fit. One of our values is One with Customers, and we know we can serve them better when we involve as many different perspectives as possible. Our team is made stronger by what makes you unique, so we hope you\u2019ll bring your whole self to the job.",
        "cleaned_desc": "  Design and evolve the architecture for the query layer that powers Clari\u2019s product suite and platform \n  Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion \n  Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers \n  Write scalable, robust, and fully tested software for deployment in mission-critical production environments \n  Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization \n  Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent \n \n  Qualifications \n \n  10+ years of software development experience using Java or similar object-oriented languages for backend development \n  Deep expertise with relational database skills and concepts \n  Experience having led multiple projects from inception through deployment, maintenance, and support \n  Experience with Postgres and non-relational databases like MongoDB \n  Experience with AWS \n  Experience building scalable systems and architectures \n  Experience with database performance tuning \n ",
        "techs": [
            "java",
            "relational database skills",
            "postgres",
            "mongodb",
            "aws"
        ],
        "cleaned_techs": [
            "java",
            "postgres",
            "mongodb",
            "aws"
        ]
    },
    "c9f696550a62823e": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 184700.0,
        "salary_max": 323300.0,
        "title": "Senior Staff Data and Services Software Engineer",
        "company": "ServiceNow",
        "desc": "Company Description \n  At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can\u2019t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you. \n With more than 7,700+ customers, we serve approximately 85% of the Fortune 500\u00ae, and we're proud to be one of FORTUNE 100 Best Companies to Work For\u00ae and World's Most Admired Companies\u2122. \n Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow. \n Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates. \n  Job Description \n  As a Senior Staff Data and Software Engineer, you will be responsible for developing and implementing cutting-edge technical solutions that align with our organization's business objectives. You will work closely with stakeholders to understand their needs, assess existing systems and infrastructure, and design robust and scalable data and mircroservices solutions that drive innovation and efficiency. Your role will require a combination of technical expertise, strategic thinking, and effective communication to bridge the gap between business and technology. \n \n  Key Responsibilities: \n \n  Solution Design:  Collaborate with business leaders, project managers, and technical teams to understand requirements and design holistic technical solutions that address current and future needs. \n  Architecture Planning:  Develop and maintain technology roadmaps, ensuring alignment with organizational goals and industry best practices. \n  Technical Leadership:  Provide technical leadership and guidance to development teams, ensuring adherence to architectural standards and best practices. \n  Risk Assessment:  Identify and evaluate technical risks and propose mitigation strategies to ensure project success and data security. \n  Documentation:  Create and maintain comprehensive architecture documentation, including diagrams, guidelines, and standards for development teams to follow. \n  Vendor Evaluation:  Assess and recommend third-party tools, products, and services that can enhance our technical solutions. \n  Prototyping:  Develop proof-of-concept and prototype solutions to validate architectural decisions and demonstrate feasibility. \n  Performance Optimization:  Continuously monitor and analyze system performance, identifying areas for improvement and optimizing existing solutions. \n  Security and Compliance:  Ensure that solutions comply with industry regulations and security standards, and proactively address security vulnerabilities. \n  Collaboration:  Foster collaboration and effective communication between cross-functional teams, promoting a culture of innovation and excellence. \n \n \n  Qualifications \n  Qualifications: \n \n Bachelor's degree in Computer Science, Information Technology, or related field (Master's degree preferred). \n Proven experience as a Lead Engineer and Solution Architect or a similar role. \n Strong knowledge of enterprise architecture principles and best practices. \n Proficiency in designing and implementing solutions using various technologies and platforms. \n Excellent problem-solving and analytical skills. \n Outstanding communication and interpersonal abilities. \n Project management skills and experience in managing complex technical projects. \n Certification in relevant technologies or architecture frameworks (e.g., TOGAF, AWS Certified Solutions Architect, Microsoft Certified: Azure Solutions Architect Expert) is a plus. \n \n Preferred Skills: \n \n Cloud computing expertise (e.g., AWS, Azure, Google Cloud Platform). \n Knowledge of DevOps practices and tools. \n Familiarity with microservices architecture, expertise a plus. \n Familiarity with graph databases, expertise a plus. \n Experience with containerization and orchestration technologies (e.g., Docker, Kubernetes). \n Strong understanding of data architecture and database technologies. \n Knowledge of cybersecurity best practices. \n Excellent presentation and facilitation skills. \n For positions in the Bay Area, we offer a base pay of $184,700 - $323,300, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.\n  \n Additional Information \n  ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law. \n At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office. \n If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance. \n For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government. \n Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site. \n   \n From Fortune. \u00a9 2022 Fortune Media IP Limited All rights reserved. Used under license. \n Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.",
        "cleaned_desc": " \n \n  Qualifications \n  Qualifications: \n \n Bachelor's degree in Computer Science, Information Technology, or related field (Master's degree preferred). \n Proven experience as a Lead Engineer and Solution Architect or a similar role. \n Strong knowledge of enterprise architecture principles and best practices. \n Proficiency in designing and implementing solutions using various technologies and platforms. \n Excellent problem-solving and analytical skills.   Outstanding communication and interpersonal abilities. \n Project management skills and experience in managing complex technical projects. \n Certification in relevant technologies or architecture frameworks (e.g., TOGAF, AWS Certified Solutions Architect, Microsoft Certified: Azure Solutions Architect Expert) is a plus. \n \n Preferred Skills: \n \n Cloud computing expertise (e.g., AWS, Azure, Google Cloud Platform). \n Knowledge of DevOps practices and tools. \n Familiarity with microservices architecture, expertise a plus. \n Familiarity with graph databases, expertise a plus. ",
        "techs": [
            "aws",
            "azure",
            "google cloud platform",
            "devops practices and tools",
            "microservices architecture",
            "graph databases"
        ],
        "cleaned_techs": [
            "aws",
            "azure",
            "gcp",
            "devops practices and tools",
            "microservices architecture",
            "graph databases"
        ]
    },
    "c5b389ff275bb80a": {
        "terms": [
            "data engineer",
            "mlops"
        ],
        "salary_min": 128242.22,
        "salary_max": 162383.22,
        "title": "Senior Data Engineer",
        "company": "FutureFit AI",
        "desc": "FutureFit AI is looking for a contract Senior Data Engineer to join our team. We have a culture of high trust, high impact, high velocity and a gritty determination to win. If you are passionate about development, like to have fun, do it right, and get things done we would love to hear from you! \n  An important note: Data shows that men on average apply for a role if they meet 3/10 requirements while women often only do so if it\u2019s 10/10. We work hard to be clear and specific about what our roles include and demand and encourage you to apply if you see a strong (but not necessarily perfect) fit between you and the opportunity. \n \n  About You: \n \n  Technology - You are a passionate data engineer, eager to learn, solve problems and share your knowledge with others \n  Data - Strong architecture and design around various data paradigms (document stores, data lakes/warehouses and relational) and databases (Mongo, Elastic, Postgres, Redshift, etc.) \n  Data Query - Experience writing highly performant data queries for large/complex data sets across relational and NoSQL databases \n  Data Pipelines - Experience in architecture and implementation of ETL processes, streaming data, data process orchestration (Airflow) \n  BI - Adept at connecting BI platforms (Looker) to data warehouses to provide critical insights via dashboards \n  MLOps - Experience with model versioning, deployment, monitoring and serving models via REST APIs \n  Understanding - You are eager to understand the business problem and collaborate with product to build a solution that meets both technology and business requirements \n  Communication - You can share your perspective clearly and constructively, you are comfortable challenging ideas and collaborating to find the best approach \n  Grit - You like to solve challenging problems, are not afraid of the unknown, eager to learn and take a systematic approach to solving problems \n  AI - You are interested in integrating advances in AI into FutureFit AI products and the development process to drive improvements in velocity, quality and engineer experience \n  Values - You are a good person who shares our values of excellence, transparency, and humanity \n \n \n  The Product\u2019s Technology Stack: \n \n  Front End - React, TypeScript, Tailwind, Storybook \n  Back End - Node.js, Nest.js, Python, Flask, GraphQL, Postgres, MongoDB, RedShift, Airflow \n  AWS - CloudFront, Cognito, ECS, RDS, S3, SQS, Elasticsearch, VPC, Lambda \n \n \n  Location: \n  Most days we work from home but everyone comes to the office at least once a week for face to face collaboration and team building. The office is located at 325 Front St West (a short walk from Union Station). \n \n  Our Company: \n  In looking at a job posting, it\u2019s often hard to get a basic picture of the company profile (size, stage, structure, etc.) which is why we are sharing it with you upfront. This helps you quickly decide and helps us focus any time we spend together on going beyond the basics: \n  Funding : We have raised one round of funding led by JP Morgan which fueled a significant growth trajectory for us and we have a safe financial runway to execute against. \n  Problem Domain:  Future of Work / Workforce Development - important that the problem domain interests you even if you haven\u2019t worked in the space before. \n  Customers : We partner with workforce development agencies, government agencies, and employers/enterprises. \n  Structure : We are organized around the following key departments: Growth, Customer Success, Product, Engineering, Data, People & Culture, Finance & Operations. \n  Team : We are a team of 30-50 across US and Canada with main hubs in New York and Toronto. This role will be based in Toronto.     Core Principles : Be Curious, Drive to Outcomes, Raise the Bar, Speed Matters, Own It, Put We Over Me \n \n  About FutureFit AI \n  At FutureFit AI, we\u2019re on a mission to unlock pathways between talent and opportunity using the power of AI. We focus on personalized, AI-powered career guidance for job seekers, emphasizing skills over extensive resumes, and partner with workforce development partners, governments, and employers to level access to opportunity. \n \n  FutureFit AI is a growing, venture-backed company focused on using technology to improve the lives and outcomes for people going through career transitions. We\u2019re a small, driven team, united by our commitment to the job seekers and workforce ecosystems we serve. We're not just building a company; we're shaping the future of work. \n \n  We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment. Please contact us to request an accommodation. \n   FutureFit AI All rights reserved, we are proud to be an equal opportunity workplace. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate on the basis of race, religion, color, gender identity, sexual orientation, age, disability, veteran status, or other applicable legally protected characteristics. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.",
        "cleaned_desc": "  Data Pipelines - Experience in architecture and implementation of ETL processes, streaming data, data process orchestration (Airflow) \n  BI - Adept at connecting BI platforms (Looker) to data warehouses to provide critical insights via dashboards \n  MLOps - Experience with model versioning, deployment, monitoring and serving models via REST APIs \n  Understanding - You are eager to understand the business problem and collaborate with product to build a solution that meets both technology and business requirements \n  Communication - You can share your perspective clearly and constructively, you are comfortable challenging ideas and collaborating to find the best approach \n  Grit - You like to solve challenging problems, are not afraid of the unknown, eager to learn and take a systematic approach to solving problems \n  AI - You are interested in integrating advances in AI into FutureFit AI products and the development process to drive improvements in velocity, quality and engineer experience \n  Values - You are a good person who shares our values of excellence, transparency, and humanity ",
        "techs": [
            "data pipelines",
            "etl processes",
            "streaming data",
            "data process orchestration (airflow)",
            "bi platforms (looker)",
            "data warehouses",
            "dashboards",
            "mlops",
            "model versioning",
            "deployment",
            "monitoring",
            "rest apis",
            "business problem",
            "collaboration",
            "communication",
            "grit",
            "ai",
            "futurefit ai products",
            "development process",
            "velocity",
            "quality",
            "engineer experience",
            "values",
            "excellence",
            "transparency",
            "humanity"
        ],
        "cleaned_techs": [
            "data pipelines",
            "etl processes",
            "streaming data",
            "data process orchestration (airflow)",
            "bi platforms (looker)",
            "data warehouses",
            "dashboards",
            "mlops",
            "model versioning",
            "deployment",
            "monitoring",
            "rest apis",
            "business problem",
            "collaboration",
            "communication",
            "grit",
            "ai",
            "futurefit ai products",
            "development process",
            "velocity",
            "quality",
            "engineer experience",
            "values",
            "excellence",
            "transparency",
            "humanity"
        ]
    },
    "9742aa23657c4edf": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 150000.0,
        "salary_max": 150000.0,
        "title": "Azure Data Engineer",
        "company": "VanderHouwen",
        "desc": "Azure Data Engineer \n  We are seeking a highly skilled and experienced Azure Data Engineer with Snowflake experience to join our dynamic team. In this role, you will play a pivotal role in designing, implementing, and managing data solutions on the Azure cloud platform while leveraging Snowflake as the data warehousing technology. The ideal candidate will have a strong background in data engineering, cloud computing, and data warehousing, with a focus on optimizing data pipelines and ensuring high-quality data delivery to support our organization's data-driven decision-making.\n  \n \n Azure Data Engineer Responsibilities \n  Data Architecture and Design:\n   Collaborate with cross-functional teams to understand data requirements and design scalable and efficient data solutions using Azure services and Snowflake.\n   Define and maintain data architecture standards, best practices, and guidelines.\n  \n  Data Integration:\n   Develop and maintain data pipelines, ETL processes, and data integration workflows.\n   Utilize Azure Data Factory, Azure Databricks, and other relevant tools to orchestrate data movement and transformation.\n  \n  Data Warehousing:\n   Administer and optimize Snowflake data warehouse, ensuring high performance, scalability, and cost-efficiency.\n   Implement proper data modeling and data warehousing techniques to support reporting and analytics.\n  \n  Data Quality and Governance:\n   Implement data quality checks, validation processes, and data governance principles to maintain high-quality data.\n   Ensure compliance with data security and privacy standards, including GDPR and HIPAA, if applicable.\n  \n  Performance Tuning:\n   Monitor and optimize data pipelines and Snowflake database performance.\n   Identify and resolve bottlenecks to maintain efficient data processing.\n  \n  Documentation and Collaboration:\n   Create and maintain comprehensive documentation for data pipelines, data models, and system configurations.\n   Collaborate with data scientists, analysts, and business stakeholders to understand their data needs and provide data solutions accordingly.\n  \n  Monitoring and Troubleshooting:\n   Develop monitoring and alerting systems to proactively identify and resolve data-related issues.\n   Troubleshoot data integration and pipeline problems in a timely manner.\n  \n  Continuous Learning:\n   Stay updated on the latest Azure and Snowflake features and best practices, and implement them where applicable.\n  \n \n Azure Data Engineer Qualifications \n  Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.\n   Proven experience as a data engineer with a focus on Azure cloud services and Snowflake.\n   Strong SQL and data modeling skills.\n   Proficiency in Azure services, including Azure Data Factory, Azure Databricks, and Azure Synapse Analytics.\n   Experience with Snowflake data warehousing technology.\n   Knowledge of ETL tools and practices.\n   Strong problem-solving skills and attention to detail.\n   Excellent communication and collaboration skills.\n   Data security and compliance knowledge is a plus.\n  \n \n Preferred Certifications \n  Microsoft Certified: Azure Data Engineer Associate.\n   Snowflake Certified Data Engineer.\n  \n \n \n Senior Solutions Architect, Salesforce \n  The Senior Salesforce Solutions Architect is responsible for setting the vision for the organization\u2019s technical roadmap through the appropriate use of Salesforce platform technologies in the Driveway Digital enterprise. Senior Software Architects work across many teams within an organizational vertical of Driveway\u2019s new products, guide other architects and engineers in the creation of complex systems and ensure teams are adhering to our organizational standards.\n  \n  Senior Solution Architects represent our engineering organization across and outside the company and stay connected with the latest industry trends. They serve as hands-on advisors with Senior leaders and stakeholders, using strong interpersonal skills, technical knowledge of the Salesforce platform to guide in delivering high value solutions. The Senior Solutions Architect reports to the Sr. Engineering Manager.\n  \n \n Responsibilities \n  Create and execute solutions that enable teams to deliver high-quality software in a consistent fashion, without restricting creativity.\n   Advise and articulate technical vision for program strategies to enable effective use of the Salesforce platform.\n   Lead discovery sessions, capturing requirements, user stories and detailed system functional designs.\n   Serve as a trusted advisor to product, engineering, design leadership and business stakeholders.\n   Serve as a leader in the development of the community of Salesforce expertise.\n   Guide scope, approach, and artifacts to keep programs on track and solutions extensible and maintainable. Proactively identify risks to ensure effective delivery.\n   Participate and represent in external Salesforce technology events and communities of practice.\n   Set standards and lead the team in the creation of world-class, audience-appropriate documentation and diagramming to demystify complex systems.\n   Learn, act and present timely opportunities to advance new concepts, business models, and technologies.\n   Drive architectural discussions during discovery, design, and implementation stages of the SDLC.\n   Serve as a leader in the Architecture team to collaboratively craft and realize a vision of our technical roadmap.\n   Pair with architects to lead engineering in vendor selection and integration with 3rd party software providers.\n   Greatly influence the discipline of how Architecture drives our organization.\n   Participate in and gain insights from incidents.\n  \n \n Skills and Qualifications \n  12+ years building, shipping, and extending high-scale, high-performance enterprise Salesforce solutions.\n   5+ years of experience in leading the delivery of enterprise-level projects using Waterfall and Agile SDLC (Software Development Life Cycle) methodologies with demonstrated success.\n   5+ years designing and delivering E2E B2B and B2C Salesforce solutions with 3rd Party enterprise systems.\n   Extensive experience defining, documenting, and presenting system architecture landscapes, identifying gaps between current and desired end-states, and delivering comprehensive solutions.\n   Extensive experience detailing business processes using process flows and systems diagrams.\n   Represent the engineering organization and company as an ambassador to the overall tech community.\n   Extensive knowledge and experience in Salesforce Platform cloud offerings and features.\n   Extensive experience with configuration and customization of the Salesforce platform.\n   Extensive experience delivering enterprise solutions in Agile and DevOps environments.\n   Relevant Salesforce certifications and consulting experience are strongly recommended.\n  \n \n Competencies \n  Persuasive communication skills, written and verbal, finding gaps and opportunities constructively.\n   Prefers to listen before speaking and thrives on consensus-building and influencing without authority.\n   Strong presentation skills; able to represent alternative views and clearly articulate pros, cons, and recommendations to a variety of executive to team level stakeholders.\n   Self-motivates, believes in accountability, focuses on results, makes plans, and follows through.\n   Believes in humility, empathy, shares best practices, desires to keep learning, measures performance and adapts to improve results.\n   Practices servant-leadership and is primarily invested in furthering the career growth and efficacy of our teams.\n  \n \n Salary: $130,000k-$150,000k/yr. (DOE) \n \n \n \n Benefits \n  Benefits are available to eligible full-time employees and can include coverage for medical, dental, life insurance, long term disability, 401k with employer match, and wellness programs.\n  \n \n About VanderHouwen \n  VanderHouwen is an award-winning, Women-Owned, WBENC certified professional staffing firm. Founded in 1987, VanderHouwen has been successfully placing experienced professionals throughout the Pacific Northwest and nationwide. Our recruitment teams are highly specialized in either Technology and IT, Engineering, or Accounting and Finance career markets. Our recruiters value building meaningful, professional relationships with each candidate as well as developing honed knowledge of companies' staffing needs and workplaces. Partner with us to land your next exciting career.\n  \n  VanderHouwen is an Equal Opportunity Employer and participates in E-Verify. VanderHouwen does not discriminate on the basis of race, color, religion, sex, national origin, age, disability, or any other characteristic protected by applicable local, state or federal civil rights laws.\n  \n  #LI-Remote",
        "cleaned_desc": "Azure Data Engineer \n  We are seeking a highly skilled and experienced Azure Data Engineer with Snowflake experience to join our dynamic team. In this role, you will play a pivotal role in designing, implementing, and managing data solutions on the Azure cloud platform while leveraging Snowflake as the data warehousing technology. The ideal candidate will have a strong background in data engineering, cloud computing, and data warehousing, with a focus on optimizing data pipelines and ensuring high-quality data delivery to support our organization's data-driven decision-making.\n  \n \n Azure Data Engineer Responsibilities \n  Data Architecture and Design:\n   Collaborate with cross-functional teams to understand data requirements and design scalable and efficient data solutions using Azure services and Snowflake.\n   Define and maintain data architecture standards, best practices, and guidelines.\n  \n  Data Integration:\n   Develop and maintain data pipelines, ETL processes, and data integration workflows.\n   Utilize Azure Data Factory, Azure Databricks, and other relevant tools to orchestrate data movement and transformation.\n  \n  Data Warehousing:\n   Administer and optimize Snowflake data warehouse, ensuring high performance, scalability, and cost-efficiency.\n   Implement proper data modeling and data warehousing techniques to support reporting and analytics.\n  \n  Data Quality and Governance:\n   Implement data quality checks, validation processes, and data governance principles to maintain high-quality data.\n   Ensure compliance with data security and privacy standards, including GDPR and HIPAA, if applicable.\n  \n  Performance Tuning:    Monitor and optimize data pipelines and Snowflake database performance.\n   Identify and resolve bottlenecks to maintain efficient data processing.\n  \n  Documentation and Collaboration:\n   Create and maintain comprehensive documentation for data pipelines, data models, and system configurations.\n   Collaborate with data scientists, analysts, and business stakeholders to understand their data needs and provide data solutions accordingly.\n  \n  Monitoring and Troubleshooting:\n   Develop monitoring and alerting systems to proactively identify and resolve data-related issues.\n   Troubleshoot data integration and pipeline problems in a timely manner.\n  \n  Continuous Learning:\n   Stay updated on the latest Azure and Snowflake features and best practices, and implement them where applicable.\n  \n \n Azure Data Engineer Qualifications \n  Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.\n   Proven experience as a data engineer with a focus on Azure cloud services and Snowflake.\n   Strong SQL and data modeling skills.\n   Proficiency in Azure services, including Azure Data Factory, Azure Databricks, and Azure Synapse Analytics.\n   Experience with Snowflake data warehousing technology.\n   Knowledge of ETL tools and practices.    Serve as a leader in the development of the community of Salesforce expertise.\n   Guide scope, approach, and artifacts to keep programs on track and solutions extensible and maintainable. Proactively identify risks to ensure effective delivery.\n   Participate and represent in external Salesforce technology events and communities of practice.\n   Set standards and lead the team in the creation of world-class, audience-appropriate documentation and diagramming to demystify complex systems.\n   Learn, act and present timely opportunities to advance new concepts, business models, and technologies.\n   Drive architectural discussions during discovery, design, and implementation stages of the SDLC.\n   Serve as a leader in the Architecture team to collaboratively craft and realize a vision of our technical roadmap.\n   Pair with architects to lead engineering in vendor selection and integration with 3rd party software providers.\n   Greatly influence the discipline of how Architecture drives our organization.\n   Participate in and gain insights from incidents.\n  \n \n Skills and Qualifications \n  12+ years building, shipping, and extending high-scale, high-performance enterprise Salesforce solutions.\n   5+ years of experience in leading the delivery of enterprise-level projects using Waterfall and Agile SDLC (Software Development Life Cycle) methodologies with demonstrated success.\n   5+ years designing and delivering E2E B2B and B2C Salesforce solutions with 3rd Party enterprise systems.\n   Extensive experience defining, documenting, and presenting system architecture landscapes, identifying gaps between current and desired end-states, and delivering comprehensive solutions.\n   Extensive experience detailing business processes using process flows and systems diagrams.\n   Represent the engineering organization and company as an ambassador to the overall tech community.\n   Extensive knowledge and experience in Salesforce Platform cloud offerings and features.\n   Extensive experience with configuration and customization of the Salesforce platform.\n   Extensive experience delivering enterprise solutions in Agile and DevOps environments.",
        "techs": [
            "azure data engineer",
            "snowflake",
            "azure data factory",
            "azure databricks",
            "azure synapse analytics",
            "etl tools",
            "sql",
            "data modeling"
        ],
        "cleaned_techs": [
            "azure",
            "snowflake",
            "etl tools",
            "sql"
        ]
    },
    "a8234e6023575cfb": {
        "terms": [
            "data engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Data Engineer (Remote-Eligible)",
        "company": "Capital One",
        "desc": "West Creek 8 (12080), United States of America, Richmond, Virginia\n   Senior Lead Data Engineer (Remote-Eligible)\n  \n  Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Senior Lead Data Engineer, you\u2019ll have the opportunity to be on the forefront of driving a major transformation within Capital One. \n \n  What You\u2019ll Do: \n \n  Manage and develop a Java-based pipeline and query tools depending on HIve Metastore, AWS S3, Kafka and ORC \n  Develop analytics tooling to solve business problems driven by scale and international expansion \n  Optimize configurations for analytics tools to support growing business and organization \n \n \n  \u201cCapital One is open to hiring a Remote Employee for this opportunity.\u201d \n \n  Basic Qualifications: \n \n  Bachelor\u2019s Degree \n  At least 8 years of experience in application development (Internship experience does not apply) \n  At least 2 years of experience in big data technologies \n  At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud) \n \n \n  Preferred Qualifications: \n \n  9+ years of experience in application development including Python, Javascript, or Java \n  4+ years of experience with AWS \n  5+ years experience with Distributed data/computing tools (Trino, Hive, Kafka or Spark) \n  4+ year experience working on real-time data and streaming applications \n  4+ years of experience with NoSQL implementation (Cassandra) \n  4+ years of experience with UNIX/Linux including basic commands and shell scripting \n  2+ years of experience with Agile engineering practices \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Data Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Data Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Data Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "a0da3422bb477ea5": {
        "terms": [
            "data engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Data Engineer (Remote-Eligible)",
        "company": "Capital One",
        "desc": "West Creek 8 (12080), United States of America, Richmond, Virginia\n   Senior Lead Data Engineer (Remote-Eligible)\n  \n  Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Senior Lead Data Engineer, you\u2019ll have the opportunity to be on the forefront of driving a major transformation within Capital One. \n \n  What You\u2019ll Do: \n \n  Manage and develop a Java-based pipeline and query tools depending on HIve Metastore, AWS S3, Kafka and ORC \n  Develop analytics tooling to solve business problems driven by scale and international expansion \n  Optimize configurations for analytics tools to support growing business and organization \n \n \n  \u201cCapital One is open to hiring a Remote Employee for this opportunity.\u201d \n \n  Basic Qualifications: \n \n  Bachelor\u2019s Degree \n  At least 8 years of experience in application development (Internship experience does not apply) \n  At least 2 years of experience in big data technologies \n  At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud) \n \n \n  Preferred Qualifications: \n \n  9+ years of experience in application development including Python, Javascript, or Java \n  4+ years of experience with AWS \n  5+ years experience with Distributed data/computing tools (Trino, Hive, Kafka or Spark) \n  4+ year experience working on real-time data and streaming applications \n  4+ years of experience with NoSQL implementation (Cassandra) \n  4+ years of experience with UNIX/Linux including basic commands and shell scripting \n  2+ years of experience with Agile engineering practices \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Data Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Data Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Data Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "a4278b8168a9614a": {
        "terms": [
            "data engineer"
        ],
        "salary_min": 90.0,
        "salary_max": 100.0,
        "title": "Sr. Data Platform Engineer",
        "company": "Primary Talent Partners",
        "desc": "***********W2 Requirement - USC or GC required - NO C2C AT ALL************       Primary Talent Partners has 1 opening for a Lead Data Platform Engineer to join our client. This is a 6-month W2 contract opening, contract extensions are based on performance, budget, and business needs. \n \n Pay: $90-100/hr \n Duration: 6-month W2 contract, no sponsorship & no c2c \n Location: St. Louis, MO or Remote \n The Data Platform Engineering Lead will be responsible for managing a cross-functional Data Warehouse platform hosted in Google Cloud Platform.\n  \n  The platform itself is still experiencing a rapid growth of end-users / demand, so there will be a need to balance both operational/support needs in additional to delivery of new capital deliverables.\n  \n  The selected candidate should have significant experience in GCP, with specific focus around BigQuery, Logging, IAM, GKE, Cloud Functions, DataFlow, Cloud Composer Cross-Cloud architecture, and Shared VPC Networking. Outside of GCP, the Data Platform Engineering Lead must also have experience with standard DevOps practices, CICD pipelines, and operating in an Agile environment.\n  \n  In addition to the technical responsibilities, the ideal candidate must be able to operate as a Product Owner for the platform \u2013 managing Azure Dev Ops boards, Aha Roadmaps, backlogs and priorities.\n   Due to the high visibility of this platform, the selected candidate must have very strong communication skills and capable of presenting to large audiences varying from developers to senior leaders.\n  \n  The Data Platform Engineering Lead should embrace the challenge of dealing with new and complex problems, understanding how to apply technologies to solve those problems via innovative solutions.\n  \n \n Key Responsibilities include: \n \n Leading a team of senior data engineers to deliver new capabilities for the platform \n Managing product backlog & priorities \n Defining Acceptance criteria \n Collaborating with Delivery teams to provide re-usable pipelines \n Hands-on technical responsibilities to ensure the platform is stable and capable to deep-dive into complex, unexpected errors \n Defining & implementing cross-cloud architecture ( GCP, AWS, Azure ) \n \n What you\u2019ll need to be successful: \n \n Expertise in GCP services and evaluating technology for specific use-cases \n Experience building / managing CICD pipelines \n DevOps experience \n Expertise in Data Warehousing \n Experience with Infrastructure as Code, preferably Terraform \n Strong organizational skills, interpersonal skills, written and oral communication \n Ability to solve complex issues with re-usable frameworks \n Ability to quickly learn technologies such as Terraform, Kubernetes, Apache Beam, and Kafka \n \n Primary Talent Partners is an Equal Opportunity / Affirmative Action employer committed to diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, age, national origin, disability, protected veteran status, gender identity, or any other factor protected by applicable federal, state, or local laws. \n \n  #PTPJobs",
        "cleaned_desc": "  The platform itself is still experiencing a rapid growth of end-users / demand, so there will be a need to balance both operational/support needs in additional to delivery of new capital deliverables.\n  \n  The selected candidate should have significant experience in GCP, with specific focus around BigQuery, Logging, IAM, GKE, Cloud Functions, DataFlow, Cloud Composer Cross-Cloud architecture, and Shared VPC Networking. Outside of GCP, the Data Platform Engineering Lead must also have experience with standard DevOps practices, CICD pipelines, and operating in an Agile environment.\n  \n  In addition to the technical responsibilities, the ideal candidate must be able to operate as a Product Owner for the platform \u2013 managing Azure Dev Ops boards, Aha Roadmaps, backlogs and priorities.\n   Due to the high visibility of this platform, the selected candidate must have very strong communication skills and capable of presenting to large audiences varying from developers to senior leaders.\n    Expertise in GCP services and evaluating technology for specific use-cases \n Experience building / managing CICD pipelines \n DevOps experience \n Expertise in Data Warehousing \n Experience with Infrastructure as Code, preferably Terraform \n Strong organizational skills, interpersonal skills, written and oral communication \n Ability to solve complex issues with re-usable frameworks ",
        "techs": [
            "gcp",
            "bigquery",
            "logging",
            "iam",
            "gke",
            "cloud functions",
            "dataflow",
            "cloud composer",
            "cross-cloud architecture",
            "shared vpc networking",
            "devops practices",
            "cicd pipelines",
            "agile",
            "azure dev ops",
            "aha roadmaps",
            "data warehousing",
            "infrastructure as code",
            "terraform"
        ],
        "cleaned_techs": [
            "gcp",
            "bigquery",
            "logging",
            "iam",
            "gke",
            "cloud functions",
            "dataflow",
            "cloud composer",
            "cross-cloud architecture",
            "shared vpc networking",
            "devops practices",
            "cicd pipelines",
            "agile",
            "azure",
            "aha roadmaps",
            "data warehousing",
            "infrastructure as code",
            "terraform"
        ]
    },
    "10ed9cd7c00ccdb4": {
        "terms": [
            "data engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Data Engineer (Remote-Eligible)",
        "company": "Capital One",
        "desc": "West Creek 8 (12080), United States of America, Richmond, Virginia\n   Senior Lead Data Engineer (Remote-Eligible)\n  \n  Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Senior Lead Data Engineer, you\u2019ll have the opportunity to be on the forefront of driving a major transformation within Capital One. \n \n  What You\u2019ll Do: \n \n  Manage and develop a Java-based pipeline and query tools depending on HIve Metastore, AWS S3, Kafka and ORC \n  Develop analytics tooling to solve business problems driven by scale and international expansion \n  Optimize configurations for analytics tools to support growing business and organization \n \n \n  \u201cCapital One is open to hiring a Remote Employee for this opportunity.\u201d \n \n  Basic Qualifications: \n \n  Bachelor\u2019s Degree \n  At least 8 years of experience in application development (Internship experience does not apply) \n  At least 2 years of experience in big data technologies \n  At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud) \n \n \n  Preferred Qualifications: \n \n  9+ years of experience in application development including Python, Javascript, or Java \n  4+ years of experience with AWS \n  5+ years experience with Distributed data/computing tools (Trino, Hive, Kafka or Spark) \n  4+ year experience working on real-time data and streaming applications \n  4+ years of experience with NoSQL implementation (Cassandra) \n  4+ years of experience with UNIX/Linux including basic commands and shell scripting \n  2+ years of experience with Agile engineering practices \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Data Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Data Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Data Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "aa10151ab9747275": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 70000.0,
        "salary_max": 110000.0,
        "title": "Machine Learning Support Engineer",
        "company": "Labelbox",
        "desc": "Labelbox is the leading data-centric AI platform for building intelligent applications. Teams looking to capitalize on the latest advances in generative AI and LLMs use the Labelbox platform to inject these systems with the right degree of human supervision and automation. Whether they are building AI products by using LLMs that require human fine-tuning, or applying AI to reduce the time associated with manually-intensive tasks like data labeling or finding business insights, Labelbox enables teams to do so effectively and quickly. \n  Current Labelbox customers are transforming industries within insurance, retail, manufacturing/robotics, healthcare, and beyond. Our platform is used by Fortune 500 enterprises including Walmart, Procter & Gamble, Genentech, and Adobe, as well as hundreds of leading AI teams. We are backed by leading investors including SoftBank, Andreessen Horowitz, B Capital, Gradient Ventures (Google's AI-focused fund), Databricks Ventures, Snowpoint Ventures and Kleiner Perkins. \n \n About the Role \n  As a Machine Learning Support Engineer, you\u2019ll act as the frontline of customer love, utilizing live chat and internal resources to resolve product issues and provide proactive guidance to customers. \n  As Labelbox continues to grow, we often have multiple openings for this role. Should your skills and experience align with our requirements and timeline, our team will reach out to you as soon as opportunities become available on this team. \n  Your Day to Day \n \n Create great experiences for our customers when they need help. Build trust and advisory relationships with customers to help them better use the product. \n Learn Labelbox's product at a deep level and help customers do the same. Learn what\u2019s happening next with the product, and help customers prepare for the use of new features. \n Go beyond the question being asked; understand how our customers define their own success with the product and help them work toward that success. \n Proactively propose creative solutions to address customers\u2019 business problems and goals. \n Be a voice for our customers during internal discussions and projects at Labelbox. Represent their needs and struggles to help drive our products in a strong direction. \n Monitor and identify trends in customer experiences. Work within the team and with other teams at Labelbox to give customers the information and tools they need to more effectively and efficiently support themselves in the use of the product. \n \n About You \n \n 1+ years experience on a Technical Support team \n BA/BS in Computer Science/Engineering degree \n Proficient in Python \n Experience with Machine Learning a plus \n An ability to navigate and advise on efforts related to complex customer requests or projects, gathering additional human resources for assistance if needed \n Empathy, patience, phenomenal people skills; \n An ability to learn quickly to understand and articulate new technologies and corresponding value propositions \n Outstanding organizational skills and ability to multitask in order to effectively prioritize and manage workflow \n A creative problem solver who isn\u2019t afraid to get their hands dirty \n Ability to quickly pick up a variety of software applications with ease \n Experience with common support software like Intercom, GitHub, Jira, etc \n \n Labelbox strives to ensure pay parity across the organization and discuss compensation transparently. The expected annual base salary range for United States-based candidates   is $70,000 - $110,000. This range is not inclusive of any potential equity packages or additional benefits. Exact compensation varies based on a variety of factors, including skills and competencies, experience, and geographical location. \n \n Excel in a Hub-centric Remote Model. \n  We\u2019re committed to excellence and understand the importance of bringing our talented people together. While we continue to embrace remote work, we\u2019ve transitioned to a Hub-Centric Remote Model with a focus on nurturing collaboration and connection within our dedicated hubs in the San Francisco Bay Area, New York City Metropolitan Area, Miami-Fort Lauderdale Area, and Warsaw, Poland. We encourage asynchronous communication, autonomy, and ownership of your tasks, with the added convenience of hub-based gatherings. \n  Your Personal Data Privacy:  Any personal information you provide Labelbox as a part of your application will be processed in accordance with Labelbox\u2019s Job Applicant Privacy notice. \n  Any emails from Labelbox team members will originate from a @labelbox.com email address. If you encounter anything that raises suspicions during your interactions, we encourage you to exercise caution and suspend or discontinue communications. If you are uncertain about the legitimacy of any communication you have received, please do not hesitate to reach out to us at recruiting@labelbox.com for clarification and verification. \n \n \n  #LI-Hybrid",
        "cleaned_desc": " An ability to navigate and advise on efforts related to complex customer requests or projects, gathering additional human resources for assistance if needed \n Empathy, patience, phenomenal people skills; \n An ability to learn quickly to understand and articulate new technologies and corresponding value propositions \n Outstanding organizational skills and ability to multitask in order to effectively prioritize and manage workflow \n A creative problem solver who isn\u2019t afraid to get their hands dirty \n Ability to quickly pick up a variety of software applications with ease \n Experience with common support software like Intercom, GitHub, Jira, etc ",
        "techs": [
            "intercom",
            "github",
            "jira"
        ],
        "cleaned_techs": [
            "intercom",
            "github",
            "jira"
        ]
    },
    "69fdad765f9de0ce": {
        "terms": [
            "machine learning engineer",
            "mlops"
        ],
        "salary_min": 120808.62,
        "salary_max": 152970.62,
        "title": "Senior Machine Learning Ops Engineer \u2013 Customer Growth Marketing",
        "company": "Dell Technologies",
        "desc": "Senior Machine Learning Ops Engineer \u2013 Customer Growth Marketing \n  The B2B Customer Growth Marketing team within Dell Field & Partner Marketing is on a mission to transform marketing through the adoption of customer-centric solutions driven by Generative AI, machine-learning, Big Data, and world-class engineering. Working very tightly with Field Marketing and Sales stakeholders, we enable for the right message to get to the right customer at the right time through their omni-channel journey at massive scale. \n  What you'll achieve \n  As a Senior MLOps Engineer on a growing team, you will bring in your industry experience to manage the machine-learning lifecycle at scale using DevOps best practices as a foundation. You will collaborate with our ML Engineering, Data Engineering, and IT teams to support the technical roadmap and world-class engineering practices. \n  Take the first step toward your dream career  \n Every Dell Technologies team member brings something unique to the table. Here\u2019s what we are looking for with this role: \n  Essential Requirements \n \n  Solid industry experience in software-engineering or infrastructure engineering in enterprise-scale environments \n  Solid industry experience supporting CI/CD systems for continuous testing and deployment of machine learning models or scalable software/services \n  Experience with automation, monitoring, logging, and troubleshooting in production \n  Experience with containerization, such as Docker or Kubernetes \n \n  Desired Requirements  \n \n Bachelor\u2019s Degree in Computer Science, Engineering, Information Technology, or equivalent professional experience \n  Experience with the Python programming language \n \n  Who we are \n  We believe that each of us has the power to make an impact. That\u2019s why we put our team members at the center of everything we do. If you\u2019re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we\u2019re looking for you. \n  Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us. \n  Application closing date: September 30th \n  Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment.",
        "cleaned_desc": "  Solid industry experience in software-engineering or infrastructure engineering in enterprise-scale environments \n  Solid industry experience supporting CI/CD systems for continuous testing and deployment of machine learning models or scalable software/services \n  Experience with automation, monitoring, logging, and troubleshooting in production \n  Experience with containerization, such as Docker or Kubernetes ",
        "techs": [
            "docker",
            "kubernetes"
        ],
        "cleaned_techs": [
            "docker",
            "kubernetes"
        ]
    },
    "fe06e230aa5a35fd": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 105950.05,
        "salary_max": 134156.36,
        "title": "Data Scientist \u2013 Customer Growth Marketing",
        "company": "Dell Technologies",
        "desc": "Principal Data Scientist \u2013 Customer Growth Marketing \n  Data Science is all about breaking new ground to enable businesses to answer their most urgent questions.Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand-new methodologies, tools, statistical methods, and models. What\u2019s more, we are in collaboration with leading academics, industry experts, and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data. \n  Join us to do the best work of your career and make a profound social impact as a  Data Scientist  on our Customer  Growth Marketing Team  working remotely in  Brazil,  or  Panama . \n  What you\u2019ll achieve \n  As a Senior Data Scientist on a growing team, you will bring your industry expertise to build machine-learning models.You will collaborate closely with our Field Marketing and Sales stakeholders to solve critical and highly-visible business problems with machine learning. \n \n \n  You will: \n \n \n \n \n \n  You will work with other Data Scientists, Data Engineers, ML Engineers, and Business Analysts to support the end-to-end ML lifecycle, from use-case identification through model productionization and business outcome measurement. \n  Play a critical role in growing and maturing our marketing capabilities with machine learning at its core \n  Engage with business stakeholders to support customer-centric design of solutions \n \n   \n \n \n \n \n \n Take the first step toward your dream career \n \n \n  Every Dell Technologies team member brings something unique to the table. Here\u2019s what we are looking for with this role: \n \n \n  Essential Requirements \n \n \n \n  Solid industry experience in machine learning, data science, statistics, or related, including a demonstrated portfolio of productionized models and model training, evaluation, validation, implementation, and monitoring \n  Solid industry experience in statistical programming, including Python (with packages such as pandas, scikit-learn, TensorFlow, or PyTorch) and Jupyter Notebooks \n  Ability to gain an understanding of domain data, business processes, and business objectives \n \n \n \n  Desired Requirements \n \n \n \n  Bachelor\u2019s degree in Data Science, Machine Learning, Statistics, Economics, Physics, other quantitative fields, or equivalent professional experience \n  Experience with model registries (i.e.MLflow) and version control (i.e.Gitlab) \n \n \n \n \n  Who we are \n \n \n  We believe that each of us has the power to make an impact. That\u2019s why we put our team members at the center of everything we do. If you\u2019re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we\u2019re looking for you.     Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us. \n  Application close date: September 15th - 2023    Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. Read the full Equal Employment Opportunity Policy here. \n \n \n  Job ID: R227251\n   Dell\u2019s Flexible & Hybrid Work Culture    At Dell Technologies, we believe our best work is done when flexibility is offered.    We know that freedom and flexibility are crucial to all our employees no matter where you are located and our flexible and hybrid work style allows team members to have the freedom to ideate, be innovative, and drive results their way. To learn more about our work culture, please visit our locations page.",
        "cleaned_desc": "  Solid industry experience in machine learning, data science, statistics, or related, including a demonstrated portfolio of productionized models and model training, evaluation, validation, implementation, and monitoring \n  Solid industry experience in statistical programming, including Python (with packages such as pandas, scikit-learn, TensorFlow, or PyTorch) and Jupyter Notebooks \n  Ability to gain an understanding of domain data, business processes, and business objectives \n \n \n \n  Desired Requirements \n \n \n \n  Bachelor\u2019s degree in Data Science, Machine Learning, Statistics, Economics, Physics, other quantitative fields, or equivalent professional experience ",
        "techs": [
            "machine learning",
            "data science",
            "statistics",
            "python",
            "pandas",
            "scikit-learn",
            "tensorflow",
            "pytorch",
            "jupyter notebooks"
        ],
        "cleaned_techs": [
            "data science",
            "statistics",
            "python",
            "pandas",
            "scikit-learn",
            "tensorflow",
            "pytorch",
            "jupyter notebooks"
        ]
    },
    "330874f0db41542a": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 116328.17,
        "salary_max": 147297.39,
        "title": "Fatigue and Damage Tolerance Engineer",
        "company": "Airbus",
        "desc": "Airbus Commercial Aircraft is looking for a Fatigue and Damage Tolerance Engineer - Product Development to join our Engineering department based in Wichita, KS. \n Relocation is possible for any candidates not currently located in the Wichita, KS metro area \n This position supports the development of aircraft structures as a Fatigue and Damage Tolerance analyst. Duties include performing F&DT analysis for metallic parts while working in close collaboration with design and static stress colleagues. Methods include the use of both classical hand calculations and computer techniques to develop high strength to weight ratio structures. \n Meet the team: \n Our Engineers have the privilege of working on complex, highly engineered machines and are involved at all stages of the aircraft life cycle, from product development to manufacturing and direct support to our airline customers. If you\u2019re interested in designing, creating, and testing the present and future of Airbus products across the globe, join our mission to connect the world. \n Your working environment: \n Located on the Wichita State University (WSU) innovation campus is Airbus\u2019 first dedicated engineering facility outside of Europe. Our team of 200+ support staff and engineers focus on aerostructure design including analysis of aircraft primary structures, such as wings and fuselages, for all major Airbus products across the globe. \n How we care for you: \n \n Financial Rewards: Competitive base salary, incentive compensation which may include profit sharing schemes, retirement savings plan and the ability to participate in an Employee Stock Ownership Plan (\u201cESOP\u201d) \n Work/Life Balance: Paid time off including personal time, holidays and a generous paid parental leave program. \n Health & Welfare: Comprehensive insurance coverage including medical (traditional and high-deductible health plans), prescription, dental, vision, life, disability, Employee Assistance Plan (\u201cEAP\u201d) and other supplemental benefit coverages. \n Individual Development: Upskilling and development opportunities through our global Leadership University, including unlimited access to 10,000+ e-learning courses focusing on ways to develop your employability, certifications, career path as well as the opportunity to participate in accelerated development programmes and both national and international mobility. \n \n At Airbus, we support you to work, connect and collaborate more easily and accessibly. Wherever possible, we support flexible working arrangements to stimulate innovative thinking. \n Your challenges: \n \n Autonomously perform routine to moderate levels of durability and damage tolerance analyses to verify that structural integrity requirements have been met. Team assignments may include detail part sizing, layout maturation, trade study analysis, weight reduction efforts, certification activity, or in-service support. \n Document analyses in support of trade studies or detail drawing sign-out for airframe structure. \n Liaise with customers, suppliers, and/or subcontractors to exchange technical data and to discuss project solutions. \n \n Your boarding pass: \n \n Bachelor of Science (BS) Degree in Engineering (Aero, Civil, Mechanical) with a strong curriculum emphasis on structural design and analysis and mechanics of materials is required. \n A master's degree with emphasis on fracture mechanics is desired. \n A Minor in Computer Science is a plus. \n 6 to 13+ years total analysis experience in airframe design/repair programs with a minimum of 4 years of demonstrated experience in F&DT. \n Proven knowledge of classical and numerical (Finite Element) stress analysis methods. \n Experience in airframe design (metallic & composites), stress analysis, SRM, MRB, FAR/JAR/CS/Part21, Certification, CATIA, NASTRAN, and PATRAN, is desired. \n Proficiency with Microsoft Office Suite and Google Suite. \n Ability to code in C, C++, Python, or other programming languages is desirable \n \n Not a 100% match? No worries! Airbus supports your personal growth with customized development solutions. \n Take your career to a new level and apply onlinenow! \n Equal Opportunity: \n Airbus is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Airbus is also committed to compliance with all fair employment practices regarding citizenship and immigration status. \n This job requires an awareness of any potential compliance risks and a commitment to act with integrity, as the foundation for the Company\u2019s success, reputation and sustainable growth. \n Company: \n Airbus Americas, Inc. \n Employment Type: \n Permanent \n Experience Level: \n Professional \n Remote Type: \n Job Family: \n Structure Design & Integration <JF-EN-EO> \n ------ \n Airbus provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, genetics, pregnancy, marital status, veteran status or other legally protected status. In addition to federal law requirements, Airbus complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, demotion, termination, layoff, recall, transfer, leaves of absence, compensation, benefits and training. Airbus expressly prohibits any form of workplace harassment based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, genetics, pregnancy, marital status, veteran status or other legally protected status. As a matter of policy, Airbus does not sponsor visas for US positions unless specified. Only applicants with current work authorization will be considered. Airbus does not offer tenured or guaranteed employment. Employment with Airbus is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice. Airbus reserves the right to revise or change job duties and responsibilities as the need arises. This position description does not constitute a written or implied contract of employment. \n By submitting your CV or application you are consenting to Airbus using and storing information about you for monitoring purposes relating to your application or future employment. This information will only be used by Airbus. Airbus is committed to achieving workforce diversity and creating an inclusive working environment. We welcome all applications irrespective of social and cultural background, age, gender, disability, sexual orientation or religious belief. \n Airbus is, and always has been, committed to equal opportunities for all. As such, we will never ask for any type of monetary exchange in the frame of a recruitment process. Any impersonation of Airbus to do so should be reported to emsom@airbus.com. \n Job Type: Full-time \n Benefits: \n \n 401(k) \n 401(k) matching \n Dental insurance \n Health insurance \n Life insurance \n Paid time off \n Professional development assistance \n Retirement plan \n Vision insurance \n \n Schedule: \n \n Monday to Friday \n \n Work Location: Remote",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "c9eb5a974d17c15a": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 168000.0,
        "salary_max": 175000.0,
        "title": "Senior Software Engineer",
        "company": "HouseCanary",
        "desc": "HouseCanary is creating a more efficient real estate marketplace, where its trusted property valuations help consumers and investors enjoy a frictionless experience in buying, financing, and improving their homes. By working directly with institutional investors, HouseCanary already powers more than $200 million in real estate transactions each month. Through partnerships with leading financial institutions, HouseCanary\u2019s ComeHome platform will enable consumers to close on more than $100 billion of real estate transactions this year. HouseCanary recently raised $65M in Series C funding as our team continues our hyper-growth. Founded in 2013, HouseCanary has built its reputation on developing unique analytics for the $30.7 trillion residential real estate market, based on big data and refined through machine learning. By combining this technology with its nationwide brokerage, HouseCanary is uniquely positioned to connect investors and consumers alike to the information that they care most about in an effort to modernize and streamline the home buying and home ownership process.\n   \n \n \n  We are recognized on Built-in's Best Places to Work, including:\n   \n \n    Colorado Best Places to Work\n   \n \n    Colorado Best Midsize Companies to Work For\n   \n \n    Colorado Best Paying Companies\n   \n \n    SF Best Midsize Companies to Work For\n   \n \n    We are Inc. Magazine's Best Workplaces Honoree 2022\n   \n \n \n  Worksite: Allowed to 100% telecommute from anywhere in the U.S.\n   \n \n \n \n What You'll Do \n \n \n  Develop frontend user interface and applications for real estate data analytics company \n  Write, analyze, and review programming, applying knowledge of computing capabilities \n  Consult with software users and system analysts/program managers to help define and resolve \n  website errors \n  Conduct tests on newly written software to ensure that it performs as intended \n  Correct errors found in the application websites, ensuring intended results are produced \n  Maintain and create computer applications to handle specific tasks, such as retrieving data \n  Produce well-commented code, understandable by other programmers \n  Perform directed revisions and repairs \n  Collaborate with management, product and engineering employees to clarify requirements and \n  make suggested changes \n  Note implemented edits to ensure that any changes or revisions are fully described \n  Develop and enhance websites \n \n \n \n \n \n \n What You'll Need \n \n \n  Education Requirements: A Bachelor\u2019s degree in Software Development, Software Engineering, or Computer Science. \n  Experience and skills requirements: \n  3 years of experience as a Software Developer or Software Engineer and one year of experience in the following: \n  Full stack development with Python and Flask frameworks. \n  Writing node libraries and Node JS \n  ReactJS, React Native \n  Postgresql \n \n \n \n \n \n \n   $168,000 - $175,000 a year\n   \n \n    RATE OF PAY: $168,000 - $175,000 annual salary. The employer will pay or exceed the prevailing wage, as determined by the U.S. Department of Labor.\n   \n \n \n \n   Resume to: tkung@housecanary.com or HouseCanary, 535 Mission Street, 14th Floor, San Francisco, CA 94105\n   \n \n \n  We strongly encourage people of color, women, lesbian, gay, bisexual, transgender, queer and non-binary people, veterans, parents, and individuals with disabilities to apply. HouseCanary is an equal opportunity employer and welcomes everyone to our team. If you need reasonable adjustments at any point in the application or interview process, please let us know. In your application, please feel free to note which pronouns you use (For example - she/her, he/him, they/them, etc).",
        "cleaned_desc": " \n  Develop frontend user interface and applications for real estate data analytics company \n  Write, analyze, and review programming, applying knowledge of computing capabilities \n  Consult with software users and system analysts/program managers to help define and resolve \n  website errors \n  Conduct tests on newly written software to ensure that it performs as intended \n  Correct errors found in the application websites, ensuring intended results are produced \n  Maintain and create computer applications to handle specific tasks, such as retrieving data \n  Produce well-commented code, understandable by other programmers \n  Perform directed revisions and repairs \n  Collaborate with management, product and engineering employees to clarify requirements and \n  make suggested changes \n  Note implemented edits to ensure that any changes or revisions are fully described \n  Develop and enhance websites \n ",
        "techs": [
            "frontend user interface",
            "programming",
            "computing capabilities",
            "software users",
            "system analysts",
            "program managers",
            "website errors",
            "tests",
            "application websites",
            "computer applications",
            "well-commented code",
            "directed revisions",
            "repairs",
            "management",
            "product and engineering employees",
            "requirements",
            "suggested changes",
            "edits",
            "websites"
        ],
        "cleaned_techs": [
            "frontend user interface",
            "programming",
            "computing capabilities",
            "software users",
            "system analysts",
            "program managers",
            "website errors",
            "tests",
            "application websites",
            "well-commented code",
            "directed revisions",
            "repairs",
            "management",
            "product and engineering employees",
            "requirements",
            "suggested changes",
            "edits",
            "websites"
        ]
    },
    "5fe77258b3fd1377": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 96682.47,
        "salary_max": 122421.55,
        "title": "Hardware Engineer",
        "company": "Sophos",
        "desc": "Sophos is a worldwide leader in next-generation cybersecurity, protecting more than 500,000 organizations and millions of consumers in more than 150 countries from today\u2019s most advanced cyberthreats. Powered by threat intelligence, AI and machine learning from SophosLabs and SophosAI, Sophos delivers a broad portfolio of advanced products and services to secure users, networks and endpoints against ransomware, malware, exploits, phishing and the wide range of other cyberattacks. Sophos provides a single integrated cloud-based management console, Sophos Central \u2013 the centerpiece of an adaptive cybersecurity ecosystem that features a centralized data lake that leverages a rich set of open APIs available to customers, partners, developers, and other cybersecurity vendors. Sophos sells its products and services through reseller partners and managed service providers (MSPs) worldwide. Sophos has major hubs around the globe. More information is available at www.sophos.com\n   \n \n \n  Role Summary \n \n \n    Sophos is seeking an experienced Hardware Development Engineer to join our development team focused on bringing cutting-edge network security products, including firewalls, wireless access points, ethernet switches, and other network access devices to customers worldwide. As a Hardware Development Engineer, you will play a pivotal role in the design, development, and testing of hardware components critical to the success of our innovative security solutions. Hardware development at Sophos provides a unique opportunity to participate in the complete development lifecycle, from early concept definition through development, validation, and mass production program phases.\n   \n \n \n \n What You Will Do \n \n \n  Collaborate with cross-functional teams, including software engineers and product managers, to define hardware requirements and specifications for network security products. \n  Perform technical evaluations of emerging technologies and assist in critical component selection for new hardware architectures.  \n Collaborate with in-house and external development teams to implement hardware designs, including schematic capture, PCB layout, and mechanical design components.  \n Develop test validation plans for new hardware designs and participate in hardware validation activities to ensure the release of high-quality designs.  \n Support the production of current and future hardware products, including triaging field issues and qualifying new components. \n \n \n \n \n \n \n What You Will Bring \n \n \n  2-4 years of experience in hardware development, with a focus on network security products or related fields. \n  Proficiency in schematic design and PCB layout using industry-standard tools (e.g., Altium Designer, Eagle, KiCad). \n  Proficiency in developing and executing comprehensive hardware test plans, including functional, performance, and reliability testing. \n  Experience using modern scripting languages (such as Python) in a Linux environment to automate hardware validation activities, improve test coverage, and shorten test cycles.  \n Excellent communication skills, both written and verbal, with the ability to articulate technical concepts to both technical and non-technical stakeholders. \n  Experience with designing and debugging products with high speed I/O protocols such as PCIe, DDR, SATA.  \n Familiarity with standard server architectures and major CPU/NPU/GPU/ASIC manufacturers is a plus. \n \n \n \n \n \n \n   Applicants in Colorado, California, Washington, and New York City may email TalentAcquisition@sophos.com for the up-to-date salary ranges for the position.\n   \n \n \n  #LI-SS1\n   \n \n    #LI-Remote\n   \n \n    #B1\n   \n \n \n  What's Great About Sophos? \n \n \n Our people \u2013 we innovate and create, all of which are accompanied by a great sense of fun and team spirit \n Employee-led diversity and inclusion networks that build community and provide education and advocacy \n Annual charity and fundraising initiatives and volunteer days for employees to support local communities \n Global employee sustainability initiatives to reduce our environmental footprint \n Global fitness and trivia competitions to keep our bodies and minds sharp \n Global wellbeing days for employees to relax and recharge \n Monthly wellbeing webinars and training to support employee health and wellbeing \n \n \n \n  Our Commitment To You \n \n \n    We\u2019re proud of the diverse and inclusive environment we have at Sophos, and we\u2019re committed to ensuring equality of opportunity. We believe that diversity, combined with excellence, builds a better Sophos, so we encourage applicants that can contribute to the diversity of our team. All applicants will be treated in a fair and equal manner and in accordance with the law regardless of gender, sex, gender reassignment, marital status, race, religion or belief, color, age, military veteran status, disability, pregnancy, maternity or sexual orientation. We want to give you every opportunity to show us your best self, so if there are any adjustments we could make to the recruitment and selection process to support you, please let us know.\n   \n \n \n  Data Protection \n \n \n    If you choose to explore an opportunity, and subsequently share your CV or other personal details with Sophos, these details will be held by Sophos for 12 months in accordance with our Privacy Policy and used by our recruitment team to contact you regarding this or other relevant opportunities at Sophos. If you would like Sophos to delete or update your details at any time, please follow the steps set out in the Privacy Policy describing your individual rights. If you have any questions about Sophos\u2019 data protection practices, please contact dataprotection@sophos.com.",
        "cleaned_desc": "  Collaborate with cross-functional teams, including software engineers and product managers, to define hardware requirements and specifications for network security products. \n  Perform technical evaluations of emerging technologies and assist in critical component selection for new hardware architectures.  \n Collaborate with in-house and external development teams to implement hardware designs, including schematic capture, PCB layout, and mechanical design components.  \n Develop test validation plans for new hardware designs and participate in hardware validation activities to ensure the release of high-quality designs.  \n Support the production of current and future hardware products, including triaging field issues and qualifying new components. \n \n \n \n \n \n \n What You Will Bring \n \n \n  2-4 years of experience in hardware development, with a focus on network security products or related fields.    Proficiency in schematic design and PCB layout using industry-standard tools (e.g., Altium Designer, Eagle, KiCad). \n  Proficiency in developing and executing comprehensive hardware test plans, including functional, performance, and reliability testing. \n  Experience using modern scripting languages (such as Python) in a Linux environment to automate hardware validation activities, improve test coverage, and shorten test cycles.  \n Excellent communication skills, both written and verbal, with the ability to articulate technical concepts to both technical and non-technical stakeholders. \n  Experience with designing and debugging products with high speed I/O protocols such as PCIe, DDR, SATA.  \n Familiarity with standard server architectures and major CPU/NPU/GPU/ASIC manufacturers is a plus. \n \n \n \n \n \n \n   Applicants in Colorado, California, Washington, and New York City may email TalentAcquisition@sophos.com for the up-to-date salary ranges for the position.\n   \n ",
        "techs": [
            "altium designer",
            "eagle",
            "kicad",
            "python",
            "linux",
            "pcie",
            "ddr",
            "sata"
        ],
        "cleaned_techs": [
            "altium designer",
            "eagle",
            "kicad",
            "python",
            "linux",
            "pcie",
            "ddr",
            "sata"
        ]
    },
    "65d91e658f9b70c5": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 58400.0,
        "salary_max": 133000.0,
        "title": "Model Based Systems Engineer",
        "company": "Booz Allen Hamilton",
        "desc": "Job Description \n \n \n \n \n \n \n \n \n \n \n         Location: \n         \n \n         El Segundo,CA,US \n         \n \n \n \n         Remote Work: \n         \n \n         Hybrid \n         \n \n \n \n         Job Number: \n         \n \n         R0182305\n         \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n         Model Based Systems Engineer\n           The Opportunity: \n  Are you looking for an opportunity to combine your technical skills with big picture thinking to make an impact on complex aerospace systems? You understand your customer\u2019s environment and how to develop the right systems for their mission. Your ability to translate real-world needs into technical specifications makes you an integral part of delivering a customer-focused engineering solution. \n \n  As a systems engineer on our team, you have the chance to shape tomorrow's systems by leading model-based systems engineering (MBSE) projects. Your customer will trust you to not only design and develop these systems, but also evolve them with advanced technology solutions. On our team, you\u2019ll be able to broaden your skillset into areas like digital transformation. Grow your skills by merging MBSE and systems-of-systems engineering and integration to create MBSE models that drive integration. \n \n  Join us. The world can\u2019t wait. \n \n  You Have: \n \n  Experience with large-scale aerospace systems \n  Experience with digital engineering tools, including Cameo or Magicdraw, Sparx EA, DOORS, or MATLAB \n  Experience with defining and supporting MBSE projects \n  Experience with using systems and digital engineering approaches to manage system architecture definition, requirements development, or technical baseline deliverables \n  Knowledge of Systems Modeling Languages \n  Secret clearance \n  Bachelor's degree in Science, Technology, Engineering, or Mathematics \n \n \n  Nice If You Have: \n \n  Knowledge of Systems Machine Learning (SysML) and DoD Architecture Framework (DoDAF) \n  Ability to perform multiple systems engineering and program management functions in support of design reviews and requirements verification  \n Ability to support proposals or business development \n  Master's degree in Science, Technology, Engineering, or Mathematics \n  INCOSE Certification \n  OMG Certified Systems Model Professional (OCSMP) Certification \n \n \n  Clearance: \n  Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information ;  Secret clearance is required. \n \n  Create Your Career: \n \n  Grow With Us \n  Your growth matters to us\u2014that\u2019s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. \n  A Place Where You Belong \n  Diverse perspectives cultivate collective ingenuity. Booz Allen\u2019s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you\u2019ll build your community in no time. \n \n  Support Your Well-Being \n  Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we\u2019ll support you as you pursue a balanced, fulfilling life\u2014at work and at home. \n \n  Your Candidate Journey \n  At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we\u2019ve compiled a list of resources so you\u2019ll know what to expect as we forge a connection with you during your journey as a candidate with us. \n \n  Compensation \n  At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen\u2019s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. \n  Salary at Booz Allen is determined by various factors, including but not limited to location, the individual\u2019s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $58,400.00 to $133,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen\u2019s total compensation package for employees.\n          \n  Work Model  Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. \n \n  If this position is listed as remote or hybrid, you\u2019ll periodically work from a Booz Allen or client site facility. \n  If this position is listed as onsite, you\u2019ll work with colleagues and clients in person, as needed for the specific role. \n \n \n  EEO Commitment \n  We\u2019re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change \u2013 no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "b995f9612384dedd": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 70000.0,
        "salary_max": 100000.0,
        "title": "Product Support Engineer (TMS)",
        "company": "MercuryGate International",
        "desc": "MercuryGate  provides powerful transportation management solutions proven to be a competitive advantage for today\u2019s most successful shippers, 3PLs, brokers and carriers. The comprehensive Software-as-a-Service (SaaS) product suite natively supports all transportation modes and segments, generating value for its users through improved cost, productivity and efficiency using artificial intelligence (AI), machine learning (ML) and connected technologies to adapt and automate transportation management functions. \n  Our  Customer Success Organization  provides an environment for our customers to satisfy their needs and goals as a priority. We drive customer utilization, adoption, and success. We are built on a customer-first philosophy to listen and operate in a way that allows our customers to realize their total value. The  Product Support Engineer  is part of a product expert team within the Customer Success organization. This high-performing team supports our full customer base and is a valued player, partner and trusted advisor to the Customer Success leadership team as well as other internal stakeholders. This role participates on a team for on-call rotation to provide coverage 24 x 7 x 365 support, every 5 to 7 weeks. \n  This is a remote position.  This opportunity can be 100% virtual from anywhere within the United States. Work Authorization: Must be able to work in the United States without any sponsorship required (currently or in the future). \n  Job Duties: \n \n Performs customer communication via Salesforce, email, phone, or video conference to determine customer needs or to resolve technical issues \n Provides advanced support for MercuryGate product Suite, TMS, Carma, Edge, MMO, Procure and Maestro \n Triage advanced and multi-functional customer support cases alongside backend teams (Integrations, Engineering, DBA, Product) \n Develop effective solutions with moderate supervision \n Utilizes product documentation to educate customers and suggests edits to the MG Education Team \n Drive continuous improvement to reduce case solve times and increase quality in services provided to customers \n Make interface recommendations based on knowledge of MercuryGate interface capabilities \n Validate and recreate defects on TEST and work with Development to test bug fixes \n Update and monitor customer support tickets using Salesforce and Jira \n Monitor alerts across all servers (Communication Servers/TMS Servers/Document Generating Servers) \n Maintain accurate time entry records for billable tasks \n \n Requirements & Qualifications: \n \n 3-8 years of directly applicable experience \n Knowledge of MercuryGate TMS software and/or logistics industry experience; TMS experience is required  \n Proficient in Excel, and Office 365 (Outlook/OneDrive) \n Solid communication and organizational skills \n Prior Technical Product Support experience is a bonus \n Ability to multi-task and prioritize in a fast-paced, detail-oriented work environment \n Ability to self-teach using documentation to supplement internal training \n \n At MercuryGate, we not only value what you do for the organization, we also value your happiness, health, and time! \n \n Flexible Time Off (take what you need, when you need it), additional paid time off for Bereavement, Jury Duty, Community Service and 8 paid Holidays \n 401(k) company match up to 4% \n Dental, Vision, Short Term Disability, Long Term Disability \n Medical, Dental, & Vision Plans with coverage beginning the first day of employment \n Health Savings Account \n Life & Disability \n Accident / Illness / Hospital Plans \n ID Theft Protection \n Wellness / EAP \n Pet Insurance \n \n \n \n  We intend to fill this position as quickly as possible. Date posted: October 16th, 2023. Application Deadline: October 27th, 2023 \n  MercuryGate does not accept unsolicited resumes from search firms or agencies. Any resume submitted to any employee of MercuryGate without a prior written search agreement will be considered unsolicited and the property of MercuryGate. Please, no phone calls or emails. \n  Compensation: $70,000 - $100,000 per year, plus a bonus calculated as a percentage of base salary. This information reflects the anticipated base salary range for this position based on current national data. Minimums and maximums may vary based on location. Individual pay is based on skills, experience and other relevant factors. \n  We encourage you to apply even if your experience is not a 100% match with the position. We are looking for someone with relevant skills and experience, not a checklist that exactly matches the job description. We want to help you grow and in return, you help us grow into a stronger, more inclusive organization. MercuryGate is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, please contact us at ApplicationAssistance@mercurygate.com \n  MercuryGate is an Equal Opportunity Employer and does not discriminate on the basis of race, national origin, religion, color, gender, sexual orientation, age, non-disqualifying physical or mental disability or any other basis covered by law. Employment decisions are based solely on qualifications and business need. At MercuryGate, we are committed to upholding the highest standards of security and compliance with ISO27001 & SOC2 requirements. All employees are expected to contribute to our security-related training and activities, helping to ensure that our organization operates at the highest levels of security and effectiveness. \n  #LI-Remote \n  #LI-DNI",
        "cleaned_desc": " \n 3-8 years of directly applicable experience \n Knowledge of MercuryGate TMS software and/or logistics industry experience; TMS experience is required  \n Proficient in Excel, and Office 365 (Outlook/OneDrive) \n Solid communication and organizational skills \n Prior Technical Product Support experience is a bonus \n Ability to multi-task and prioritize in a fast-paced, detail-oriented work environment \n Ability to self-teach using documentation to supplement internal training \n ",
        "techs": [
            "mercurygate tms software",
            "excel",
            "office 365 (outlook/onedrive)"
        ],
        "cleaned_techs": [
            "mercurygate tms software",
            "excel",
            "office 365 (outlook/onedrive)"
        ]
    },
    "28e2701d77545d9b": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 172994.0,
        "salary_max": 241000.0,
        "title": "Graphics Software Engineer, Rendering - Reality Labs",
        "company": "Meta",
        "desc": "Reality Labs at Meta is building products that make it easier for people to connect with the ones they love most, enjoy top-notch, wire-free VR, and push the future of computing platforms. We are a team of world-class experts developing and shipping products at the intersection of hardware, software and content.As a Graphics Software Engineer on the Reality Labs team at Meta, you can help build new, innovative hardware and software that radically redefine the way people work, play and connect. What we build today could one day be the norm. So to be here today is to truly be at the heart of change and the frontier of what's to come. We're the people helping to define the metaverse. We may not have all the answers. But together, we're getting closer.\n  \n \n \n Graphics Software Engineer, Rendering - Reality Labs Responsibilities:    \n \n Develop innovative graphics frameworks, algorithms, and tools to maximize graphics quality and performance \n  Partner closely with various infra and product teams across Meta, on camera, graphics, upcoming hardware, media enhancements, and more to create real-time rendering architecture \n  Building tools and pipelines for generating very realistic synthetic images \n  Enable high fidelity experiences through remote compute solutions on smaller devices with limited battery \n  Building rendering subsystems for platforms such as Spark AR and Horizon \n  Build a platform for cloud streamed games \n  Document and support graphics features \n  Write high-quality, performant, and maintainable code \n  Collaborate with cross-functional engineering teams to deliver innovation into AR/VR products \n \n \n \n \n Minimum Qualifications:   \n \n  Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience. \n  6+ years of graphics software engineering experience or 2+ years of graphics software engineering experience with PhD \n  6+ years of experience with C/C++ programming \n  6+ years of object-oriented and component-based design experience \n  Problem-solving and communication skills \n \n \n \n \n Preferred Qualifications:   \n \n  Experience delivering AAA Games, working on Graphics subsystems or the Game Engine AR/VR experience \n  Knowledge of ray tracing, rasterization and linear algebra \n  Experience with low level performance profiling and optimization \n  Experience implementing 3D graphics features such as lighting, effects, shaders and other low-level systems \n  Experience with tools such as Maya, Houdini, Blender, 3Ds Max, Arnold, RenderMan, or Cycles \n  Experience with either DirectX/Vulkan/OpenGL/Metal \n \n \n \n \n About Meta:    Meta builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps like Messenger, Instagram and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. People who choose to build their careers by building with us at Meta help shape a future that will take us beyond what digital connection makes possible today\u2014beyond the constraints of screens, the limits of distance, and even the rules of physics.\n  \n \n \n \n  Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment. \n   \n  Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",
        "cleaned_desc": "  Enable high fidelity experiences through remote compute solutions on smaller devices with limited battery \n  Building rendering subsystems for platforms such as Spark AR and Horizon \n  Build a platform for cloud streamed games \n  Document and support graphics features \n  Write high-quality, performant, and maintainable code \n  Collaborate with cross-functional engineering teams to deliver innovation into AR/VR products \n \n \n   \n Minimum Qualifications:   \n \n  Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience. \n  6+ years of graphics software engineering experience or 2+ years of graphics software engineering experience with PhD \n  6+ years of experience with C/C++ programming \n  6+ years of object-oriented and component-based design experience \n  Problem-solving and communication skills \n   \n \n \n Preferred Qualifications:   \n \n  Experience delivering AAA Games, working on Graphics subsystems or the Game Engine AR/VR experience \n  Knowledge of ray tracing, rasterization and linear algebra \n  Experience with low level performance profiling and optimization \n  Experience implementing 3D graphics features such as lighting, effects, shaders and other low-level systems ",
        "techs": [
            "spark ar",
            "horizon",
            "cloud streamed games",
            "graphics features",
            "ar/vr products",
            "bachelor's degree in computer science",
            "computer engineering",
            "c/c++ programming",
            "object-oriented and component-based design experience",
            "aaa games",
            "graphics subsystems",
            "game engine ar/vr experience",
            "ray tracing",
            "rasterization",
            "linear algebra",
            "low level performance profiling",
            "optimization",
            "implementing 3d graphics features"
        ],
        "cleaned_techs": [
            "spark ar",
            "horizon",
            "cloud streamed games",
            "graphics features",
            "ar/vr products",
            "computer engineering",
            "c/c++ programming",
            "object-oriented and component-based design experience",
            "aaa games",
            "graphics subsystems",
            "game engine ar/vr experience",
            "ray tracing",
            "rasterization",
            "linear algebra",
            "low level performance profiling",
            "optimization",
            "implementing 3d graphics features"
        ]
    },
    "fa5ce90b4deb0d69": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 113259.914,
        "salary_max": 143412.28,
        "title": "Senior Rockwell Automation Engineer",
        "company": "E Tech Group",
        "desc": "At  E Tech Group , joining our team means joining a group of passionate and forward-thinking experts. We're one of the largest engineering and system integration firms in the United States providing value for our clients through IT automation and control solutions for more than 25 years to the Life Sciences, Mission Critical, Metals, Material Handling, Consumer Products and Food & Beverage industries. Our national scale allows us to help our clients develop and implement standardized solutions at any of their facilities regardless of physical location or the local resources available at that site. \n \n \n  We are seeking a  Senior Rockwell Automation Engineer  role requires an individual have a diverse engineering skill set with the ability to work independently to perform all aspects of process automation engineering and system integration, including initial concepts, specifications, design engineering, documentation, project implementation, testing, startup, and training of operating and maintenance personnel. You will be a key member of a diverse team of professionals developing solutions for our client's most complex automation challenges. \n  Location:  Hybrid of work from home and local driving travel within 1.5 hours of Merrimack after onboarding. \n  You will: \n \n Lead or work on a project team responsible for the design, configuration, and testing of continuous and batch control applications. \n Understand the project scope and financials and exercise good time management skills to plan & coordinate the work as part of a project team by developing project schedules and budgets in collaboration with project leadership and identify change orders and justify changes to Project Management ensuring completion of all project deliverables to the satisfaction of the client. \n Anticipate and provide solutions to project leadership and/or customer giving high priority to customer satisfaction offering strong technical expertise and advisement as part of the project team. \n Adapt to a flexible work schedule & travel, if necessary, to participate in the design, development, and start-up of control systems at the client's facility. \n \n \n \n  You have: \n \n Hands on experience executing automation projects for process plants, minimum of 3-5 years in industry, preferably in Life Sciences domain. \n BS Engineering degree or equivalent educational background \n Detail oriented with strong technical aptitude and desire for learning new technologies \n Knowledge of industry and regulatory standards, design criteria and codes relevant to Instrumentation and Controls \n    \n ISA S88 Batch standard \n ISA S95 Controls System Architecture standard \n GAMP 5 & cGMP \n 21 CFR Part 11 \n \n Demonstrated ability to develop process control system applications, utilizing Programmable Logic Controllers (PLC), Human Machine Interfaces (HMI), and Supervisory Control and Data Acquisition (SCADA) Systems, configuration of thin client server applications, design, developing & maintaining databases for data collection, configuring process historians and using reporting tools to present process data to operations & management, tuning control loops, providing control system startup services, troubleshooting existing control systems and providing control system qualifications. \n Solid understanding of object-oriented software development & delivery methodologies \n Expertise with the design, implementation, maintenance and troubleshooting of virtual server-based control system architectures, and understanding of control system network topology, i.e. Ethernet, Ethernet/IP, ControlNet, DeviceNet, etc. \n Knowledge of process instrumentation specification, installation & troubleshooting, and control panel design, check-out, commissioning & troubleshooting \n Hands-on experience rapidly troubleshooting and solving equipment & instrument-related issues during start-up and commissioning and perform complex system testing \n Experience working at customer sites supporting the installation & commissioning of equipment and systems. \n \n \n \n  We are seeking Application Knowledge in: \n \n PLC Programming with Rockwell Studio5000, RSLogix5000 \n \n Ability to program in RLL, SFC and ST \n \n HMI Programming using one or more of Rockwell FTView ME and SE, Wonderware System Platform, Inductive Automation Ignition \n Configuration & use of one or more of Wonderware DAS/OI Server, FactoryTalk Gateway, RSLinx, RSNetWorx, KepServerEx \n Configuration & development of one or both of Wonderware InBatch & Rockwell FTBatch \n Configuration & development using one or more of Wonderware Historian, FTHistorian, OSI PI \n Use of SQL for database creation, query & management.Ability to develop stored procedures, functions & views and use of SQL commands for integration of control systems to back-end databases. Familiarity with SSRS for reporting \n Configure & operate Allen Bradley PowerFlex Drives via manual HIM or through PLC programming \n Configuration, use & troubleshooting of Allen Bradley Stratix switches \n Experience programming various other control systems is desired (i.e., GE Fanuc, Siemens, DeltaV, Honeywell DCS, GE iFix) \n \n \n \n  You may have experience with: \n \n Bio Reactors \n Skid System Controls \n Clean/Steam in Place \n Freeze Dryers \n Autoclave Control Systems \n Centrifuge Systems \n Building Management & HVAC Control Systems \n Thermal Oxidizer Systems \n Purified Water \n Finished Product Transfer Systems \n Granulator Control Systems \n Boilers and Boiler Control Systems \n Bulk Material Handling and Processing \n Laboratory Systems \n Pasteurization and Homogenization \n Wastewater Treatment \n Weighing and Metering Systems \n Wet or Dry Blending System \n And many more\u2026 \n \n E Technologies Group is an Equal Opportunity / Affirmative Action Employer. Applicants are considered for employment without regard to race, color, religion, sex, age, disability, military status, genetic information, gender identity, sexual orientation, citizenship status, or any other basis prohibited by law. E Technologies Group will provide reasonable accommodations to qualified individuals with disabilities and for religious beliefs.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "69df72a599ddd354": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 120000.0,
        "salary_max": 200000.0,
        "title": "Technical Product Manager",
        "company": "Novity",
        "desc": "Novity\u00ae, a spin-out from Xerox's Palo Alto Research Center (PARC), brings truly predictive maintenance (PdM) technology to the industrial manufacturing sector to reduce costly unplanned downtime. The Novity solution is an Industrial Internet of Things (IIoT) technology that uses IIoT sensors and proprietary algorithms to enable industrial manufacturers to see the future health of their production assets. The Novity TruPrognostics\u2122 engine relies on a combination of machine learning and physics-based models of equipment. This allows Novity to predict equipment failures with 90 percent or better accuracy and lead times of months, not weeks or days. \n Novity has an immediate opening for a technical product manager or product development engineer with experience in product management and a strong technical background. The ideal candidate will be passionate about defining and driving a strong product vision within the engineering and product teams. If you are exceptionally adept at both product management work and technical work and would like an opportunity to leverage both skill sets simultaneously, then this opportunity can be a great fit! \n This position is a remote position with a strong preference for West Coast-based candidates due to time zone considerations. \n Technical Responsibilities \n \n Serve as an SME and source of knowledge for equipment operation, maintenance practices, and condition monitoring with respect to typical process control equipment. \n \n \n Document technical specifications for algorithms, including software requirements, hardware/sensor requirements, performance metrics, algorithm functionality, features, and expected outputs. \n  Develop predictive maintenance and condition monitoring solution architectures for typical industrial equipment configurations. \n  Produce internal technical training materials to train sales and marketing team members on algorithm requirements, features, and capabilities. \n  Collaborate with algorithm development engineers and instrumentation engineers to define sensor requirements. \n \n \n Develop prototype prognostics and anomaly detection algorithms, as needed. \n \n Product Management Responsibilities \n \n Work closely with the scientists and engineers in our research and development team to define and productize our predictive maintenance solutions for industrial equipment. \n  Managing Novity's algorithm product roadmap. \n  Perform market research and collect customer requirements through voice of the customer to drive new algorithm development. \n  Develop product marketing content and materials, such as white papers, product specification sheets, brochures, and technical marketing presentations. \n  Write and post technical content on social media (e.g. LinkedIn). \n \n \n Represent Novity as a technical specialist at tradeshows and webinars. \n \n Required Experience \n \n M.S. or Ph.D. in an engineering field (mechanical or chemical engineering preferred, Ph.D. preferred). \n \n \n At least 3 years of experience in one or more of the following: \n \n \n Predictive maintenance or condition monitoring algorithm development for the process industries (oil and gas, chemical, wastewater, etc.). \n  Chemical process modeling. \n  Computational physics (CFD/FEA/Multiphysics) applied to industrial equipment, such as pumps, compressors, heat exchangers, separators, etc. \n \n \n At least 1 year of experience in a technical product management role or closely related role, managing predictive analytics, condition monitoring, or industrial IoT software products. \n \n Preferred Experience and Skills \n \n Experience with process or design failure modes and effects analysis (FMEA). \n  Programming experience in Python, Matlab, or similar scientific computing language. \n  Experience developing technology and product roadmaps. \n  Experience reading process and instrumentation diagrams, process flow diagrams, and similar. \n \n Travel Requirements \n Must be available to travel approximately 10% of the time. \n Eligibility \n This position is only open to applicants currently eligible to work in the United States. \n Additional Information \n The base salary range for this full-time position is $120,000-$200,000 + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on this job posting reflects the minimum and maximum target for a new hire salary for this position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. The high end of the base salary range is reserved for exceptionally qualified candidates only. \n Novity is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, creed, religion, ancestry, national origin, age, gender identity or expression, sex, marital status, sexual orientation, physical or mental disability, use of a guide dog or service animal, military/veteran status, citizenship status, basis of genetic information, or any other group protected by law. People with disabilities who need reasonable accommodation to apply or compete for employment with Novity may request such accommodation(s). \n \n \u00a9 2023 Novity, Inc. All rights reserved. Novity and TruPrognostics are trademarks of Novity in the United States and/or other countries.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "34f109dbf6b014bc": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 58400.0,
        "salary_max": 133000.0,
        "title": "Model Based Systems Engineer, Junior",
        "company": "Booz Allen Hamilton",
        "desc": "Job Description \n \n \n \n \n \n \n \n \n \n \n         Location: \n         \n \n         El Segundo,CA,US \n         \n \n \n \n         Remote Work: \n         \n \n         Hybrid \n         \n \n \n \n         Job Number: \n         \n \n         R0182303\n         \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n         Model Based Systems Engineer, Junior\n           The Opportunity: \n  Are you looking for an opportunity to combine your technical skills with big picture thinking to make an impact to complex aerospace systems? You understand your customer\u2019s environment and how to develop the right systems for their mission. Your ability to translate real-world needs into technical specifications makes you an integral part of delivering a customer-focused engineering solution. \n \n  As a systems engineer on our team, you have the chance to shape tomorrow's systems by leading model-based systems engineering (MBSE) projects. Your customer will trust you to not only design and develop these systems, but also evolve them with advanced technology solutions. On our team, you\u2019ll be able to broaden your skillset into areas like digital transformation. Grow your skills by merging MBSE and systems-of-systems engineering and integration to create MBSE models that drive integration. \n \n  Join us. The world can\u2019t wait. \n \n  You Have: \n \n  Experience with large-scale aerospace systems \n  Experience with digital engineering tools, including Cameo or Magicdraw, Sparx EA, DOORS, or MATLAB \n  Experience defining and supporting MBSE projects \n  Experience using systems and digital engineering approaches to manage system architecture definition, requirements development, or technical baseline deliverables \n  Knowledge of Systems Modeling Languages \n  Secret clearance \n  Bachelor's degree in Science, Technology, Engineering, or Mathematics \n \n \n  Nice If You Have: \n \n  Knowledge of Systems Machine Learning (SysML) and DoD Architecture Framework (DoDAF) \n  Ability to perform multiple systems engineering and program management functions in support of design reviews and requirements verification  \n Ability to support proposals or business development \n  Master's degree in Science, Technology, Engineering, or Mathematics \n  INCOSE Certification \n  OMG Certified Systems Model Professional (OCSMP) Certification \n \n \n  Clearance: \n  Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information ;  Secret clearance is required. \n \n  Create Your Career: \n  Grow With Us \n  Your growth matters to us\u2014that\u2019s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. \n \n  A Place Where You Belong \n  Diverse perspectives cultivate collective ingenuity. Booz Allen\u2019s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you\u2019ll build your community in no time. \n \n  Support Your Well-Being \n  Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we\u2019ll support you as you pursue a balanced, fulfilling life\u2014at work and at home. \n \n  Your Candidate Journey \n  At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we\u2019ve compiled a list of resources so you\u2019ll know what to expect as we forge a connection with you during your journey as a candidate with us. \n \n  Compensation \n  At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen\u2019s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. \n  Salary at Booz Allen is determined by various factors, including but not limited to location, the individual\u2019s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $58,400.00 to $133,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen\u2019s total compensation package for employees.\n          \n  Work Model  Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. \n \n  If this position is listed as remote or hybrid, you\u2019ll periodically work from a Booz Allen or client site facility. \n  If this position is listed as onsite, you\u2019ll work with colleagues and clients in person, as needed for the specific role. \n \n \n  EEO Commitment \n  We\u2019re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change \u2013 no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "519e0aa4d970df8d": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 116751.09,
        "salary_max": 147832.9,
        "title": "Sr. Software Engineer \u2013 Oklahoma",
        "company": "FORTUNE PERSONNEL CONSULTANTS",
        "desc": "Senior Software Engineer: S eeking a Senior Software Engineer who will lead the development of software frameworks and applications through architecture, design, implementation, testing and documentation. The successful candidate will lead or support various project tasks and organize, schedule and task team activities. As a senior engineer you will influence engineering design and software development, supervise other software engineers, and act as a mentor and technical resource. There is a remote work option for this position. \n  Qualifications \n \n v Bachelor\u2019s degree in Computer Science or Computer Engineering \n v Proficiency in Java, JavaScript, Python, SQL, Linux \n v Experience with UI/UX design and development \n v Experience in DevOps tools (Jira, Git, Jenkins) \n v Experience in Software Development Life Cycle Processes \n v Proficiency in MS Office Suite \n v 5+ years relevant professional experience \n v United States Citizenship \n \n Desired Qualifications \n \n v Master\u2019s degree in Computer Science or Computer Engineering  \n v C/C++ programming skills \n v Understanding of Agile Methodologies \n v Experience in Backend design, development, and integration \n v Experience in Software architectures, Data frameworks and Security \n v Understanding of Secure Communications and Message Delivery Tools \n v Production deployment of Machine Learning (ML) models \n \n Duties / Responsibilities \n \n v Lead software development projects to meet customer/product requirements ensuring high quality through unit, functional, integration, and regression testing \n v Document architecture, design, test plan and user guide \n v Deploy software applications to production \n v Participate in design reviews and present your work to the company \n v Track project/product milestones and releases ensuring schedule and budget are met \n v Train & mentor junior engineers, perform code reviews \n v Lead continuous process and productivity improvements in the team \n v Engage in professional development activities to enhance your skillset \n \n Exceptional work ethic, willingness to learn, tenacity not to quit, aptitude to surpass, and strong desire to work in a fast-paced environment are necessary for success. Collaboration and cross pollination with other teams will be frequent, thus communication, openness, and willingness to share both success and failure is a must. A team-centric organization, there are no individuals, we win and lose together.",
        "cleaned_desc": "Senior Software Engineer: S eeking a Senior Software Engineer who will lead the development of software frameworks and applications through architecture, design, implementation, testing and documentation. The successful candidate will lead or support various project tasks and organize, schedule and task team activities. As a senior engineer you will influence engineering design and software development, supervise other software engineers, and act as a mentor and technical resource. There is a remote work option for this position. \n  Qualifications \n \n v Bachelor\u2019s degree in Computer Science or Computer Engineering \n v Proficiency in Java, JavaScript, Python, SQL, Linux \n v Experience with UI/UX design and development ",
        "techs": [
            "java",
            "javascript",
            "python",
            "sql",
            "linux"
        ],
        "cleaned_techs": [
            "java",
            "javascript",
            "python",
            "sql",
            "linux"
        ]
    },
    "182aad49dc53af0d": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Director - ML Engineering",
        "company": "Cisco Systems",
        "desc": "Who We Are \n  \n \n  We are part of the Outshift Group passionate about identifying breakthrough emerging solutions that create new markets and businesses for Cisco. We incubate these new opportunities in partnership with our business groups, partners and other startups. The team also continues to progress Cisco\u2019s important work with Standards bodies and manage research partnerships with leading-edge Universities.\n   \n \n \n \n \n  Our organization is anticipating high growth. We are seeking talent with agility and creativity to explore opportunities and fill in needs as they arise. Applicants should be seeking a flexible role, in which they not only provide leadership in their primary function, but also contribute in meaningful ways to other projects. We are looking for individuals who are excited about adjusting quickly to different domains that may be outside of their normal scope of responsibilities.\n   \n \n \n \n \n  Learn more about us at https://eti.cisco.com.\n   \n \n \n \n \n Who You'll Work With   \n \n \n \n \n  The team is a highly visible group which reports directly to Cisco\u2019s CEO. Outshift focuses on the next wave of innovation by anticipating, investing in, and incubating new technologies and business ventures. As part of the incubation team, you will be responsible for developing new products and bring them to market in a startup-like environment.\n   \n \n \n \n \n What You'll Do   \n \n \n \n \n  In this role you will build and establish AI/ML engineering team. You will be responsible for delivery of new AI/ML products at scale for new markets to drive business growth. As a founding senior leader, you will work across various Cisco teams to create new AI/ML products that make tangible business impacts and contribute to shaping Cisco\u2019s AI strategy for the next decade. Your team will be responsible for evaluation and standardization of AI platform for all AI application development at Cisco.\n   \n \n \n \n \n The Impact You\u2019ll Make   \n \n \n \n \n  If you want the challenge of fast-paced growth, the satisfaction of seeing your thoughtful ideas come to life, and the pride in building a best-in-class data team, this is the place for you. You can help build pioneering full-stack technologies that will create new products by monetizing data.\n   \n \n \n \n \n Your Responsibilities Will Include   \n \n \n \n \n  Build and lead AI/ML engineering team. \n  \n \n  Own end to end delivery of new AI/ML products at scale for new persona and markets to drive business growth. \n  \n \n  Evaluate, perform due diligence, and standardize AI platform for application development. \n  \n \n  Stay updated on emerging technologies and trends in data science and architecture, making informed decisions on technology adoption and investment. \n  \n \n  Collaborate with cross-functional teams, including strategy, marketing, product management, engineering, and sales, to align data initiatives with business objectives.\n   \n \n \n \n \n Minimum Requirements   \n \n \n \n \n  7+ years\u2019 experience in AI, Machines Learning, Automation, Technology Modernization, Generative AI and Cloud development. \n  \n \n  7+ years in a management role leading a team of software engineers, ML engineers, and architects. \n  \n \n  Bachelors degree and 17+ years of Engineering exp OR \n  \n \n  Masters degree and 14+ years of Engineering exp OR \n  \n \n  PhD and 10+ years of Engineering experience\n   \n \n \n \n \n Preferred Requirements   \n \n \n \n \n  Strong leadership skills and the ability to build and lead hard-working teams. \n  \n \n  Extraordinarily resourceful & thorough, you can operate successfully among forward-thinking people. \n  \n \n  Equally comfortable and capable interacting with technologists as with business executives. \n  \n \n  Excellent verbal and written communication skills.\n   \n \n \n \n \n Why Cisco?   \n \n \n \n \n  #WeAreCisco. We are all unique, but collectively we bring our talents to work as a team, to develop innovative technology and power a more inclusive, digital future for everyone. How do we do it? Well, for starters \u2013 with people like you!\n   \n \n \n \n \n  Nearly every internet connection around the world touches Cisco. We\u2019re the Internet\u2019s optimists. Our technology makes sure the data traveling at light speed across connections does so securely, yet it\u2019s not what we make but what we make happen which marks us out. We\u2019re helping those who work in the health service to connect with patients and each other; schools, colleges, and universities to teach in even the most challenging of times. We\u2019re helping businesses of all shapes and sizes to connect with their employees and customers in new ways, providing people with access to the digital skills they need and connecting the most remote parts of the world \u2013 whether through 5G, or otherwise.\n   \n \n \n \n \n  We tackle whatever challenges come our way. We have each other\u2019s backs, we recognize our accomplishments, and we grow together. We celebrate and support one another \u2013 from big and small things in life to big career moments. And giving back is in our DNA (we get 10 days off each year to do just that).\n   \n \n \n \n \n  We know that powering an inclusive future starts with us. Because without diversity and a dedication to equality, there is no moving forward. Our 30 Inclusive Communities, that bring people together around commonalities or passions, are leading the way. Together we\u2019re committed to learning, listening, caring for our communities, whilst supporting the most vulnerable with a collective effort to make this world a better place either with technology, or through our actions.\n   \n \n \n \n \n  So, you have colorful hair? Don\u2019t care. Tattoos? Show off your ink. Like polka dots? That\u2019s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us! #WeAreCisco\n   \n \n \n \n \n  #LI-TA2 \n  \n \n  #LI-Remote\n  \n \n \n \n \n Message to applicants applying to work in the U.S.: \n \n \n \n  When available, the salary range posted for this position reflects the projected hiring range for new hire, full-time salaries in U.S. locations, not including equity or benefits. For non-sales roles the hiring ranges reflect base salary only; employees are also eligible to receive annual bonuses. Hiring ranges for sales positions include base and incentive compensation target. Individual pay is determined by the candidate's hiring location and additional factors, including but not limited to skillset, experience, and relevant education, certifications, or training. Applicants may not be eligible for the full salary range based on their U.S. hiring location. The recruiter can share more details about compensation for the role in your location during the hiring process.\n  \n \n  U.S. employees have \n   access  to quality medical, dental and vision insurance, a 401(k) plan with a Cisco matching contribution, short and long-term disability coverage, basic life insurance and numerous wellbeing offerings. Employees receive up to twelve paid holidays per calendar year, which includes one floating holiday, plus a day off for their birthday. Employees accrue up to 20 days of Paid Time Off (PTO) each year and have access to paid time away to deal with critical or emergency issues without tapping into their PTO. We offer additional paid time to volunteer and give back to the community. Employees are also able to purchase company stock through our Employee Stock Purchase Program.\n  \n \n  Employees on sales plans earn performance-based incentive pay on top of their base salary, which is split between quota and non-quota components. For quota-based incentive pay, Cisco pays at the standard rate of 1% of incentive target for each 1% revenue attainment against the quota up to 100%. Once performance exceeds 100% quota attainment, incentive rates may increase up to five times the standard rate with no cap on incentive compensation. For non-quota-based sales performance elements such as strategic sales objectives, Cisco may pay up to 125% of target. Cisco sales plans do not have a minimum threshold of performance for sales incentive compensation to be paid.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "e130592fb4cbef96": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 166999.69,
        "salary_max": 211458.81,
        "title": "Senior Software Engineer, Fault Handling and Mode Management",
        "company": "Latitude AI",
        "desc": "Latitude AI (lat.ai) is an automated driving technology company developing a hands-free, eyes-off driver assist system for next-generation Ford vehicles at scale. We're driven by the opportunity to reimagine what it's like to drive and make travel safer, less stressful, and more enjoyable for everyone. \n  When you join the Latitude team, you'll work alongside leading experts across machine learning and robotics, cloud platforms, mapping, sensors and compute systems, test operations, systems and safety engineering \u2013  all dedicated to making a real, positive impact on the driving experience for millions of people. \n  As a Ford Motor Company subsidiary, we operate independently to develop automated driving technology at the speed of a technology startup. Latitude is headquartered in Pittsburgh with engineering centers in Dearborn, Mich., and Palo Alto, Calif. \n  Meet the team: \n  The Faults, Signals, and Modes team builds the fault monitoring and mode management software that tells our vehicles whether they're ready for autonomous operation. That includes normal monitoring, like determining whether the system is in the right mode to activate a driver assistance feature, as well as safety-critical fault monitoring to tell the vehicle when it needs to alert the driver or stop driving autonomously to minimize risk. We're building a safety-critical fault monitoring and response system that has to run efficiently and reliably. \n  Our team leverages a variety of backgrounds, ranging from robotics, systems engineering, high performance software development, and embedded and industrial software engineering. We collaborate closely with experts in machine learning, prediction/planning, sensor processing, and simulation from across Latitude AI's product development teams to understand the systems they build, the possible failure modes, and how to detect and respond to those failures. \n  What you'll do: \n \n Build a distributed fault-monitoring software system for a safety-critical application on a resource-constrained hardware platform \n Build a diagnostics and observability software system for non-critical faults \n Build the mode management and monitoring software that handles how our system transitions between operating modes \n Create automated unit and integration tests that verify the software works as expected and meets the requirements \n Analyze logs to root-cause problems found in hardware-in-the-loop tests \n Build tooling to collect metrics and support reliability engineering \n Work with systems engineers to refine requirements into software architecture and designs \n Optimize software performance. We're running state-of-the-art autonomy and machine learning software on an automotive embedded system, so every cycle counts \n \n What you'll need to succeed: \n \n A Bachelor's or Master's Degree in Computer Engineering, Computer Science, Electrical Engineering, Robotics, or a related field and 4 years experience \n Proven experience in developing embedded software or robotics software on a resource-constrained compute platform \n At least 3 years of development experience in Python/C++ environments \n Ability to design software components and systems and clearly communicate your designs through documents, diagrams, and presentations \n Track record of operating effectively on cross-functional product development teams culminating in successful launches/releases/etc \n \n Nice to have: \n \n Experience with Mathworks Simulink \n Experience with code generation or compilers \n Experience creating software tools to analyze logged data and root-cause events \n Experience with ISO-26262 or similar safety standards \n Experience with design and architecture documentation, such as UML and SysML \n Experience with AUTOSAR development \n \n What we offer you: \n \n Competitive compensation packages \n High-quality individual and family medical, dental, and vision insurance \n Health savings account with available employer match \n Employer-matched 401(k) retirement plan with immediate vesting \n Employer-paid group term life insurance and the option to elect voluntary life insurance \n Paid parental leave \n Paid medical leave \n Unlimited vacation \n 13 paid holidays \n Complimentary daily lunches, beverages, and snacks for onsite employees \n Pre-tax spending accounts for healthcare and dependent care expenses \n Pre-tax commuter benefits \n Monthly wellness stipend \n Adoption/Surrogacy support program \n Backup child and elder care program \n Professional development reimbursement \n Employee assistance program \n Discounted programs that include legal services, identity theft protection, pet insurance, and more \n Company and team bonding outlets: employee resource groups, quarterly team activity stipend, and wellness initiatives \n \n Learn more about Latitude's team, mission and career opportunities at lat.ai! \n  Candidates for positions with Latitude AI must be legally authorized to work in the United States on a permanent basis. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is available for this position. \n  We are an Equal Opportunity Employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status. \n  At Latitude, the health and safety of our employees is our top priority. Vaccination has been proven to play a critical role in combatting COVID-19. As a result, Latitude has made the decision to require U.S. employees to be fully vaccinated against COVID-19, unless employees require an accommodation for religious or medical reasons. Being fully vaccinated means that an individual is at least two weeks past their final dose of an authorized COVID-19 vaccine regimen. As a condition of employment, newly hired employees will be required to provide proof of their COVID-19 vaccination or an approved medical or religious exemption.",
        "cleaned_desc": " Create automated unit and integration tests that verify the software works as expected and meets the requirements \n Analyze logs to root-cause problems found in hardware-in-the-loop tests \n Build tooling to collect metrics and support reliability engineering \n Work with systems engineers to refine requirements into software architecture and designs \n Optimize software performance. We're running state-of-the-art autonomy and machine learning software on an automotive embedded system, so every cycle counts \n \n What you'll need to succeed: \n \n A Bachelor's or Master's Degree in Computer Engineering, Computer Science, Electrical Engineering, Robotics, or a related field and 4 years experience \n Proven experience in developing embedded software or robotics software on a resource-constrained compute platform \n At least 3 years of development experience in Python/C++ environments   Ability to design software components and systems and clearly communicate your designs through documents, diagrams, and presentations \n Track record of operating effectively on cross-functional product development teams culminating in successful launches/releases/etc \n \n Nice to have: \n \n Experience with Mathworks Simulink \n Experience with code generation or compilers \n Experience creating software tools to analyze logged data and root-cause events \n Experience with ISO-26262 or similar safety standards \n Experience with design and architecture documentation, such as UML and SysML \n Experience with AUTOSAR development ",
        "techs": [
            "automated unit tests",
            "integration tests",
            "log analysis",
            "tooling",
            "metrics collection",
            "reliability engineering",
            "software architecture",
            "software designs",
            "software performance optimization",
            "automotive embedded system",
            "autonomy software",
            "machine learning software",
            "resource-constrained compute platform",
            "python",
            "c++",
            "software components design",
            "system design",
            "cross-functional product development teams",
            "mathworks simulink",
            "code generation",
            "compilers",
            "software tools",
            "data analysis",
            "root-cause analysis",
            "iso-26262",
            "safety standards",
            "design documentation",
            "uml",
            "sysml",
            "autosar development"
        ],
        "cleaned_techs": [
            "automated unit tests",
            "integration tests",
            "log analysis",
            "tooling",
            "metrics collection",
            "reliability engineering",
            "software architecture",
            "software designs",
            "software performance optimization",
            "automotive embedded system",
            "autonomy software",
            "machine learning software",
            "resource-constrained compute platform",
            "python",
            "c++",
            "software components design",
            "system design",
            "cross-functional product development teams",
            "mathworks simulink",
            "code generation",
            "compilers",
            "software tools",
            "root-cause analysis",
            "iso-26262",
            "safety standards",
            "uml",
            "sysml",
            "autosar development"
        ]
    },
    "91841150b9b5499e": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 100000.0,
        "salary_max": 220000.0,
        "title": "Senior Full Stack Software Engineer - Remote",
        "company": "Reality Defender",
        "desc": "About Reality Defender \n \n \n \n  Reality Defender is a groundbreaking security platform offering comprehensive deepfake detection. A Y Combinator graduate, Comcast NBCUniversal LIFT Labs alumni, and winner of SXSW Pitch 2023, Reality Defender's proactive deepfake and AI-generated content detection technology is developed by a leadership team with over 20 years of experience in applied research at the intersection of machine learning, data science, and cybersecurity.\n  \n \n \n  With models defending against present and future fabrication techniques, Reality Defender is the best way to detect and deter fraudulent text, audio, and visual content, partnering with government agencies and enterprise clients to enhance security and detect fraud.\n  \n \n \n  About the Role \n \n \n \n  Are you passionate about leveraging cutting-edge technology to defend against emerging threats in the digital landscape? Do you thrive in a dynamic, collaborative environment where your skills can make a significant impact? If so, we invite you to join Reality Defender, a leading innovator in the field of deep fake detection and misinformation prevention.\n  \n \n \n  Key Responsibilities \n \n \n \n  As a full stack engineer, you will tackle a wide array of intriguing and challenging problems. Key responsibilities include, but are not limited to:\n  \n \n \n  Backend API Design:  Develop robust and secure backend APIs to provide both public interfaces for external applications and internal APIs for our own dashboards. Your role will involve designing APIs that facilitate seamless communication between our deep fake detection algorithms and external applications, ensuring data integrity and security. Internally, you will create APIs that power our own user interfaces, ensuring a smooth user experience for our clients and team.\n  \n \n \n  Backend Testing and Code Coverage:  Write comprehensive tests for the backend infrastructure, ensuring the reliability and stability of our systems. Your responsibility will also include measuring and improving code coverage, ensuring that our tests are thorough and effective in catching potential issues.\n  \n \n \n  Architect Scalable Inference Pipelines:  Your expertise will be pivotal in architecting and optimizing scalable inference pipelines, harnessing the power of multiple GPUs on AWS, Google Cloud, Azure and On-Premises. This involves crafting efficient algorithms, implementing parallel processing techniques, and ensuring seamless integration with our deep fake detection models. Your focus will be on enhancing the speed and accuracy of our detection mechanisms, enabling real-time analysis of vast data sets.\n  \n \n \n  Scalable Preprocessing Pipeline Design and Implementation:  Delve into the intricacies of crafting preprocessing pipelines essential to our deep fake detection systems. You will handle substantial data loads, ensuring seamless flow from diverse sources to our models even under high concurrent request volumes. Building fault-tolerant, high-throughput pipelines is pivotal, enhancing the adaptability of our machine learning algorithms to diverse data sources and emerging misinformation patterns.\n  \n \n \n  Hybrid Cloud Optimization:  Leverage your hybrid cloud expertise to optimize our infrastructure for reliability, scalability, and cost efficiency. You will be responsible for crafting strategies that seamlessly integrate on-premises and cloud-based resources, ensuring our systems operate efficiently across diverse environments. Your role will involve continuous evaluation of cloud services, selecting the best tools for specific tasks, and orchestrating their seamless collaboration.\n  \n \n \n  Frontend Updates:  Contribute to the development of intuitive and visually appealing user interfaces. Collaborate with UI/UX designers to implement frontend updates that enhance user experience, ensuring our clients and internal users have a seamless experience while interacting with our deep fake detection platform.\n  \n \n \n  Collaborative Problem-Solving:  Engage in collaborative problem-solving with cross-functional teams. You will actively participate in brainstorming sessions, contribute innovative ideas, and work closely with machine learning engineers, data scientists and other developers to address complex challenges. Your ability to collaborate effectively will be instrumental in devising comprehensive solutions that encompass both frontend user experiences and backend computational efficiency.\n  \n \n \n  Continuous Learning and Innovation:  Stay at the forefront of technology trends, exploring emerging tools, frameworks, and methodologies. Your curiosity and enthusiasm for learning will drive the innovation engine within our team, ensuring that Reality Defender remains a leader in the field of deep fake detection and misinformation prevention.\n  \n \n \n  Qualifications and Skills \n \n \n \n  If you are a forward-thinking engineer who thrives on challenges and is excited about revolutionizing the landscape of digital security, we invite you to apply. Join us at Reality Defender, where your skills will be honed, your ideas will be valued, and your contributions will shape the future of deep fake detection technologies. Together, we will defend against digital fraud and uphold the integrity of information in the digital age.\n  \n \n \n  We encourage candidates who may not meet all the specified requirements to still apply.  We value diverse perspectives and skills, and believe that unique experiences can contribute significantly to our team. If you are passionate about the role and confident in your ability to make a meaningful impact, we welcome your application. Your enthusiasm, adaptability, and potential for growth are equally important to us.\n  \n \n \n  Basic Requirements: \n \n \n \n  5+ years of professional experience in software development with a bachelor's or master's degree in computer science, engineering, math, or STEM discipline \n \n \n \n  We are unable to engage with firms due to regulatory constraints \n \n \n \n  Preferred Skills and Experience : \n \n \n \n  Proficiency in NodeJs, Typescript, Go and/or Rust with a strong emphasis on scalable software design and efficient data processing \n \n \n \n  Experience in designing and building scalable inference pipelines, preferably in GPU-accelerated environments on cloud platforms \n \n \n \n  Experience working with Docker and Kubernetes \n \n \n \n  Experience with AWS services, especially Lambda, SQS, DynamoDB, RDS, EKS, and ECS. Preferably also multi cloud and on-premises experience \n \n \n \n  Database experience with PostgreSQL, SQL Server, or similar database technologies \n \n \n \n  Writing and consuming REST and GraphQL \n \n \n \n  Experience in frontend technologies, including TypeScript, Remix, Next.js, Remix, Tailwind CSS \n \n \n \n  Strong expertise in implementing intuitive user interfaces and interactive experiences, ensuring seamless user interactions \n \n \n \n  Experience with version control, continuous integration, and continuous delivery concepts \n \n \n \n  Deep understanding of testing, continuous integration, build, deployment & monitoring \n \n \n \n  Expertise in profiling and improving application performance \n \n \n \n  Established skills in strategic and critical thinking, decision-making, and relationship-building \n \n \n \n  Highly organized, detail-oriented, and possess a proven ability to thrive under deadline pressure \n \n \n \n  Successful experience working in a fast-paced, dynamic, results-oriented team environment \n \n \n \n  Additional Requirements :\n  \n \n \n  Willing to work extended hours when needed \n \n \n \n  #LI-Remote",
        "cleaned_desc": " \n \n  Backend Testing and Code Coverage:  Write comprehensive tests for the backend infrastructure, ensuring the reliability and stability of our systems. Your responsibility will also include measuring and improving code coverage, ensuring that our tests are thorough and effective in catching potential issues.\n  \n \n \n  Architect Scalable Inference Pipelines:  Your expertise will be pivotal in architecting and optimizing scalable inference pipelines, harnessing the power of multiple GPUs on AWS, Google Cloud, Azure and On-Premises. This involves crafting efficient algorithms, implementing parallel processing techniques, and ensuring seamless integration with our deep fake detection models. Your focus will be on enhancing the speed and accuracy of our detection mechanisms, enabling real-time analysis of vast data sets.\n  \n \n \n  Scalable Preprocessing Pipeline Design and Implementation:  Delve into the intricacies of crafting preprocessing pipelines essential to our deep fake detection systems. You will handle substantial data loads, ensuring seamless flow from diverse sources to our models even under high concurrent request volumes. Building fault-tolerant, high-throughput pipelines is pivotal, enhancing the adaptability of our machine learning algorithms to diverse data sources and emerging misinformation patterns.\n  \n \n \n  Hybrid Cloud Optimization:  Leverage your hybrid cloud expertise to optimize our infrastructure for reliability, scalability, and cost efficiency. You will be responsible for crafting strategies that seamlessly integrate on-premises and cloud-based resources, ensuring our systems operate efficiently across diverse environments. Your role will involve continuous evaluation of cloud services, selecting the best tools for specific tasks, and orchestrating their seamless collaboration.\n  \n \n \n  Frontend Updates:  Contribute to the development of intuitive and visually appealing user interfaces. Collaborate with UI/UX designers to implement frontend updates that enhance user experience, ensuring our clients and internal users have a seamless experience while interacting with our deep fake detection platform.\n  \n \n \n  Collaborative Problem-Solving:  Engage in collaborative problem-solving with cross-functional teams. You will actively participate in brainstorming sessions, contribute innovative ideas, and work closely with machine learning engineers, data scientists and other developers to address complex challenges. Your ability to collaborate effectively will be instrumental in devising comprehensive solutions that encompass both frontend user experiences and backend computational efficiency.\n  \n \n \n  Continuous Learning and Innovation:  Stay at the forefront of technology trends, exploring emerging tools, frameworks, and methodologies. Your curiosity and enthusiasm for learning will drive the innovation engine within our team, ensuring that Reality Defender remains a leader in the field of deep fake detection and misinformation prevention.\n  \n \n   \n \n  Experience in designing and building scalable inference pipelines, preferably in GPU-accelerated environments on cloud platforms \n \n \n \n  Experience working with Docker and Kubernetes \n \n \n \n  Experience with AWS services, especially Lambda, SQS, DynamoDB, RDS, EKS, and ECS. Preferably also multi cloud and on-premises experience \n \n \n \n  Database experience with PostgreSQL, SQL Server, or similar database technologies \n \n \n \n  Writing and consuming REST and GraphQL \n \n \n \n  Experience in frontend technologies, including TypeScript, Remix, Next.js, Remix, Tailwind CSS \n \n \n \n  Strong expertise in implementing intuitive user interfaces and interactive experiences, ensuring seamless user interactions \n \n \n ",
        "techs": [
            "backend testing and code coverage",
            "architect scalable inference pipelines",
            "scalable preprocessing pipeline design and implementation",
            "hybrid cloud optimization",
            "frontend updates",
            "collaborative problem-solving",
            "continuous learning and innovation",
            "gpu-accelerated environments",
            "docker and kubernetes",
            "aws services (lambda",
            "sqs",
            "dynamodb",
            "rds",
            "eks",
            "ecs)",
            "postgresql",
            "sql server",
            "rest and graphql",
            "frontend technologies (typescript",
            "remix",
            "next.js",
            "tailwind css)",
            "intuitive user interfaces."
        ],
        "cleaned_techs": [
            "backend testing and code coverage",
            "architect scalable inference pipelines",
            "scalable preprocessing pipeline design and implementation",
            "hybrid cloud optimization",
            "frontend updates",
            "collaborative problem-solving",
            "continuous learning and innovation",
            "gpu-accelerated environments",
            "docker and kubernetes",
            "aws",
            "sqs",
            "dynamodb",
            "rds",
            "eks",
            "ecs)",
            "postgresql",
            "sql",
            "rest and graphql",
            "frontend technologies (typescript",
            "remix",
            "next.js",
            "tailwind css)",
            "intuitive user interfaces."
        ]
    },
    "d60aa31b36ff18a2": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 114368.24,
        "salary_max": 144815.67,
        "title": "Sr. Engineer II, Software Engineering BE",
        "company": "Oak Street Health",
        "desc": "Company:  Oak Street Health    Title:  Sr .  Engineer II, Software Engineer BE   Location:  Remote \n \n  Role Description \n  Our product suite, lovingly known as Canopy, is a transformative clinical operations and population health platform for value-based primary care. Coupling a high-performance tech platform with machine learning-driven analytics, Canopy enables our clinical and service teams to provide excellent care to every single patient. From facilitating the delivery of a visit in our centers to supporting patients that have been admitted to a hospital, there is not a point in the patient journey that Canopy does not touch. At Oak Street Health, you will be influential in the development of innovative products that do more than just building scalable services. Everything you do will have an immediate impact on the health and wellbeing of our patients. \n  As a Software Engineer, you will have an impactful contribution in the delivery of a new kind of platform for healthcare, one built specifically for the clinical team. From design to implementation, you will partner with our stellar engineering, product, and design teams in a fast-paced, agile environment to transform ideas into a reality. Utilizing modern methodologies and open source tools, you will be empowered to push the boundaries of development as we seek to deliver applications that will directly and immediately impact the experience of our teams and our patients. \n \n  There is no better time to join our rapidly growing team at Oak Street Health. We can\u00e2\u20ac\u2122t think of a mission any more motivating; and, with your help, we can drive change in healthcare for the patients that need us most. \n \n  Core Responsibilities: \n \n \n  Contribute in all aspects of SDLC process( SCRUM, Design, Code, Test, Deploy & Maintain) \n  Collaborate with cross functional teams \n  Participate in code reviews \n  Improve overall code quality and maintainability \n  Address Technical Debt, Bugs \n  Mentoring Junior Engineers \n  Implementing SecDevops best practices. \n \n \n  What we\u00e2\u20ac\u2122re looking for \n  We\u00e2\u20ac\u2122re looking for a motivated, experienced Engineer with: \n \n  Bachelors in CS or relevant degree / industry experience \n  10+ years\u00e2\u20ac\u2122 experience building consumer-facing products with large distributed system backend using .NET running in Linux environment. \n  Expert is threading and memory management in building microservices. \n  Expert in building resilient, scalable and secure software. \n  Expert using and doing performance optimization of MongoDb. \n  Expert in building a distributed, event-driven, cloud native environment either in Azure/AWS using Docker/Kubernetes \n  Expert in Design First approach to software development. \n  Expert Domain Driven Design (DDD). \n  Experience with TDD achieving 80% plus code coverage on microservices. \n  Experience working in an Agile/Devops environment \n  Experience with Kafka and Kafka Streams preferred \n  Experience with Event Sourcing preferred \n  Experience with using Protobuf and gRPC preferred \n \n \n  What does being \u00e2\u20ac\u0153Oaky\u00e2\u20ac\u009d look like? \n \n  Radiating positive energy \n  Assuming good intentions \n  Creating an unmatched patient experience \n  Driving clinical excellence \n  Taking ownership and delivering results \n  Being relentlessly determined \n \n \n  Why Oak Street Health? \n \n  Oak Street Health is on a mission to \u00e2\u20ac\u0153Rebuild healthcare as it should be'', providing personalized primary care for older adults on Medicare, with the goal of keeping patients healthy and living life to the fullest. Our innovative care model is centered right in our patient\u00e2\u20ac\u2122s communities, and focused on the quality of care over volume of services. We\u00e2\u20ac\u2122re an organization on the move! With over 150 locations and an ambitious growth trajectory, Oak Street Health is attracting and cultivating team members who embody \u00e2\u20ac\u0153Oaky\u00e2\u20ac\u009d values and passion for our mission. \n \n  Oak Street Health Benefits: \n \n  Mission-focused career impacting change and measurably improving health outcomes for medicare patients \n  Paid vacation, sick time, and investment/retirement 401K match options \n  Health insurance, vision, and dental benefits \n  Opportunities for leadership development and continuing education stipends \n  New centers and flexible work environments \n  Opportunities for high levels of responsibility and rapid advancement \n \n \n  Oak Street Health is an equal opportunity employer. We embrace diversity and encourage all interested readers to apply. \n \n  Learn more at www.oakstreethealth.com/diversity-equity-and-inclusion-at-oak-street-health",
        "cleaned_desc": "  10+ years\u00e2\u20ac\u2122 experience building consumer-facing products with large distributed system backend using .NET running in Linux environment. \n  Expert is threading and memory management in building microservices. \n  Expert in building resilient, scalable and secure software. \n  Expert using and doing performance optimization of MongoDb. \n  Expert in building a distributed, event-driven, cloud native environment either in Azure/AWS using Docker/Kubernetes \n  Expert in Design First approach to software development. \n  Expert Domain Driven Design (DDD). \n  Experience with TDD achieving 80% plus code coverage on microservices. \n  Experience working in an Agile/Devops environment \n  Experience with Kafka and Kafka Streams preferred \n  Experience with Event Sourcing preferred \n  Experience with using Protobuf and gRPC preferred ",
        "techs": [
            ".net",
            "linux",
            "threading",
            "memory management",
            "microservices",
            "mongodb",
            "azure/aws",
            "docker/kubernetes",
            "design first approach",
            "domain driven design (ddd)",
            "tdd",
            "agile/devops",
            "kafka",
            "kafka streams",
            "event sourcing",
            "protobuf",
            "grpc"
        ],
        "cleaned_techs": [
            ".net",
            "linux",
            "threading",
            "memory management",
            "microservices",
            "mongodb",
            "azure",
            "docker/kubernetes",
            "design first approach",
            "domain driven design (ddd)",
            "tdd",
            "agile/devops",
            "kafka",
            "kafka streams",
            "event sourcing",
            "protobuf",
            "grpc"
        ]
    },
    "be44c75176246b65": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 133006.2,
        "salary_max": 168415.48,
        "title": "Senior Front-End Engineer - Fakespot",
        "company": "Mozilla",
        "desc": "To learn the Hiring Ranges for this position, please select your location from the Apply Now dropdown menu. \n  To learn more about our Hiring Range System, please click this link. \n  Why Mozilla? \n  Mozilla Corporation is the non-profit-backed technology company that has shaped the internet for the better over the last 25 years. We make pioneering brands like Firefox, the privacy-minded web browser, and Pocket, a service for keeping up with the best content online. Now, with more than  225  million people around the world using our products each month, we're shaping the next 25 years of technology. Our work focuses on diverse areas including AI, social media, security and more. And we're doing this while never losing our focus on our core mission \u2013 to make the internet better for everyone. \n  The Mozilla Corporation is wholly owned by the non-profit 501(c) Mozilla Foundation. This means we aren't beholden to any shareholders \u2014 only to our mission. Along with  60,000 + volunteer contributors and collaborators all over the world, Mozillians design, build and distribute  open-source  software that enables people to enjoy the internet on their terms. \n  About this team and role: \n  Fakespot is now part of Mozilla, where our mission to bring trust and transparency back to the eCommerce space is now aligned with ensuring the Internet is a global resource, open and accessible to all. Fakespot leverages machine learning and other state-of-the-art technologies to automatically filter out spurious reviews of products and vendors so consumers can make informed purchasing decisions based on real feedback by real people. This role involves being on the frontline of e-commerce and learning what is necessary to protect shoppers from online scams. Developers in this role will be met with fun but challenging puzzles regarding how to process data and present it to the user without compromising their experience or privacy. \n  What you'll do: \n \n Develop front-end extension code to scrape, process, and present e-commerce safety data to our users \n Implement solutions to protect users in the realm of online shopping \n Collaborate with other developers regarding design, implementation, testing, and maintenance \n \n What you'll bring: \n \n Experience working in a fast paced startup environment focused on constant delivery and production system reliability \n Minimum 5 years of experience with JavaScript/TypesScript, CSS, and HTML \n Minimum 2 years of Ruby on Rails experience \n Ability to write efficient algorithms, and clean code \n \n Bonus Points for experience\u2026 \n \n writing and maintaining tests written with Jest, Playwright/Puppeteer \n working on consumer shopping experiences \n writing/maintaining browser extensions \n working with react.js/redux \n \n About Mozilla \n  Mozilla exists to build the Internet as a public resource accessible to all because we believe that open and free is better than closed and controlled. When you work at Mozilla, you give yourself a chance to make a difference in the lives of Web users everywhere. And you give us a chance to make a difference in your life every single day. Join us to work on the Web as the platform and help create more opportunity and innovation for everyone online. \n  Commitment to diversity, equity, inclusion, and belonging \n  Mozilla understands that valuing diverse creative practices and forms of knowledge are crucial to and enrich the company's core mission. We encourage applications from everyone, including members of all equity-seeking communities, such as (but certainly not limited to) women, racialized and Indigenous persons, persons with disabilities, persons of all sexual orientations, gender identities, and expressions. \n  We will ensure that qualified individuals with disabilities are provided reasonable accommodations to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment, as appropriate. Please contact us at hiringaccommodation@mozilla.com to request accommodation. \n  We are an equal opportunity employer. We do not discriminate on the basis of race (including hairstyle and texture), religion (including religious grooming and dress practices), gender, gender identity, gender expression, color, national origin, pregnancy, ancestry, domestic partner status, disability, sexual orientation, age, genetic predisposition, medical condition, marital status, citizenship status, military or veteran status, or any other basis covered by applicable laws. Mozilla will not tolerate discrimination or harassment based on any of these characteristics or any other unlawful behavior, conduct, or purpose. \n  Group: D \n  #LI-DNI \n  Req ID: R2324",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "f65257542c684c2b": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Chief Technology Officer (CTO) \u2013 Equity Based",
        "company": "Inteliparent LLC",
        "desc": "Chief Technology Officer (CTO) \u2013 Equity Based Compensation \n InteliParent is looking for a passionate, innovative and seasoned technology leader to join our mission to improve the mental health of parents and children everywhere. This role will be crucial to building products that are technologically advanced, intuitive to use, valuable and delights the consumer. As the CTO you will play a pivotal role in laying the very foundation of our company, helping to sculpt its identity and chart its course to success. This role is not compensated; however, equity will be provided upon the achievement of set objectives. \n About Us: \n InteliParent is a company with a heartfelt dedication to leverage today\u2019s technology to gift the ability to parent with confidence and raise thriving children by focusing on the parent\u2019s and child\u2019s mental health needs. We are more than just a company; we are a partner in the extraordinary journey of parenthood. \n Our approach is to do so by designing and deploying impactful digital products that are: \n \n parent and child centered \n hyper-personalized \n science-backed, by leveraging research available to-date and a network of specialized clinicians \n AI-powered \n \n Given that we are building solutions to some of the most critical challenges faced by society today (declining mental health of parents and children), we believe that it is paramount to approach our work with: honesty, love, empowerment, fairness, respect, a learning mindset, teamwork and gratitude. \n About the  Chief Technology Officer at InteliParent  Role: \n \n Recruit and manage the technical team, including engineers (in-house or contractor) \n Effectively partner with the CEO/Founder to set the company\u2019s technology vision and strategy \n Lead the development and launch of new products and features \n Stay up-to-date on the latest advancements in AI and machine learning \n Effectively manage relationships with partners and vendors, as needed \n \n MUST-HAVE EXPERIENCE & SKILLS:  \n \n Excellent leadership and interpersonal skills, with a record of effectively leading and partnering with technical teams, including developers, data scientists, and engineers. \n Exceptional communication skills, including communicating clearly and effectively, and able to simplify complex tech concepts for diverse stakeholders (non-technical). \n Proven record of strategic and innovative thinking, with the ability to align technical decisions with the company's short and long-term goals. \n A proven record as CTO or similar leadership role, developing and launching successful AI-powered mobile apps at scale (large datasets, training machine learning models). \n Strong technical background in app development, AI technologies (generative AI, NLP, LLMs, and machine learning); and familiarity with popular AI frameworks and libraries. \n Ability to lead the design of scalable and robust technical architectures. \n Understanding of cybersecurity best practices to anticipate and identify potential tech vulnerabilities and strengthen our infrastructure with robust security measures. \n Familiarity with data privacy regulations (e.g., GDPR, CCPA). \n Knowledge of cloud computing platforms (e.g., AWS, Azure, GCP). \n \n ADDITIONAL DESIRED EXPERIENCE \n \n Prior experience in a startup environment, including the ability to tackle complex, uncertain problems and make informed decisions in ambiguous situations; take calculated risks. \n Prior experience with: \n Programming languages: Python, R, C++, Java, and Scala \n Machine learning frameworks: TensorFlow, PyTorch, and scikit-learn \n Big data processing tools: Hadoop, Spark, and Hive \n Data visualization tools: Tableau, Power BI, and Matplotlib \n \n \n A deep understanding of the R&D process. \n Experience with creating patentable technologies and a track record of innovating and developing unique products or features. \n Experience in fundraising, securing venture capital, or managing investor relationships. \n Prior experience in the health and wellness industry. \n Connections or partnerships within the health and wellness industry. \n \n Our Commitment to You and Why You Should Join Us \n \n An opportunity to make a major impact to a critical issue today: the increased need for a solution to support the mental health of parents and their children. \n Freedom and accountability to develop life-changing consumer products in an industry with tremendous opportunities and demand. \n A pivotal role in laying the very foundation of our company, helping to sculpt its identity and chart its course to success. \n Work with supportive, passionate and dedicated people, and build a company with an amazing culture and vision. \n Flexible schedule. \n Equity as part of the founding team. \n \n Job Types: Part-time, Contract. Likely will change to Full-time as the company grows. \n Work Location: Remote; however, InteliParent is based out of the Washington D.C. area. \n To Apply : \n We are looking to build the best team and a 200 years old company! If this sounds like an amazing opportunity for you (and it is!), send us your resume and cover letter to  info@ InteliParent.com . \n Job Type: Contract \n Benefits: \n \n Flexible schedule \n \n Experience: \n \n IT management: 1 year (Preferred) \n Technology management: 1 year (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": " Recruit and manage the technical team, including engineers (in-house or contractor) \n Effectively partner with the CEO/Founder to set the company\u2019s technology vision and strategy \n Lead the development and launch of new products and features \n Stay up-to-date on the latest advancements in AI and machine learning \n Effectively manage relationships with partners and vendors, as needed \n \n MUST-HAVE EXPERIENCE & SKILLS:  \n \n Excellent leadership and interpersonal skills, with a record of effectively leading and partnering with technical teams, including developers, data scientists, and engineers. \n Exceptional communication skills, including communicating clearly and effectively, and able to simplify complex tech concepts for diverse stakeholders (non-technical). \n Proven record of strategic and innovative thinking, with the ability to align technical decisions with the company's short and long-term goals. \n A proven record as CTO or similar leadership role, developing and launching successful AI-powered mobile apps at scale (large datasets, training machine learning models). \n Strong technical background in app development, AI technologies (generative AI, NLP, LLMs, and machine learning); and familiarity with popular AI frameworks and libraries. \n Ability to lead the design of scalable and robust technical architectures.   Understanding of cybersecurity best practices to anticipate and identify potential tech vulnerabilities and strengthen our infrastructure with robust security measures. \n Familiarity with data privacy regulations (e.g., GDPR, CCPA). \n Knowledge of cloud computing platforms (e.g., AWS, Azure, GCP). \n \n ADDITIONAL DESIRED EXPERIENCE \n \n Prior experience in a startup environment, including the ability to tackle complex, uncertain problems and make informed decisions in ambiguous situations; take calculated risks. \n Prior experience with: \n Programming languages: Python, R, C++, Java, and Scala \n Machine learning frameworks: TensorFlow, PyTorch, and scikit-learn \n Big data processing tools: Hadoop, Spark, and Hive \n Data visualization tools: Tableau, Power BI, and Matplotlib \n \n ",
        "techs": [
            "recruit and manage the technical team",
            "including engineers (in-house or contractor)",
            "ai technologies",
            "generative ai",
            "nlp",
            "llms",
            "machine learning",
            "ai frameworks",
            "app development",
            "cybersecurity best practices",
            "cloud computing platforms",
            "python",
            "r",
            "c++",
            "java",
            "scala",
            "tensorflow",
            "pytorch",
            "scikit-learn",
            "hadoop",
            "spark",
            "hive",
            "tableau",
            "power bi",
            "matplotlib"
        ],
        "cleaned_techs": [
            "recruit and manage the technical team",
            "including engineers (in-house or contractor)",
            "ai",
            "generative ai",
            "nlp",
            "llm",
            "app development",
            "cloud computing platforms",
            "python",
            "r",
            "c++",
            "java",
            "scala",
            "tensorflow",
            "pytorch",
            "scikit-learn",
            "hadoop",
            "spark",
            "hive",
            "tableau",
            "powerbi",
            "matplotlib"
        ]
    },
    "6c693c35654e3237": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 100000.0,
        "salary_max": 200000.0,
        "title": "Senior Forward Deployed Software Engineer - Remote",
        "company": "Reality Defender",
        "desc": "About Reality Defender \n \n \n \n  Reality Defender is a groundbreaking security platform offering comprehensive deepfake detection. A Y Combinator graduate, Comcast NBCUniversal LIFT Labs alumni, and winner of SXSW Pitch 2023, Reality Defender's proactive deepfake and AI-generated content detection technology is developed by a leadership team with over 20 years of experience in applied research at the intersection of machine learning, data science, and cybersecurity.\n  \n \n \n  With models defending against present and future fabrication techniques, Reality Defender is the best way to detect and deter fraudulent text, audio, and visual content, partnering with government agencies and enterprise clients to enhance security and detect fraud.\n  \n \n \n  About the Role \n \n \n \n  Are you motivated by the opportunity to address real-world challenges directly, applying your technical expertise to create a significant impact? Join our innovative team at Reality Defender as a Forward Deployed Software Engineer (FDSE). In this role, you will directly collaborate with our diverse range of clients, understanding their most pressing challenges and crafting tailored detection solutions.\n  \n \n \n  Key Responsibilities \n \n \n \n  As a Forward Deployed Software Engineer, you will play a pivotal role in high-stakes projects, owning end-to-end execution and working in small, dynamic teams. Your responsibilities will include:\n  \n \n \n  Client Collaboration:  Engage directly with clients, understanding their core issues, and collaborating on innovative data and detection solutions. You will communicate complex technical concepts effectively to client executives, ensuring alignment between technical implementations and organizational objectives.\n  \n \n \n  Architectural Design:  Discuss and design robust architectures with fellow engineers. Your role will involve crafting scalable and efficient solutions, leveraging your deep understanding of software engineering principles and data processing techniques while keeping the clients' environment in mind.\n  \n \n \n  Custom Web App Development:  Develop custom web applications tailored to meet specific client requirements. Adapt internally developed solutions to meet clients' needs and continuously contribute to make internal solutions reusable. This role will manage and execute the entire end to end software life cycle. From planning, development to deployment.\n  \n \n \n  Backend API Design:  Help the team to continuously develop robust and secure backend APIs to provide both public interfaces for external applications and internal APIs for our own dashboards. Your role will involve designing APIs that facilitate seamless communication between our deep fake detection algorithms and external applications, ensuring data integrity and security. Internally, you will create APIs that power our own user interfaces, ensuring a smooth user experience for our clients and team.\n  \n \n \n  Strategic Planning:  Collaborate with your team to establish strategic plans, ensuring the successful execution of projects and integrations. Your ability to think critically and strategically will contribute to the formulation of effective project and integration strategies and successful delivery.\n  \n \n \n  Architect Scalable Inference Pipelines:  Your expertise will be pivotal in architecting and optimizing scalable inference pipelines, harnessing the power of multiple GPUs on AWS, Google Cloud, Azure and On-Premises. This involves crafting efficient algorithms, implementing parallel processing techniques, and ensuring seamless integration with our deep fake detection models. Your focus will be on enhancing the speed and accuracy of our detection mechanisms, enabling real-time analysis of vast data sets.\n  \n \n \n  Error Handling:  Develop effective error handling and logging mechanisms to ensure graceful degradation of services during unexpected scenarios. Monitor and analyze logs to proactively identify and address potential issues.\n  \n \n \n  Documentation:  Create and maintain comprehensive technical documentation, including APIs, algorithms, and system architecture. Document code functionalities, usage guidelines, and troubleshooting procedures for reference and knowledge sharing.\n  \n \n \n  Technical Support:  Provide technical support to resolve complex issues escalated from customer support teams. Collaborate with cross-functional teams to diagnose and troubleshoot production incidents.\n  \n \n \n  Collaborative Problem-Solving:  Engage in collaborative problem-solving with cross-functional teams. You will actively participate in brainstorming sessions, contribute innovative ideas, and work closely with machine learning engineers, data scientists and other developers to address complex challenges. Your ability to collaborate effectively will be instrumental in devising comprehensive solutions that encompass both frontend user experiences and backend computational efficiency.\n  \n \n \n  Continuous Learning and Innovation:  Stay at the forefront of technology trends, exploring emerging tools, frameworks, and methodologies. Your curiosity and enthusiasm for learning will drive the innovation engine within our team, ensuring that Reality Defender remains a leader in the field of deep fake detection and misinformation prevention.\n  \n \n \n  Qualifications and Skills \n \n \n \n  If you are a forward-thinking engineer who thrives on challenges and is excited about revolutionizing the landscape of digital security, we invite you to apply. Join us at Reality Defender, where your skills will be honed, your ideas will be valued, and your contributions will shape the future of deep fake detection technologies. Together, we will defend against digital fraud and uphold the integrity of information in the digital age.\n  \n \n \n  We encourage candidates who may not meet all the specified requirements to still apply.  We value diverse perspectives and skills, and believe that unique experiences can contribute significantly to our team. If you are passionate about the role and confident in your ability to make a meaningful impact, we welcome your application. Your enthusiasm, adaptability, and potential for growth are equally important to us.\n  \n \n \n  Basic Requirements \n \n \n \n  5+ years of professional experience in software development with a bachelor's or master's degree in computer science, engineering, math, or STEM discipline \n \n \n \n  We are unable to engage with firms due to regulatory constraints \n \n \n \n  Preferred Skills and Experience \n \n \n \n  Strong communication skills \n \n \n \n  Proficiency in Python, Java, NodeJs, Typescript, Go with a strong emphasis on adapting scalable software solutions to customer needs \n \n \n \n  Experience working with Docker and Kubernetes \n \n \n \n  Experience with AWS, Google Cloud, Azure and On-Premises \n \n \n \n  Database experience with PostgreSQL, SQL Server, NoSQL, or similar database technologies \n \n \n \n  Experience in frontend technologies, like TypeScript, Remix, Next.js, Remix, Tailwind CSS \n \n \n \n  Experience in implementing intuitive user interfaces and interactive experiences, ensuring seamless user interactions \n \n \n \n  Experience with version control, continuous integration, and continuous delivery concepts \n \n \n \n  Deep understanding of testing, continuous integration, build, deployment & monitoring \n \n \n \n  Expertise in profiling and improving application performance \n \n \n \n  Established skills in strategic and critical thinking, decision-making, and relationship-building \n \n \n \n  Highly organized, detail-oriented, and possess a proven ability to thrive under deadline pressure \n \n \n \n  Successful experience working in a fast-paced, dynamic, results-oriented team environment \n \n \n \n  Additional Requirements \n \n \n \n  Willing to work extended hours when needed \n \n \n \n  Willing to occasionally work from or travel to client\u2019s location \n \n \n \n  #LI-Remote",
        "cleaned_desc": "  Architectural Design:  Discuss and design robust architectures with fellow engineers. Your role will involve crafting scalable and efficient solutions, leveraging your deep understanding of software engineering principles and data processing techniques while keeping the clients' environment in mind.\n  \n \n \n  Custom Web App Development:  Develop custom web applications tailored to meet specific client requirements. Adapt internally developed solutions to meet clients' needs and continuously contribute to make internal solutions reusable. This role will manage and execute the entire end to end software life cycle. From planning, development to deployment.\n  \n \n \n  Backend API Design:  Help the team to continuously develop robust and secure backend APIs to provide both public interfaces for external applications and internal APIs for our own dashboards. Your role will involve designing APIs that facilitate seamless communication between our deep fake detection algorithms and external applications, ensuring data integrity and security. Internally, you will create APIs that power our own user interfaces, ensuring a smooth user experience for our clients and team.\n  \n \n \n  Strategic Planning:  Collaborate with your team to establish strategic plans, ensuring the successful execution of projects and integrations. Your ability to think critically and strategically will contribute to the formulation of effective project and integration strategies and successful delivery.\n  \n \n \n  Architect Scalable Inference Pipelines:  Your expertise will be pivotal in architecting and optimizing scalable inference pipelines, harnessing the power of multiple GPUs on AWS, Google Cloud, Azure and On-Premises. This involves crafting efficient algorithms, implementing parallel processing techniques, and ensuring seamless integration with our deep fake detection models. Your focus will be on enhancing the speed and accuracy of our detection mechanisms, enabling real-time analysis of vast data sets.\n  \n \n \n  Error Handling:  Develop effective error handling and logging mechanisms to ensure graceful degradation of services during unexpected scenarios. Monitor and analyze logs to proactively identify and address potential issues.\n  \n \n \n  Documentation:  Create and maintain comprehensive technical documentation, including APIs, algorithms, and system architecture. Document code functionalities, usage guidelines, and troubleshooting procedures for reference and knowledge sharing.\n  \n \n \n  Technical Support:  Provide technical support to resolve complex issues escalated from customer support teams. Collaborate with cross-functional teams to diagnose and troubleshoot production incidents.\n  \n \n    Preferred Skills and Experience \n \n \n \n  Strong communication skills \n \n \n \n  Proficiency in Python, Java, NodeJs, Typescript, Go with a strong emphasis on adapting scalable software solutions to customer needs \n \n \n \n  Experience working with Docker and Kubernetes \n \n \n \n  Experience with AWS, Google Cloud, Azure and On-Premises \n \n \n \n  Database experience with PostgreSQL, SQL Server, NoSQL, or similar database technologies \n \n \n \n  Experience in frontend technologies, like TypeScript, Remix, Next.js, Remix, Tailwind CSS \n \n \n \n  Experience in implementing intuitive user interfaces and interactive experiences, ensuring seamless user interactions \n \n \n ",
        "techs": [
            "architectural design",
            "custom web app development",
            "backend api design",
            "strategic planning",
            "architect scalable inference pipelines",
            "error handling",
            "documentation",
            "technical support",
            "python",
            "java",
            "nodejs",
            "typescript",
            "go",
            "docker",
            "kubernetes",
            "aws",
            "google cloud",
            "azure",
            "on-premises",
            "postgresql",
            "sql server",
            "nosql",
            "typescript",
            "remix",
            "next.js",
            "tailwind css"
        ],
        "cleaned_techs": [
            "architectural design",
            "custom web app development",
            "backend api design",
            "strategic planning",
            "architect scalable inference pipelines",
            "error handling",
            "technical support",
            "python",
            "java",
            "nodejs",
            "typescript",
            "go",
            "docker",
            "kubernetes",
            "aws",
            "gcp",
            "azure",
            "on-premises",
            "postgresql",
            "sql",
            "nosql",
            "remix",
            "next.js",
            "tailwind css"
        ]
    },
    "e8b510366f325810": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Director of Technology - U.S. Citizenship Required",
        "company": "Ardent",
        "desc": "Why do you need to choose between doing important work and having a fulfilling life?  At Ardent , we have both. Ardent employees are committed to solving our customers' most difficult problems\u2014and we are committed to the well-being, personal goals, and professional development of our employee. We are \"All In.\" We put forth our strongest effort possible to get the mission accomplished and we do it together. We respect the skills and experience you bring to the Ardent team. And we provide a rewarding environment to help you succeed. \n  We offer highly competitive benefits, professional development opportunities, and an exceptional culture that embraces flexibility, innovation, collaboration, and career growth. A collective service mindset underpins our work, and a shared camaraderie to serve clients, colleagues and our communities set us apart. Our full commitment to being \"All In\" for our employees and our clients is not just our approach, it is our standard. If this sounds like the perfect fit for you, choose Ardent and make a difference with us. \n \n Ardent  is seeking a  Director of Technology  to join our team. \n  This is a  Remote position  but will require regular travel to the  Washington, D.C. Metro area  and/or client sites as needed. \n  Position Description: \n  Ardent  is seeking a passionate  Director of Technology  who is experienced in performing and leading complex cloud-based solutions and products for Federal needs. Our  Director  will lead corporate innovation, go-to-market strategies, and proposal solution development in collaboration with the best business and technical strategists and top engineers. When needed, you may lead-by-example system architecture work for a complex enterprise system/product with emphasis on scalability and efficiency by adhering to enterprise architecture framework. \n  Responsibilities: \n \n \n Working with the government, vendors, and teaming partners to integrate systems for the overall success. \n Creating effective business and technical cloud and DevSecOps strategies and solutions that drive growth and execution. \n Leading solutions and technical proposal responses, ultimately responsible for high quality and compliant technical volumes. \n Identifies and owns entire technical solution requirements in the development of enterprise-wide data-driven information systems. \n Creates specific technical design, product and vendor selection, application, and technical architectures. \n Provides subject matter expertise on applicable technologies and platforms and leads decision process to identify the best options. \n Ensures strategic alignment of technical design and architecture to meet business growth and direction and stay on top of emerging technologies. \n Develop and design complex solutions, through collaboration, across a variety of cloud, applications data, data-centric applications, AI/ML and/or Big-Data cloud architectures. \n Hands-On Solution Architect, owning and responsible for complete technical solutions. \n Lead an organization of machine learning, software, DevOps and data engineers to deliver on an ambitious agenda around building an industry leading machine learning tools and products. \n Recruit, mentor, and retain top engineering talent. \n \n Qualifications: \n \n \n Bachelor's Degree in Computer Science, Engineering, Math, Business Administration or relevant field \n 7+ years of experience in enterprise architecture, designing and architecting (minimum of 3+ categories): large cloud scale applications, data-centric engineering, cloud engineering, AI/ML, and/or DevSecOps solutions \n Industry experience in the Federal government consulting environment \n 5+ years of experience in decomposing application components and determining how to leverage various technologies found with modern cloud services \n 7+ years implementing engineering best practices, iterative/continuous engineering principles \n Experienced designing cost-optimized cloud scalable solutions \n Experienced fixing architecture issues that impact scalability and performance for large systems with large datasets. \n Experience with Agile project collaboration tools (e.g. Atlassian Jira, Confluence) \n Excellent verbal and written communication \n Experience working in an Agile or SAFe environment \n Ability to obtain and maintain a DHS clearance; residing in the United States \n \n Due to the nature of the work we support, all candidates in consideration for this role must be U.S. Citizens willing to undergo the government issued background investigation process. \n \n Ardent  is an equal opportunity employer. We will not discriminate and will take affirmative action measures to ensure against discrimination in employment, recruitment, advertisements for employment, compensation, termination, upgrading, promotions, and other conditions of employment against any employee or job applicant on the bases of race, color, gender, national origin, age, religion, creed, disability, veteran's status, sexual orientation, gender identity or gender expression.",
        "cleaned_desc": " Creates specific technical design, product and vendor selection, application, and technical architectures. \n Provides subject matter expertise on applicable technologies and platforms and leads decision process to identify the best options. \n Ensures strategic alignment of technical design and architecture to meet business growth and direction and stay on top of emerging technologies. \n Develop and design complex solutions, through collaboration, across a variety of cloud, applications data, data-centric applications, AI/ML and/or Big-Data cloud architectures. \n Hands-On Solution Architect, owning and responsible for complete technical solutions. \n Lead an organization of machine learning, software, DevOps and data engineers to deliver on an ambitious agenda around building an industry leading machine learning tools and products. \n Recruit, mentor, and retain top engineering talent.   \n Qualifications: \n \n \n Bachelor's Degree in Computer Science, Engineering, Math, Business Administration or relevant field \n 7+ years of experience in enterprise architecture, designing and architecting (minimum of 3+ categories): large cloud scale applications, data-centric engineering, cloud engineering, AI/ML, and/or DevSecOps solutions \n Industry experience in the Federal government consulting environment   5+ years of experience in decomposing application components and determining how to leverage various technologies found with modern cloud services \n 7+ years implementing engineering best practices, iterative/continuous engineering principles \n Experienced designing cost-optimized cloud scalable solutions \n Experienced fixing architecture issues that impact scalability and performance for large systems with large datasets. \n Experience with Agile project collaboration tools (e.g. Atlassian Jira, Confluence) \n Excellent verbal and written communication \n Experience working in an Agile or SAFe environment ",
        "techs": [
            "technical design",
            "product and vendor selection",
            "application architectures",
            "technical architectures",
            "applicable technologies and platforms",
            "cloud architectures",
            "data-centric applications",
            "ai/ml",
            "big-data cloud architectures",
            "enterprise architecture",
            "large cloud scale applications",
            "data-centric engineering",
            "cloud engineering",
            "ai/ml",
            "devsecops solutions",
            "federal government consulting environment",
            "decomposing application components",
            "modern cloud services",
            "engineering best practices",
            "iterative/continuous engineering principles",
            "cost-optimized cloud scalable solutions",
            "architecture issues",
            "scalability",
            "performance",
            "large systems",
            "large datasets",
            "agile project collaboration tools",
            "atlassian jira",
            "confluence",
            "verbal communication",
            "written communication",
            "agile environment",
            "safe environment"
        ],
        "cleaned_techs": [
            "technical design",
            "product and vendor selection",
            "application architectures",
            "technical architectures",
            "applicable technologies and platforms",
            "cloud architectures",
            "ai",
            "big-data cloud architectures",
            "enterprise architecture",
            "data-centric engineering",
            "cloud engineering",
            "devsecops solutions",
            "federal government consulting environment",
            "decomposing application components",
            "modern cloud services",
            "iterative/continuous engineering principles",
            "cost-optimized cloud scalable solutions",
            "architecture issues",
            "scalability",
            "performance",
            "large systems",
            "large datasets",
            "agile project collaboration tools",
            "atlassian jira",
            "confluence",
            "verbal communication",
            "written communication",
            "agile environment",
            "safe environment"
        ]
    },
    "137c6a62938c2bad": {
        "terms": [
            "machine learning engineer",
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Sr Distinguished Engineer, Generative AI Systems - (Remote- Eligible)",
        "company": "Capital One",
        "desc": "Locations: VA - McLean, United States of America, McLean, Virginia\n   Sr Distinguished Engineer, Generative AI Systems - (Remote- Eligible)\n  \n  Sr Distinguished Engineer, Generative AI Systems \n \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Senior Distinguished Engineer, AI Systems, to help us build the foundations of our enterprise AI Capabilities. You will work on a wide range of initiatives, whether that\u2019s designing robust, secure infrastructure, building large-scale distributed training clusters, deploying LLMs on GPU instances for real-time use cases, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work with a team of AI engineers and researchers to envision the target state of our capabilities while helping to design and implement key services. Examples of projects you will work on include: \n \n \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build infrastructure for serving large ML models, in our public cloud. \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 9 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 6 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud. \n \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $315,100 - $359,700 for Sr Distinguished Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $333,900 - $381,000 for Sr Distinguished Machine Learning Engineer\n   Remote (Regardless of Location): $267,100 - $304,800 for Sr Distinguished Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 9 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 6 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud.   \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n ",
        "techs": [
            "llms and fms",
            "mlops",
            "python",
            "c/c++",
            "ml development frameworks",
            "pytorch",
            "tensorflow",
            "lightning"
        ],
        "cleaned_techs": [
            "llm",
            "mlops",
            "python",
            "c/c++",
            "ml development frameworks",
            "pytorch",
            "tensorflow",
            "lightning"
        ]
    },
    "67ea6df1e77f57e7": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 133006.2,
        "salary_max": 168415.48,
        "title": "Senior Front-End Engineer - Fakespot",
        "company": "Mozilla",
        "desc": "To learn the Hiring Ranges for this position, please select your location from the Apply Now dropdown menu. \n  To learn more about our Hiring Range System, please click this link. \n  Why Mozilla? \n  Mozilla Corporation is the non-profit-backed technology company that has shaped the internet for the better over the last 25 years. We make pioneering brands like Firefox, the privacy-minded web browser, and Pocket, a service for keeping up with the best content online. Now, with more than  225  million people around the world using our products each month, we're shaping the next 25 years of technology. Our work focuses on diverse areas including AI, social media, security and more. And we're doing this while never losing our focus on our core mission \u2013 to make the internet better for everyone. \n  The Mozilla Corporation is wholly owned by the non-profit 501(c) Mozilla Foundation. This means we aren't beholden to any shareholders \u2014 only to our mission. Along with  60,000 + volunteer contributors and collaborators all over the world, Mozillians design, build and distribute  open-source  software that enables people to enjoy the internet on their terms. \n  About this team and role: \n  Fakespot is now part of Mozilla, where our mission to bring trust and transparency back to the eCommerce space is now aligned with ensuring the Internet is a global resource, open and accessible to all. Fakespot leverages machine learning and other state-of-the-art technologies to automatically filter out spurious reviews of products and vendors so consumers can make informed purchasing decisions based on real feedback by real people. This role involves being on the frontline of e-commerce and learning what is necessary to protect shoppers from online scams. Developers in this role will be met with fun but challenging puzzles regarding how to process data and present it to the user without compromising their experience or privacy. \n  What you'll do: \n \n Develop front-end extension code to scrape, process, and present e-commerce safety data to our users \n Implement solutions to protect users in the realm of online shopping \n Collaborate with other developers regarding design, implementation, testing, and maintenance \n \n What you'll bring: \n \n Experience working in a fast paced startup environment focused on constant delivery and production system reliability \n Minimum 5 years of experience with JavaScript/TypesScript, CSS, and HTML \n Minimum 2 years of Ruby on Rails experience \n Ability to write efficient algorithms, and clean code \n \n Bonus Points for experience\u2026 \n \n writing and maintaining tests written with Jest, Playwright/Puppeteer \n working on consumer shopping experiences \n writing/maintaining browser extensions \n working with react.js/redux \n \n About Mozilla \n  Mozilla exists to build the Internet as a public resource accessible to all because we believe that open and free is better than closed and controlled. When you work at Mozilla, you give yourself a chance to make a difference in the lives of Web users everywhere. And you give us a chance to make a difference in your life every single day. Join us to work on the Web as the platform and help create more opportunity and innovation for everyone online. \n  Commitment to diversity, equity, inclusion, and belonging \n  Mozilla understands that valuing diverse creative practices and forms of knowledge are crucial to and enrich the company's core mission. We encourage applications from everyone, including members of all equity-seeking communities, such as (but certainly not limited to) women, racialized and Indigenous persons, persons with disabilities, persons of all sexual orientations, gender identities, and expressions. \n  We will ensure that qualified individuals with disabilities are provided reasonable accommodations to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment, as appropriate. Please contact us at hiringaccommodation@mozilla.com to request accommodation. \n  We are an equal opportunity employer. We do not discriminate on the basis of race (including hairstyle and texture), religion (including religious grooming and dress practices), gender, gender identity, gender expression, color, national origin, pregnancy, ancestry, domestic partner status, disability, sexual orientation, age, genetic predisposition, medical condition, marital status, citizenship status, military or veteran status, or any other basis covered by applicable laws. Mozilla will not tolerate discrimination or harassment based on any of these characteristics or any other unlawful behavior, conduct, or purpose. \n  Group: D \n  #LI-REMOTE \n  Req ID: R2324 \n \n \n \n \n To learn more about our Hiring Range System, please click this  link. \n  Hiring Ranges: \n \n  US Tier 1 Locations \n \n    $137,000\u2014$199,000 USD\n   \n  US Tier 2 Locations \n \n    $126,000\u2014$183,000 USD\n   \n  US Tier 3 Locations \n \n    $116,000\u2014$169,000 USD",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "d891c9055c3dbcc1": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)",
        "company": "Capital One",
        "desc": "314 Main Street (21020), United States of America, Cambridge, Massachusetts\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity. \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 8 years of experience designing and building data-intensive solutions using distributed computing \n  At least 4 years of experience with HPCs, vector embedding, or semantic search technologies \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 3 years of experience building, scaling, and optimizing training and inferencing systems for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Machine Learning Engineer\n   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "314 Main Street (21020), United States of America, Cambridge, Massachusetts\n   Senior Lead Engineer - Generative AI Infrastructure (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Sr. Lead Engineer, Generative AI Infrastructure to help us build the foundations of our AI capabilities. You will work on a wide range of initiatives, whether that\u2019s building large-scale distributed training clusters, or deploying LLMs on GPU instances for real-time applications and decisioning systems, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work closely with our cloud and container infrastructure teams as well as our world-class team of AI researchers to design and implement key capabilities. Examples of projects you will work on: \n \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build run-time infrastructure for serving large ML models such as LLMs and FMs in our public cloud. \n  Build infrastructure for deploying search indexes and embeddings in vector databases that will work closely with the rest of our capabilities. \n    Preferred Qualifications: \n \n  Master's or Doctoral degree in Computer science, Computer Engineering, Electrical engineering, Mathematics, or a similar field. \n  Background in machine learning with experience in large scale training and deployment of deep neural nets and/or transformer architectures. \n  Experience with machine learning frameworks such as TensorFlow or Pytorch, Lightning, Mosaic ML etc. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. ",
        "techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ],
        "cleaned_techs": [
            "tensorflow",
            "pytorch",
            "lightning",
            "mosaic ml"
        ]
    },
    "e7955a8fa0eb785c": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 66.0,
        "salary_max": 68.0,
        "title": "Senior Software Engineer",
        "company": "Brooksource",
        "desc": "Job Title: Senior Software Engineer \n Duration: 3-6 month Contract \n Location: Remote (Must be willing to work 9-5 EST) \n Company Overview: \n Brooksource is a looking for a Senior Software Engineer to join our Fortune 20 client\u2019s ML Ops team as they replatform a current tool. We are seeking a talented Senior Software Engineer with 3-5 years of experience in Python, Java, SQL, Angular, GCP and Tableau to join our team for a short-term project, lasting 3-6 months. In this role, you will play a key part in re-platforming a legacy Java application into Python and Angular, while working extensively with SQL and Tableau to improve data visualization and analytics capabilities. \n Key Responsibilities: \n Collaborate with cross-functional teams to understand the existing Java platform's architecture and identify areas that require updates and improvements. \n Develop and maintain Python-based solutions to modernize the legacy Java codebase, optimizing performance and functionality. \n Work closely with the SQL database, design efficient queries, and implement data transformations to support the project's goals. \n Leverage Tableau for data visualization, creating interactive dashboards and reports to facilitate decision-making. \n Troubleshoot and debug issues in the codebase and database, implementing necessary fixes and improvements. \n Ensure code quality and maintainability through unit testing and code reviews. \n Document the project's progress, code changes, and system updates for knowledge transfer and future reference. \n Collaborate with other team members to ensure the successful delivery of the project within the specified timeframe. \n Qualifications: \n Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field. \n 3-5 years of proven experience as a Software Engineer, with a strong background in Python development. \n Experience in Java, with the ability to understand and update legacy Java code and retire Java platforms. \n Proficiency in SQL, including the ability to design complex queries and optimize database performance. \n Experience with Google Cloud Platform is strongly recommended and its tools such as VertexAI and BitQuery. \n Hands-on experience with Tableau for data visualization and reporting. \n Understanding of Machine Learning Models and ongoing Maintenace with existing models \n Strong problem-solving skills and the ability to debug and troubleshoot issues effectively. \n Excellent communication and collaboration skills, with the ability to work effectively in a team. \n Self-motivated and able to work independently to meet project milestones. \n Familiarity with software development best practices, including code version control and testing. \n Experience with modern software development methodologies, such as Agile or Scrum. \n If you are a talented Senior Software Engineer with 3-5 years of experience and a passion for modernizing legacy systems using Python, Java, SQL, and Tableau, we invite you to apply for this short-term project. Join us in our mission to deliver innovative solutions and drive meaningful change for our clients. \n Eight Eleven Group provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws. \n Job Type: Contract \n Pay: $66.00 - $68.00 per hour \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Experience: \n \n Python: 4 years (Required) \n SQL: 4 years (Required) \n \n Work Location: Remote",
        "cleaned_desc": " Work closely with the SQL database, design efficient queries, and implement data transformations to support the project's goals. \n Leverage Tableau for data visualization, creating interactive dashboards and reports to facilitate decision-making. \n Troubleshoot and debug issues in the codebase and database, implementing necessary fixes and improvements. \n Ensure code quality and maintainability through unit testing and code reviews. \n Document the project's progress, code changes, and system updates for knowledge transfer and future reference. \n Collaborate with other team members to ensure the successful delivery of the project within the specified timeframe. \n Qualifications: \n Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field.   3-5 years of proven experience as a Software Engineer, with a strong background in Python development. \n Experience in Java, with the ability to understand and update legacy Java code and retire Java platforms. \n Proficiency in SQL, including the ability to design complex queries and optimize database performance. \n Experience with Google Cloud Platform is strongly recommended and its tools such as VertexAI and BitQuery. \n Hands-on experience with Tableau for data visualization and reporting. \n Understanding of Machine Learning Models and ongoing Maintenace with existing models \n Strong problem-solving skills and the ability to debug and troubleshoot issues effectively. \n Excellent communication and collaboration skills, with the ability to work effectively in a team. ",
        "techs": [
            "sql database",
            "tableau",
            "python",
            "java",
            "google cloud platform",
            "vertexai",
            "bitquery",
            "tableau"
        ],
        "cleaned_techs": [
            "sql",
            "tableau",
            "python",
            "java",
            "gcp",
            "vertexai",
            "bitquery"
        ]
    },
    "d996f083bf1bfb8d": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": 83000.0,
        "salary_max": 138200.0,
        "title": "Product Analyst II - Data & Analytics",
        "company": "DICK'S Sporting Goods",
        "desc": "At  DICK\u2019S Sporting Goods , we believe in how positively sports can change lives. On our team, everyone plays a critical role in creating confidence and excitement by personally equipping all athletes to achieve their dreams. We are committed to creating an inclusive and diverse workforce, reflecting the communities we serve. \n  If you are ready to make a difference as part of the world\u2019s greatest sports team, apply to join our team today! \n OVERVIEW: \n Welcome to Product Management at DICK\u2019S Sporting Goods, where we specialize in delivering retail excellence along every aspect of the customer journey. Our mission focuses on crafting intuitive, easy-to-use products that span across our business. We are looking for a new team member to join our technology team, partnering with product teams to bring product-market viability ideas that inspire, innovate and deliver elevated experiences for our customers and products. \n This role is part of the Data & Analytics (D&A) portfolio, where we provide industry-leading data services and capabilities that empower DICK'S Sporting Goods to deliver outcomes and grow our business. D&A builds, buys and manages all things data \u2013 platforms, foundation, products plus reporting and analytics.  \n We live, breathe, eat and sleep data, and are passionate about ensuring the business has the data assets they need to make quick and informed decisions. Our team is comprised of product management, architecture, engineering, program management and data analysts \u2013 all coming together to uncover innovative (but scalable!) ways to solve DICK'S Sporting Goods' toughest data challenges. \n As a Product Analyst II, you will work closely with product managers, product designers and software engineers to deliver products and services that align to strategic priorities as well as achieve product-market fit. You will be an active participant in the organization and deliver high-quality products through the following: \n Analytical Services  As a Product Analyst II, you will support strategic analytic services in partnership with technology and business teammates. You will work within a portfolio domain and function to develop an in-depth understanding of the business, processes, services and operations. \n Data Steward  Where appropriate, you will act as a technical data steward for priority data domains where you will support efforts in documenting data definitions and key business metric calculations. This role is a critical enabler to ensuring the data moving through the system is governed and accurate. This role is a critical enabler to ensuring the data moving through the system is governed and accurate within data domains. \n Technical Requirements  The Product Analyst II role partners closely with product managers to understand the strategic vision and roadmap of a product. You will work with the product manager on your team to identify and document technical specifications for product development. You will work with software engineering, data engineering and architecture to determine how to develop the solution to meet the product and customer expectations. As part of the team, you are accountable to partner with software engineering and product management on the testing and acceptance of technical requirements before shipping to production. \n Operations Support  You will implement maintenance and efficiency procedures, monitor product health and stability, gather system statistics and troubleshoot reported errors. Working back with the product team to ensure prioritization and resolution as quickly as possible. \n Job Duties & Responsibilities \n Strategy \u2013 How you understand and plan within a product team that solves customer problems and drives market impact \n Embrace Curiosity and Continuous Learning \n \n Collects and organizes data to solve problems or answer questions   \n Asks insightful questions to drive actionable insights   \n Partners with the product management organization to share insights that inform product roadmaps   \n Works closely with Product Management and Product Design to define opportunities for improvement within product portfolio   \n Solves problems with a fresh perspective rooted in data   \n \n Execution \u2013 How you support and provide oversight to the design and delivery of solutions to our customers \n The Product Analyst II role works with a product team, supporting day-to-day operations as follows: \n Effectively Communicate Data \n \n Transforms meaningful data into digestible and actionable insights   \n Provides critical analysis without bias or expectations to inform product strategy   \n Uses data analysis to effectively communicate the \u201cwhy\u201d   \n Creates methodical and systemic guides for appropriate use and transformation of data   \n Uses graphs, infographics and other methods to visualize data and support KPI measurement   \n \n Be a Steward of Data \n \n Monitors and plans for appropriate compliance requirements   \n Acts as a data steward and implements data governance requirements   \n \n Execute & Deliver \n \n Performs ad hoc analytics to resolve and inform accurate requirements gathering   \n Documents data definitions and key business metric calculations   \n Documents technical requirements in user story format for data engineering, machine learning engineering and software engineering teams to execute against   \n Performs quality assurance testing on technical specifications to confirm feature acceptance   \n Mines and merges data to develop insights on key business and technology challenges   \n \n #LI-JN1 \n QUALIFICATIONS: \n Candidates for this role should have: \n \n Experience working with product management, product design and software engineering in a modern development environment   \n Understanding of software development lifecycle   \n Foundational understanding of Scrum and Agile methodologies   \n Knowledge of business intelligence tooling and code base analytical tools   \n Advanced understanding of metrics and monitoring methodologies   \n Experience in building data requirements, data models and source to target mappings   \n Experience as a data steward and executing on data governance requirements   \n Comfortable working, combining and analyzing data from different source and maturity levels   \n Use of graphs, infographics and other methods to visualize data and support KPI measurement   \n Ability to navigate and collaborate across technology and business partners   \n Strong written and verbal communication skills   \n Excellent organizational skills   \n Strong deductive reasoning skills   \n Curious attitude and desire to learn   \n \n This is a fully remote position \n  This role is not eligible for immigration sponsorship. \n Targeted Pay Range: $83,000 \u2013 $138,200. This is part of a competitive total rewards package that could include other components such as: incentive, equity and benefits. Individual pay is determined by a number of factors including experience, location, internal pay equity, and other relevant business considerations. We review all teammate pay regularly to ensure competitive and equitable pay.",
        "cleaned_desc": " Documents data definitions and key business metric calculations   \n Documents technical requirements in user story format for data engineering, machine learning engineering and software engineering teams to execute against   \n Performs quality assurance testing on technical specifications to confirm feature acceptance   \n Mines and merges data to develop insights on key business and technology challenges   \n \n #LI-JN1 \n QUALIFICATIONS: \n Candidates for this role should have: \n \n Experience working with product management, product design and software engineering in a modern development environment   \n Understanding of software development lifecycle   \n Foundational understanding of Scrum and Agile methodologies   \n Knowledge of business intelligence tooling and code base analytical tools     Advanced understanding of metrics and monitoring methodologies   \n Experience in building data requirements, data models and source to target mappings   \n Experience as a data steward and executing on data governance requirements   \n Comfortable working, combining and analyzing data from different source and maturity levels   \n Use of graphs, infographics and other methods to visualize data and support KPI measurement   \n Ability to navigate and collaborate across technology and business partners   \n Strong written and verbal communication skills   \n Excellent organizational skills   \n Strong deductive reasoning skills   \n Curious attitude and desire to learn   \n \n This is a fully remote position \n  This role is not eligible for immigration sponsorship. ",
        "techs": [
            "scrum",
            "agile methodologies",
            "business intelligence tooling",
            "code base analytical tools",
            "metrics and monitoring methodologies",
            "data requirements",
            "data models",
            "source to target mappings",
            "data steward",
            "data governance requirements",
            "graphs",
            "infographics",
            "kpi measurement",
            "technology and business partners collaboration",
            "written and verbal communication skills",
            "organizational skills",
            "deductive reasoning skills"
        ],
        "cleaned_techs": [
            "scrum",
            "agile methodologies",
            "business intelligence tooling",
            "code base analytical tools",
            "metrics and monitoring methodologies",
            "data requirements",
            "data models",
            "source to target mappings",
            "data steward",
            "data governance requirements",
            "graphs",
            "infographics",
            "kpi measurement",
            "technology and business partners collaboration"
        ]
    },
    "2a17410aa23e4a32": {
        "terms": [
            "machine learning engineer"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Lead Engineer - Generative AI Product Engineering (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Center 1 (19052), United States of America, McLean, Virginia\n   Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Lead Generative AI Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services that enable real-time customer-facing applications.  \n \n Examples of projects you will work on include: \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience. \n  Enable our users to build new GenAI capabilities. \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field \n  At least 6 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 2 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n \n \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  At this time, Capital One will not sponsor a new applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $197,400 - $225,300 for Lead Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $209,200 - $238,700 for Lead Machine Learning Engineer\n   Remote (Regardless of Location): $167,400 - $191,000 for Lead Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "Center 1 (19052), United States of America, McLean, Virginia\n   Lead Engineer - Generative AI Product Engineering (Remote-Eligible)\n  \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Lead Generative AI Engineer to help build and maintain APIs and SDKs to train, fine-tune and access AI models at scale. You will work as part of our Enterprise AI team and build systems that will enable our users to work with Large-Language Models (LLMs) and Foundation Models (FMs), using our public cloud infrastructure. You will work with a team of world-class AI engineers and researchers to design and implement key API products and services that enable real-time customer-facing applications.  \n \n Examples of projects you will work on include: \n \n  Architect, build and deploy well-managed core APIs and SDKs to access LLMs and our proprietary FMs including training, fine-tuning and prompting tasks, including orchestration SDKs. \n  Design APIs for performance, real-time applications, scale, ease of use and governance automation. \n  Develop application-specific interfaces that leverage LLMs and FMs to continue to enhance the associate and customer experience.    Enable our users to build new GenAI capabilities. \n  Develop tools and processes to monitor API access patterns and operational health. \n  Design and implement AI safety and guardrails in the API layer working closely with researchers. \n \n \n  Basic Qualifications: \n \n  Bachelor\u2019s degree in Computer Science, Computer Engineering or a technical field \n  At least 6 years of experience designing and building data-intensive solutions using distributed computing and cache optimization techniques \n  At least 4 years of experience programming with Python, Go, Scala, or Java \n  At least 2 years of experience building, scaling, and optimizing training and inferencing APIs for deep neural networks \n   \n  Preferred Qualifications: \n \n  Familiarity with building large-scale AI products or platforms for NLP, speech, computer vision, or recommendation systems serving millions of users. \n  Ability to move fast in an environment with ambiguity at times, and with competing priorities and deadlines. Experience at tech and product-driven companies/startups preferred. \n  Ability to iterate rapidly with researchers and engineers to improve a product experience while building the foundational capabilities. \n  Familiarity with deploying large neural network models in demanding production environments. \n  Have experience with API security, observability, cloud access control and privacy best practices. \n \n \n  At this time, Capital One will not sponsor a new applicant for employment authorization for this position. \n ",
        "techs": [
            "none"
        ],
        "cleaned_techs": []
    },
    "227132aeb43222dd": {
        "terms": [
            "mlops"
        ],
        "salary_min": 60.0,
        "salary_max": 70.0,
        "title": "product owner manager with medical algorithms MLOps",
        "company": "Formac Inc",
        "desc": "Product owner with medical algorithm productization commercialization MLOps and DevOps \n REMOTE \n Long TERM 70/hr All inclusive \n Required Qualifications: \n \u25cf Domain knowledge in medical imaging, medical algorithm productization/commercialization, MLOps, and DevOps \n \u25cf 4+ years of experience in product ownership, product management, or a similar role \n \u25cf In-depth experience with agile methodology and the product lifecycle \n \u25cf Detailed understanding of business value related to features and functions \n \u25cf Strong business writing, communication, and presentation skills \n \u25cf Sharp analytical and problem-solving skills \n \u25cf Ability to effectively harmonize or otherwise manage conflicting stakeholder needs \n Recommended Qualifications: \n \u25cf Domain expertise in medical imaging, medical algorithm productization/commercialization, MLOps, and DevOps \n \u25cf Experience in ML model development and productization \n \u25cf 6+ years of experience in product ownership, product management, or a similar role \n \u25cf Product lifecycle management expertise \n Thanks \n Jay \n 628-215-2224 \n Job Types: Contract, Permanent, Full-time \n Pay: $60.00 - $70.00 per hour \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n Paid time off \n Vision insurance \n \n Experience level: \n \n 7 years \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Application Question(s): \n \n Number and Time to connect \n \n Education: \n \n High school or equivalent (Preferred) \n \n Experience: \n \n product owner: 7 years (Required) \n mlops: 1 year (Required) \n DevOps: 1 year (Required) \n medical imaging, medical algorithm productization: 1 year (Required) \n \n Work Location: Remote",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "f2ed68f4721e1c02": {
        "terms": [
            "mlops"
        ],
        "salary_min": 97549.84,
        "salary_max": 182415.7,
        "title": "Devops Architect",
        "company": "FACEBOOK APP",
        "desc": "DevOps Architect Responsibilities: Facilitating the development process and operations. Identifying setbacks and shortcomings. Creating suitable DevOps channels across the organization. Establishing continuous build environments to speed up software development. Designing efficient practices. Delivering comprehensive best practices. Managing and reviewing technical operations. Guiding the development team. \n Job Types: Contract, Full-time \n Salary: $97,549.84 - $182,415.71 per year \n Experience level: \n \n 10 years \n 11+ years \n 9 years \n \n Experience: \n \n Devops Architect: 10 years (Preferred) \n AWS: 10 years (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "94de7a1209430355": {
        "terms": [
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior DevOps Engineer",
        "company": "Nagarro",
        "desc": "Full-time \n  Service Region: Eastern Europe \n \n \n \n \n \n  Company Description \n \n \n  We're Nagarro. \n  We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale \u2014 across all devices and digital mediums, and our people exist everywhere in the world (19,000+ experts across 35 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in! \n  By this point in your career, it is not just about the tech you know or how well you can code. It is about what more you want to do with that knowledge. Can you help your teammates proceed in the right direction? Can you tackle the challenges our clients face while always looking to take our solutions one step further to succeed at an even higher level? Yes? You may be ready to join us. \n \n \n \n \n \n  Job Description \n \n \n \n  Delivering custom developed products for a top diagnostics company in Switzerland. \n  Managing the CI/CD processes for over 200 medical devices projects. \n  Developing new features for the build solution (C# wrapper over Nuke). \n  Full autonomy when it comes to developing and maintaining the build system. \n  Using the client methodology complemented by the Nagarro software delivery process and best practices. \n  Providing general guidance related to development teams and resolving support tickets around building, testing, and deploying existing and new applications. \n  Implementing solutions which aim for a full automation of manual tasks, delivery processes and monitoring. \n \n \n \n \n \n \n  Qualifications \n \n \n  Must have skills: \n \n  Azure DevOps \n  Gitlab CI/CD \n  Docker \n  PowerShell \n  Bash \n  Windows and Linux Server Administration \n  GitOps \n \n \n \n \n \n \n  Nice to have: \n \n  C# \n  Experience in highly regulated industries (e.g. Life Sciences). \n  AWS Knowledge \n  Experience with Jenkins. \n  Exposure to monitoring tools (Prometheus, Grafana). \n  Familiarity with Docker Compose. \n  Server administration. \n  Client-facing role, directly interacting with Nagarro and customer project stakeholders. \n  Organized, structured. Problem-solving and effective communication skills.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "990248cc89bced26": {
        "terms": [
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Senior Level DevOps",
        "company": "Nagarro",
        "desc": "Full-time \n  Service Region: Eastern Europe \n \n \n \n \n \n  Company Description \n \n \n  We're Nagarro. \n  We are a digital product engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale \u2014 across all devices and digital mediums, and our people exist everywhere in the world (19,000+ experts across 35 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in! \n  By this point in your career, it is not just about the tech you know or how well you can code. It is about what more you want to do with that knowledge. Can you help your teammates proceed in the right direction? Can you tackle the challenges our clients face while always looking to take our solutions one step further to succeed at an even higher level? Yes? You may be ready to join us. \n \n \n \n \n \n  Job Description \n \n \n  We are looking for a Senior Level DevOps Engineer having 4-6 years\u2019 experience with Docker, Gitlab, and Gitlab CI. \n  WHAT ARE YOUR RESPONSIBILITIES: \n \n  Collaborate with clients to understand their needs and tailor our DevOps solutions accordingly. \n  Manage and enhance  Gitlab-based CI/CD  pipelines to streamline development and deployment processes. \n  Maintain and optimize  Docker  containers for various applications and services. \n  Oversee and monitor  AWS Cloud Infrastructure , including EC2 instances, S3 buckets, RDS databases, and networking components. \n  Work closely with development teams to ensure code integration and deployment align with project goals. \n  Create and maintain automation scripts (e.g., Bash) to streamline routine tasks and deployment processes. \n  Swiftly troubleshoot and resolve infrastructure and deployment issues to minimize disruptions. \n  Implement robust security practices to safeguard infrastructure and applications. \n  Configure Nginx to serve as a reliable reverse proxy for applications. \n  Utilize  Terraform  or equivalent technologies to define and manage infrastructure as code for Cloud resources. \n \n \n \n \n \n \n  Qualifications \n \n \n \n  Strong problem-solving skills and attention to detail. \n  Expertise in Docker, Gitlab, and Gitlab CI. \n  Ability to work independently and collaboratively in a fast-paced environment. \n  Excellent communication and teamwork skills.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "01046b503d1bc1e6": {
        "terms": [
            "mlops"
        ],
        "salary_min": 100000.0,
        "salary_max": 100000.0,
        "title": "DevOps Engineer",
        "company": "Rapidiant",
        "desc": "We are looking for a new DevOps Engineer to join our United States Internal Revenue Service (IRS) team supporting the CI/CD pipeline. Applicants must be US Citizens capable of obtaining a US Government public trust security clearance. \n \n 5-10 years' experience in Linux Systems Administration or working in a DevOps environment. \n Experience (2 years) with administering and using Nexus Repository Pro. \n Experience administering GitLab a big plus. \n Experience with Ansible a plus \n Experience administering Web Applications -- native or Cloud (AWS, Azure, etc.) \n DevOps Engineer with high level scripting experience (e.g., Unix shell, Perl, Python, Groovy, PowerShell, Ruby, Go, JavaScript). \n \n Job Type: Full-time \n Pay: $100,000.00 per year \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Experience: \n \n Ansible: 2 years (Required) \n Scripting: 3 years (Required) \n Linux: 6 years (Required) \n \n Work Location: Remote",
        "cleaned_desc": " Experience administering GitLab a big plus. \n Experience with Ansible a plus \n Experience administering Web Applications -- native or Cloud (AWS, Azure, etc.) \n DevOps Engineer with high level scripting experience (e.g., Unix shell, Perl, Python, Groovy, PowerShell, Ruby, Go, JavaScript). ",
        "techs": [
            "gitlab",
            "ansible",
            "aws",
            "azure",
            "unix shell",
            "perl",
            "python",
            "groovy",
            "powershell",
            "ruby",
            "go",
            "javascript"
        ],
        "cleaned_techs": [
            "gitlab",
            "ansible",
            "aws",
            "azure",
            "unix shell",
            "perl",
            "python",
            "groovy",
            "powershell",
            "ruby",
            "go",
            "javascript"
        ]
    },
    "6cd8f1e0824761b2": {
        "terms": [
            "mlops"
        ],
        "salary_min": 118000.0,
        "salary_max": 155000.0,
        "title": "DevOps Manager",
        "company": "Cotiviti",
        "desc": "Overview: \n  \n   Working in an agile software development environment, the \n   DevOps Manager  is responsible for the vision and technical integrity of the automation infrastructure and services in our Cloud Environment (AWS and Azure). As manager of the Cloud Platform Engineering Team, you will focus on standardizing the automation platform across multiple software products. This position involves developing roadmaps, reporting and detailed plans for new automation, automation standardization and enhancements of existing automat for distributed, highly available applications. The DevOps Manager is a thought leader who will be collaborating with the DevOps Architect / Technical leads, Business Operations Leadership, Product Leadership, Development Leadership as well as Security and Compliance Leadership.\n   Responsibilities: \n  \n Responsible for developing plans, processes and visions in multiple enterprise systems in a fast-paced, agile, service-oriented environment. \n  Good understanding of the full stack software development (databases, services, REST, client-side, user interface). \n  Understand and effectively communicate the needs of automation, infrastructure and security at scale to meet rapidly increasing demand. \n  Develop and enhance reporting on SLA\u2019s, process, requirements and progress toward a resilient, secure, and efficient SaaS application platform. \n  Collaborate with Development Leadership to bring new features and services into the delivery organization. \n  Collaboration with Business Operations develop, report and enhance monitoring and automation for the application platform. \n  Evangelize automation, implement processes, procedures, and best practice guidelines for code management. \n  Collaborate with Engineering, Product, Services, and other departments to define requirements to meet the platform standards. \n  Work with Architecture to drive technical designs to consensus and approval. \n  Clearly communicate changed both verbally and in writing the benefits, uses, purpose and vision for the DevOps Platform \n  Mange the DevOps team in developing and communicating best practices, including improved scalability, performance, reliability, and speed to market. \n  Interface with leadership, Group Leads, and Lead Architects to convey infrastructure requirements, plan, and schedule deployment of tasks, and resolve any issues that impact deployment of the Application delivery systems. \n  Qualifications: \n  \n Master's or Bachelor\u2019s in engineering in IT/ Electronics / Communication / Computer Science / Information Systems. \n  5+ years of work experience in the field or in a related area with proven enterprise level experience in a software configuration management role, test automation, application stack deployment and support experience, and experience being a member of a software project life cycle team. \n  2+ years of experience managing teams deploying cloud-based applications in Azure or AWS. \n  Experience with collaborative version control systems (Git/ GitHub/ Subversion/ Bitbucket). \n  Experience with configuration management tools, processes and best practices. \n  Experience with cloud computing and container best practices, tools and processes. \n  Experience developing vision and leading teams to implement enterprise level platforms for automated code delivery. \n  5+ years Agile software delivery experience. \n  Professional with ability to properly handle confidential information. \n  Excellent written and verbal communication skills. \n  Ability to develop roadmaps, report progress and clearly communicate risk and reward for technical achievements \n  Strong organizational skills and adaptive capacity for rapidly changing priorities and workloads \n  Comfort in working with team members that are remote and located in the US, India or other geographies \n  Ability to work within a matrix organization \n \n \n   Base compensation ranges from $118,000 to $155,000. Specific offers are determined by various factors, such as experience, education, skills, certifications, and other business needs.\n  \n \n \n  Cotiviti offers team members a competitive benefits package to address a wide range of personal and family needs, including medical, dental, vision, disability, and life insurance coverage, 401(k) savings plans, paid family leave, 9 paid holidays per year, and 17-27 days of Paid Time Off (PTO) per year, depending on specific level and length of service with Cotiviti. For information about our benefits package, please refer to our Careers page.\n  \n \n \n  This role is based remotely and all interviews will be conducted virtually.\n  \n \n   #LI-Remote\n    #LI-RA1\n    #senior\n  \n \n   #Manager",
        "cleaned_desc": "Overview: \n  \n   Working in an agile software development environment, the \n   DevOps Manager  is responsible for the vision and technical integrity of the automation infrastructure and services in our Cloud Environment (AWS and Azure). As manager of the Cloud Platform Engineering Team, you will focus on standardizing the automation platform across multiple software products. This position involves developing roadmaps, reporting and detailed plans for new automation, automation standardization and enhancements of existing automat for distributed, highly available applications. The DevOps Manager is a thought leader who will be collaborating with the DevOps Architect / Technical leads, Business Operations Leadership, Product Leadership, Development Leadership as well as Security and Compliance Leadership.\n   Responsibilities: \n  \n Responsible for developing plans, processes and visions in multiple enterprise systems in a fast-paced, agile, service-oriented environment. \n  Good understanding of the full stack software development (databases, services, REST, client-side, user interface). \n  Understand and effectively communicate the needs of automation, infrastructure and security at scale to meet rapidly increasing demand. \n  Develop and enhance reporting on SLA\u2019s, process, requirements and progress toward a resilient, secure, and efficient SaaS application platform.   Master's or Bachelor\u2019s in engineering in IT/ Electronics / Communication / Computer Science / Information Systems. \n  5+ years of work experience in the field or in a related area with proven enterprise level experience in a software configuration management role, test automation, application stack deployment and support experience, and experience being a member of a software project life cycle team. \n  2+ years of experience managing teams deploying cloud-based applications in Azure or AWS. \n  Experience with collaborative version control systems (Git/ GitHub/ Subversion/ Bitbucket). \n  Experience with configuration management tools, processes and best practices. \n  Experience with cloud computing and container best practices, tools and processes. \n  Experience developing vision and leading teams to implement enterprise level platforms for automated code delivery. \n  5+ years Agile software delivery experience. \n  Professional with ability to properly handle confidential information. \n  Excellent written and verbal communication skills. ",
        "techs": [
            "aws",
            "azure",
            "git",
            "github",
            "subversion",
            "bitbucket"
        ],
        "cleaned_techs": [
            "aws",
            "azure",
            "git",
            "github",
            "subversion",
            "bitbucket"
        ]
    },
    "1fc2f7c75e3bfae7": {
        "terms": [
            "mlops"
        ],
        "salary_min": 43.44,
        "salary_max": 85.84,
        "title": "Senior Devops Engineer",
        "company": "FACEBOOK APP",
        "desc": "Understanding customer requirements and project KPIs Implementing various development, testing, automation tools, and IT infrastructure Planning the team structure, activities, and involvement in project management activities. Managing stakeholders and external interfaces Setting up tools and required infrastructure Defining and setting development, test, release, update, and support processes for DevOps operation Have the technical skill to review, verify, and validate the software code developed in the project. Troubleshooting techniques and fixing the code bugs Monitoring the processes during the entire lifecycle for its adherence and updating or creating new processes for improvement and minimizing the wastage Encouraging and building automated processes wherever possible Identifying and deploying cybersecurity measures by continuously performing vulnerability assessment and risk management Incidence management and root cause analysis Coordination and communication within the team and with customers Selecting and deploying appropriate CI/CD tools Strive for continuous improvement and build continuous integration, continuous development, and constant deployment pipeline (CI/CD Pipeline) Mentoring and guiding the team members Monitoring and measuring customer experience and KPIs Managing periodic reporting on the progress to the management and the customer \n Job Types: Contract, Full-time \n Salary: $43.44 - $85.84 per hour \n Experience level: \n \n 10 years \n 11+ years \n \n Experience: \n \n Devops Engineer: 10 years (Preferred) \n AWS: 1 year (Preferred) \n \n Work Location: Remote",
        "cleaned_desc": "Understanding customer requirements and project KPIs Implementing various development, testing, automation tools, and IT infrastructure Planning the team structure, activities, and involvement in project management activities. Managing stakeholders and external interfaces Setting up tools and required infrastructure Defining and setting development, test, release, update, and support processes for DevOps operation Have the technical skill to review, verify, and validate the software code developed in the project. Troubleshooting techniques and fixing the code bugs Monitoring the processes during the entire lifecycle for its adherence and updating or creating new processes for improvement and minimizing the wastage Encouraging and building automated processes wherever possible Identifying and deploying cybersecurity measures by continuously performing vulnerability assessment and risk management Incidence management and root cause analysis Coordination and communication within the team and with customers Selecting and deploying appropriate CI/CD tools Strive for continuous improvement and build continuous integration, continuous development, and constant deployment pipeline (CI/CD Pipeline) Mentoring and guiding the team members Monitoring and measuring customer experience and KPIs Managing periodic reporting on the progress to the management and the customer \n Job Types: Contract, Full-time ",
        "techs": [
            "understanding customer requirements",
            "project kpis",
            "development",
            "testing",
            "automation tools",
            "it infrastructure",
            "team structure",
            "project management activities",
            "stakeholders",
            "external interfaces",
            "tools",
            "required infrastructure",
            "development processes",
            "test processes",
            "release processes",
            "update processes",
            "support processes",
            "devops operation",
            "software code",
            "troubleshooting techniques",
            "code bugs",
            "process monitoring",
            "process improvement",
            "automated processes",
            "cybersecurity measures",
            "vulnerability assessment",
            "risk management",
            "incidence management",
            "root cause analysis",
            "coordination",
            "communication",
            "ci/cd tools",
            "continuous improvement",
            "continuous integration",
            "continuous development",
            "constant deployment pipeline",
            "mentoring",
            "team members",
            "customer experience",
            "periodic reporting",
            "contract",
            "full-time"
        ],
        "cleaned_techs": [
            "understanding customer requirements",
            "project kpis",
            "development",
            "testing",
            "automation tools",
            "it infrastructure",
            "team structure",
            "project management activities",
            "stakeholders",
            "external interfaces",
            "tools",
            "required infrastructure",
            "development processes",
            "test processes",
            "release processes",
            "update processes",
            "support processes",
            "devops operation",
            "software code",
            "troubleshooting techniques",
            "code bugs",
            "process monitoring",
            "process improvement",
            "automated processes",
            "cybersecurity measures",
            "vulnerability assessment",
            "risk management",
            "incidence management",
            "root cause analysis",
            "coordination",
            "communication",
            "ci/cd tools",
            "continuous improvement",
            "continuous integration",
            "continuous development",
            "constant deployment pipeline",
            "mentoring",
            "team members",
            "customer experience",
            "periodic reporting",
            "contract",
            "full-time"
        ]
    },
    "30918e7a91597420": {
        "terms": [
            "mlops"
        ],
        "salary_min": 58400.0,
        "salary_max": 133000.0,
        "title": "DevOps Engineer, Mid",
        "company": "Booz Allen Hamilton",
        "desc": "Job Description \n \n \n \n \n \n \n \n \n \n \n         Location: \n         \n \n         McLean,VA,US \n         \n \n \n \n         Remote Work: \n         \n \n         Hybrid \n         \n \n \n \n         Job Number: \n         \n \n         R0182414\n         \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n         DevOps Engineer, Mid\n           The Opportunity: \n  As a DevOps engineer, you know how to set up cloud environments and provision computer networking, storage, and virtual networks\u2014ultimately, how to \u201charness the cloud.\u201d We\u2019re looking for a seasoned DevOps infrastructure engineer like you to support our clients as they modernize their IT infrastructures and meet their most challenging missions. \n \n  As a lead DevOps infrastructure engineer at Booz Allen, you\u2019ll oversee a team of cloud architects and engineers as they manage server configuration for modern cloud solutions. You\u2019ll apply your skills and experience within a DevOps framework to establish or provision virtual machines or networks and use cloud service providers to further your clients\u2019 meaningful missions. \n \n  Join us. The world can\u2019t wait. \n \n  You Have: \n \n  3+ years of experience working within a technical, cloud-based environment \n  3+ years of experience working in a Linux based environment \n  3+ years of experience with commercial Cloud service providers, including Amazon Web Services or Microsoft Azure, or GCP Cloud providers \n  Ability to troubleshoot development builds or pipelines with service desk tools \n  Secret clearance \n  Bachelor\u2019s degree \n \n \n  Nice If You Have: \n \n  1+ year of experience in a technical support role \n  Experience with managing a support desk and creating documentation or training related to solving user problems \n  Knowledge of Jenkins and Gitlab pipelines, mechanisms, and build workflows \n  Possession of client service skills \n  Ability to obtain a security plus Certification \n \n \n  Clearance: \n  Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required. \n \n  Create Your Career: \n \n  Grow With Us \n  Your growth matters to us\u2014that\u2019s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. \n \n  A Place Where You Belong \n  Diverse perspectives cultivate collective ingenuity. Booz Allen\u2019s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you\u2019ll develop your community in no time. \n \n  Support Your Well-Being \n  Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we\u2019ll support you as you pursue a balanced, fulfilling life\u2014at work and at home. \n \n  Your Candidate Journey \n  At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we\u2019ve compiled a list of resources so you\u2019ll know what to expect as we forge a connection with you during your journey as a candidate with us. \n \n  Compensation \n  At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen\u2019s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. \n  Salary at Booz Allen is determined by various factors, including but not limited to location, the individual\u2019s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $58,400.00 to $133,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen\u2019s total compensation package for employees.\n          \n  Work Model  Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. \n \n  If this position is listed as remote or hybrid, you\u2019ll periodically work from a Booz Allen or client site facility. \n  If this position is listed as onsite, you\u2019ll work with colleagues and clients in person, as needed for the specific role. \n \n \n  EEO Commitment \n  We\u2019re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change \u2013 no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "0114e1ee6c7bdbe7": {
        "terms": [
            "mlops"
        ],
        "salary_min": 113100.0,
        "salary_max": 203600.0,
        "title": "Sr DevOps Engineer",
        "company": "Parsons",
        "desc": "In a world of possibilities, pursue one with endless opportunities. Imagine Next! \n  When it comes to what you want in your career, if you can imagine it, you can do it at Parsons. Imagine a career working with intelligent, diverse people sharing a common quest. Imagine a workplace where you can be yourself. Where you can thrive. Where you can find your next, right now. We\u2019ve got what you\u2019re looking for. \n  \n Job Description:  \n Parsons is looking for an amazingly talented  Sr DevOps Engineer  to join our team! In this role you will get to  lead a team of DevOps Engineers across the Cyber Warfare Platforms Portfolio  .  \n \n \n \n Responsibilities  \n \n \n \n Design, develop, and maintain DevOps framework environments to support the automation and deployment of operational capabilities.  \n Perform maintenance, upgrades, and troubleshooting of existing systems.  \n Configure and manage Kubernetes clusters.  \n \n \n \n \n \n \n Work with software engineering team to build CI/CD pipelines.  \n Engineer solutions for automating deployments and all associated software configurations to increase reliability of software applications.  \n Manage and lead a small/mid-size DevOps team and serve as the primary interface between team, management, and customers  \n Provide technical support and training to project team members and customers.   \n \n \n \n \n \n \n Required Skills:  \n \n \n \n Infrastructure-as-Code experience (e.g., Python, Bash, YAML, JSON, CloudFormation, Ansible, Terraform)  \n AWS or other cloud experience  \n Kubernetes management and configuration experience  \n Experience with the Atlassian product suite (Confluence, Jira), vSphere, Gitlab and/or SonarQube  \n \n \n \n \n \n \n Top Secret security clearance with required, preferred TS/SCI w/ poly  \n Bachelor of Science degree in Computer Science, Information Systems Management, or a related discipline or comparable work experience  \n \n \n \n Desired Skills:  \n \n \n \n Object Oriented or other programming experience (C, C++, Java)  \n CISSP, Security+, AWS Solutions Architect Associate/Professional, or other certifications  \n \n \n \n \n Minimum Clearance Required to Start:  \n \n \n Top Secret  \n \n \n Minimum Clearance Required to Start:  Top Secret \n  This position is part of our Federal Solutions team. \n  Our Federal Solutions segment delivers resources to our US government customers that ensure the success of missions around the globe. Our diverse, intelligent employees drive the state of the art as they provide services and solutions in the areas of defense, security, intelligence, infrastructure, and environmental. We promote a culture of excellence and close-knit teams that take pride in delivering, protecting, and sustaining our nation's most critical assets, from Earth to cyberspace. Throughout the company, our people are anticipating what\u2019s next to deliver the solutions our customers need now. \n  \n Salary Range:  $113,100.00 - $203,600.00 \n  We value our employees and want our employees to take care of their overall wellbeing, which is why we offer best-in-class benefits such as medical, dental, vision, paid time off, 401(k), life insurance, flexible work schedules, and holidays to fit your busy lifestyle! \n  \n The position may require a COVID vaccination or an approved accommodation/exemption for a disability/medical condition or religious belief as required by customer requirements and some cases federal, state, provincial or local mandates.  \n Parsons is an equal opportunity employer committed to diversity, equity, inclusion, and accessibility in the workplace. Diversity is ingrained in who we are, how we do business, and is one of our company\u2019s core values. Parsons equally employs representation at all job levels for minority, female, disabled, protected veteran and LGBTQ+. \n  We truly invest and care about our employee\u2019s wellbeing and provide endless growth opportunities as the sky is the limit, so aim for the stars! Imagine next and join the Parsons quest\u2014APPLY TODAY!",
        "cleaned_desc": " \n Required Skills:  \n \n \n \n Infrastructure-as-Code experience (e.g., Python, Bash, YAML, JSON, CloudFormation, Ansible, Terraform)  \n AWS or other cloud experience  \n Kubernetes management and configuration experience  \n Experience with the Atlassian product suite (Confluence, Jira), vSphere, Gitlab and/or SonarQube  \n \n \n \n \n \n   Top Secret security clearance with required, preferred TS/SCI w/ poly  \n Bachelor of Science degree in Computer Science, Information Systems Management, or a related discipline or comparable work experience  \n \n \n \n Desired Skills:  \n \n \n \n Object Oriented or other programming experience (C, C++, Java)  \n CISSP, Security+, AWS Solutions Architect Associate/Professional, or other certifications  \n \n \n \n ",
        "techs": [
            "python",
            "bash",
            "yaml",
            "json",
            "cloudformation",
            "ansible",
            "terraform",
            "aws",
            "kubernetes",
            "atlassian product suite (confluence",
            "jira)",
            "vsphere",
            "gitlab",
            "sonarqube",
            "top secret security clearance",
            "ts/sci w/ poly",
            "bachelor of science degree in computer science",
            "information systems management",
            "object oriented programming experience (c",
            "c++",
            "java)",
            "cissp",
            "security+",
            "aws solutions architect associate/professional"
        ],
        "cleaned_techs": [
            "python",
            "bash",
            "yaml",
            "json",
            "cloudformation",
            "ansible",
            "terraform",
            "aws",
            "kubernetes",
            "atlassian product suite (confluence",
            "jira)",
            "vsphere",
            "gitlab",
            "sonarqube",
            "ts/sci w/ poly",
            "information systems management",
            "object oriented programming experience (c",
            "c++",
            "java)",
            "cissp"
        ]
    },
    "0e2458eaf0f1cd41": {
        "terms": [
            "mlops"
        ],
        "salary_min": 110000.0,
        "salary_max": 130000.0,
        "title": "SFDC Devops Engineer",
        "company": "Ascendion",
        "desc": "Description \n About Ascendion \n  Ascendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. \n  Ascendion | Engineering to elevate life \n  We have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us: \n \n Build the coolest tech for world\u2019s leading brands \n Solve complex problems \u2013 and learn new skills \n Experience the power of transforming digital engineering for Fortune 500 clients \n Master your craft with leading training programs and hands-on experience \n \n Experience a community of change makers! \n  Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion. \n  About the Role: \n  Job Title: SFDC Devops Engineer \n  K ey Responsibilities: \n \n This engineer will be taking over the devops pipeline from an engineering perspective and working to keep the pipeline healthy, make enhancements, and resolve break issues. \n \n Minimum Qualifications: \n \n 5+ years of experience as Salesforce/SFDC \n Strong experience as a DevOps Engineer \n Strong coding experience (Apex preferred, but any backend language acceptable) \n Strong container experience in RHEL, Docker, or Kubernetes, etc. \n Strong experience in DevOps principles including automation, Jenkins, and GitHub \n Experience/knowledge around cloud computing \n \n Location: Remote \n  Salary Range:  The salary for this position is between $110K \u2013 $130K annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate. \n  Benefits : The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal leaves accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 leaves of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System] \n  Want to change the world? Let us know. \n  Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let\u2019s talk! \n  If interested do share me your resume on shagun.chourasia@ascendion.com \n Preferred Skills: \n \n SFDC \n  Devops \n  Apex \n  Salesforce \n \n Job details \n \n \n Job ID \n \n \n   328856\n   \n \n \n \n Job Requirements \n \n \n   SFDC Devops Engineer\n   \n \n \n \n \n Location \n \n \n   Raleigh, North Carolina, US\n   \n \n \n \n \n Recruiter \n \n \n   Shagun\n   \n \n \n \n Email \n \n \n   shagun.chourasia@ascendion.com",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "894351f7f411b504": {
        "terms": [
            "mlops"
        ],
        "salary_min": 131000.0,
        "salary_max": 164000.0,
        "title": "Senior Technical Program Manager (DevOps and InfoSec Enablement)",
        "company": "Tango",
        "desc": "What We're Up to at Tango \n  Tango believes that rewards, incentives, and payments are more than transactions. They're opportunities to fuel stronger relationships with people, and better performance for business. Whether you're focused on customer loyalty or employee engagement, wellness or research participation, we can help you seamlessly send smiles and drive results. \n  Why Work for Us \n  We care about each other and our customers. Our team is autonomous, collaborative, creative, and eager to learn. We are an inclusive and diverse company that offers excellent health, dental, and vision benefits, flexible PTO, and competitive compensation packages. We're doing incredible things with awesome people, and we'd love for you to join us! \n \n  The Job \n  Are you seeking an opportunity where you'll get to work with fun, curious people, building innovative products, in a fresh and modern tech stack? We're hiring a Senior Technical Program Manager to do just that. In this role (which sits in engineering) you will hone development areas, craft a roadmap, and have a high degree of ownership over programs in your domain areas. \n  This particular TPM will support the InfoSec and DevOps teams. That's why our ideal candidate would be: a former developer who has experience in working in a CI/CD, cloud-hosted (ideally AWS), SaaS environment. They would have some contextual knowledge about risk mitigation, especially in a high-transaction SaaS environment. They are equipped to discover and drive tactical delivery tasks alongside InfoSec and DevOps leadership, supporting goals and removing roadblocks. \n  This human should be solution-oriented and action-driven, as they will be managing multiple projects at once in an ambiguous, fast-paced environment. As a senior TPM, they will be expected to lead by influence and should elevate and empower those they work alongside daily. While you are closely connected to DevOps and InfoSec initiatives, you will work across varied cross-functional teams at Tango. \n  Tango's Engineering teams play an influential role in our mission: making rewards easy to send and awesome to receive! Working here, you will see the impact of your work on thousands of businesses and reward recipients every day. We're proud of our cutting edge, product-oriented, fully remote engineering org. We like being a place people like to work - come see for yourself! \n  Reports To:  Chief Technology Officer \n  As Senior Technical Program Manager (TPM) at Tango, you will: \n \n Deliver on key product and engineering initiatives: be responsible for scoping with Product teams, planning with engineering teams, and working across Architecture, Compliance, InfoSec, and BizSystems to deliver complete solutions. \n Support the Product Owner in standups, sprint planning, and reviews. \n Construct and prioritize user stories and other requirement artifacts and build a development timeline. \n Be responsible for initiatives that have complex dependencies, processes, and technologies. \n Develop and execute a SCRUM team's roadmap in close coordination with Product Management. \n Facilitate technical design and track dependencies on domains in our platform. \n Proactively identify and resolve risks that may impair the team's ability to meet goals. \n Manage prioritization and trade-offs amongst multiple work streams and domain areas. \n Collaborate with a cross-functional team to deliver the best experience for our customers. \n Be part of an on-call rotation: respond to and prioritize production bugs, and perform the role of Incident Communicator during production incidents. \n Mentor junior TPMs and other teammates. \n \n To be effective in our Senior TPM role, you  must  have: \n \n 8+ years experience in a TPM OR software engineering role, developing or operating a cloud based software product. \n 3+ years' experience with cloud infrastructure and technologies (AWS preferred) \n Experience working with Development Operations or Information Security teams. This includes crossfunctional stakeholders in   IT, HR, Compliance, Legal and others. \n 3+ Experience with security frameworks, risk management, operations management, application security, or compliance management \n Experience with building complex, highly scalable cloud-based systems that have been successfully delivered to customers. \n Knowledge of security architecture for identity management, network management, SSDLC. Knowledge of OWASP, PCI, ISO27001 or ISO27701 \n Experience leading multiple engineering or information security projects successfully in the same period \n Ability to participate effectively and influence multi-functional teams without formal authority. \n Strong verbal and written communication skills, including ability to present clearly to all levels of management. \n BS in Computer Science, Engineering, or equivalent industry experience \n \n Our ideal Senior TPM at Tango will have most of the following skills and experience: \n \n 5+ years experience as Technical Program Manager, working with a cloud-based software product   and securing IaaS, PaaS, SaaS \n Experience with red teaming, blue teaming, or operations teams \n Knowledge of how to perform or lead Security Incident Response \n Knowledge of common security tools for endpoints, vulnerability management, application security, behavioral analysis \n Knowledge of information security risk management framework or maturity models \n Experience in high-volume, transaction-based technology business \n \n \n \n  Salary: The targeted pay range for this position is between $131,000 and $164,000.  Please note that the actual salary offer will carefully consider a wide range of factors, including your skills, qualifications, and experience. Certain positions are eligible for additional forms of compensation such as bonus. \n  This posting is open for applications through October 23, 2023. After this date, we will remove the posting while we review the applicant pool. \n \n \n  #Li-DNI #Li-Remote #BI-Remote \n \n  What You'll Get From Us \n \n Competitive compensation package (money isn't everything, but it helps) \n Medical, dental, and vision benefits (100% employer paid premium) \n Flex PTO and a generous holiday schedule \n 401(k) matching up to 6% and equity opportunities \n Choice-First Plan allows employees to choose to work from home, at our Seattle HQ, or hybrid \n A work-from-home monthly stipend for all employees \n Award-winning culture that fosters autonomy, creativity, inclusion, transparency, and ownership \n Dog-friendly Seattle office! \n \n Remote Work Eligibility \n  Tango has a flexible work policy, allowing most employees to work from home, in our Seattle office, or in a hybrid format. We are currently set up to employ people in most states, but there are a few where we aren't able to operate yet. We anticipate expanding our operations to include more states in the future, and we welcome your application if you reside in an unapproved state. For now, we can only consider applicants from unapproved states if relocation to an approved state is an option for you. Approved states are: Alabama, Arizona, California, Colorado, Delaware, Florida, Georgia, Idaho, Illinois, Indiana, Iowa, Kentucky, Maine, Maryland, Michigan, Minnesota, Missouri, Montana, Nebraska, Nevada, New Mexico, North Carolina, Oklahoma, Oregon, Tennessee, Texas, Utah, Virginia, Washington, West Virginia, Wisconsin. \n \n \n  This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.  If E-Verify cannot confirm that you are authorized to work, this employer is required to give you written instructions and an opportunity to contact Department of Homeland Security (DHS) or Social Security (SSA) so you can begin to resolve the issue before the employer can take any action against you, including terminating your employment.  Employers can only use E-Verify once you have accepted a job offer and completed the Form I-9.  \n Tango is an equitable and equal opportunity employer; we're strong because we're diverse and prioritize inclusive practices. All applicants will be recruited and considered for employment regardless of their social identities, including but not limited to race, gender, color, religion, belief, national origin, age, sexual orientation, disability, and other protected classes. We thrive in the areas of recruiting and hiring when we have a growing awareness of the challenges faced by underrepresented candidates during the application and hiring process. We encourage all candidates to apply for roles they feel they have the skills to fulfill or a willingness to learn. \n  We at Tango take the protection of your personal information very seriously. We will never ask for financial information of any kind or for payment during the job application process. Please take extra caution while examining the email address of the individual you are communicating with, as scammers may misspell an official Tango email address or use a slightly modified version. Emails from us will come only from an official  @tangocard.com  email address. If you suspect a role or job ad is fraudulent, or that you are being contacted by a scammer pretending to be a representative of Tango, please contact us. If you believe you are the victim of fraud resulting from a job listing, please contact your local authorities.",
        "cleaned_desc": " 3+ years' experience with cloud infrastructure and technologies (AWS preferred) \n Experience working with Development Operations or Information Security teams. This includes crossfunctional stakeholders in   IT, HR, Compliance, Legal and others. \n 3+ Experience with security frameworks, risk management, operations management, application security, or compliance management \n Experience with building complex, highly scalable cloud-based systems that have been successfully delivered to customers. \n Knowledge of security architecture for identity management, network management, SSDLC. Knowledge of OWASP, PCI, ISO27001 or ISO27701 \n Experience leading multiple engineering or information security projects successfully in the same period \n Ability to participate effectively and influence multi-functional teams without formal authority. \n Strong verbal and written communication skills, including ability to present clearly to all levels of management. \n BS in Computer Science, Engineering, or equivalent industry experience \n \n Our ideal Senior TPM at Tango will have most of the following skills and experience: \n \n 5+ years experience as Technical Program Manager, working with a cloud-based software product   and securing IaaS, PaaS, SaaS \n Experience with red teaming, blue teaming, or operations teams ",
        "techs": [
            "aws",
            "development operations",
            "information security teams",
            "it",
            "hr",
            "compliance",
            "legal",
            "security frameworks",
            "risk management",
            "operations management",
            "application security",
            "compliance management",
            "complex",
            "highly scalable cloud-based systems",
            "security architecture",
            "identity management",
            "network management",
            "ssdlc",
            "owasp",
            "pci",
            "iso27001",
            "iso27701",
            "engineering projects",
            "information security projects",
            "multi-functional teams",
            "verbal and written communication skills",
            "computer science",
            "engineering",
            "technical program manager",
            "cloud-based software product",
            "securing iaas",
            "paas",
            "saas",
            "red teaming",
            "blue teaming",
            "operations teams"
        ],
        "cleaned_techs": [
            "aws",
            "development operations",
            "it",
            "hr",
            "compliance",
            "legal",
            "risk management",
            "operations management",
            "compliance management",
            "complex",
            "highly scalable cloud-based systems",
            "identity management",
            "network management",
            "ssdlc",
            "owasp",
            "pci",
            "iso27001",
            "iso27701",
            "engineering projects",
            "multi-functional teams",
            "computer science",
            "engineering",
            "technical program manager",
            "cloud-based software product",
            "securing iaas",
            "paas",
            "saas",
            "red teaming",
            "blue teaming",
            "operations teams"
        ]
    },
    "b918783bd482719e": {
        "terms": [
            "mlops"
        ],
        "salary_min": 15.0,
        "salary_max": 15.0,
        "title": "DevOps Engineer",
        "company": "AchieveIt",
        "desc": "AchieveIt is a leading provider of cloud-based enterprise software for planning and execution and was just recognized by Inc. Magazine as both a Best Workplace of 2021 and one of the top 5,000 fastest-growing private companies in America for the 5th year in a row. If you\u2019re looking to join a fast-paced software company where your work will have a direct impact on our growth and the lives of our customers - this may be the right opportunity for you! You will work on an Agile software development team and focus on engineering and supporting our cloud infrastructure and CI/CD pipelines. We are a lean and fast-moving team, and you will have the opportunity to work directly with engineering, QA, product management, and security team members. \n \n  Responsibilities \n \n Write high-quality infrastructure-as-code that automates the provisioning, deployment, scaling, and monitoring of AchieveIt\u2019s infrastructure \n Deploy releases to multiple environments in Microsoft Azure using CI/CD tools (Azure DevOps, GitHub Actions) \n Collaborate with engineering and QA to understand and automate software development and testing workflows \n Identify, troubleshoot, and resolves system and application issues, and implement preventive measures to avoid future problems \n Proactively track our capacity, quotas, and other performance limits to plan for growth \n Work with security and compliance employees to implement and monitor security processes and standards \n Participate in all team planning activities (Sizing Meetings, Retrospectives, etc) \n \n Requirements \n \n Must be a United States citizen or permanent resident (currently hold a Green Card) \n 3+ years of DevOps or software engineering experience for a SaaS application hosted in a public cloud (Azure, AWS, Google) \n Experience with continuous integration/continuous delivery tools such as Azure DevOps or GitHub Actions \n Strong understanding of computer science fundamentals \n Passionate about learning new technologies building great product \n \n Benefits \n  What we do \n  Our mission is to empower organizations to achieve world-class execution on their most important initiatives. Our team is dedicated to building solutions that give leaders the cross-plan visibility they need to see what's working and where they need to pivot to grow their business faster and farther. We are innovators, strategizers, thinkers, builders, get-stuff-doners, learners, competitors, and pioneers. We are a group of Achievers who are passionate about results. \n \n  Our beliefs drive our culture \n  An Achiever  starts . We believe great things aren\u2019t achieved by waiting around.  An Achiever  adapts . We thrive on change.  An Achiever  serves . We believe that success for our company and customers is not achieved alone.  An Achiever  delivers . We are committed to delivering results. \n \n  What we offer \n \n 100% work from home \n $1,500 annual training budget \n Great hardware \n Competitive salary \n 15 vacation days (5 additional days at two years of service) \n Up to 32 paid holidays (8 standard, 2 floating, & 22 additional company holidays) \n Unlimited sick days (sick days do not count against PTO and include things like dentist visits and regular checkups \u2013 we want you to invest in staying healthy) \n 401K (with matching), Medical, Vision, Life and Dental insurance \n Short-term and long-term disability insurance \n Community events (team building, company parties, etc) \n \n \n \n Don't take our word for it  \n \n Inc. Best Workplaces 2023 \n 4.5 Glassdoor Rating - 92% would recommend us to a friend \n 4.5 out of 5.0 rating on G2, 4.7 out of 5.0 rating on Capterra",
        "cleaned_desc": "AchieveIt is a leading provider of cloud-based enterprise software for planning and execution and was just recognized by Inc. Magazine as both a Best Workplace of 2021 and one of the top 5,000 fastest-growing private companies in America for the 5th year in a row. If you\u2019re looking to join a fast-paced software company where your work will have a direct impact on our growth and the lives of our customers - this may be the right opportunity for you! You will work on an Agile software development team and focus on engineering and supporting our cloud infrastructure and CI/CD pipelines. We are a lean and fast-moving team, and you will have the opportunity to work directly with engineering, QA, product management, and security team members. \n \n  Responsibilities \n \n Write high-quality infrastructure-as-code that automates the provisioning, deployment, scaling, and monitoring of AchieveIt\u2019s infrastructure \n Deploy releases to multiple environments in Microsoft Azure using CI/CD tools (Azure DevOps, GitHub Actions) \n Collaborate with engineering and QA to understand and automate software development and testing workflows \n Identify, troubleshoot, and resolves system and application issues, and implement preventive measures to avoid future problems \n Proactively track our capacity, quotas, and other performance limits to plan for growth   Work with security and compliance employees to implement and monitor security processes and standards \n Participate in all team planning activities (Sizing Meetings, Retrospectives, etc) \n \n Requirements \n \n Must be a United States citizen or permanent resident (currently hold a Green Card) \n 3+ years of DevOps or software engineering experience for a SaaS application hosted in a public cloud (Azure, AWS, Google) \n Experience with continuous integration/continuous delivery tools such as Azure DevOps or GitHub Actions \n Strong understanding of computer science fundamentals ",
        "techs": [
            "achieveit",
            "inc. magazine",
            "microsoft azure",
            "ci/cd tools (azure devops",
            "github actions)",
            "infrastructure-as-code",
            "agile software development",
            "engineering",
            "qa",
            "product management",
            "security",
            "provisioning",
            "deployment",
            "scaling",
            "monitoring",
            "software development",
            "testing",
            "troubleshooting",
            "system and application issues",
            "preventive measures",
            "capacity",
            "quotas",
            "performance limits",
            "security processes",
            "standards",
            "united states citizen",
            "permanent resident",
            "green card",
            "devops",
            "software engineering",
            "saas application",
            "public cloud (azure",
            "aws",
            "google)",
            "continuous integration/continuous delivery tools",
            "computer science fundamentals"
        ],
        "cleaned_techs": [
            "achieveit",
            "inc. magazine",
            "microsoft azure",
            "ci/cd tools (azure devops",
            "github actions)",
            "infrastructure-as-code",
            "agile software development",
            "engineering",
            "qa",
            "product management",
            "provisioning",
            "deployment",
            "scaling",
            "monitoring",
            "software development",
            "testing",
            "troubleshooting",
            "system and application issues",
            "preventive measures",
            "capacity",
            "quotas",
            "performance limits",
            "standards",
            "united states citizen",
            "permanent resident",
            "green card",
            "devops",
            "software engineering",
            "saas application",
            "public cloud (azure",
            "aws",
            "google)",
            "continuous integration/continuous delivery tools",
            "computer science fundamentals"
        ]
    },
    "eccaf2b8bb8ab709": {
        "terms": [
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "DevOps Engineer",
        "company": "DMI",
        "desc": "About DMI: \n  \n   DMI is a leading global provider of digital services working at the intersection of public and private sectors. With broad capabilities across IT managed services, cybersecurity, cloud migration and application development, DMI provides on-site and remote support to clients within governments, healthcare, financial services, transportation, manufacturing, and other critical infrastructure sectors. DMI has grown to over 2,100+ employees globally and has been continually recognized as a Top Workplace in both regional and national categories.\n   About the Opportunity: \n  \n   DMI, LLC is seeking a \n   Senior DevOps Engineer  to join our team. As a Senior DevOps Engineer at DMI, you will take a leadership role in the end-to-end development and maintenance of our enterprise-wide SaaS platforms, pipelines, and cloud infrastructure. You will collaborate closely with internal stakeholders, product development teams, and other enterprise teams to determine DevOps and infrastructure technical requirements for automation and infrastructure.\n  \n \n \n  Duties and Responsibilities: \n \n \n  Provide technical leadership and guidance to the team, ensuring alignment with internal stakeholders, product development teams, business units, and other enterprise teams at DMI. \n  Lead and facilitate technical requirements gathering meetings, driving discussions, and reviewing designs with the business to ensure effective communication and understanding, specifically within the context of DMI's goals and objectives. \n  Coordinate and lead the release and deployment change process, collaborating with other teams as necessary to ensure smooth and efficient deployments. \n  Identify opportunities for improvement in stability, performance, and scalability across major business-critical systems. Drive the implementation of robust solutions aligned with DMI's priorities. \n  Champion the implementation of processes to reduce barriers and roadblocks in projects, services, and processes, aiming for increased operational efficiency. \n  Define and develop the scope and objectives of DevOps initiatives, overseeing the preparation of technical and functional specifications for pipelines and infrastructure. \n  Develop and maintain data integration pipelines sourcing data from various systems into a data lake. \n  Conduct technical and architectural reviews with product teams to ensure high-quality deliverables aligned with industry standards and best practices. \n  Ensure source code merge processes are complete, meet the test plan technical requirements, and oversee the completion of system testing and implementation according to plan within DMI's development environment. \n  Demonstrate an extensive knowledge of development and operations, including coding, infrastructure management, system administration, and DevOps toolchains, specific to DMI's technology landscape. \n  Act as a vendor manager for the DMI/AWS relationship. \n  Lead the provisioning and administration of infrastructure, including deploying and maintaining servers, storage, and networking resources required to host applications within DMI's environment. \n  Utilize Terraform for provisioning and managing infrastructure, specifically for DMI's AWS Cloud resources, ensuring efficient and scalable operations.   \n  Qualifications: \n  \n  Education and Years of Experience: \n \n \n \n \n  Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent practical experience). \n  5 years\u2019 experience as a Senior DevOps Engineer or similar leadership role, with a strong understanding of DevOps principles and practices. \n  Experience managing Amazon S3, Amazon EC2, Amazon ECR, Amazon T3, Splunk, PostgreSQL, Redis, Route 53, Trend Micro Deep Security, Burp Suite Professional, Tenable Nessus Pro, Teleport, Okta, Docker, Terraform, ElasticCache, CloudWatch, and other platforms used to manage a SaaS platform within an AWS environment. \n  Proficient working knowledge in setting up, maintaining, and modifying DigitalOcean, Teleport, Datadog, GitHub, OneLogin, 1Password, Ansible, WAL-E. \n \n \n  Required and Desired Skills/Certifications: \n \n \n \n \n  Expert proficiency in coding and scripting languages such as Ruby, Ruby on Rails (RoR), Python, Bash, or PowerShell, suitable for DMI's technology stack. \n  Expertise in operating an Agile software development. \n  Extensive experience with infrastructure provisioning and system administration, including deploying and maintaining servers, storage, and networking resources, aligned with DMI's infrastructure requirements. \n  Solid understanding of cloud platforms, preferably AWS, and experience with cloud resource provisioning using tools like Terraform within DMI's cloud environment. \n  Strong knowledge of DevOps toolchains and experience in implementing CI/CD pipelines, tailored to DMI's development processes. \n \n \n  Experience with RESTful API design, implementation, and integration with 3rd party systems (i.e.: AWS S3, Salesforce, third-party APIs) \n  Experience with server administration and deployment via Capistrano and Docker. \n  Ability to configure and troubleshoot Linux servers. \n \n \n  Excellent problem-solving and troubleshooting skills, with the ability to analyze and resolve complex infrastructure and deployment issues within DMI's systems. \n  Demonstrated leadership skills, with the ability to provide guidance, mentorship, and technical direction to the team. \n  Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams within DMI's organizational structure. \n  Proactive mindset with a focus on continuous improvement and automation, aligned with DMI's culture of innovation and efficiency. \n \n \n  Additional Requirements: \n \n \n \n \n  Ability to work in an office environment. \n  Ability to sit for long periods of time. \n  Ability to utilize a keyboard and mouse. \n  Ability to work in front of a computer screen for long periods of time. \n \n \n  Min Citizenship Status Required:  US Citizen\n  \n \n \n  Physical Requirements:  Possible Travel\n  \n \n \n  Location:  Remote United States\n  \n \n \n  Working at DMI: \n \n \n \n  DMI is a diverse, prosperous, and rewarding place to work. Being part of the DMI family means we care about your wellbeing. As such, we offer a variety of perks and benefits that help meet various interests and needs, while still having the opportunity to work directly with a number of our award-winning, Fortune 1000 clients. The following categories make up your DMI wellbeing:\n  \n \n  Community \u2013 Blood drives, volunteering opportunities, Holiday parties, summer picnics, Tech Chef, Octoberfest just to name a few ways DMI comes together as a community \n  Convenience/Concierge - Virtual visits through health insurance, pet insurance, commuter benefits, discount tickets for movies, travel, and many other items to provide convenience \n  Development \u2013 Annual performance management, continuing education, and tuition assistance, internal job opportunities along with career enrichment and advancement to help each employee with their professional and personal development \n  Financial \u2013 Generous 401k match for both pre-tax and post-tax (ROTH) contributions along with financial wellness education, EAP, Life Insurance and Disability help provide financial stability for each DMI employee \n  Recognition \u2013 Great achievements do not go unnoticed by DMI through Annual Awards ceremony, service anniversaries, peer-to-peer acknowledgment through Spotlight, employee referral \n  Wellness \u2013 Healthcare benefits, Wellness programs provide employees with several wellness options \n \n \n   Employees are valued for their talents and contributions. We all take pride in helping our customers achieve their goals, which in turn contributes to the overall success of the company. The company does and will take affirmative action to employ and advance in employment individuals with disabilities and protected veterans, and to treat qualified individuals without discrimination based on their physical or mental disability or veteran status. DMI is an Equal Opportunity Employer Minority/Female/Veterans/Disability. DMI maintains a drug-free workplace.\n  \n \n \n  ***************** No Agencies Please *****************\n  \n \n \n  Applicants selected may be subject to a government security investigation and must meet eligibility requirements for access to classified information. US citizenship may be required for some positions.",
        "cleaned_desc": "About DMI: \n  \n   DMI is a leading global provider of digital services working at the intersection of public and private sectors. With broad capabilities across IT managed services, cybersecurity, cloud migration and application development, DMI provides on-site and remote support to clients within governments, healthcare, financial services, transportation, manufacturing, and other critical infrastructure sectors. DMI has grown to over 2,100+ employees globally and has been continually recognized as a Top Workplace in both regional and national categories.\n   About the Opportunity: \n  \n   DMI, LLC is seeking a \n   Senior DevOps Engineer  to join our team. As a Senior DevOps Engineer at DMI, you will take a leadership role in the end-to-end development and maintenance of our enterprise-wide SaaS platforms, pipelines, and cloud infrastructure. You will collaborate closely with internal stakeholders, product development teams, and other enterprise teams to determine DevOps and infrastructure technical requirements for automation and infrastructure.\n  \n \n \n  Duties and Responsibilities: \n \n \n  Provide technical leadership and guidance to the team, ensuring alignment with internal stakeholders, product development teams, business units, and other enterprise teams at DMI. \n  Lead and facilitate technical requirements gathering meetings, driving discussions, and reviewing designs with the business to ensure effective communication and understanding, specifically within the context of DMI's goals and objectives. \n  Coordinate and lead the release and deployment change process, collaborating with other teams as necessary to ensure smooth and efficient deployments. \n  Identify opportunities for improvement in stability, performance, and scalability across major business-critical systems. Drive the implementation of robust solutions aligned with DMI's priorities. \n  Champion the implementation of processes to reduce barriers and roadblocks in projects, services, and processes, aiming for increased operational efficiency. \n  Define and develop the scope and objectives of DevOps initiatives, overseeing the preparation of technical and functional specifications for pipelines and infrastructure. \n  Develop and maintain data integration pipelines sourcing data from various systems into a data lake. \n  Conduct technical and architectural reviews with product teams to ensure high-quality deliverables aligned with industry standards and best practices.    Ensure source code merge processes are complete, meet the test plan technical requirements, and oversee the completion of system testing and implementation according to plan within DMI's development environment. \n  Demonstrate an extensive knowledge of development and operations, including coding, infrastructure management, system administration, and DevOps toolchains, specific to DMI's technology landscape. \n  Act as a vendor manager for the DMI/AWS relationship. \n  Lead the provisioning and administration of infrastructure, including deploying and maintaining servers, storage, and networking resources required to host applications within DMI's environment. \n  Utilize Terraform for provisioning and managing infrastructure, specifically for DMI's AWS Cloud resources, ensuring efficient and scalable operations.   \n  Qualifications: \n  \n  Education and Years of Experience: \n \n \n \n \n  Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent practical experience). \n  5 years\u2019 experience as a Senior DevOps Engineer or similar leadership role, with a strong understanding of DevOps principles and practices. \n  Experience managing Amazon S3, Amazon EC2, Amazon ECR, Amazon T3, Splunk, PostgreSQL, Redis, Route 53, Trend Micro Deep Security, Burp Suite Professional, Tenable Nessus Pro, Teleport, Okta, Docker, Terraform, ElasticCache, CloudWatch, and other platforms used to manage a SaaS platform within an AWS environment. \n  Proficient working knowledge in setting up, maintaining, and modifying DigitalOcean, Teleport, Datadog, GitHub, OneLogin, 1Password, Ansible, WAL-E. \n \n \n  Required and Desired Skills/Certifications: \n \n   \n \n  Expert proficiency in coding and scripting languages such as Ruby, Ruby on Rails (RoR), Python, Bash, or PowerShell, suitable for DMI's technology stack. \n  Expertise in operating an Agile software development. \n  Extensive experience with infrastructure provisioning and system administration, including deploying and maintaining servers, storage, and networking resources, aligned with DMI's infrastructure requirements. \n  Solid understanding of cloud platforms, preferably AWS, and experience with cloud resource provisioning using tools like Terraform within DMI's cloud environment. \n  Strong knowledge of DevOps toolchains and experience in implementing CI/CD pipelines, tailored to DMI's development processes. \n \n \n  Experience with RESTful API design, implementation, and integration with 3rd party systems (i.e.: AWS S3, Salesforce, third-party APIs) \n  Experience with server administration and deployment via Capistrano and Docker. \n  Ability to configure and troubleshoot Linux servers. \n \n \n  Excellent problem-solving and troubleshooting skills, with the ability to analyze and resolve complex infrastructure and deployment issues within DMI's systems. \n  Demonstrated leadership skills, with the ability to provide guidance, mentorship, and technical direction to the team. \n  Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams within DMI's organizational structure. \n  Proactive mindset with a focus on continuous improvement and automation, aligned with DMI's culture of innovation and efficiency. \n \n \n  Additional Requirements: ",
        "techs": [
            "dmi",
            "aws",
            "splunk",
            "postgresql",
            "redis",
            "route 53",
            "trend micro deep security",
            "burp suite professional",
            "tenable nessus pro",
            "teleport",
            "okta",
            "docker",
            "terraform",
            "elasticcache",
            "cloudwatch",
            "ruby",
            "ruby on rails (ror)",
            "python",
            "bash",
            "powershell",
            "digitalocean",
            "datadog",
            "github",
            "onelogin",
            "1password",
            "ansible",
            "wal-e",
            "agile software development",
            "ci/cd pipelines",
            "restful api design",
            "capistrano",
            "linux servers."
        ],
        "cleaned_techs": [
            "dmi",
            "aws",
            "splunk",
            "postgresql",
            "redis",
            "route 53",
            "burp suite professional",
            "tenable nessus pro",
            "teleport",
            "okta",
            "docker",
            "terraform",
            "elasticcache",
            "cloudwatch",
            "ruby",
            "ruby on rails (ror)",
            "python",
            "bash",
            "powershell",
            "digitalocean",
            "datadog",
            "github",
            "onelogin",
            "1password",
            "ansible",
            "wal-e",
            "agile software development",
            "ci/cd pipelines",
            "restful api design",
            "capistrano",
            "linux servers."
        ]
    },
    "1469e1b6e7a28d83": {
        "terms": [
            "mlops"
        ],
        "salary_min": 90000.0,
        "salary_max": 225000.0,
        "title": "Senior DevOps Engineer",
        "company": "Imply",
        "desc": "At Imply, we are on a mission to help developers become the new heroes of analytics. Our unique database, built from Apache Druid, enables them to develop the next generation of analytics applications. With Imply, developers can build without constraints as our database lets them create interactive data experiences on streaming and batch data with limitless scale and at the best economics. \n  Backed by leading investors including a16z and Bessemer Venture Partners, Imply is on a fast growth trajectory - disrupting the $100B database market - with customers including Salesforce, Reddit, and Intercontinental Exchange. Come join our team of disruptors, pioneers, and innovators! \n  The Role: \n  We're looking for a talented DevOps Engineer who is passionate about infrastructure as code, automation, observability, and generating efficiency for developers. You will join a team of multi-talented engineers to build and maintain environments that support Imply products and employees. You will be part of investing in our deployment pipelines and improving developer efficiency. Having an inclusive and collaborative philosophy is paramount to success within our group. \n  Responsibilities: \n \n Proactively interact with other Engineers, supporting their experience building and deploying software \n Architect and write tools to automate manual tasks, speeding up time to release \n Maintain open source deployment pipeline related software and tool components \n Observability end to end, from system health to deployment traceability \n AWS and Azure SaaS Infrastructure support, automating using infrastructure as code wherever possible \n Mentorship and training of more junior members, sharing knowledge and best practices \n \n Requirements: \n \n 5+ Years in a DevOps or SRE based role. \n Deep understanding of infrastructure as code, build and deploy pipelines, containers and linux based operating systems \n Passion for automation and reducing developer toil \n Strong empathy for developers and patience to help them solve their problems \n Excellent communication, verbally and in writing, internally and externally \n An eye for observability, and passion to improve visibility into systems for data driven decision making \n Core values including integrity, honesty, a willingness to help and reach out to others in need \n Ability to learn new technologies quickly \n \n Bonus Points: \n \n 4+ years of experience in developer relations, software development, or a role where you showcased your innate ability to earn trust and credibility with technical teams \n Multi-cloud experience, especially Azure and GCP \n Experience with Terraform, Kubernetes, ArgoCD, and Big Data systems is a plus \n \n What we offer: \n \n 100% Paid Medical, Dental and Vision Benefits \n 401(k) Program \n Fertility Coverage Option \n Pet Insurance \n Dependent Care FSA \n Mental Health Support \n Life and AD&D Insurance \n Unlimited Paid Time Off - USA Only \n Wellness Stipend \n Home Office Equipment Reimbursement \n Pre-Tax Commuter Benefits \n \n Individual compensation will be determined based on the candidate's experience and qualifications aligned with Imply's internal levelings guidelines and benchmarks. The below range encompasses roles with on target earnings. This role is also eligible to participate in Imply's equity plan subject to the terms of the applicable plans and policies. \n  Compensation: $90,000 - $225,000 \n  Don't meet every single requirement? Studies have shown that certain minority groups are less likely to apply to jobs unless they meet every qualification. At Imply, we are dedicated to building a diverse, inclusive and authentic workplace. If you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or for other roles in the future. \n  Imply is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, color, gender identity or expression, marital status, national origin, disability, protected veteran status, race, religion, pregnancy, sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. \n  Please note, applications and candidate submissions are subject to our privacy policy and, for California residents, the CCPA terms available at https://imply.io/privacy. \n  \u2014 \n  Attention: Imply Applicants \n  Due to reports of phishing, we're requesting that all Imply applicants apply through our official Careers page at  imply.io/careers . All official communication from Imply will come from email addresses ending with @imply.io.",
        "cleaned_desc": " AWS and Azure SaaS Infrastructure support, automating using infrastructure as code wherever possible \n Mentorship and training of more junior members, sharing knowledge and best practices \n \n Requirements: \n \n 5+ Years in a DevOps or SRE based role. \n Deep understanding of infrastructure as code, build and deploy pipelines, containers and linux based operating systems \n Passion for automation and reducing developer toil \n Strong empathy for developers and patience to help them solve their problems \n Excellent communication, verbally and in writing, internally and externally   An eye for observability, and passion to improve visibility into systems for data driven decision making \n Core values including integrity, honesty, a willingness to help and reach out to others in need \n Ability to learn new technologies quickly \n \n Bonus Points: \n \n 4+ years of experience in developer relations, software development, or a role where you showcased your innate ability to earn trust and credibility with technical teams \n Multi-cloud experience, especially Azure and GCP \n Experience with Terraform, Kubernetes, ArgoCD, and Big Data systems is a plus \n ",
        "techs": [
            "aws",
            "azure saas infrastructure",
            "infrastructure as code",
            "build and deploy pipelines",
            "containers",
            "linux-based operating systems",
            "automation",
            "developer toil reduction",
            "observability",
            "data-driven decision making",
            "integrity",
            "honesty",
            "communication",
            "terraform",
            "kubernetes",
            "argocd",
            "big data systems"
        ],
        "cleaned_techs": [
            "aws",
            "azure",
            "infrastructure as code",
            "build and deploy pipelines",
            "containers",
            "linux-based operating systems",
            "automation",
            "developer toil reduction",
            "observability",
            "data-driven decision making",
            "integrity",
            "honesty",
            "communication",
            "terraform",
            "kubernetes",
            "argocd",
            "big data systems"
        ]
    },
    "a86e098718f8433d": {
        "terms": [
            "mlops"
        ],
        "salary_min": 100000.0,
        "salary_max": 135000.0,
        "title": "REMOTE- Mid-Level DevOps Engineer (BP)",
        "company": "Sigma Defense [SOLUTE]",
        "desc": "Sigma Defense Systems  is a leading technology company serving the Department of Defense (DoD), providing tactical communications systems and services for digital modernization since 2006. Through our acquisitions of SOLUTE in January 2022 and Sub U Systems in May 2022, we have expanded our software and communications hardware solutions to better support JADC2, C5ISR, SATCOM, and DEVSECOPS for customers in the Army, Navy, Air Force, Marine Corps, and Space Force. Through a combination of hardware, software, and industry expertise, we provide a complete portfolio of solutions and services that accelerates information collection and sharing for faster decision making and better mission outcomes. \n  We are a company of innovative professionals thriving in a highly motivating work environment that fosters creativity and independent thinking. If you are a motivated individual with a desire to support our service men and women, now is a great time to join Sigma Defense! \n  Why would you work for us? Quite simply, the work we do is meaningful and stimulating. We promote initiative and independent thought; we encourage direct client engagement to ensure we are delivering what the customer wants; and our engineers and scientists are working on cutting-edge projects that move the state-of-the-art closer to the people who need them. If you're looking for technical challenges and an opportunity to take a leadership role in an environment that encourages you to excel, then WE are your destination. \n  Want to know what it's like to work at Sigma?  Find out what our employees are saying. \n  Discover Your Next Mission with Sigma Defense  - Find and follow us at Sigma Defense Systems LLC: Overview | LinkedIn and visit Sigma Defense | A Leading Technology Company for more information.  \n Equal Opportunity Employer/Veterans/Disabled:  Sigma Defense Systems is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. \n \n \n \n Sigma Defense is seeking a platform engineer who is enthusiastic about learning and contributing ideas to multiple teams. As a member of the team, the  Mid-Level DevSecOps Engineer  will help accelerate software modernization efforts through platform engineering in unique environments and help instill good DevSecOps practices across the industry. An ideal candidate will bring flexible and humble mindset to the team, and willingness to take on new and unique challenges. As this is a fully remote position, embracing the remote-first culture with the understanding of responsibility for intentional communication as part of daily activity is critical for success. \n  Requirements \n \n Must be a citizen of the United States. \n At minimum 2+ years of experience in DevOps role.  \n Candidate must have experience in a customer service capacity as they will be working directly with customers.  \n Excellent interpersonal and communication skills are needed as providing follow-ups and reports to the customer is required.  \n Candidate must have a customer-first attitude. \n \n Personnel Clearance Level: \n \n Candidate must possess or have the ability to obtain an active, DoD issued Secret security clearance.  A clearance will be sponsored for the right candidate, but employment is contingent upon their ability to successfully obtain a clearance.   \n Candidates with an active DoD issued Secret security clearance, or higher, are preferred. \n \n Education Requirements: \n \n  High School Diploma or GED \n \n  Software/Programs Experience: \n \n Kubernetes experience is critical (operational and developmental.) \n GitOps \n Terraform > VMWare, Azure, AWS \n Helm or Kustomize \n Agile \n Docker \n Linux \n \n Candidate Differentiators: \n \n \n Familiarity with code security and static analysis tools. \n Experience with Git-based infrastructure as code development processes. \n Familiarity with Agile Manifesto. \n Experience documenting processes and workflows with attention to detail. \n Experience with:\n    \n Gitlab CI/CD, Terraform, ArgoCD, Sonarqube, Anchore, Fortify, or, \n a generalized background in CI/CD, static code analysis, and Git infrastructure. \n \n \n Essential Job Duties (Not All-Inclusive): \n \n Contribute to daily Standup meetings. \n Development and maintenance of Infrastructure as Code. \n Development and maintenance of Helm and Kustomize deployments. \n Contribute to individual and team growth, skills, and culture. \n Contribute ideas for continuous improvement. \n \n Salary Range:  $100,000 - $135,000 annually.  \n Benefits \n \n Dental and Vision Insurance \n Medical Insurance to Include an HSA Plan and HRA Plan Which Features a $6,000 Health Reimbursement \n Life and A&D coverage \n Employee Assistance Program (EAP) \n 401(k) Plan with Company Matching Contributions \n 160 Hours of Paid Time Off (PTO) with Carry-Over up to 240 hours \n 12 (Floating) Holidays \n Educational Assistance \n Highly Competitive Salary \n Flexible Schedule",
        "cleaned_desc": " \n Kubernetes experience is critical (operational and developmental.) \n GitOps \n Terraform > VMWare, Azure, AWS \n Helm or Kustomize \n Agile \n Docker \n Linux \n \n Candidate Differentiators: \n \n \n Familiarity with code security and static analysis tools. \n Experience with Git-based infrastructure as code development processes. ",
        "techs": [
            "kubernetes",
            "gitops",
            "terraform",
            "vmware",
            "azure",
            "aws",
            "helm",
            "kustomize",
            "agile",
            "docker",
            "linux",
            "code security",
            "static analysis tools",
            "git-based infrastructure"
        ],
        "cleaned_techs": [
            "kubernetes",
            "gitops",
            "terraform",
            "vmware",
            "azure",
            "aws",
            "helm",
            "kustomize",
            "agile",
            "docker",
            "linux",
            "static analysis tools",
            "git-based infrastructure"
        ]
    },
    "06d987b3f86f453e": {
        "terms": [
            "mlops"
        ],
        "salary_min": 118052.69,
        "salary_max": 149481.0,
        "title": "DevOps Engineer Site Reliability Engineer - Mid Level",
        "company": "Apogee Integration LLC",
        "desc": "Apogee Integration, LLC \u2013 DevOps Engineer Site Reliability Engineer, Reston, VA \n \n  Security Clearance : Active TS/SCI   required \n \n \n  Position \n \n \n  As a DevOps Infrastructure Engineer, you\u2019ll collaborate with cloud architects and engineers specializing in Ansible, Terraform, Puppet, and Chef to manage server configuration for modern cloud solutions. You\u2019ll have the chance to demonstrate your expertise within a DevOps framework and use cloud service providers like Amazon Web Services, Microsoft Azure, Google Cloud Platform to further your clients\u2019 meaningful missions. \n \n   \n With access to internal innovative labs, you\u2019ll have the space and time to explore different ways of solving client challenges and you\u2019ll work with your team and stakeholders to deliver the right technical solutions. \n \n   \n Whether developing, deploying, or managing IT infrastructures for crucial server and network components, you\u2019ll have the latest tech and brightest teammates at your fingertips! \n \n \n  What You'll Do \n \n \n With access to internal innovative labs, you\u2019ll have the space and time to explore different ways of solving our clients\u2019 challenges, and you\u2019ll work with your team and stakeholders to deliver the right technical solutions \n Whether developing, deploying, or managing IT infrastructures for crucial server and network components, you\u2019ll have the latest tech and brightest teammates at your fingertips \n \n \n \n  Required \n \n \n \n 2+ years of experience with DevOps and Agile methodologies  \n 2+ years of experience with automation tools, including Terraform, CloudFormation, Ansible, or Puppet  \n Experience with automating system configurations and orchestrating network operations and DevOps pipelines, scaling, releases, and day-to-day system operations in Cloud environments Res \n Experience with managing Git-based version control systems, including GitLab and GitHub  \n Experience with architecting and deploying applications on Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform \n \n \n Desired \n \n \n Experience with infrastructure-as-code environments, including activities around the automated server or network configurations, large-scale software deployments, or monitoring and testing, such as continuous integration and continuous delivery (CI/CD) \n Experience with provisioning and managing infrastructure and applications in Cloud environments, including Amazon Web Services (AWS), Azure, or Google Cloud Platform (GCP)  \n Experience with system integration using a variety of protocols, including JSON, REST, and XML  \n Experience with scripting in Bash, PowerShell, Python, Groovy, or Ruby  Experience with leveraging JSON, YAML, or XML for configuration \n         \n \n What You'll Love About Apogee \n \n \n \n Challenging work in support of US Intel Community - a Mission that Matters! \n Access to our cool ApogeePlex facility \n Support for new ideas & encouragement to take risks \n \n \n \n \n Professional Development Assistance (PDA) \n Wicked smart and collaborative coworkers \n Regular interfacing with company leadership \n \n \n \n \n 401(k) with huge company match \n Paid Time Off / Floating & Fixed Holidays \n Medical, Dental, Vision \n Health Savings Accounts / Dependent Care Flexible Spending Accounts \n Life Insurance, Disability (Short and Long Term), Accidental Death and Dismemberment (AD&D) \n \n \n \n  Apogee's Mission \n \n \n \n Be the PROVIDER  of choice for government & commercial organizations with an unwavering commitment to responsiveness, accuracy, integrity, collaboration, and innovation \n Be the EMPLOYER of choice committed to an open & transparent corporate atmosphere and progressive culture that attracts and empowers world class professionals to explore cutting-edge technical solutions while fostering professional growth \n Be the preferred SOURCE for cutting-edge Analytic Products, Systems & Software Engineering, Big Data Integration, IT and Business Services that directly contribute to customer success \n \n \n \n  Apogee is an M/F Disabled and Vet EEO/AA Employer",
        "cleaned_desc": " Experience with managing Git-based version control systems, including GitLab and GitHub  \n Experience with architecting and deploying applications on Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform \n \n \n Desired \n \n \n Experience with infrastructure-as-code environments, including activities around the automated server or network configurations, large-scale software deployments, or monitoring and testing, such as continuous integration and continuous delivery (CI/CD) \n Experience with provisioning and managing infrastructure and applications in Cloud environments, including Amazon Web Services (AWS), Azure, or Google Cloud Platform (GCP)  \n Experience with system integration using a variety of protocols, including JSON, REST, and XML  \n Experience with scripting in Bash, PowerShell, Python, Groovy, or Ruby  Experience with leveraging JSON, YAML, or XML for configuration \n         \n \n What You'll Love About Apogee \n \n ",
        "techs": [
            "gitlab",
            "github",
            "amazon web services (aws)",
            "microsoft azure",
            "google cloud platform",
            "infrastructure-as-code environments",
            "continuous integration and continuous delivery (ci/cd)",
            "json",
            "rest",
            "xml",
            "bash",
            "powershell",
            "python",
            "groovy",
            "ruby",
            "json",
            "yaml",
            "xml"
        ],
        "cleaned_techs": [
            "gitlab",
            "github",
            "aws",
            "microsoft azure",
            "gcp",
            "infrastructure-as-code environments",
            "continuous integration and continuous delivery (ci/cd)",
            "json",
            "rest",
            "xml",
            "bash",
            "powershell",
            "python",
            "groovy",
            "ruby",
            "yaml"
        ]
    },
    "dd11674e85772a58": {
        "terms": [
            "mlops"
        ],
        "salary_min": 141864.6,
        "salary_max": 179632.19,
        "title": "ServiceNow Cloud Observability Developer Velocity Engineer (DevOps)",
        "company": "ServiceNow",
        "desc": "Company Description \n  At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can\u2019t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you. \n  ServiceNow Cloud Observability (formerly Lightstep) enables teams to detect and resolve regressions quickly, regardless of system scale or complexity. We integrate seamlessly into daily workflows, whether you are proactively optimizing performance or investigating a root cause so you can quickly get back to building features. We continue to grow our talented, humble team! We believe that we should always align our actions toward a common purpose, that trust is paramount and must be cultivated, that the most valuable team members multiply rather than add, and that we shouldn\u2019t take ourselves too seriously (except when we\u2019re writing way-too-long sentences about our values). We believe that assembling and retaining an inclusive and diverse team is the best way to grow a successful company. \n  We understand the massive architectural transformation underway in the software industry, and we saw an opportunity to accelerate it, all while improving the quality of the developer and end-user experience. While we learned a great deal from our experiences building Dapper (Google\u2019s distributed tracing solution) and Monarch (Google\u2019s high-availability metrics solution), ServiceNow Cloud Observability in many ways is a reaction to and a generational improvement beyond those approaches. Our story as individuals and as a company revolves around continuous learning, careful listening, and the belief that these fundamentally new software architectures require fundamentally new solutions. \n \n  With more than 7,700+ customers, we serve approximately 85% of the Fortune 500\u00ae, and we're proud to be one of FORTUNE 100 Best Companies to Work For\u00ae and World's Most Admired Companies\u2122. \n  Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow. \n  Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates \n  Job Description \n  What you get to do in this role: \n  As a Developer Velocity Engineer, you will own our developer experience, ensuring that our engineers have all of the best tooling and support to do their jobs efficiently and enjoyably. We're looking for someone who is passionate about continuously improving the way developers work, while treating those developers as the primary stakeholders in the journey. \n \n  Qualifications \n  To be successful in this role: \n \n  You are excited to optimize the developer experience. \n  You have familiarity with some or all of the following: Codefresh, CircleCI, Git, GitHub Actions, test frameworks, builds, packaging and deployment, container technologies and automated developer environment creation. \n  You may have a particular area of expertise, but are still excited to explore new areas (and want to work with a team that encourages you to do so). \n  You value empathy as much as, if not more than, intelligence in your teammates. \n  You\u2019ve worked on (and/or have a lot of curiosity about) large-scale distributed systems, performance, security, microservice-based architectures, and multi-region/multi-cloud environments. \n  You're excited about serving the engineering team that developers at dozens of companies rely on for 5 9's and incident response! \n \n  Bonus points if\u2026 \n \n  You have experience leading the design and implementation of automated test environments. \n  You contribute to open source projects, enjoy giving talks, and/or do community-building work. \n  You have a track record of driving adoption of new methods and technologies across multiple development teams. \n \n  FD21 \n  Additional Information \n  ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law. \n At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office. \n If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance. \n For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government. \n Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site. \n   \n From Fortune. \u00a9 2022 Fortune Media IP Limited All rights reserved. Used under license. \n Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.",
        "cleaned_desc": "",
        "techs": "",
        "cleaned_techs": []
    },
    "95763c28b7036165": {
        "terms": [
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Sr Distinguished Engineer, Generative AI Systems - (Remote- Eligible)",
        "company": "Capital One",
        "desc": "Locations: VA - McLean, United States of America, McLean, Virginia\n   Sr Distinguished Engineer, Generative AI Systems - (Remote- Eligible)\n  \n  Sr Distinguished Engineer, Generative AI Systems \n \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Senior Distinguished Engineer, AI Systems, to help us build the foundations of our enterprise AI Capabilities. You will work on a wide range of initiatives, whether that\u2019s designing robust, secure infrastructure, building large-scale distributed training clusters, deploying LLMs on GPU instances for real-time use cases, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work with a team of AI engineers and researchers to envision the target state of our capabilities while helping to design and implement key services. Examples of projects you will work on include: \n \n \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build infrastructure for serving large ML models, in our public cloud. \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 9 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 6 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud. \n \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $315,100 - $359,700 for Sr Distinguished Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $333,900 - $381,000 for Sr Distinguished Machine Learning Engineer\n   Remote (Regardless of Location): $267,100 - $304,800 for Sr Distinguished Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 9 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 6 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud.   \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n ",
        "techs": [
            "llms",
            "fms",
            "mlops",
            "hpc",
            "ml",
            "ai",
            "python",
            "c/c++",
            "ml frameworks",
            "public cloud",
            "cloud environments (aws",
            "azure",
            "gcp)",
            "security",
            "availability",
            "performance",
            "scalability",
            "cost",
            "gpu clusters",
            "storage",
            "networking",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "llm hosting",
            "fine-tuning",
            "research publications",
            "neural networks",
            "distributed training",
            "sysml"
        ],
        "cleaned_techs": [
            "llm",
            "fms",
            "mlops",
            "hpc",
            "ml",
            "ai",
            "python",
            "c/c++",
            "ml frameworks",
            "public cloud",
            "cloud environments (aws",
            "azure",
            "gcp",
            "availability",
            "performance",
            "scalability",
            "cost",
            "gpu clusters",
            "storage",
            "networking",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "fine-tuning",
            "research publications",
            "neural networks",
            "distributed training",
            "sysml"
        ]
    },
    "86bebde4141e09fe": {
        "terms": [
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Distinguished Engineer, Generative AI Systems (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Locations: NY - New York, United States of America, New York, New York\n   Distinguished Engineer, Generative AI Systems (Remote-Eligible)\n  \n  Distinguished Engineer, Generative AI Systems \n \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Senior Distinguished Engineer, AI Systems, to help us build the foundations of our enterprise AI Capabilities. You will work on a wide range of initiatives, whether that\u2019s designing robust, secure infrastructure, building large-scale distributed training clusters, deploying LLMs on GPU instances for real-time use cases, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work with a team of AI engineers and researchers to envision the target state of our capabilities while helping to design and implement key services. Examples of projects you will work on include: \n \n \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build infrastructure for serving large ML models, in our public cloud. \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 7 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 5 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud. \n \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $269,400 - $307,500 for Distinguished Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $285,400 - $325,700 for Distinguished Machine Learning Engineer\n   Remote (Regardless of Location): $228,300 - $260,600 for Distinguished Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 7 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 5 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud.   \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n ",
        "techs": [
            "llms",
            "fms",
            "mlops",
            "python",
            "c/c++",
            "ml development lifecycle",
            "ai",
            "ml frameworks",
            "public cloud",
            "modern ai techniques",
            "large-scale distributed platforms",
            "cloud environments",
            "aws",
            "azure",
            "gcp",
            "security",
            "availability",
            "performance",
            "scalability",
            "cost",
            "mlops life cycle",
            "gpu clusters",
            "ml compilers",
            "distributed training frameworks",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "llm hosting",
            "fine-tuning",
            "research publications",
            "neural networks",
            "distributed training",
            "sysml."
        ],
        "cleaned_techs": [
            "llm",
            "fms",
            "mlops",
            "python",
            "c/c++",
            "ai",
            "ml frameworks",
            "public cloud",
            "modern ai techniques",
            "large-scale distributed platforms",
            "cloud environments",
            "aws",
            "azure",
            "gcp",
            "availability",
            "performance",
            "scalability",
            "cost",
            "mlops life cycle",
            "gpu clusters",
            "ml compilers",
            "distributed training frameworks",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "fine-tuning",
            "research publications",
            "neural networks",
            "distributed training",
            "sysml."
        ]
    },
    "fe97af589791c8a0": {
        "terms": [
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Distinguished Engineer, Generative AI Systems (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Locations: NY - New York, United States of America, New York, New York\n   Distinguished Engineer, Generative AI Systems (Remote-Eligible)\n  \n  Distinguished Engineer, Generative AI Systems \n \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Senior Distinguished Engineer, AI Systems, to help us build the foundations of our enterprise AI Capabilities. You will work on a wide range of initiatives, whether that\u2019s designing robust, secure infrastructure, building large-scale distributed training clusters, deploying LLMs on GPU instances for real-time use cases, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work with a team of AI engineers and researchers to envision the target state of our capabilities while helping to design and implement key services. Examples of projects you will work on include: \n \n \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build infrastructure for serving large ML models, in our public cloud. \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 7 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 5 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud. \n \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $269,400 - $307,500 for Distinguished Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $285,400 - $325,700 for Distinguished Machine Learning Engineer\n   Remote (Regardless of Location): $228,300 - $260,600 for Distinguished Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 7 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 5 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud.   \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n ",
        "techs": [
            "llms",
            "fms",
            "mlops",
            "python",
            "c/c++",
            "ai and ml frameworks",
            "aws",
            "azure",
            "gcp",
            "gpu clusters",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases",
            "llm hosting"
        ],
        "cleaned_techs": [
            "llm",
            "fms",
            "mlops",
            "python",
            "c/c++",
            "ai",
            "aws",
            "azure",
            "gcp",
            "gpu clusters",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases/knowledge bases"
        ]
    },
    "7723a8b73972042e": {
        "terms": [
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Distinguished Engineer, Generative AI Systems (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Locations: NY - New York, United States of America, New York, New York\n   Distinguished Engineer, Generative AI Systems (Remote-Eligible)\n  \n  Distinguished Engineer, Generative AI Systems \n \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Senior Distinguished Engineer, AI Systems, to help us build the foundations of our enterprise AI Capabilities. You will work on a wide range of initiatives, whether that\u2019s designing robust, secure infrastructure, building large-scale distributed training clusters, deploying LLMs on GPU instances for real-time use cases, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work with a team of AI engineers and researchers to envision the target state of our capabilities while helping to design and implement key services. Examples of projects you will work on include: \n \n \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build infrastructure for serving large ML models, in our public cloud. \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 7 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 5 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud. \n \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $269,400 - $307,500 for Distinguished Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $285,400 - $325,700 for Distinguished Machine Learning Engineer\n   Remote (Regardless of Location): $228,300 - $260,600 for Distinguished Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 7 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 5 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud.   \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n ",
        "techs": [
            "benchmarking tools",
            "llms",
            "fms",
            "mlops",
            "distributed computing",
            "hpc",
            "ml systems",
            "ai",
            "ml algorithms",
            "python",
            "c/c++",
            "ml development frameworks",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases",
            "knowledge bases",
            "llm hosting",
            "fine-tuning",
            "neural networks",
            "distributed training",
            "sysml"
        ],
        "cleaned_techs": [
            "benchmarking tools",
            "llm",
            "fms",
            "mlops",
            "distributed computing",
            "hpc",
            "ml systems",
            "ai",
            "ml algorithms",
            "python",
            "c/c++",
            "ml development frameworks",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases",
            "knowledge bases",
            "fine-tuning",
            "neural networks",
            "distributed training",
            "sysml"
        ]
    },
    "a923d7333d367c9c": {
        "terms": [
            "mlops"
        ],
        "salary_min": null,
        "salary_max": null,
        "title": "Distinguished Engineer, Generative AI Systems (Remote-Eligible)",
        "company": "Capital One",
        "desc": "Locations: NY - New York, United States of America, New York, New York\n   Distinguished Engineer, Generative AI Systems (Remote-Eligible)\n  \n  Distinguished Engineer, Generative AI Systems \n \n  Our mission at Capital One is to create trustworthy, reliable and human-in-the-loop AI systems, changing banking for good. For years, Capital One has been leading the industry in using machine learning to create real-time, intelligent, automated customer experiences. From informing customers about unusual charges to answering their questions in real time, our applications of AI & ML are bringing humanity and simplicity to banking. Because of our investments in public cloud infrastructure and machine learning platforms, we are now uniquely positioned to harness the power of AI. We are committed to building world-class applied science and engineering teams and continue our industry leading capabilities with breakthrough product experiences and scalable, high-performance AI infrastructure. At Capital One, you will help bring the transformative power of emerging AI capabilities to reimagine how we serve our customers and businesses who have come to love the products and services we build. \n \n  We are looking for an experienced Senior Distinguished Engineer, AI Systems, to help us build the foundations of our enterprise AI Capabilities. You will work on a wide range of initiatives, whether that\u2019s designing robust, secure infrastructure, building large-scale distributed training clusters, deploying LLMs on GPU instances for real-time use cases, or supporting cutting-edge AI research and development, all in our public cloud infrastructure. You will work with a team of AI engineers and researchers to envision the target state of our capabilities while helping to design and implement key services. Examples of projects you will work on include: \n \n \n  Design and build fault-tolerant infrastructure to support long-running large-scale training tasks reliably despite failure of individual nodes, using containers and check-pointing libraries. \n  Design and build infrastructure for serving large ML models, in our public cloud. \n  Deploy a thousand-node training cluster optimizing storage and networking stack, with tightly coupled training pipelines to take advantage of multiple parallelism strategies, in our public cloud. \n  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 7 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 5 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud. \n \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n \n \n  Capital One will consider sponsoring a new qualified applicant for employment authorization for this position. \n \n  The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. \n  New York City (Hybrid On-Site): $269,400 - $307,500 for Distinguished Machine Learning Engineer\n   San Francisco, California (Hybrid On-Site): $285,400 - $325,700 for Distinguished Machine Learning Engineer\n   Remote (Regardless of Location): $228,300 - $260,600 for Distinguished Machine Learning Engineer\n  \n  Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter. \n  This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n  \n \n \n \n \n  Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. \n \n \n \n \n  No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n  \n  If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. \n \n  For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com \n \n  Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. \n \n  Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "cleaned_desc": "  Design and implement benchmarks to measure the performance of software systems within AI capabilities and make recommendations on technology selection \n  Develop applications that leverage LLMs and FMs. \n  Design and implement capabilities to support MLOps for foundation models. \n \n \n  Capital One is open to hiring a Remote Employee for this opportunity \n \n  Basic Qualifications: \n \n  Bachelor's degree in Computer Science, Computer Engineering or a technical field \n  At least 7 years of experience designing and building distributed computing HPC and large-scale ML systems \n  At least 5 years of experience developing AI and ML algorithms in Python or C/C++ \n  At least 3 years of experience with the full ML development lifecycle using AI and ML frameworks and public cloud.   \n \n  Preferred Qualifications: \n \n  Master\u2019s degree or PhD in Engineering, Computer Science, a related technical field, or equivalent practical experience with a focus on modern AI techniques. \n  Experience designing large-scale distributed platforms and/or systems in cloud environments such as AWS, Azure, or GCP. \n  Experience architecting cloud systems for security, availability, performance, scalability, and cost. \n  Experience with delivering very large models through the MLOps life cycle from exploration to serving. \n  Experience with building GPU clusters in the public cloud with tightly-coupled storage and networking. \n  Experience with the complete stack for distributed training of large models including ML compilers, distributed training frameworks, and ML development frameworks such as Pytorch, Tensorflow, Lightning etc. \n  Experience with one or multiple areas of AI technology stack including prompt engineering, guardrails, vector databases/knowledge bases, LLM hosting and fine-tuning. \n  Authored research publications in top peer-reviewed conferences, or industry-recognized contributions in the space of neural networks, distributed training and SysML. \n ",
        "techs": [
            "benchmarks",
            "software systems",
            "ai capabilities",
            "technology selection",
            "llms",
            "fms",
            "mlops",
            "distributed computing",
            "hpc",
            "ml systems",
            "ai algorithms",
            "python",
            "c/c++",
            "ml development lifecycle",
            "ml frameworks",
            "public cloud",
            "ai techniques",
            "distributed platforms",
            "cloud environments",
            "aws",
            "azure",
            "gcp",
            "cloud systems",
            "security",
            "availability",
            "performance",
            "scalability",
            "cost",
            "mlops life cycle",
            "exploration",
            "serving",
            "gpu clusters",
            "storage",
            "networking",
            "ml compilers",
            "distributed training frameworks",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases",
            "knowledge bases",
            "llm hosting",
            "fine-tuning",
            "research publications",
            "neural networks",
            "distributed training",
            "sysml"
        ],
        "cleaned_techs": [
            "benchmarks",
            "software systems",
            "ai",
            "technology selection",
            "llm",
            "fms",
            "mlops",
            "distributed computing",
            "hpc",
            "ml systems",
            "python",
            "c/c++",
            "ml frameworks",
            "public cloud",
            "distributed platforms",
            "cloud environments",
            "aws",
            "azure",
            "gcp",
            "cloud systems",
            "availability",
            "performance",
            "scalability",
            "cost",
            "mlops life cycle",
            "serving",
            "gpu clusters",
            "storage",
            "networking",
            "ml compilers",
            "distributed training frameworks",
            "pytorch",
            "tensorflow",
            "lightning",
            "prompt engineering",
            "guardrails",
            "vector databases",
            "knowledge bases",
            "fine-tuning",
            "research publications",
            "neural networks",
            "distributed training",
            "sysml"
        ]
    },
    "metadata": {
        "keywords": [
            "data science",
            "data analyst",
            "data engineer",
            "machine learning engineer",
            "mlops"
        ],
        "locations": [
            "remote"
        ],
        "time_ran": "11:23:35-17-10-23",
        "num_jobs": 278,
        "timings": {
            "start_drivers": 45.77246642112732,
            "find_job_ids": 437.96008372306824,
            "get_job_descs": 129.12773823738098
        },
        "models": {
            "classifier": {
                "clf": "data/classifier_models/job_desc_classifier_v1.0.pkl",
                "tfidf": "data/classifier_models/job_desc_tfidf_vectorizer_v1.0.pkl"
            },
            "NER": "gpt-3.5-turbo"
        }
    }
}