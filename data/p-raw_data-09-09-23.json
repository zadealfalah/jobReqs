{"5797d2bf25c48cdf": {"terms": ["data science"], "salary_min": 140900.0, "salary_max": 225100.0, "title": "Senior Data Scientist", "company": "Zillow", "desc": "About the team  Zillow Group\u2019s mission is to give people the power to unlock life's next chapter. The Customer Conversion Analytics team builds models that enable smarter products and deliver strategic insights that help customers transition from browsing to successfully buying or selling a home.\n  \n  Zillow, the top real estate website in the U.S., is building an on-demand real estate experience. Whether selling, buying, renting or financing, customers can turn to Zillow to find and get into their next home with speed, certainty and ease.\n  \n  About the role \n  This role will employ a broad variety of data science skills, all with the aim of creating a fantastic product that supports Flex agents who partner with Zillow via revenue sharing to help buyers navigate their home-buying journey. This role helps understand questions such as: \n \n  Real estate transactions are long processes with a large range of possible duration to transaction. Given this and idiosyncrasies associated with buyers, markets and properties, when we deliver new leads to agents, how many of those will eventually convert to transactions? \n  How can we leverage existing data and experimentation to provide improved guidance on revenue sharing with Flex agents in different markets? \n  How can we advise Finance and Operations partners on strategic decisions regarding the Flex program? \n \n \n  To that end, you will: \n \n  Maintain and improve an existing production conversion model that supports our Flex Agent business. \n  Proactively conduct hypothesis-focused deep dives and explorations to understand the seller journey and experience with Zillow Group products \n  Design and execute experiments (ex: A/B tests) that measure impact of product changes. Frame experimentation problems, both statistically and within the business and customer context \n  Present and interpret customer insights, metric development, testing methodologies, and experimentation results to technical and non-technical partners \n  Work with engineering teams to improve data collection procedures to ensure integrity and quality of the data. \n \n  This role has been categorized as a Remote position. \u201cRemote\u201d employees do not have a permanent corporate office workplace and, instead, work from a physical location of their choice which must be identified to the Company. Employees may live in any of the 50 US States, with limited exceptions. In certain cases, an employee in a remote-designated job may need to live in a specific region or time zone to support customers or clients as part of their role.\n   In California, Colorado, Connecticut, Nevada, New York City and Washington the standard base pay range for this role is $140,900.00 - $225,100.00 Annually. This base pay range is specific to California, Colorado, Connecticut, Nevada, New York City and Washington and may not be applicable to other locations.\n   In addition to a competitive base salary this position is also eligible for equity awards based on factors such as experience, performance and location. Actual amounts will vary depending on experience, performance and location.\n  \n  Who you are \n  Are you passionate about all things \u201chome\u201d? Would you describe yourself as a curious and ambitious problem solver? Come help us guide our business into the future with powerful insights and recommendations! We're seeking a data scientist with the follow qualifications: \n \n \n  Possesses either an undergraduate or Master's degree in a quantitative field (e.g. mathematics, finance, statistics, or similar) or confirmed experience within data science and analytics \n  3+ years of work experience involving quantitative data analysis and complex problem solving \n  Excellent communication skills with the ability to distill complex issues and detailed analysis into simple, structured frameworks with concrete action plans \n  Experience building statistical models to yield insights from complex user journeys. Experience in maintaining and developing production-grade models is a must. \n  Strong proficiency in Python and/or another programming language; experience using SQL, Tableau, Excel and Airflow. \n  Experience in experimentation methodologies, causal inferences and pricing \n  Strong product sense \n \n \n  Get to know us \n  Zillow is reimagining real estate to make home a reality for more and more people. \n \n  As the most-visited real estate website in the United States, Zillow\u00ae and its affiliates help movers find and win their home through digital solutions, first class partners, and easier buying, selling, financing and renting experiences. Millions of people visit Zillow Group sites every month to start their home search, and now they can rely on Zillow to help make it easier to move. The work we do helps people get home and no matter what job you're in, you will play a critical role in making home a reality for more and more people. \n \n  Our efforts to streamline the real estate transaction are supported by a deep-rooted culture of innovation, our passion to redefine the employee experience, a fundamental commitment to Equity and Belonging, and world-class benefits. These benefits include comprehensive medical, dental, vision, life, and disability coverages as well as parental leave, family benefits, retirement contributions, and paid time off. We\u2019re also setting the standard for work experiences of the future, where our employees are supported in doing their best work and living a flexible, well-balanced life. But don\u2019t just take our word for it. Read recent reviews on Glassdoor and recent recognition from multiple organizations, including: the 100 Best Companies to Work For, Glassdoor Employees\u2019 Choice Award, Bloomberg Gender-Equality Index, Human Rights Campaign (HRC) Corporate Equity Index, and TIME 100 Most Influential Companies list. \n \n  Zillow Group is an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. If you have a disability or special need that requires accommodation, please contact us at RecruitingAccessibility@zillowgroup.com. \n \n  Applicants who receive job offers from Zillow Group will be asked to sign a Proprietary Rights Agreement which includes confidentiality, intellectual property assignment, customer and employee non-solicitation, and non-competition provisions. If you are contacted for a role at Zillow Group and wish to review a copy of the Proprietary Rights Agreement prior to receiving an offer, you may request a copy from your Recruiter.", "cleaned_desc": " \n  Possesses either an undergraduate or Master's degree in a quantitative field (e.g. mathematics, finance, statistics, or similar) or confirmed experience within data science and analytics \n  3+ years of work experience involving quantitative data analysis and complex problem solving \n  Excellent communication skills with the ability to distill complex issues and detailed analysis into simple, structured frameworks with concrete action plans \n  Experience building statistical models to yield insights from complex user journeys. Experience in maintaining and developing production-grade models is a must. \n  Strong proficiency in Python and/or another programming language; experience using SQL, Tableau, Excel and Airflow. \n  Experience in experimentation methodologies, causal inferences and pricing \n  Strong product sense \n ", "techs": ["python", "sql", "tableau", "excel", "airflow"]}, "5726d0c9f7a246bf": {"terms": ["data science"], "salary_min": 90000.0, "salary_max": 125000.0, "title": "Data Scientist: Food Industry", "company": "Marler Search Group", "desc": "Summary: \n We are seeking a highly skilled and innovative Food Data Scientist to join our team and play a key role in the development of a cutting-edge 3D food modeling database. As a Food Data Scientist, you will leverage your expertise in data analysis, database architecture and food science to create a comprehensive and dynamic database model of food items in three dimensions. Your work will contribute to advancing the field of food technology and revolutionize how consumers interact with food-related information. \n Responsibilities: \n Data Collection and Curation:  Collect, organize, and curate a diverse dataset of 3D representations of food items, including raw ingredients, prepared dishes, and food packaging. \n Data Modeling:  Develop and implement database models to extract meaningful insights from 3D food data, such as nutritional information, texture, and appearance. \n Image Processing:  Utilize image processing techniques to enhance the quality and accuracy of 3D food models, ensuring realistic and visually appealing representations. \n Data Integration:  Collaborate with cross-functional teams to integrate 3D food data into various applications, including augmented reality (AR), virtual reality (VR), and mobile apps. \n Quality Assurance:  Continuously monitor and improve data quality, addressing any discrepancies or inconsistencies in the 3D food database. \n Research and Development:  Stay up-to-date with the latest developments in food technology, data science, and machine learning to propose and implement innovative approaches to enhance the database. \n Data Visualization:  Create visually engaging and informative visualizations to communicate insights and findings from the 3D food data. \n Interdisciplinary Collaboration:  Work closely with food scientists, computer vision experts, software developers, and designers to ensure seamless integration of 3D food data into consumer-facing products. \n Documentation:  Maintain detailed documentation of data collection processes, data models, and algorithms used in the development of the 3D food database. \n Compliance and Security:  Ensure that data handling and storage comply with relevant data privacy and security regulations. \n Qualifications: \n 5 years experience with 3D database modeling and rendering software/tools. \n Strong proficiency in computer vision, and image processing techniques. \n Proficiency in programming languages such as Python, R, or similar for data analysis and modeling. \n Strong knowledge of databases and data storage systems. \n Bachelor's, Master's, or Ph.D. in Computer Science, Data Science, Food Science, or a related field. \n Excellent problem-solving skills and the ability to work independently or as part of a team. \n Strong communication skills to collaborate effectively with cross-functional teams and convey complex concepts to non-technical stakeholders. \n Knowledge of food science and nutrition is a plus. \n Experience in data collection, curation, and management is advantageous. \n Passion for food technology and a drive to innovate in the food industry. \n Job Type: Full-time \n Salary: $90,000.00 - $125,000.00 per year \n Benefits: \n \n Health insurance \n \n Experience level: \n \n 3 years \n \n Schedule: \n \n Monday to Friday \n \n Education: \n \n Bachelor's (Required) \n \n Experience: \n \n Python and R: 3 years (Required) \n Database Modelling: 3 years (Required) \n Food/Nutrition Industray: 2 years (Required) \n \n Work Location: Remote", "cleaned_desc": "Summary: \n We are seeking a highly skilled and innovative Food Data Scientist to join our team and play a key role in the development of a cutting-edge 3D food modeling database. As a Food Data Scientist, you will leverage your expertise in data analysis, database architecture and food science to create a comprehensive and dynamic database model of food items in three dimensions. Your work will contribute to advancing the field of food technology and revolutionize how consumers interact with food-related information. \n Responsibilities: \n Data Collection and Curation:  Collect, organize, and curate a diverse dataset of 3D representations of food items, including raw ingredients, prepared dishes, and food packaging. \n Data Modeling:  Develop and implement database models to extract meaningful insights from 3D food data, such as nutritional information, texture, and appearance. \n Image Processing:  Utilize image processing techniques to enhance the quality and accuracy of 3D food models, ensuring realistic and visually appealing representations. \n Data Integration:  Collaborate with cross-functional teams to integrate 3D food data into various applications, including augmented reality (AR), virtual reality (VR), and mobile apps. \n Quality Assurance:  Continuously monitor and improve data quality, addressing any discrepancies or inconsistencies in the 3D food database. \n Research and Development:  Stay up-to-date with the latest developments in food technology, data science, and machine learning to propose and implement innovative approaches to enhance the database.   Data Visualization:  Create visually engaging and informative visualizations to communicate insights and findings from the 3D food data. \n Interdisciplinary Collaboration:  Work closely with food scientists, computer vision experts, software developers, and designers to ensure seamless integration of 3D food data into consumer-facing products. \n Documentation:  Maintain detailed documentation of data collection processes, data models, and algorithms used in the development of the 3D food database. \n Compliance and Security:  Ensure that data handling and storage comply with relevant data privacy and security regulations. \n Qualifications: \n 5 years experience with 3D database modeling and rendering software/tools. \n Strong proficiency in computer vision, and image processing techniques. \n Proficiency in programming languages such as Python, R, or similar for data analysis and modeling. \n Strong knowledge of databases and data storage systems. ", "techs": ["3d database modeling and rendering software/tools", "computer vision", "image processing techniques", "programming languages (python", "r)", "databases", "data storage systems"]}, "5a40ec2b5cfe588e": {"terms": ["data science"], "salary_min": 105137.15, "salary_max": 133127.06, "title": "Data Scientist - 23274", "company": "Enverus", "desc": "Data Scientist \n  Why YOU want this position \n  Enverus is the leading energy SaaS company delivering highly technical insights and predictive/prescriptive analytics that empower customers to make decisions that increase profit. Enverus\u2019 innovative technologies drive production and investment strategies, enable best practices for energy and commodity trading and risk management, and reduce costs through automated processes across critical business functions. Enverus is a strategic partner to more than 6,000 customers in 50 countries. \n  We are currently seeking a Data Scientist to join our Power & Renewables team. This role offers the opportunity to join a rapidly growing company delivering industry-leading solutions to customers in the world\u2019s most dynamic and fastest-growing sector. \n \n  Key values to the Enverus culture: \n \n   Collaboration and Teamwork-  You thrive in an environment that supports effective teamwork and collaboration by earning the trust and respect of those around you. \n   Job Knowledge-  You have the drive to be the best at what you do. Enverus will support your professional growth as well as encourage you to share your knowledge with your team. \n   Quality of Work-  You are passionate about what you do and every detail counts. \n   Communication-  You recognize that active listening is just as important as clearly communicating your message. This is key to our collaborative and transparent environment. \n   Respect \u2013  You believe respect is the basis of all relationships both inside and outside of Enverus. \n   Reliability \u2013 You are someone your team can rely on to finish what you start. \n   Integrity - You are honest and ethical in all of your relationships and decisions. \n \n \n  Performance Objectives \n \n  You will work with the CRCL Solutions team reporting directly to the VP of Power & Renewables. You will work on Senior Leadership agenda and high priority projects that are critical to the success of Enverus. \n  You will take a strategic approach to implement tangible and practical solutions, developing the most complex analytical algorithms to tackle Enverus and its clients\u2019 most difficult and crucial problems. We have a focus to embrace cutting-edge development in computer vision, pattern recognition, and a wide range of machine learning techniques to build patentable algorithms to meaningfully contribute to the digital revolution of energy sector. \n  You will enjoy a range of firm-wide social activities. The teams also engage in regular socials around the year fostering a collaborative and intellectual environment for innovation. \n  Admission is selective, candidates may be offered a set of optional math and programming challenges to gauge skills and aptitude levels. \n \n  Competitive Candidate Profile \n \n \n \n  Masters or PhD in mathematics, physics, computer science, engineering, or a similar quantitative field. \n  Prior experience with Python (preferred), R, or MATLAB. \n  Experience in applying ML/AI techniques in an R&D or professional setting. \n  Ability to work independently and as a member of a team on assigned projects and tasks with limited supervision. \n  Ability to prioritize, raise issues and resolve tough problems in a timely fashion to meet business deadlines. \n  Ability to identify and execute R&D initiatives that could improve Enverus' suite of renewable power forecasting products. \n  Self-motivated, task-oriented personality with a strong work ethic and desire to learn. \n  Portfolio of algorithm designs in any coding language (excel welcomed).", "cleaned_desc": "", "techs": ""}, "759d15fcab541a9b": {"terms": ["data science", "data analyst"], "salary_min": 69473.97, "salary_max": 87969.53, "title": "Data Analytics Analyst", "company": "IQVIA", "desc": "The Data Analyst plays a vital role in supporting clients working with IQVIA's Contract Performance Solution (CPS) team. In addition, the analyst will support development of CPS team Master Data Management tool . The analyst will also provide executive summaries and perform client brand specific analysis as required. Analysts begin under the guidance of more senior team members but are quickly provided with opportunities to contribute to all aspects of client engagements. Team members will begin to assume an active ownership in running projects and mentoring more junior colleagues. \n \n  Responsibilities include: \n \n  Monitor daily transmissions of inbound data to ensure required data transactions are received \n  Perform Data Loads using on Oracle and Big Data Factory using SQL's and Alteryx \n  Setting up data exports in the required format, assuring the exports contain all necessary data requested by Business Operations Teams \n  Quality control of the data files/ inbound data feeds \n  Perform Data analysis and validation \n  Support Development of Master Data Management Tool using IQVIA Integrated Development Platform and Python \n  Implement automation and innovation in legacy data load pipelines \n \n \n  Minimum Requirements \n \n  Bachelor's Degree . Computer science preferred \n  Experience in Data Management and Governance Mythology \n  Hands on expertise in data loading, validation and analysis using ETL tools \n  Exceptional IT literacy in Excel, SQL, Airflow and Python \n  Big Data Factory (BDF) and AI/ML is a plus \n  Knowledge of Pharmaceutical Revenue Management Systems Data is plus \n  Fluent English (written and spoken - FCE level or higher) is a must \n  Strong Microsoft Office skills, specifically MS Excel \n  Good attention to detail, multi-tasking, and organizational abilities \n  A strong analytical record, excellent problem-solving abilities, and strong quantitative skills \n  Exceptional communication skills and be a proven team contributor \n  An interest in and desire to learn about the constantly evolving healthcare industry \n  Good project management, time management and organizational skills \n \n \n  IQVIA is a leading global provider of advanced analytics, technology solutions and clinical research services to the life sciences industry. We believe in pushing the boundaries of human science and data science to make the biggest impact possible \u2013 to help our customers create a healthier world. Learn more at https://jobs.iqvia.com \n \n  We are committed to providing equal employment opportunities for all, including veterans and candidates with disabilities. https://jobs.iqvia.com/eoe \n \n  As the COVID-19 virus continues to evolve, IQVIA\u2019s ability to operate and provide certain services to customers and partners necessitates IQVIA and its employees meet specific requirements regarding vaccination status. https://jobs.iqvia.com/covid-19-vaccine-status", "cleaned_desc": "  Setting up data exports in the required format, assuring the exports contain all necessary data requested by Business Operations Teams \n  Quality control of the data files/ inbound data feeds \n  Perform Data analysis and validation \n  Support Development of Master Data Management Tool using IQVIA Integrated Development Platform and Python \n  Implement automation and innovation in legacy data load pipelines \n   \n  Minimum Requirements \n \n  Bachelor's Degree . Computer science preferred \n  Experience in Data Management and Governance Mythology \n  Hands on expertise in data loading, validation and analysis using ETL tools    Exceptional IT literacy in Excel, SQL, Airflow and Python \n  Big Data Factory (BDF) and AI/ML is a plus \n  Knowledge of Pharmaceutical Revenue Management Systems Data is plus \n  Fluent English (written and spoken - FCE level or higher) is a must \n  Strong Microsoft Office skills, specifically MS Excel \n  Good attention to detail, multi-tasking, and organizational abilities ", "techs": ["iqvia integrated development platform", "python", "excel", "sql", "airflow", "big data factory (bdf)", "ai/ml", "pharmaceutical revenue management systems data", "microsoft office"]}, "2b038a24f2df9b91": {"terms": ["data science"], "salary_min": 64009.117, "salary_max": 81049.805, "title": "AFIT Instructor of Data Science (Analytics / Management)", "company": "BTAS", "desc": "Position:  Instructor of Data Science (Analytics / Management)\n   \n Work Location:  Wright-Patterson Air Force Base, Dayton, OH\n   \n Clearance Requirement:  Tier 1 / (NACI)\n   \n Position Type / Standard Work Hours:  40 Hrs. / Full-Time / On-Site (Remote support will be assessed for out of state candidates on an individual basis.)\n   \n \n Summary/ Objective: \n \n \n  Develop and administer PCE developed to educate/train joint service active duty, reservist, and National Guard, officers, enlisted, and civilians on tactical, strategic, and operational levels.\n  \n \n \n Primary Responsibilities: \n \n \n Interact with various course sponsors, subject matter experts, and instructional systems designers to determine education needs in order to develop, maintain, and instruct course content appropriately. All curriculums shall be approved by the course leadership and designed to facilitate understanding and engagement in accordance with curriculum standards. \n Coordinate course offerings, document courses in accordance with Air Force and AFIT policies and conduct post-offering assessments. \n Instruct students across multiple approved educational platforms and settings to include traditional in residence/on-site classrooms and distance learning services. \n Available to travel and instruct at different locations, both CONUS and OCONUS as mission requires and approved by the Government. \n Participate in consultation efforts, and attend or participate in workshops, conferences, technical interchanges or similar venues as approved by the Government on a case-by-case basis. \n Assist faculty and staff with graduate-level research, including literature reviews, data collection, technical writing, instruction, and other duties as required. \n \n \n \n Supervisory Responsibilities: \n  This position does not have supervisory responsibilities.\n   \n \n Travel: \n \n \n  Position may require occasional travel.\n   \n \n Required Education and/or Experience: \n \n \n Bachelor\u2019s degree in Education, Business Management, Program Management, or related technical field. \n Demonstrated knowledge of the DoD acquisition cycle \n \n \n \n AAP / EEO Statement: \n  BTAS is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.\n  \n \n \n  BTAS is an E-Verify program participant.\n  \n \n \n Other Duties: \n \n \n  Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and/or activities may change at any time with or without notice.", "cleaned_desc": "", "techs": ""}, "8618761c1a6b28b2": {"terms": ["data science"], "salary_min": 93433.58, "salary_max": 118307.734, "title": "Data Scientist - Generative AI", "company": "HP", "desc": "** Locations include Spring, Texas (preferred location), and US remote.   ** Typically need at least Master's degree and minimum 0-2 years full-time relevant work experience. \n \n  The Team \n  We are a growing centralized team helping HP take advantage of new AI/ML technology, especially around Generative AI and large language models. We engage with business units to advise and prototype solutions, and we develop and run software applications for internal use. \n \n  The Role \n  As a Data Scientist with a focus on Generative AI you will work on multiple engagements across HP involving large language models and other new Generative AI capabilities. Beyond our team, you will work with business stakeholders and developers from other business units. Your primary focus and mindset is to help deliver business solutions to our (mostly internal) customers. You are expected to stay up to date on important new Gen AI papers and releases, but note that this is not an AI research role. \n \n  Skills and Profile \n \n Knowledge of data science and machine learning core skills. \n Experience with NLP, Large Language Models, LLM prompt engineering, vector databases, Retrieval Augmented Generation (RAG), and search engines. \n Good computer science skills. Experience in a software development team is a big plus, especially with business applications in a cloud environment. \n Tools you may use include Azure services such as Azure Machine Learning and Azure prompt flow, as well as python, langchain, streamlit, docker, git, and elastic search. \n Some experience from a large complex organization is a plus. \n Ability to participate in meetings with a multitude of stakeholders and communicate in a crisp manner. Mastery in English is required. \n You enjoy explaining technical concepts (such as LLM's) to a non-technical audience. \n You propose pragmatic solutions that are as simple as possible, which sometimes mean that no Gen AI component is necessary. \n Interest in working in a distributed team with diverse backgrounds. \n The recent AI progress is disruptive, and our team is in the midst of it. As a consequence, day-to-day priorities, tasks, and team structure may change rapidly. We are looking for somebody who thrives in such an environment. The role is not a fit if you value \"business as usual\". \n \n \n  Education and Length of Experience  For this position, we prefer at least a relevant Master's degree (e.g. Computer Science) or demonstrated competence, and a minimum of 0-2 years experience. \n \n  About HP \n \n \n \n \n \n  You\u2019re out to reimagine and reinvent what\u2019s possible\u2014in your career as well as the world around you.\n  \n \n   So are we. We love taking on tough challenges, disrupting the status quo, and creating what\u2019s next. We\u2019re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.\n  \n \n \n \n   HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.\n  \n \n \n \n   Our history: HP\u2019s commitment to diversity, equity and inclusion \u2013 it's just who we are.\n  \n \n   From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you\u2019re more innovative and that helps grow our bottom line. Come to HP and thrive!", "cleaned_desc": " \n Knowledge of data science and machine learning core skills. \n Experience with NLP, Large Language Models, LLM prompt engineering, vector databases, Retrieval Augmented Generation (RAG), and search engines. \n Good computer science skills. Experience in a software development team is a big plus, especially with business applications in a cloud environment. \n Tools you may use include Azure services such as Azure Machine Learning and Azure prompt flow, as well as python, langchain, streamlit, docker, git, and elastic search. \n Some experience from a large complex organization is a plus. \n Ability to participate in meetings with a multitude of stakeholders and communicate in a crisp manner. Mastery in English is required. \n You enjoy explaining technical concepts (such as LLM's) to a non-technical audience. \n You propose pragmatic solutions that are as simple as possible, which sometimes mean that no Gen AI component is necessary. ", "techs": ["nlp", "large language models (llm)", "llm prompt engineering", "vector databases", "retrieval augmented generation (rag)", "search engines", "azure machine learning", "azure prompt flow", "python", "langchain", "streamlit", "docker", "git", "elastic search"]}, "d8de00a566ddf72a": {"terms": ["data science"], "salary_min": null, "salary_max": null, "title": "Epidemiologist - US/Canada", "company": "PHASTAR", "desc": "Overview: \n  \n  THE COMPANY \n \n \n \n  Phastar is a multiple award-winning global biometric Contract Research Organization (CRO) that is accredited as an outstanding company to work for by Best Companies. We partner with pharmaceutical, biotechnology and medical device organizations to provide the expertise and processes to manage and deliver on time, quality biostatistics, programming, data management and data science services. With offices across the UK, US, Germany, Denmark, Kenya, Australia, India, China and Japan, Phastar is the second largest specialized biometrics provider globally, and the largest in the UK.\n  \n \n \n  Our unique approach to data analysis, \u201cThe Phastar Discipline\u201d, has led us to build a reputation for outstanding quality. With this as our core focus, we\u2019re looking for talented individuals who share our passion for quality and technical expertise to join our team.\n  \n \n \n  WHY PHASTAR \n \n \n \n  Accredited as an outstanding company to work for, Phastar is committed to employee engagement, workplace satisfaction and ensuring a healthy work-life balance. We offer flexible working, part-time hours, involvement in developing company-wide initiatives, structured training and development plans, and a truly supportive, fun and friendly environment.\n  \n \n \n  What\u2019s more, when you join our team, Phastar will plant a tree in your honour, as one of our Environmental, Social and Governance (ESG) initiatives. So, not only would you get your dream job, you\u2019ll also be helping to save the planet!\n  \n \n \n  THE ROLE \n \n \n \n  Demand for our Functional Service Provision is growing, and we are looking for an experienced Epidemiologist to join our FSP team.\n  \n \n \n  This is a fantastic opportunity to work for a growing CRO that is recognized for its continuous learning and development opportunities, whilst also gaining direct experience of working within a pharmaceutical environment.\n   Responsibilities: \n  \n Propose, design, initiate, and report epidemiologic study analyses for their assigned Registry \n  Coordinate work between Biostatistics and Statistical Programming resources for planning/implementation of registry analytical deliverables \n  Lead regularly scheduled team meetings with Biostatistics and Statistical Programming \n  Ensure project timelines are maintained \n  Review analysis plans, TLF shells, and TLFs. Prepare report summary and/or slides based on analytical project results (including SAB meeting slides) \n  Accountable to lead the project planning, management, and execution of epidemiology deliverables for their assigned Registry \n  Provide epidemiology support to Medical Affairs, Pharmacovigilance, Regulatory Affairs, and Clinical Development related to their assigned Registry \n  Partcicipate on cross-functional Registry Leadership Teams as appropriate \n  Lead sub-team meetings with Registry Biostatisticians and Programmers as appropriate \n  Provide epidemiology perspective into study protocols, case report forms (CRFs), and regulatory documents and other reports for their assigned Registry \n  May interact with authors, and in-house reviewers on scientific manuscripts, clinical conference abstracts, presentations, and posters based on data generated from their assigned Registry \n  May participate with Epidemiology department during interactions with key external stakeholders including Registry Scientific Advisory Boards and regulatory authorities \n  Qualifications: \n  \n A minimum of 3 years of hands-on experience, within the pharmaceutical industry, biotechnology, or consulting environment for Doctoral-level candidates, or 6 years of industry experience for Master\u2019s-level candidates \n  PhD / DSc / DrPH in Epidemiology, Biostatistics or related degree with 3 years of relevant experience, or MPH / MSc in Epidemiology, Biostatistics or related degree with 6 years of relevant experience \n  Thorough and up-to-date technical knowledge of epidemiology and biostatistics methods \n  Experience with study design, analysis planning and execution of analyses \n  Strong interpersonal skills and ability to work effectively in multidisciplinary teams \n  Record of high-quality publications in peer-reviewed journals  \n Experience working within a clinical trials environment (CRO, pharma or academia) \n  Excellent written and verbal communication skills \n \n \n \n  APPLY NOW \n \n \n \n  With the world\u2019s eyes focused on clinical trial data, this is a fantastic time to join an award-winning specialized biometric CRO that is renowned for its technical expertise, outstanding quality and cutting-edge data science techniques. We offer flexible working, part-time hours, structured training and development plans, continuous learning opportunities, and a competitive salary and benefits package. We\u2019re committed to ensuring our employees achieve a healthy work-life balance, within a supportive, fun and friendly working environment.\n  \n \n \n  Should you feel that you have the right skill set and motivations for this position, please apply! Please note that we are considering candidates located anywhere in the US or Canada as this role can be carried out remotely.\n  \n \n \n  Phastar is committed to the principles and practices of equal opportunities and to encouraging the establishment of a diverse workforce. It is our policy to employ individuals on the basis of their suitability for the work to be performed and their potential for development, regardless of age, sex, race, colour, nationality, ethnic or national origin, disability, marital status, pregnancy or maternity, sexual orientation, gender reassignment, religion, or belief. This includes creating a culture that fully reflects our commitment to equal opportunities for all. \n \n \n \n  Important notice to Employment businesses/ Agencies \n \n \n \n  Phastar does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact Phastar's Head of Talent Acquisition to obtain prior written authorization before referring any candidates to Phastar. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and Phastar. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of Phastar. Phastar shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.", "cleaned_desc": " \n \n  This is a fantastic opportunity to work for a growing CRO that is recognized for its continuous learning and development opportunities, whilst also gaining direct experience of working within a pharmaceutical environment.\n   Responsibilities: \n  \n Propose, design, initiate, and report epidemiologic study analyses for their assigned Registry \n  Coordinate work between Biostatistics and Statistical Programming resources for planning/implementation of registry analytical deliverables \n  Lead regularly scheduled team meetings with Biostatistics and Statistical Programming \n  Ensure project timelines are maintained \n  Review analysis plans, TLF shells, and TLFs. Prepare report summary and/or slides based on analytical project results (including SAB meeting slides) \n  Accountable to lead the project planning, management, and execution of epidemiology deliverables for their assigned Registry \n  Provide epidemiology support to Medical Affairs, Pharmacovigilance, Regulatory Affairs, and Clinical Development related to their assigned Registry \n  Partcicipate on cross-functional Registry Leadership Teams as appropriate \n  Lead sub-team meetings with Registry Biostatisticians and Programmers as appropriate \n  Provide epidemiology perspective into study protocols, case report forms (CRFs), and regulatory documents and other reports for their assigned Registry \n  May interact with authors, and in-house reviewers on scientific manuscripts, clinical conference abstracts, presentations, and posters based on data generated from their assigned Registry    May participate with Epidemiology department during interactions with key external stakeholders including Registry Scientific Advisory Boards and regulatory authorities \n  Qualifications: \n  \n A minimum of 3 years of hands-on experience, within the pharmaceutical industry, biotechnology, or consulting environment for Doctoral-level candidates, or 6 years of industry experience for Master\u2019s-level candidates \n  PhD / DSc / DrPH in Epidemiology, Biostatistics or related degree with 3 years of relevant experience, or MPH / MSc in Epidemiology, Biostatistics or related degree with 6 years of relevant experience \n  Thorough and up-to-date technical knowledge of epidemiology and biostatistics methods \n  Experience with study design, analysis planning and execution of analyses \n  Strong interpersonal skills and ability to work effectively in multidisciplinary teams \n  Record of high-quality publications in peer-reviewed journals  \n Experience working within a clinical trials environment (CRO, pharma or academia) \n  Excellent written and verbal communication skills \n \n \n \n  APPLY NOW \n ", "techs": ["none"]}, "0fbfad58e2cb1ed6": {"terms": ["data science"], "salary_min": 61131.2, "salary_max": 91707.2, "title": "Statistical Programmer (Remote)", "company": "Mayo Clinic", "desc": "Responsibilities \n \n  As a Statistical Programmer, the candidate will support Mayo Clinic research by providing high-value analytical collaborations in the form of data manipulations, analysis and reporting. Current Statistical Programmer needs are focused in various clinical departments such as urology, nephrology, obstetrics & gynecology, oncology, and bioinformatics. \n  Key roles and responsibilities include the following: \n \n Interact and collaborate within multidisciplinary teams, under the direction of project team leads \n Familiarity with programming languages and techniques (e.g., SAS, R) \n Contribute to project deliverables by critically reviewing and preparing data for data processing, analysis, and/or summarization \n Demonstrate strengths in organization, documentation, logical and systematic thinking, written and oral communication \n Perform tasks with strong problem-solving skills, with high attention to detail focusing on delivering high quality results \n Manage multiple tasks with concurrent deadlines \n Work independently and remotely as well as in a team environment \n Seek educational opportunities and share knowledge within teams to enhance professional development \n \n \n \n Qualifications \n \n  Visa sponsorship is not available for this position; Must be a US citizen, refugee or asylee. \n  A minimum of a Bachelor's degree with a major in statistics, biostatistics, bioinformatics, mathematics, computer science, data science or quantitative degree relevant to the current needs is required. \n  Required to be considered: A complete application includes a cover letter, resume, and academic transcripts. \n  Preferred qualifications include the following: \n \n  Applied experience and/or coursework in programming (e.g., SAS, R, Python), data capture systems, data management, statistical and/or bioinformatics analysis \n  Commitment to customer service \n  Basic knowledge of human physiology and/or medical terminology \n  Interest in professional growth and continuing education \n  GPA of 3.0 or greater \n \n \n  Exemption Status \n \n  Exempt\n  \n \n Compensation Detail \n \n  $61,131.20 - $91,707.20 / year; Education, experience and tenure may be considered along with internal equity when job offers are extended.\n  \n \n Benefits Eligible \n \n  Yes\n  \n \n Schedule \n \n  Full Time\n  \n \n Hours/Pay Period \n \n  80\n  \n \n Schedule Details \n \n  Monday-Friday, 8:00am-5:00pm (with some flexibility)\n  \n \n Weekend Schedule \n \n  As needed\n  \n \n International Assignment \n \n  No\n  \n \n Recruiter \n \n  Stephanie Baird", "cleaned_desc": "Responsibilities \n \n  As a Statistical Programmer, the candidate will support Mayo Clinic research by providing high-value analytical collaborations in the form of data manipulations, analysis and reporting. Current Statistical Programmer needs are focused in various clinical departments such as urology, nephrology, obstetrics & gynecology, oncology, and bioinformatics. \n  Key roles and responsibilities include the following: \n \n Interact and collaborate within multidisciplinary teams, under the direction of project team leads \n Familiarity with programming languages and techniques (e.g., SAS, R) \n Contribute to project deliverables by critically reviewing and preparing data for data processing, analysis, and/or summarization \n Demonstrate strengths in organization, documentation, logical and systematic thinking, written and oral communication \n Perform tasks with strong problem-solving skills, with high attention to detail focusing on delivering high quality results \n Manage multiple tasks with concurrent deadlines \n Work independently and remotely as well as in a team environment \n Seek educational opportunities and share knowledge within teams to enhance professional development \n   \n \n Qualifications \n \n  Visa sponsorship is not available for this position; Must be a US citizen, refugee or asylee. \n  A minimum of a Bachelor's degree with a major in statistics, biostatistics, bioinformatics, mathematics, computer science, data science or quantitative degree relevant to the current needs is required. \n  Required to be considered: A complete application includes a cover letter, resume, and academic transcripts. \n  Preferred qualifications include the following: \n \n  Applied experience and/or coursework in programming (e.g., SAS, R, Python), data capture systems, data management, statistical and/or bioinformatics analysis \n  Commitment to customer service \n  Basic knowledge of human physiology and/or medical terminology \n  Interest in professional growth and continuing education \n  GPA of 3.0 or greater ", "techs": ["sas", "r", "python"]}, "77ce9517bd1f42fd": {"terms": ["data science"], "salary_min": 96165.35, "salary_max": 121766.76, "title": "Data Scientist", "company": "Eastport Analytics", "desc": "Dat \n a Scientist \n \n \n   Eastport Analytics has an immediate opportunity for a full time Data Scientist with strong SQL & Python experience. Our data subject matter experts, technologists, and domain/industry specialists work directly with financial investigators, auditors, and analysts to gain better insights out of existing structured/unstructured data and IT systems. These teams drive innovation that allows their clients to modernize their data strategy, understand and improve their data management, automate, and optimize their business and technological procedures, and build insights through modeling, reporting, and generalized data analytics. We measure success by our client\u2019s mission accomplishments, be it higher compliance, improved asset recovery, or enhanced law/tax enforcement.\n   \n \n \n \n  As a Data Scientist, you will be performing data manipulation, data analysis, and data wrangling/ETL. You will be using statistical software and other exploratory data analysis (EDA) tools/techniques to inform senior leadership decision-making. You will be responsible for building, maintaining, executing, and documenting SQL models to ensure data is managed efficiently and effectively for our client. This role includes developing and implementing databases and data collection systems, acquiring data from primary and secondary sources, maintaining data ingest processes and identifying/analyzing/interpreting trends or patterns in complex datasets.\n  \n \n Requirements \n \n \n Bachelors degree in quantitative field such as applied math, advanced or applied statistics, computer engineering, computer science, and econometrics. \n 2 plus years of demonstrated experience in a corporate environment with SQL (i.e., ETL data and performing analytics) including the ability to build, maintain, understand, and modify complex SQL code. \n Must have advanced Python, SQL, and advanced Excel skills. \n Experience with data definition, data manipulation, and data wrangling using one or more relational databases including SQL Relational Database Management (RDMS). \n Experience with ETL and EDA processes, data mapping/processing/management. \n Experience with descriptive statistics and inferences to understand relationships between fields. \n Experience communicating with both technical and non-technical stakeholders, collaborating with cross-functional teams, and delivering projects through a variety of mediums (i.e., demonstrations, briefings, slide decks, etc.). \n Visualization skills-ability to visualize the data and use storytelling using advanced dashboarding skills to convey deep technical information to non-technical and/or domain users. \n Experience using Python libraries - Pandas, Numpy, Mat Plotlib. \n Ability to synthesize and summarize analytical project results that address client problems to client stakeholders. \n Demonstrated ability to work in a collaborative environment with strong organizational skills, attention to detail and the ability to work simultaneously on various projects with strict deadlines. \n Remote or hybrid options are available . Must be willing to support East Coast core work hours and be interested in attending company quarterly meetings in Washington, DC vicinity as well as significant client milestones. \n Must be able to successfully pass a background investigation to obtain and maintain a public trust clearance. The  requirements  are either  US Citizenship or Legal Permanent Resident (LPR)  status with at least three consecutive years of U.S. residency, from the date of legal entry, as an LPR. \n \n \n Desirable:  \n \n \n In addition to Bachelors degree, a rigorous data science boot camp or Masters degree in quantitative related field such as statistical analysis or advanced statistics. \n MD, VA, DC local candidates are preferred. \n Experience with government contracting (Treasury) and/or federal financial regulatory environment is desirable. \n R/SAS, Plotly, no SQL and graph analytics, pyrtorch, TensorFlow, scikit, sidekick, NLP machine learning is a plus. \n Experience/understanding of regression modeling. \n \n \n  Do you value making a measurable impact on the client\u2019s critical mission? Do you thrive in a work environment that emphasizes and fosters collaboration, creativity, intellectual curiosity, innovation, flexibility, and professionalism? Eastport Analytics is an established small thriving MD based company offering a work-life balanced culture with full benefits and a competitive compensation package. This is a full-time salaried W-2 opportunity. Check us out at www.EastportAnalytics.com. Come join our team!\n  \n \n \n  We offer competitive salary with a full benefits package including full medical, dental, and vision health insurance, life, short and long-term disability, 401k program, flexible spending account, radical flex time and generous leave, quarterly discretionary performance-based bonus program, professional development opportunities, employee assistance program, fun quarterly events, and a work-life balance culture.\n  \n \n \n  Eastport Analytics is an Equal Opportunity Employer committed to providing equal employment opportunity without regard to an individual\u2019s race, color, national origin, gender, gender identity, religion, age, status as a protected veteran, or status as an individual with a disability. Employment decisions are made in accordance with a merit system.", "cleaned_desc": "  As a Data Scientist, you will be performing data manipulation, data analysis, and data wrangling/ETL. You will be using statistical software and other exploratory data analysis (EDA) tools/techniques to inform senior leadership decision-making. You will be responsible for building, maintaining, executing, and documenting SQL models to ensure data is managed efficiently and effectively for our client. This role includes developing and implementing databases and data collection systems, acquiring data from primary and secondary sources, maintaining data ingest processes and identifying/analyzing/interpreting trends or patterns in complex datasets.\n  \n \n Requirements \n \n \n Bachelors degree in quantitative field such as applied math, advanced or applied statistics, computer engineering, computer science, and econometrics. \n 2 plus years of demonstrated experience in a corporate environment with SQL (i.e., ETL data and performing analytics) including the ability to build, maintain, understand, and modify complex SQL code. \n Must have advanced Python, SQL, and advanced Excel skills.   Experience with data definition, data manipulation, and data wrangling using one or more relational databases including SQL Relational Database Management (RDMS). \n Experience with ETL and EDA processes, data mapping/processing/management. \n Experience with descriptive statistics and inferences to understand relationships between fields. \n Experience communicating with both technical and non-technical stakeholders, collaborating with cross-functional teams, and delivering projects through a variety of mediums (i.e., demonstrations, briefings, slide decks, etc.). \n Visualization skills-ability to visualize the data and use storytelling using advanced dashboarding skills to convey deep technical information to non-technical and/or domain users. \n Experience using Python libraries - Pandas, Numpy, Mat Plotlib. \n Ability to synthesize and summarize analytical project results that address client problems to client stakeholders. \n Demonstrated ability to work in a collaborative environment with strong organizational skills, attention to detail and the ability to work simultaneously on various projects with strict deadlines. \n Remote or hybrid options are available . Must be willing to support East Coast core work hours and be interested in attending company quarterly meetings in Washington, DC vicinity as well as significant client milestones. ", "techs": ["sql", "statistical software", "exploratory data analysis (eda) tools", "sql models", "databases", "data collection systems", "python", "advanced excel skills", "relational databases", "data definition", "data manipulation", "data wrangling", "etl", "eda processes", "data mapping", "data processing", "descriptive statistics", "inferences", "visualization skills", "advanced dashboarding skills", "python libraries (pandas", "numpy", "matplotlib)"]}, "8c3d2458aed15b8f": {"terms": ["data science"], "salary_min": 87828.805, "salary_max": 111210.836, "title": "Data Science & Analytics Specialist (remote)", "company": "H+M Industrial EPC", "desc": "Department\n    \n \n     IT\n    \n \n \n \n     Location\n    \n \n \n      Pasadena\n     \n \n \n      TX\n     \n \n \n \n \n     Flsa Status\n    \n \n     Exempt\n    \n \n \n \n     reports to\n    \n \n     IT\n    \n \n \n \n \n     work schedule\n    \n \n \n \n \n \n \n \n \n \n \n  Core Job Responsibilities \n \n  Purpose:  H+M Industrial EPC is a rapidly growing company that specializes in engineering, procurement, and construction services. One of our strategic goals is to be industry leaders in managing, interpreting, and using data to create predictable outcomes for our projects. To that end, we are seeking a highly motivated Data Science & Analytics Specialist to join our dynamic team. \n  The ideal candidate will possess a unique blend of business and technical savvy; a big-picture vision, and the drive to make that vision a reality. You must enjoy spending time in the data to understand and analyze business problems and be able to utilize your insights to make actionable recommendations. This role is pivotal for making data-driven decisions across the organization and contributing to our strategic goal of leveraging data for project predictability. \n \n  Data Analysis and Visualization:  Utilize tools such as Power BI to create dashboards, scorecards, and KPIs that bring data to life and tell compelling stories. \n  Data Modeling and Algorithms:  Apply data science techniques using Python to analyze complex data sets and provide insights that align with our strategic goal of project predictability. \n  Database Management:  Work with database systems, manage data hygiene, and perform data extraction, transformation, and load (ETL) processes. \n  Business Recommendations:  Develop data-driven insights to influence business strategy, make recommendations that drive operational efficiencies, business growth, and project predictability. \n  Stakeholder Communication:  Effectively communicate findings, insights, and recommendations to both technical and non-technical stakeholders at various organizational levels. \n  Collaboration:  Work closely with cross-functional teams, including engineering, procurement, and construction to identify business needs and opportunities for improvements. \n  Quality Assurance:  Ensure data accuracy and consistent reporting by designing and creating optimal processes and procedures for analytics employees to follow. \n  Documentation:  Maintain rigorous documentation of models, code, and projects to ensure reproducibility and knowledge retention. \n  Professional Development:  Keep up-to-date with industry best practices and emerging technologies. \n \n \n \n \n  Essential Qualifications \n \n \n  Bachelor\u2019s degree in Computer Science, Statistics, Applied Math, Business Analytics, or related field; Master\u2019s preferred. \n  3+ years of experience in data science, analytics, or a related field. \n  Strong expertise in Power BI, SQL, and Python. \n  Experience with various database technologies (SQL, NoSQL). \n  Highly skilled in KPI creation, and developing and managing scorecards. \n  Exceptional communication skills, both written and verbal, with the ability to convey complex data in a clear, concise manner. \n  Strong problem-solving skills with an emphasis on product development and project predictability. \n  Experience using statistical methods to analyze data and generate useful business insights. \n  Ability to work collaboratively across departments and contribute to our rapid growth. \n  Detail-oriented with strong organizational skills. \n \n \n \n \n  Physical Requirements \n \n \n  Occasionally lift objects weighing up to 25 pounds \n  Sit for long periods of time \n  Occasionally stoop, kneel or crouch \n  Use hands and arms to reach for, grasp and manipulate objects \n \n \n \n \n  Tool Requirements \n \n \n \n  Environment \n  Most work time is spent in an office or home office environment.", "cleaned_desc": "    \n \n \n \n \n \n \n \n \n \n \n  Core Job Responsibilities \n \n  Purpose:  H+M Industrial EPC is a rapidly growing company that specializes in engineering, procurement, and construction services. One of our strategic goals is to be industry leaders in managing, interpreting, and using data to create predictable outcomes for our projects. To that end, we are seeking a highly motivated Data Science & Analytics Specialist to join our dynamic team. \n  The ideal candidate will possess a unique blend of business and technical savvy; a big-picture vision, and the drive to make that vision a reality. You must enjoy spending time in the data to understand and analyze business problems and be able to utilize your insights to make actionable recommendations. This role is pivotal for making data-driven decisions across the organization and contributing to our strategic goal of leveraging data for project predictability. \n \n  Data Analysis and Visualization:  Utilize tools such as Power BI to create dashboards, scorecards, and KPIs that bring data to life and tell compelling stories. \n  Data Modeling and Algorithms:  Apply data science techniques using Python to analyze complex data sets and provide insights that align with our strategic goal of project predictability. \n  Database Management:  Work with database systems, manage data hygiene, and perform data extraction, transformation, and load (ETL) processes. \n  Business Recommendations:  Develop data-driven insights to influence business strategy, make recommendations that drive operational efficiencies, business growth, and project predictability.    Stakeholder Communication:  Effectively communicate findings, insights, and recommendations to both technical and non-technical stakeholders at various organizational levels. \n  Collaboration:  Work closely with cross-functional teams, including engineering, procurement, and construction to identify business needs and opportunities for improvements. \n  Quality Assurance:  Ensure data accuracy and consistent reporting by designing and creating optimal processes and procedures for analytics employees to follow. \n  Documentation:  Maintain rigorous documentation of models, code, and projects to ensure reproducibility and knowledge retention. \n  Professional Development:  Keep up-to-date with industry best practices and emerging technologies. \n \n \n \n \n  Essential Qualifications \n \n \n  Bachelor\u2019s degree in Computer Science, Statistics, Applied Math, Business Analytics, or related field; Master\u2019s preferred. \n  3+ years of experience in data science, analytics, or a related field. \n  Strong expertise in Power BI, SQL, and Python. \n  Experience with various database technologies (SQL, NoSQL). \n  Highly skilled in KPI creation, and developing and managing scorecards. \n  Exceptional communication skills, both written and verbal, with the ability to convey complex data in a clear, concise manner. \n  Strong problem-solving skills with an emphasis on product development and project predictability. \n  Experience using statistical methods to analyze data and generate useful business insights. ", "techs": ["power bi", "python", "sql", "kpi creation", "scorecard development", "database technologies", "statistical methods"]}, "b143f48b6e6943bc": {"terms": ["data science", "data analyst"], "salary_min": 58300.0, "salary_max": 133000.0, "title": "Data Analyst", "company": "Booz Allen Hamilton", "desc": "Job Description \n \n \n \n \n \n \n \n \n \n \n         Location: \n         \n \n         Camp Lejeune,NC,US \n         \n \n \n \n         Remote Work: \n         \n \n         Hybrid \n         \n \n \n \n         Job Number: \n         \n \n         R0179470\n         \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n         Data Analyst\n           The Opportunity:  As data analyst, you love diving into data and turning it into meaningful insights. With the abundance of structured and unstructured data, you understand the importance of transforming complex data sets into useful information to solve challenges. As a data analyst at Booz Allen, you can use your skills and experience to support a mission and use data for good. We need a data analyst like you to bring your knowledge to support decision makers mitigate risk through data-informed decision tools. \n \n  As a client-facing data analyst on our Navy Marine Corps team, you\u2019ll work closely with your clients to understand their questions and needs and then dig into their data-rich environments to find the pieces of their information puzzle. Not only will you provide a deep understanding of their data, you\u2019ll also advise your client on what the information means and how it can be used to make an impact by providing decision space to leaders, validating progress and milestones for program implementation, and expanding the client's vision of how to incorporate data and data science into their current decision cycle. \n \n  How You\u2019ll Contribute:  As a data analyst on our team, you\u2019ll: \n \n  Use your technical knowledge and analytical mindset to support client and stakeholder relationships. \n  Research, develop, and test data methodologies, and generate cross-functional solutions through collection, interpretation, evaluation, analysis, and visualization of large data sets. \n  Contribute to impactful work and guide decision-making across multiple organizations. \n  Apply data visualization skills and data analytics experience by simplifying technical requirements and trends, based on audience. \n  Present data findings and recommendations to clients and stakeholders using your knowledge of databases, scripting languages such as Python, data visualization programs, including Qlik, Tableau, PowerBI, and Microsoft Office Suite. \n  Establish quantitative and qualitative metrics and key performance indicators to drive technical outcomes. \n  Apply data visualization through different formats. \n  Grow your communication and technical skills by merging consulting and big data to create data-centric solutions across the Marine Corps. \n \n \n  Work with us to help drive large-scale business and process decisions through data insights. \n \n  Join us. The world can\u2019t wait. \n \n  You Have: \n \n  2+ years of experience with producing analytic products \n  2+ years of experience with data wrangling using SQL, Python, or R \n  Experience with working in Agile environments \n  Experience with data querying, data wrangling, and visualization libraries and platforms \n  Experience in engaging with clients and gathering requirements \n  Ability to document source-to-target mappings, data dictionaries, and general documentation \n  Ability to think strategically and comprehend data implications across the enterprise ecosystem \n  Secret clearance  \n Bachelor's degree or 3+ years of experience as a data analyst in lieu of a degree \n \n \n  Nice If You Have: \n \n  Experience with Amazon Web Services (AWS) or Microsoft Point \n  Experience in working with data science and BI teams \n  Experience with normalization of unstructured data \n  Experience with business workflows and dashboarding tools \n  Experience in working with the DoD \n  Knowledge of implementing a variety of supervised and unsupervised machine learning techniques, including clustering, decision tree learning, and artificial neural networks \n  Knowledge of the real-world advantages and drawbacks of supervised and unsupervised machine learning \n  Knowledge of data standardization policies and standards sufficient to develop data management tools, including data dictionaries, data models, and metadata repositories \n  Possession of excellent verbal and written communication skills in documentation and focused on accuracy, consistency, and standardized terminology  \n \n \n Clearance:  Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required.     Create Your Career: \n  Grow With Us  Your growth matters to us\u2014that\u2019s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. \n \n  A Place Where You Belong  Diverse perspectives cultivate collective ingenuity. Booz Allen\u2019s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you\u2019ll build your community in no time. \n \n  Support Your Well-Being  Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we\u2019ll support you as you pursue a balanced, fulfilling life\u2014at work and at home. \n \n  Your Candidate Journey  At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we\u2019ve compiled a list of resources so you\u2019ll know what to expect as we forge a connection with you during your journey as a candidate with us. \n \n  Compensation \n  At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen\u2019s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. \n  Salary at Booz Allen is determined by various factors, including but not limited to location, the individual\u2019s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $58,300.00 to $133,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen\u2019s total compensation package for employees.\n          \n  Work Model  Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. \n \n  If this position is listed as remote or hybrid, you\u2019ll periodically work from a Booz Allen or client site facility. \n  If this position is listed as onsite, you\u2019ll work with colleagues and clients in person, as needed for the specific role. \n \n \n  EEO Commitment \n  We\u2019re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change \u2013 no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.", "cleaned_desc": " \n \n \n \n         Data Analyst\n           The Opportunity:  As data analyst, you love diving into data and turning it into meaningful insights. With the abundance of structured and unstructured data, you understand the importance of transforming complex data sets into useful information to solve challenges. As a data analyst at Booz Allen, you can use your skills and experience to support a mission and use data for good. We need a data analyst like you to bring your knowledge to support decision makers mitigate risk through data-informed decision tools. \n \n  As a client-facing data analyst on our Navy Marine Corps team, you\u2019ll work closely with your clients to understand their questions and needs and then dig into their data-rich environments to find the pieces of their information puzzle. Not only will you provide a deep understanding of their data, you\u2019ll also advise your client on what the information means and how it can be used to make an impact by providing decision space to leaders, validating progress and milestones for program implementation, and expanding the client's vision of how to incorporate data and data science into their current decision cycle. \n \n  How You\u2019ll Contribute:  As a data analyst on our team, you\u2019ll: \n \n  Use your technical knowledge and analytical mindset to support client and stakeholder relationships. \n  Research, develop, and test data methodologies, and generate cross-functional solutions through collection, interpretation, evaluation, analysis, and visualization of large data sets. \n  Contribute to impactful work and guide decision-making across multiple organizations. \n  Apply data visualization skills and data analytics experience by simplifying technical requirements and trends, based on audience. \n  Present data findings and recommendations to clients and stakeholders using your knowledge of databases, scripting languages such as Python, data visualization programs, including Qlik, Tableau, PowerBI, and Microsoft Office Suite. \n  Establish quantitative and qualitative metrics and key performance indicators to drive technical outcomes. \n  Apply data visualization through different formats. \n  Grow your communication and technical skills by merging consulting and big data to create data-centric solutions across the Marine Corps. \n \n \n  Work with us to help drive large-scale business and process decisions through data insights. \n    Join us. The world can\u2019t wait. \n \n  You Have: \n \n  2+ years of experience with producing analytic products \n  2+ years of experience with data wrangling using SQL, Python, or R \n  Experience with working in Agile environments \n  Experience with data querying, data wrangling, and visualization libraries and platforms \n  Experience in engaging with clients and gathering requirements \n  Ability to document source-to-target mappings, data dictionaries, and general documentation \n  Ability to think strategically and comprehend data implications across the enterprise ecosystem \n  Secret clearance  \n Bachelor's degree or 3+ years of experience as a data analyst in lieu of a degree \n \n \n  Nice If You Have: \n \n  Experience with Amazon Web Services (AWS) or Microsoft Point \n  Experience in working with data science and BI teams \n  Experience with normalization of unstructured data \n  Experience with business workflows and dashboarding tools \n  Experience in working with the DoD \n  Knowledge of implementing a variety of supervised and unsupervised machine learning techniques, including clustering, decision tree learning, and artificial neural networks ", "techs": ["sql", "python", "r", "qlik", "tableau", "powerbi", "microsoft office suite", "amazon web services (aws)", "microsoft point", "dod"]}, "aae512db6720e143": {"terms": ["data science", "data engineer", "machine learning engineer", "mlops"], "salary_min": 119400.7, "salary_max": 151187.89, "title": "Machine Learning Ops Data Engineer/Software Engineer", "company": "Dell Technologies", "desc": "MLOps Data Engineer/Senior Software Engineer \n  Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand new methodologies, tools, statistical methods and models. What\u2019s more, we are in collaboration with leading academics, industry experts and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data. \n  Be a part of a team that\u2019s ensuring Dell Technologies' product integrity and customer satisfaction. Our MLOps team turns business requirements into technology solutions by designing, coding, and testing/debugging applications, as well as documenting procedures for use and constantly seeking quality improvements. \n  Join us as a  Data Engineer/Senior Software Engineer  on our  MLOps  team to work remotely in  Panama or Brazil  to do the best work of your career and make a profound social impact. \n  What you\u2019ll achieve \n  As an MLOps Data Engineer/Senior Software Engineer, you will deliver products and improvements for a changing world. Working at the cutting edge, you will craft and develop software for platforms, peripherals, applications, and diagnostics \u2014 all with the most sophisticated technologies, tools, software engineering methodologies, and partnerships. \n  What you\u2019ll do: \n  Feature engineering: Stitching together and aggregating multiple large data sets working with the data scientist to the desired format. You will need to optimize both queries and architecture to support big data sets. \n  Data pipelining: Once the initial dataset is prepared, it is normally run through a further pipeline to prepare it for modeling. Here you will create features that make Machine Learning/Artificial algorithms work (e.g. translating the text into category variables). Expect to work with datasets with billions of rows and thousands of columns. Python/R is usually the preferred language, some larger tasks require Spark. \n  Define standards for how data pipeline should be defined, designed and documented. Create pipeline definition that can be run in any runtime environment from day 1. Document pipeline and share best practices and knowledge with entire DataOps organization. \n  Take the first step towards your dream career.  \n Every Dell Technologies team member brings something unique to the table. Here\u2019s what we are looking for with this role:  \n Essential Requirements \n \n  Engineering Degree in Computer Science/Engineering, or equivalent professional experience \n  Experience with big data technology (Hadoop, Apache Spark, etc.) and data migration from relational database to big data technologies \n  Proficiency in data modelling, data optimization for both relational and non-relational databases (Oracle, MySQL, SqlServer, Mongo, Cassandra, Couchbase, Hadoop, Redis ) \n  The primary skillset of building and maintaining complex ETL orchestrating pipelines preferably using Airflow \n  Ability to work with varied data infrastructures \u2013 including relational databases, column stores, NoSQL databases, and file-based storage solutions. Exposure to machine learning or machine learning pipelines is a plus. Ability to set up containerized services using Kubernetes and Docker. \n \n  Desirable Requirements \n \n  Experience with building and designing enterprise data pipelines with various levels of priority, concurrency, and versioning \n  Experience with dbt, OpenMetadata, data catalogues and data lineage tools \n \n  Who we are \n  We believe that each of us has the power to make an impact. That\u2019s why we put our team members at the center of everything we do. If you\u2019re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we\u2019re looking for you. \n  Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us. \n  Application closing date:  September 15th - 2023 \n  Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment.", "cleaned_desc": "  As an MLOps Data Engineer/Senior Software Engineer, you will deliver products and improvements for a changing world. Working at the cutting edge, you will craft and develop software for platforms, peripherals, applications, and diagnostics \u2014 all with the most sophisticated technologies, tools, software engineering methodologies, and partnerships. \n  What you\u2019ll do: \n  Feature engineering: Stitching together and aggregating multiple large data sets working with the data scientist to the desired format. You will need to optimize both queries and architecture to support big data sets. \n  Data pipelining: Once the initial dataset is prepared, it is normally run through a further pipeline to prepare it for modeling. Here you will create features that make Machine Learning/Artificial algorithms work (e.g. translating the text into category variables). Expect to work with datasets with billions of rows and thousands of columns. Python/R is usually the preferred language, some larger tasks require Spark. \n  Define standards for how data pipeline should be defined, designed and documented. Create pipeline definition that can be run in any runtime environment from day 1. Document pipeline and share best practices and knowledge with entire DataOps organization.    Experience with big data technology (Hadoop, Apache Spark, etc.) and data migration from relational database to big data technologies \n  Proficiency in data modelling, data optimization for both relational and non-relational databases (Oracle, MySQL, SqlServer, Mongo, Cassandra, Couchbase, Hadoop, Redis ) \n  The primary skillset of building and maintaining complex ETL orchestrating pipelines preferably using Airflow \n  Ability to work with varied data infrastructures \u2013 including relational databases, column stores, NoSQL databases, and file-based storage solutions. Exposure to machine learning or machine learning pipelines is a plus. Ability to set up containerized services using Kubernetes and Docker. \n    Desirable Requirements \n \n  Experience with building and designing enterprise data pipelines with various levels of priority, concurrency, and versioning \n  Experience with dbt, OpenMetadata, data catalogues and data lineage tools \n ", "techs": ["mlops data engineer/senior software engineer", "software engineering methodologies", "partnerships", "feature engineering", "data scientist", "queries", "architecture", "big data sets", "python", "r", "spark", "data pipeline", "machine learning/artificial algorithms", "data pipeline definition", "runtime environment", "big data technology", "hadoop", "apache spark", "data migration", "relational database", "non-relational databases", "oracle", "mysql", "sqlserver", "mongo", "cassandra", "couchbase", "hadoop", "redis", "etl orchestrating pipelines", "airflow", "varied data infrastructures", "relational databases", "column stores", "nosql databases", "file-based storage solutions", "machine learning", "machine learning pipelines", "containerized services", "kubernetes", "docker", "enterprise data pipelines", "priority", "concurrency", "versioning", "dbt", "openmetadata", "data catalogues", "data lineage tools"]}, "b4d3207a21a5d41e": {"terms": ["data science", "machine learning engineer"], "salary_min": 136000.0, "salary_max": 219650.0, "title": "Director - Data Science (AI solutions)", "company": "Merkle", "desc": "About the job \n \n \n \n The Integrated Solutions / Analytics Innovation team at Merkle is a fast-growing team of thought leaders and problem solvers who apply rigorous cutting-edge analytics to help our clients assess and solve a multitude of business challenges. We combine our deep analytical heritage, platform knowledge, cross channel expertise and sharp business acumen with emerging technologies to drive performance. \n  We are seeking a talented and experienced Data Scientist with a strong background in building Generative AI solutions to join our dynamic team. This is an exciting opportunity to shape the future of AI and drive groundbreaking solutions in diverse domains. \n  Key Responsibilities: \n \n  Design, develop, and implement cutting-edge generative models using state-of-the-art deep learning techniques to enhance customer experiences and drive marketing effectiveness. \n  Collaborate closely with cross-functional teams, including marketing professionals, data engineers, and product managers, to understand business goals and define project requirements. \n  Conduct exploratory data analysis and preprocessing to ensure high-quality input data for training generative models. \n  Train, fine-tune, and validate generative models on diverse datasets representing customer preferences and behaviors. \n  Optimize and iterate on existing models to improve performance, scalability, and efficiency. \n  Evaluate and benchmark generative models against key performance metrics, comparing them to industry standards and best practices. \n  Stay up to date with the latest research and advancements in generative AI and propose innovative solutions to drive customer engagement and improve marketing strategies. \n  Mentor and provide guidance to junior data scientists, fostering a collaborative and knowledge-sharing environment. \n \n \n \n \n \n \n Qualifications \n \n \n \n Master's or Ph.D. degree in Computer Science, Data Science, or a related field. \n  Proven experience (3+ years) as a Data Scientist, with a focus on building Generative AI solutions. \n  Strong theoretical and practical understanding of deep learning architectures. \n  Proficiency in machine learning frameworks (such as TensorFlow, PyTorch) and libraries (such as Keras, scikit-learn). \n  Solid programming skills in Python, with experience in data manipulation, analysis, and visualization. \n  Strong mathematical and statistical background, with expertise in probability theory and optimization techniques. \n  Experience in working with large-scale datasets and distributed computing frameworks (e.g., Spark) is desirable. \n  Excellent problem-solving skills, with the ability to think critically and propose innovative solutions. \n  Strong communication skills, with the ability to present complex technical concepts to both technical and non-technical stakeholders. \n  A proven track record of delivering high-quality results within tight deadlines. \n  Must have superior multi-tasking, networking, time management and inter-personal skills. \n  Self-starter high intellectual curiosity, drive, determination, and persuasion skills. \n  Comfort and experience working within a highly matrixed organization in roles with a wide degree of latitude. \n  Publications or contributions to the field of Generative AI will be highly regarded. \n \n \n       If you are passionate about pushing the boundaries of AI and have a deep understanding of Generative AI techniques, we invite you to join our team and contribute to our mission of transforming industries through groundbreaking solutions. Apply now and be part of our exciting journey towards innovation.\n      \n \n \n \n \n Additional Information \n \n \n Employees from diverse or underrepresented backgrounds encouraged to apply.  Dentsu (the \"Company\") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying. \n  The anticipated salary range for this position is $136,000-$219,650. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. \n  #LI-MZ1 \n \n  About dentsu  Dentsu is the network designed for what\u2019s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com. \n  We are champions for meaningful progress and we strive to be a force for good\u2014for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all. \n \n       Dentsu (the \"Company\") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.", "cleaned_desc": "About the job \n \n \n \n The Integrated Solutions / Analytics Innovation team at Merkle is a fast-growing team of thought leaders and problem solvers who apply rigorous cutting-edge analytics to help our clients assess and solve a multitude of business challenges. We combine our deep analytical heritage, platform knowledge, cross channel expertise and sharp business acumen with emerging technologies to drive performance. \n  We are seeking a talented and experienced Data Scientist with a strong background in building Generative AI solutions to join our dynamic team. This is an exciting opportunity to shape the future of AI and drive groundbreaking solutions in diverse domains. \n  Key Responsibilities: \n \n  Design, develop, and implement cutting-edge generative models using state-of-the-art deep learning techniques to enhance customer experiences and drive marketing effectiveness. \n  Collaborate closely with cross-functional teams, including marketing professionals, data engineers, and product managers, to understand business goals and define project requirements. \n  Conduct exploratory data analysis and preprocessing to ensure high-quality input data for training generative models.   Qualifications \n \n \n \n Master's or Ph.D. degree in Computer Science, Data Science, or a related field. \n  Proven experience (3+ years) as a Data Scientist, with a focus on building Generative AI solutions. \n  Strong theoretical and practical understanding of deep learning architectures. \n  Proficiency in machine learning frameworks (such as TensorFlow, PyTorch) and libraries (such as Keras, scikit-learn). \n  Solid programming skills in Python, with experience in data manipulation, analysis, and visualization. \n  Strong mathematical and statistical background, with expertise in probability theory and optimization techniques. \n  Experience in working with large-scale datasets and distributed computing frameworks (e.g., Spark) is desirable. ", "techs": ["merkle", "integrated solutions", "analytics innovation", "data scientist", "generative ai", "deep learning techniques", "customer experiences", "marketing effectiveness", "cross-functional teams", "marketing professionals", "data engineers", "product managers", "exploratory data analysis", "preprocessing", "master's degree", "ph.d. degree", "computer science", "data science", "tensorflow", "pytorch", "keras", "scikit-learn", "python programming", "data manipulation", "analysis", "visualization", "probability theory", "optimization techniques", "large-scale datasets", "distributed computing frameworks", "spark"]}, "0691b22e6642b1e8": {"terms": ["data science"], "salary_min": 121113.586, "salary_max": 153356.78, "title": "Data Scientist - Generative AI", "company": "HP", "desc": "** Locations include Spring, Texas (preferred location), and US remote.   ** Typically need at least Master's degree and minimum 5-8 years full-time relevant work experience. \n \n  The Team \n  We are a growing centralized team helping HP take advantage of new AI/ML technology, especially around Generative AI and large language models. We engage with business units to advise and prototype solutions, and we develop and run software applications for internal use. \n \n  The Role \n  As a Data Scientist with a focus on Generative AI you will work on multiple engagements across HP involving large language models and other new Generative AI capabilities. Beyond our team, you will work with business stakeholders and developers from other business units. Your primary focus and mindset is to help deliver business solutions to our customers. This is not an AI research role. \n \n  Skills and Profile \n  \u2013 Solid knowledge of data science and machine learning core skills. \n  \u2013 Experience with NLP, Large Langauge Models, LLM prompt engineering, vector databases, Retreival Augmented Generation (RAG), and search engines. \n  \u2013 Good computer science skills. Experience in a software development team is a big plus, especially with business applications in a cloud environment.  \u2013 Tools you may use include Azure services such as Azure Machine Learning and Azure prompt flow, as well as python, langchain, streamlit, docker, git, and elastic search. \n \n Experience from a large complex organization, and ideally also some experience from a fast-moving SaaS company. \n Ability to represent the team in meetings with a multitude of stakeholders and help drive discussions in a crisp but respectful manner. Mastery in English is required. \n You enjoy explaining technical concepts (such as LLM's) to a non-technical audience. \n You propose pragmatic solutions that are as simple as possible, which sometimes mean that no AI component is necessary. \n \n  \u2013 Experience working in a distributed team with diverse backgrounds. \n  \u2013 The recent AI progress is disruptive, and our team is in the midst of it. As a consequence, day-to-day priorities, tasks, and team structure may change rapidly. We are looking for somebody who thrives in such an environment. The role is not a fit if you value \"business as usual\". \n \n  Education and Length of Experience  For this position, we prefer at least a relevant Master's degree (e.g. Computer Science) or demonstrated competence and a minimum of 5-8 years experience. \n \n  About HP \n \n \n \n \n \n  You\u2019re out to reimagine and reinvent what\u2019s possible\u2014in your career as well as the world around you.\n  \n \n   So are we. We love taking on tough challenges, disrupting the status quo, and creating what\u2019s next. We\u2019re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.\n  \n \n \n \n   HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.\n  \n \n \n \n   Our history: HP\u2019s commitment to diversity, equity and inclusion \u2013 it's just who we are.\n  \n \n   From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you\u2019re more innovative and that helps grow our bottom line. Come to HP and thrive!", "cleaned_desc": "  \u2013 Solid knowledge of data science and machine learning core skills. \n  \u2013 Experience with NLP, Large Langauge Models, LLM prompt engineering, vector databases, Retreival Augmented Generation (RAG), and search engines. \n  \u2013 Good computer science skills. Experience in a software development team is a big plus, especially with business applications in a cloud environment.  \u2013 Tools you may use include Azure services such as Azure Machine Learning and Azure prompt flow, as well as python, langchain, streamlit, docker, git, and elastic search. \n \n Experience from a large complex organization, and ideally also some experience from a fast-moving SaaS company. \n Ability to represent the team in meetings with a multitude of stakeholders and help drive discussions in a crisp but respectful manner. Mastery in English is required. \n You enjoy explaining technical concepts (such as LLM's) to a non-technical audience. \n You propose pragmatic solutions that are as simple as possible, which sometimes mean that no AI component is necessary. \n ", "techs": ["nlp", "large language models", "llm prompt engineering", "vector databases", "retreival augmented generation (rag)", "search engines", "azure machine learning", "azure prompt flow", "python", "langchain", "streamlit", "docker", "git", "elastic search"]}, "d78772488477d565": {"terms": ["data science", "data analyst", "machine learning engineer", "mlops"], "salary_min": 85410.74, "salary_max": 108149.04, "title": "Senior Data Science Analyst", "company": "Freeport McMoRan", "desc": "Freeport-McMoRan  is a leading international mining company with headquarters in Phoenix, Arizona. We operate large, long-lived, geographically diverse assets with significant proven and probable reserves of copper, gold, and molybdenum. The company has a dynamic portfolio of operating, expansion and growth projects in the copper industry. Freeport-McMoRan is one of the world\u2019s largest publicly traded copper producers, the world\u2019s largest producer of molybdenum and a significant gold producer. We have a long and successful history of conducting our business in a safe, highly efficient and socially-responsible manner. \n  We have the assets, the talent, the drive and the financial strength to provide attractive and rewarding careers of our employees. We encourage you to take the time to explore the opportunity to advance your career at Freeport-McMoRan. \n  Please note:  This position has the possibility to work remotely up to 100% of the time. The position will require occasional travel to the Phoenix corporate offices and/or site locations . This position may be performed anywhere in the U.S. except California, Connecticut, New Hampshire, Massachusetts, Michigan, Illinois, Kentucky and New York. Additional states may be excluded from remote work based on business factors. Should the positions shift to in-office work in the future, the company will offer relocation benefits at that time should the position meet the established eligibility for these benefits. \n \n \n \n  Description \n \n \n  You will be a lead contributor within a fast-growing Data Science team pursuing a vision of analytics-driven mining at Freeport. You will work in close collaboration with global mining operations, subject matter experts, data scientists, and software engineers to develop novel approaches to complex problems. As a lead contributor you will use your data analysis and modeling skills to independently develop models and exploratory data analysis while also coaching jr. team members to meet workstream deliverables. \n \n  Collaborate with technical and business experts to understand business requirements and translate into solution, mathematical features \n  Plan, create, and implement complex models and algorithms independently while also coaching jr. team members. \n  Utilize modern cloud technologies and employ best practices from DevOps/MLOps to produce enterprise quality production Python and SQL code with minimal errors. Identify and direct the implementation code optimization opportunities \n  Independently conduct advanced analysis and visualize exploratory analysis clearly and concisely to provide a broad audience actionable insight, identify trends, and measure performance \n  Constructively challenge while soliciting participation in problem solving to enrich possible solutions and flexibly seek out new work or training opportunities to broaden experience \n  Collaborate with a wide range of technical and business experts to communicate the design, functioning, and output of models, analysis, and solutions developed \n \n \n \n \n \n  Qualifications \n \n \n  Minimum Requirements: \n \n  Bachelor\u2019s degree in a technical engineering or analytical field (Statistics, Mathematics, etc.) or related discipline and four (4) years of relevant work experience, OR \n  Master\u2019s degree in a technical engineering or analytical field (Statistics, Mathematics, etc.) or related discipline and two (2) years of relevant work experience \n  Python and SQL Programming Experience \n  Knowledgeable Practitioner of at least three of the following analytical areas and the ability to articulate theoretical concepts from at least three\n       \n  Statistics & Statistical Modeling, including Time-Series Modeling \n  Simulation Techniques, including MCMC or an equivalent \n  Neural Networks / Deep Learning \n  Tree Based Machine Learning Algorithms \n  Unsupervised Learning \n  Classification techniques, including Support Vector Machines or an equivalent \n  Optimization Heuristics \n  Text Analytics \n \n  Ability to visualize data utilizing programmatic techniques \n  Experience executing the Data Science development workflow including data manipulation and cleaning, feature engineering, model selection, model training, modeling validation, model deployment \n \n \n  Preferred Qualifications: \n \n  Working knowledge of MLOps/DevOps concepts (Version Control, CI/CD, Trunk Based Development/PR Based Development/GIT, Test driven development) \n  Working knowledge of Azure Machine Learning Environment \n  Working knowledge of Software Engineering and Object Orient Programming Principles \n  Working knowledge of Distributed Parallel Processing Environments such as Spark or Snowflake \n  Working knowledge of Edge Analytics, embedded systems, or computer vision. \n  Working knowledge of Data Architecture, engineering, and ETL teams \n  Working knowledge of problem solving/root cause analysis on Production workloads \n  Working knowledge of Agile, Scrum, and Kanban \n  Experience influencing and building mindshare convincingly within a project team. Confident and experienced in public speaking to large audiences and storytelling with data. \n  Strong verbal and written skills in English language. \n \n \n  Criteria/Conditions: \n \n  Position is in busy, non-smoking office located in downtown Phoenix, AZ \n  Location requires mobility in an office environment; each floor is accessible by elevator. Occasionally work will be performed in a mine, outdoor or manufacturing plant setting. \n  Must be able to frequently sit, stand and walk. \n  Must be able to frequently lift and carry up to 10 pounds. \n  Must be able to work in a potentially stressful environment. \n  Personal protective equipment is required when performing work in a mine, outdoor, manufacturing or plant environment, including hard hat, hearing protection, safety glasses, safety footwear, and as needed, respirator, rubber steel-toe boots, protective clothing, gloves and any other protective equipment as required. \n  Freeport-McMoRan promotes a drug/alcohol-free work environment through the use of mandatory pre-employment drug testing and on-going random drug testing as allowed by applicable state laws \n \n \n \n \n  At Freeport, we are committed to providing an employment package that recognizes excellence, rewards value and impact, and encourages safe production. Benefits and compensation are foundational elements of this package, along with career development opportunities, job progression and a culture supported by our core values, among others. Learn more at: FCX Jobs - Working Here \n \n  Benefits: \n  We provide an industry-leading benefits package with some of the lowest cost to employees \u2013 offering health, wellness, life insurance, paid time off, retirement savings and more. These benefits are available to you and your dependents starting day one. Our comprehensive benefits program is important to how we support the health and wellness of employees and their families. For further benefits information please click here:  Benefits Details \n \n  Compensation: \n  The estimated annual pay range for this role is currently  $94,000 - $132,000.  This range reflects base salary only and does not include bonus payments, benefits or retirement contributions. Actual base pay is determined by experience, qualifications, skills and other job-related factors. This role is eligible for additional discretionary and incentive payment considerations based on company and individual performance. More details will be shared during the hiring process. To view an example of a Total Rewards Estimate for this role click here:  Total Rewards Estimate \n \n  Safety / Work Conditions: \n  Candidates will be required to participate in a post-offer, pre-employment medical examination for the following positions which may have essential job duties that can impact both their own safety and the safety of others:  \n \n Site-based positions, or positions which require unescorted access to site-based operational areas, which are held by employees who are required to receive MSHA, OSHA, DOT, HAZWOPER and/or Hazard Recognition Training; or \n  Positions which are held by employees who operate equipment, machinery or motor vehicles in furtherance of performing the essential functions of their job duties, including operating motor vehicles while on Company business or travel (for this purpose \u201cmotor vehicles\u201d includes Company owned or leased motor vehicles and personal motor vehicles used by employees in furtherance of Company business or while on Company travel). \n \n \n  Equal Opportunity Employer", "cleaned_desc": " \n \n \n \n \n  Qualifications \n \n \n  Minimum Requirements: \n \n  Bachelor\u2019s degree in a technical engineering or analytical field (Statistics, Mathematics, etc.) or related discipline and four (4) years of relevant work experience, OR \n  Master\u2019s degree in a technical engineering or analytical field (Statistics, Mathematics, etc.) or related discipline and two (2) years of relevant work experience \n  Python and SQL Programming Experience \n  Knowledgeable Practitioner of at least three of the following analytical areas and the ability to articulate theoretical concepts from at least three\n       \n  Statistics & Statistical Modeling, including Time-Series Modeling \n  Simulation Techniques, including MCMC or an equivalent    Neural Networks / Deep Learning \n  Tree Based Machine Learning Algorithms \n  Unsupervised Learning \n  Classification techniques, including Support Vector Machines or an equivalent \n  Optimization Heuristics \n  Text Analytics \n \n  Ability to visualize data utilizing programmatic techniques \n  Experience executing the Data Science development workflow including data manipulation and cleaning, feature engineering, model selection, model training, modeling validation, model deployment \n \n \n  Preferred Qualifications: \n \n  Working knowledge of MLOps/DevOps concepts (Version Control, CI/CD, Trunk Based Development/PR Based Development/GIT, Test driven development) \n  Working knowledge of Azure Machine Learning Environment \n  Working knowledge of Software Engineering and Object Orient Programming Principles \n  Working knowledge of Distributed Parallel Processing Environments such as Spark or Snowflake ", "techs": ["python", "sql", "statistics", "statistical modeling", "time-series modeling", "simulation techniques", "mcmc", "neural networks", "deep learning", "tree based machine learning algorithms", "unsupervised learning", "support vector machines", "optimization heuristics", "text analytics", "visualization", "data science development workflow", "mlops", "devops concepts", "version control", "ci/cd", "trunk based development", "pr based development", "git", "test driven development", "azure machine learning environment", "software engineering", "object orient programming principles", "distributed parallel processing environments", "spark", "snowflake"]}, "5fe63a6693cdabdc": {"terms": ["data science"], "salary_min": 80075.91, "salary_max": 101393.94, "title": "Statistical Analyst (contract)", "company": "Elevance Health", "desc": "Job ID: JP00043672 \n \n  Elevance Health  is a health company dedicated to improving lives and communities \u2013 and making healthcare simpler. Previously known as Anthem, Inc., we have evolved into a company focused on whole health and updated our name to better reflect the direction the company is heading. \n \n  We are looking for contract workers ( via BCforward)  who are passionate about making an impact on our members and the communities we serve. You will thrive in a complex and collaborative environment where you take action and ownership to solve problems and lead change. Do you want to be part of a larger purpose and an evolving, high-performance culture that empowers you to make an impact? \n \n  Responsible for conducting statistical analysis for the HealthCore research team. \n \n  Primary duties may include, but are not limited to: \n \n  Manipulates and creates complex datasets. \n  Cleans datasets to identify outliers. \n  Provides statistical review/input for quality control during the data collection phase of a study. \n  Drafts written summaries of analysis results. \n  Participates in the writing of manuscripts, especially Statistical Methods and Results sections. \n  Summarizes results of requested analysis in table and graphic format. \n  Write statistical analysis programming. \n \n \n  Requirements : \n \n  Masters degree in Statistics or Biostatistics or a closely allied field with substantial statistical preparation. \n  Knowledge of statistical methodology and analysis. \n  Practical experience in data analysis in public health or the social sciences. \n  Demonstrated expertise in statistical consulting/collaboration and technical writing. \n  Proficiency in Statistical Analysis System (SAS) is required. \n  1-2 years of experience in data analysis in clinical research preferred. \n \n \n  Additional Details: \n \n  9 am to 5 pm, M-F - flexible with time zone but prefer as close to 9am-5pm as possible.  \n Prefer candidates in Boston, MA area but open to anywhere - fully remote role.  \n Flexibility with the hours to some degree, centered around core hours of 10 am to 3 pm ET. \n \n \n  BCForward is An Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. \n \n  Privacy Notice for California Residents", "cleaned_desc": "  Masters degree in Statistics or Biostatistics or a closely allied field with substantial statistical preparation. \n  Knowledge of statistical methodology and analysis. \n  Practical experience in data analysis in public health or the social sciences. \n  Demonstrated expertise in statistical consulting/collaboration and technical writing. \n  Proficiency in Statistical Analysis System (SAS) is required. \n  1-2 years of experience in data analysis in clinical research preferred. \n ", "techs": ["sas"]}, "7fc69cf668811f02": {"terms": ["data science", "data engineer"], "salary_min": 115840.62, "salary_max": 146680.03, "title": "Google GenAI Data Engineer", "company": "Microagility", "desc": "Collaborate closely with cross-functional teams including data scientists, product managers, and software developers to understand requirements and objectives for building a benchmark chat application. \n Leverage your in-depth knowledge of the Google Cloud Platform and associated tools to design, develop, and deploy scalable and reliable solutions. \n Utilize Google Gen AI App Builder to create a chat application that showcases the capabilities of AI-powered conversation models. \n Work with existing data sets to ensure consistent and relevant input for the chat application, ensuring optimal performance and benchmarking accuracy. \n \n Qualifications and Skills: \n \n Data engineer or software engineer who must be hands on with GCP platform and tools, and understand Gen AI technologies (Google Gen AI App Builder). \n Use Google Gen AI App Builder to build benchmark chat based on same data sets \n Deep understanding of General Artificial Intelligence (Gen AI) technologies, particularly in the context of natural language processing and conversation models. \n \n Prior experience with AI-powered Chabot development and integration is highly desirable \n Job Type: Contract \n Experience level: \n \n 8 years \n \n Schedule: \n \n 8 hour shift \n \n Experience: \n \n GCP: 6 years (Preferred) \n Natural language processing: 4 years (Preferred) \n \n Work Location: Remote", "cleaned_desc": "Collaborate closely with cross-functional teams including data scientists, product managers, and software developers to understand requirements and objectives for building a benchmark chat application. \n Leverage your in-depth knowledge of the Google Cloud Platform and associated tools to design, develop, and deploy scalable and reliable solutions. \n Utilize Google Gen AI App Builder to create a chat application that showcases the capabilities of AI-powered conversation models. \n Work with existing data sets to ensure consistent and relevant input for the chat application, ensuring optimal performance and benchmarking accuracy. \n   Qualifications and Skills: \n \n Data engineer or software engineer who must be hands on with GCP platform and tools, and understand Gen AI technologies (Google Gen AI App Builder). \n Use Google Gen AI App Builder to build benchmark chat based on same data sets \n Deep understanding of General Artificial Intelligence (Gen AI) technologies, particularly in the context of natural language processing and conversation models.   \n Experience: \n \n GCP: 6 years (Preferred) \n Natural language processing: 4 years (Preferred) ", "techs": ["google cloud platform", "google gen ai app builder", "natural language processing"]}, "e05b1f76ed443262": {"terms": ["data science"], "salary_min": 113268.63, "salary_max": 143423.33, "title": "Data Scientist - US/Canada", "company": "PHASTAR", "desc": "Overview: \n  \n  THE COMPANY \n \n \n \n  Phastar is a multiple award-winning global biometric Contract Research Organization (CRO) that is accredited as an outstanding company to work for by Best Companies. We partner with pharmaceutical, biotechnology and medical device organizations to provide the expertise and processes to manage and deliver on time, quality biostatistics, programming, data management and data science services. With offices across the UK, US, Germany, Denmark, Kenya, Australia, India, China and Japan, Phastar is the second largest specialized biometrics provider globally, and the largest in the UK.\n  \n \n \n  Our unique approach to data analysis, \u201cThe Phastar Discipline\u201d, has led us to build a reputation for outstanding quality. With this as our core focus, we\u2019re looking for talented individuals who share our passion for quality and technical expertise to join our team.\n  \n \n \n  WHY PHASTAR \n \n \n \n  Accredited as an outstanding company to work for, Phastar is committed to employee engagement, workplace satisfaction and ensuring a healthy work-life balance. We offer flexible working, part-time hours, involvement in developing company-wide initiatives, structured training and development plans, and a truly supportive, fun and friendly environment.\n  \n \n \n  What\u2019s more, when you join our team, Phastar will plant a tree in your honour, as one of our Environmental, Social and Governance (ESG) initiatives. So, not only would you get your dream job, you\u2019ll also be helping to save the planet!\n  \n \n \n  THE ROLE \n \n \n \n  Demand for our Functional Service Provision is growing, and we are looking for an experienced Senior Data Scientist to join our FSP team.\n  \n \n \n  This is a fantastic opportunity to work for a growing CRO that is recognized for its continuous learning and development opportunities, whilst also gaining direct experience of working within a pharmaceutical environment.\n   Responsibilities: \n  \n Collaborate with Payer and Epidemiology teams to maximise the value derived from large observational research data \n  Deliver analyses of data from EMR, claims and primary observational data required by TA RWE strategies \n  Support the development of IVS strategies and selection of optimised contact models for prioritised markets through analysis of RWD \n  Provide scientific guidance on the application of Real World Evidence and observational research data to address issues across the Oncology and Biopharmaceuticals business units \n  Provide technical input, options and directions to strategic decisions made by the client observational study teams on study design, data partner selection and best practices in RWE data utilization \n  Support technical teams to provide access to analytical tools and develop visual analytics to enable self-serving applications for end customers \n  Provide clear technical input, options, and direction to strategic decisions on RWE platform and capability build \n  Provide support for strategic decisions on client Medical Evidence and Observational Research external collaborations in the US and other markets \n  Assist in building a capability that becomes a source of sustained competitive advantage for the client in identifying, acquiring, integrating and mining diverse RW data from multiple geographic and healthcare system sources to support evidence generation and real-world studies \n  Evaluate and assess strengths and weaknesses of external RW data sources, and potential partners for advancing the data strategy for specific therapeutic areas \n  Maintain a strong insight into the capabilities of potential external partners in RWE, especially for US and emerging markets \n  Qualifications: \n  \n Experience in real-world evidence and familiarity with health economics/epidemiology \n  Expertise in methods development and application using statistical languages such as R/Matlab/SAS/SQL/Hadoop/Python \n  Preferred to have experience in advanced visualisation and visual analytics of routinely collected healthcare data \n  MSc or PhD in Data Science or related/advanced analytical degree field i.e. Biostatistics/Statistics, Epidemiology, Public Health \n  Experience working within a clinical trials environment (CRO, pharma or academia) \n  Excellent written and verbal communication skills \n \n \n \n  APPLY NOW \n \n \n \n  With the world\u2019s eyes focused on clinical trial data, this is a fantastic time to join an award-winning specialized biometric CRO that is renowned for its technical expertise, outstanding quality and cutting-edge data science techniques. We offer flexible working, part-time hours, structured training and development plans, continuous learning opportunities, and a competitive salary and benefits package. We\u2019re committed to ensuring our employees achieve a healthy work-life balance, within a supportive, fun and friendly working environment.\n  \n \n \n  Should you feel that you have the right skill set and motivations for this position, please apply! Please note that we are considering candidates located anywhere in the US or Canada as this role can be carried out remotely.\n  \n \n \n  Phastar is committed to the principles and practices of equal opportunities and to encouraging the establishment of a diverse workforce. It is our policy to employ individuals on the basis of their suitability for the work to be performed and their potential for development, regardless of age, sex, race, colour, nationality, ethnic or national origin, disability, marital status, pregnancy or maternity, sexual orientation, gender reassignment, religion, or belief. This includes creating a culture that fully reflects our commitment to equal opportunities for all. \n \n \n \n  Important notice to Employment businesses/ Agencies \n \n \n \n  Phastar does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact Phastar's Head of Talent Acquisition to obtain prior written authorization before referring any candidates to Phastar. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and Phastar. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of Phastar. Phastar shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.", "cleaned_desc": "  Assist in building a capability that becomes a source of sustained competitive advantage for the client in identifying, acquiring, integrating and mining diverse RW data from multiple geographic and healthcare system sources to support evidence generation and real-world studies \n  Evaluate and assess strengths and weaknesses of external RW data sources, and potential partners for advancing the data strategy for specific therapeutic areas \n  Maintain a strong insight into the capabilities of potential external partners in RWE, especially for US and emerging markets \n  Qualifications: \n  \n Experience in real-world evidence and familiarity with health economics/epidemiology \n  Expertise in methods development and application using statistical languages such as R/Matlab/SAS/SQL/Hadoop/Python \n  Preferred to have experience in advanced visualisation and visual analytics of routinely collected healthcare data \n  MSc or PhD in Data Science or related/advanced analytical degree field i.e. Biostatistics/Statistics, Epidemiology, Public Health \n  Experience working within a clinical trials environment (CRO, pharma or academia) \n  Excellent written and verbal communication skills \n \n \n \n  APPLY NOW ", "techs": ["r/matlab/sas/sql/hadoop/python"]}, "549cdd95c7f5ee7b": {"terms": ["data science", "machine learning engineer"], "salary_min": 168393.33, "salary_max": 213223.48, "title": "Sr. Data Scientist, Content", "company": "Pinterest", "desc": "About Pinterest : \n  Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. \n  Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences. \n  Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our  PinFlex  landing page to learn more. \n \n  We are looking for a Senior Data Scientist for the Content org. You will shape the future of people-facing and business-facing products we build at Pinterest. Your expertise in quantitative modeling, experimentation and algorithms will be utilized to solve some of the most complex engineering challenges at the company. You will collaborate on a wide array of product and business problems with a diverse set of cross-functional partners across Product, Engineering, Design, Research, Product Analytics, Data Engineering and others. The results of your work will influence and uplevel our product development teams while introducing greater scientific rigor into the real world products serving hundreds of millions of pinners, creators, advertisers and merchants around the world. \n  What you'll do: \n \n Develop best practices for instrumentation and experimentation and communicate those to product engineering teams to help us fulfill our mission - to bring everyone the inspiration to create a life they love \n Bring scientific rigor and statistical methods to the challenges of product creation, development and improvement with an appreciation for the behaviors of our Pinners \n Build and prototype analysis pipelines iteratively to provide insights at scale while developing comprehensive knowledge of data structures and metrics, advocating for changes where needed for product development \n Work cross-functionally to build and communicate key insights, and collaborate closely with product managers, engineers, designers, and researchers to help build the next experiences on Pinterest \n \n What we're looking for: \n \n 5+ years of experience analyzing data in a fast-paced, data-driven environment with proven ability to apply scientific methods to solve real-world problems on web-scale data \n Extensive experience solving analytical problems using quantitative approaches including in the fields of Statistics, Machine Learning, Forecasting, Econometrics or other related fields \n Familiarity with online experiment and its pitfalls \n Statistical rigor, can guide the team and others on statistically valid approaches to problems \n A scientifically rigorous approach to analysis and data, and a well-tuned sense of skepticism, attention to detail and commitment to high-quality, results-oriented output \n Ability to manipulate large data sets with high dimensionality and complexity; fluency in SQL (or other database languages) and a scripting language (Python or R) \n Excellent communication skills and ability to explain learnings to both technical and non-technical partners \n A team player who's able to partner with cross-functional leadership to quickly turn insights into actions \n \n This position is not eligible for relocation assistance. \n  #LI-JT6 \n  #LI-REMOTE \n \n \n  At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. \n  Information regarding the culture at Pinterest and benefits available for this position can be found  here . \n \n  US based applicants only \n \n    $114,750\u2014$236,000 USD\n   \n \n \n  Our Commitment to Diversity: \n  Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.", "cleaned_desc": " \n 5+ years of experience analyzing data in a fast-paced, data-driven environment with proven ability to apply scientific methods to solve real-world problems on web-scale data \n Extensive experience solving analytical problems using quantitative approaches including in the fields of Statistics, Machine Learning, Forecasting, Econometrics or other related fields \n Familiarity with online experiment and its pitfalls \n Statistical rigor, can guide the team and others on statistically valid approaches to problems \n A scientifically rigorous approach to analysis and data, and a well-tuned sense of skepticism, attention to detail and commitment to high-quality, results-oriented output \n Ability to manipulate large data sets with high dimensionality and complexity; fluency in SQL (or other database languages) and a scripting language (Python or R) ", "techs": ["sql", "python", "r"]}, "76ddbbfdb7b878db": {"terms": ["data science", "machine learning engineer"], "salary_min": 100735.45, "salary_max": 127553.53, "title": "Manager, Data Analytics - Medical Devices (Remote, US)", "company": "International Consulting Associates, Inc.", "desc": "Manager, Data Analytics - Medical Devices - International Consulting Associates, Inc. (Remote) \n About ICA \n International Consulting Associates, Inc. is a rapidly growing company, located in the D.C. Metro area. We were founded in 2009 to assist firms with evaluating and achieving their objectives. We have become a trusted advisor helping our clients by offering cutting-edge innovation and solutions to complex projects. Our small company has grown significantly, and we're overjoyed at the opportunity to expand yet again! \n We offer a range of management solutions and technical services on domestic and international programs. We are results-focused and have a proven track record supporting federal agencies and large government services primes in three main areas: Research and Data Analysis, Advanced-Data Science, and Strategic Services. And if that isn't amazing enough, ICA has established itself as a critical partner of the US Food and Drug Administration (FDA). We currently support multiple analytics and research programs with offices including the Center for Devices and Radiological Health (CDRH) and the Center for Medicaid/Medicare Services (CMS). \n We believe in our people! We support our high-performing team and encourage work/life balance. We strive to create and foster an environment of collaboration - one team. We care about our mission and delivering value that makes a real-world impact. We are committed to hiring and developing diverse, dynamic, and highly skilled teams in an atmosphere that is safe and inclusive. We are passionate about creating solutions that leverage technology, data science, and machine learning to simply help make our world a better place! \n We are looking for an experienced  Manager of Data Analytics  to join our team! \n About the Data Analytics Manager Role \n The Data Analytics Manager will serve as liaison between the Artificial Intelligence (AI) leadership team for medical devices, Data Science Team Leads and Data Scientists to facilitate and lead key business aspects and applications of AI Solution Development for one of ICA's key client accounts, the U.S. Food and Drug Administration (FDA). In this position, the Data Analytics (DA) Manager will work alongside a multidisciplinary team of data scientists, engineers, UI/UX designers, project managers and other analysts to support, lead, and facilitate developing the business solution requirements to deploy an AI tool tailored for FDA's Center for Devices and Radiological Health (CDRH). The goal of this role is to assist the overall team with developing a working prototype, or minimum viable product (MVP) designed to pilot the various prioritized applications for the FDA, using an agile development framework to develop this initial prototype before scaling this to a fully productionized tool ready for enterprise-wide deployment. \n  The key tasks and areas of responsibilities vary greatly for a DA Manager in support of this program but will follow the typical development lifecycle, whereby the DA Manager will lead and drive business tasks and activities aligned to this lifecycle. The following list of key responsibilities are packaged by the stages of solution development and where the data analyst will lead or support with these respective tasks to see the tool development to completion. A key part of the DA Manager role is to serve clients and key stakeholders and assess the overall effectiveness of the tool and impact it will have to CDRH to support their regulatory review of medical devices. \n \n Qualifications & Requirements \n \n Ability to effectively lead and manage a team of data analysts. \n  Experience leading and managing a cross-disciplinary team. \n  Ability to perform data research analyzation and provide visualizations to show findings \n  Excellent analytical, critical thinking, and problem-solving skills \n  Ability to work in a fast-paced environment \n  Excellent oral and written communication skills including the ability to create and deliver high-quality presentations \n  Excellent customer service and collaboration skills \n  Wireframing capability to show tool interaction, user interface capabilities and data mapping (Preferred) \n  Experience understanding and analyzing medical device related data \n  Four or more years of experience as a Data Analytics Manager (Preferred) \n  Bachelor's degree or higher in data science, biomedical engineering or a related field. (Preferred) \n  Ability to obtain a Public Trust Clearance (required, will help obtain) \n \n Nice If You Have: \n \n Experience working in a consultative environment \n \n Clearance: \n Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Public Trust clearance is required. \n \n What we offer: \n \n Health Insurance   \n  Dental Insurance \n  Vision insurance \n  Health Spending Account \n  Flexible Spending Account \n  Life and Disability insurance \n  401(k) plan with company match \n  Paid Time Off (Vacation, Sick Leave and Holidays) \n  Education and Professional Development Assistance \n  Remote work from anywhere within the continental United States   \n \n Location and Telework: \n This position is remote, but our clients are primarily based in the DC-metro area. While we encourage flexible work hours based on location, it is essential that the successful candidate be responsive to client scheduling (EST). Occasional travel (< 5% time) to the DC-metro area for mandatory In-person meetings required. \n Additional Information: \n ICA is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, national origin, genetics, disability status, protected veteran status, age, or any other characteristic protected by state, federal or local laws. \n This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.", "cleaned_desc": " \n Qualifications & Requirements \n \n Ability to effectively lead and manage a team of data analysts. \n  Experience leading and managing a cross-disciplinary team. \n  Ability to perform data research analyzation and provide visualizations to show findings \n  Excellent analytical, critical thinking, and problem-solving skills \n  Ability to work in a fast-paced environment \n  Excellent oral and written communication skills including the ability to create and deliver high-quality presentations    Excellent customer service and collaboration skills \n  Wireframing capability to show tool interaction, user interface capabilities and data mapping (Preferred) \n  Experience understanding and analyzing medical device related data \n  Four or more years of experience as a Data Analytics Manager (Preferred) \n  Bachelor's degree or higher in data science, biomedical engineering or a related field. (Preferred) \n  Ability to obtain a Public Trust Clearance (required, will help obtain) \n \n Nice If You Have: \n ", "techs": ["data research analyzation", "visualizations", "analytical skills", "critical thinking skills", "problem-solving skills", "oral communication skills", "written communication skills", "high-quality presentations", "customer service skills", "collaboration skills", "wireframing capability", "user interface capabilities", "data mapping", "understanding medical device related data", "experience as a data analytics manager", "bachelor's degree in data science", "biomedical engineering or a related field", "public trust clearance."]}, "3511abc7137d6949": {"terms": ["data science"], "salary_min": 106683.2, "salary_max": -1.0, "title": "Lead Data Scientist", "company": "Ascension", "desc": "Details  \n \n Department:  Ascension Data Science Institute \n  Schedule:  Full-time / Monday - Friday / Days \n  Location:  Fully Remote \n \n  Benefits  \n \n  Paid time off (PTO)\n    Various health insurance options & wellness plans\n    Retirement benefits including employer match plans\n    Long-term & short-term disability\n    Employee assistance programs (EAP)\n    Parental leave & adoption assistance\n    Tuition reimbursement\n    Ways to give back to your community\n  \n \n \n  As a military friendly organization, Ascension promotes career flexibility and offers many benefits to help support the well-being of our military families, spouses, veterans and reservists. Our associates are empowered to apply their military experience and unique perspective to their civilian career with Ascension.\n  \n \n \n   Please note, benefits and benefits eligibility can vary by position, exclusions may apply for some roles (for example: PRN, Short-Term Option, etc.). Connect with your Talent Advisor today for additional specifics. \n \n \n  Responsibilities  \n \n  Iteratively prototype and build machine learning pipelines at scale to provide data-driven insights. Develop comprehensive knowledge of data structures, metrics, and advocate for improvements where needed for product development.\n  \n \n  Develop advanced end-to-end ML models for different use cases: tabular data, images, video, speech, unstructured text. \n  Leads discussions with project stakeholders, senior management, and the user community, to collaboratively define project scope and capabilities in accordance with data science best practices and organizational needs. \n  Clearly communicate technical knowledge and provide training in a client-facing technical consulting role. \n  Enhance, harmonize, and transform data from large relational databases to support ML projects (e.g. SQL, R, Python). \n  Create and manage a machine learning pipeline that ingests data, performs sophisticated feature engineering, and deploys a scalable and repeatable solution using serverless cloud ML tools. Architect, optimize, and deploy production ML systems on the cloud (e.g., AWS, GCP). \n  Demonstrate leadership in a rapidly evolving environment. Proactively navigate ambiguity and technological hurdles to deliver state-of-the-art cloud data science solutions to healthcare problems. \n \n \n   Develop predictive and prescriptive analytics models, create efficient algorithms and innovating use of data.\n  \n \n  Analyze complex business and competitive issues and discern the implications for systems support. \n  Identify, define, direct and preform project issue analysis to resolve issues, including analysis of technical and economic feasibility of proposed data solution. \n  Create and manage a machine learning pipeline, from raw data acquisitions to merging and normalizing to sophisticated feature engineering development to model execution. \n  Design, lead and actively engage in projects with broad implication for the business and/or the future architecture, successfully addressing cross-technology and cross-platform issues. \n  Select tools and methodologies for projects and negotiate terms and conditions with vendors. \n  Communicate highly technical information to numerous audiences, including senior management, the user community, and less-experienced staff. \n  Lead the organizations planning for the future of the data model and data architecture. \n \n  Requirements  \n \n  Education:\n  \n \n  High School diploma equivalency with 3 years of cumulative experience OR Associate's degree/Bachelor's degree with 2 years of cumulative experience OR 7 years of applicable cumulative job specific experience required. \n  3 years of leadership or management experience preferred. \n \n  Additional Preferences  \n \n Advanced degree in Computer Science, Mathematics, or other quantitative field; with some evidence of ML related work. \n  Minimum of 5 years of hands-on experience in data science, machine learning, and analytics, with at least 3 years of managerial or team leadership experience. \n  Proficiency in programming languages such as Python or R. Extensive experience with data manipulation, data visualization, and machine learning libraries (e.g., TensorFlow, scikit-learn, PyTorch). Comfortable using Github for version control of code. \n  Experience working with production ML systems in the cloud (e.g. AWS, GCP). GCP preferred. \n  Demonstrated ability to understand business objectives and translate them into data-driven strategies and actionable insights. \n  Excellent verbal and written communication skills, with the ability to present complex technical concepts to non-technical stakeholders. \n \n \n \n  #LI-Remote #ADSI\n  \n  Why Join Our Team  \n \n  Ascension associates are key to our commitment of transforming healthcare and providing care to all, especially those most in need. Join us and help us drive impact through reimagining how we can deliver a people-centered healthcare experience and creating the solutions to do it. Explore career opportunities across our ministry locations and within our corporate headquarters.\n  \n \n \n  Ascension is a leading non-profit, faith-based national health system made up of over 150,000 associates and 2,600 sites of care, including more than 140 hospitals and 40 senior living communities in 19 states.\n  \n \n \n  Our Mission, Vision and Values encompass everything we do at Ascension. Every associate is empowered to give back, volunteer and make a positive impact in their community. Ascension careers are more than jobs; they are opportunities to enhance your life and the lives of the people around you.\n  \n  Equal Employment Opportunity Employer  \n \n  Ascension will provide equal employment opportunities (EEO) to all associates and applicants for employment regardless of race, color, religion, national origin, citizenship, gender, sexual orientation, gender identification or expression, age, disability, marital status, amnesty, genetic information, carrier status or any other legally protected status or status as a covered veteran in accordance with applicable federal, state and local laws.\n  \n \n \n  For further information, view the EEO Know Your Rights (English) poster or EEO Know Your Rights (Spanish) poster.\n  \n \n \n  Pay Non-Discrimination Notice\n  \n \n \n  Please note that Ascension will make an offer of employment only to individuals who have applied for a position using our official application. Be on alert for possible fraudulent offers of employment. Ascension will not solicit money or banking information from applicants.\n  \n \n  E-Verify Statement  \n \n  This employer participates in the Electronic Employment Verification Program. Please click the E-Verify link below for more information.\n   \n  E-Verify", "cleaned_desc": "  \n \n \n   Please note, benefits and benefits eligibility can vary by position, exclusions may apply for some roles (for example: PRN, Short-Term Option, etc.). Connect with your Talent Advisor today for additional specifics. \n \n \n  Responsibilities  \n \n  Iteratively prototype and build machine learning pipelines at scale to provide data-driven insights. Develop comprehensive knowledge of data structures, metrics, and advocate for improvements where needed for product development.\n  \n \n  Develop advanced end-to-end ML models for different use cases: tabular data, images, video, speech, unstructured text. \n  Leads discussions with project stakeholders, senior management, and the user community, to collaboratively define project scope and capabilities in accordance with data science best practices and organizational needs. \n  Clearly communicate technical knowledge and provide training in a client-facing technical consulting role. \n  Enhance, harmonize, and transform data from large relational databases to support ML projects (e.g. SQL, R, Python). \n  Create and manage a machine learning pipeline that ingests data, performs sophisticated feature engineering, and deploys a scalable and repeatable solution using serverless cloud ML tools. Architect, optimize, and deploy production ML systems on the cloud (e.g., AWS, GCP). \n  Demonstrate leadership in a rapidly evolving environment. Proactively navigate ambiguity and technological hurdles to deliver state-of-the-art cloud data science solutions to healthcare problems. \n \n \n   Develop predictive and prescriptive analytics models, create efficient algorithms and innovating use of data.   \n \n  Analyze complex business and competitive issues and discern the implications for systems support. \n  Identify, define, direct and preform project issue analysis to resolve issues, including analysis of technical and economic feasibility of proposed data solution. \n  Create and manage a machine learning pipeline, from raw data acquisitions to merging and normalizing to sophisticated feature engineering development to model execution. \n  Design, lead and actively engage in projects with broad implication for the business and/or the future architecture, successfully addressing cross-technology and cross-platform issues. \n  Select tools and methodologies for projects and negotiate terms and conditions with vendors. \n  Communicate highly technical information to numerous audiences, including senior management, the user community, and less-experienced staff. \n  Lead the organizations planning for the future of the data model and data architecture. \n \n  Requirements  \n \n  Education:\n  \n \n  High School diploma equivalency with 3 years of cumulative experience OR Associate's degree/Bachelor's degree with 2 years of cumulative experience OR 7 years of applicable cumulative job specific experience required. \n  3 years of leadership or management experience preferred. \n \n  Additional Preferences  \n   Advanced degree in Computer Science, Mathematics, or other quantitative field; with some evidence of ML related work. \n  Minimum of 5 years of hands-on experience in data science, machine learning, and analytics, with at least 3 years of managerial or team leadership experience. \n  Proficiency in programming languages such as Python or R. Extensive experience with data manipulation, data visualization, and machine learning libraries (e.g., TensorFlow, scikit-learn, PyTorch). Comfortable using Github for version control of code. \n  Experience working with production ML systems in the cloud (e.g. AWS, GCP). GCP preferred. \n  Demonstrated ability to understand business objectives and translate them into data-driven strategies and actionable insights. \n  Excellent verbal and written communication skills, with the ability to present complex technical concepts to non-technical stakeholders. \n \n \n \n  #LI-Remote #ADSI\n  \n  Why Join Our Team  \n \n  Ascension associates are key to our commitment of transforming healthcare and providing care to all, especially those most in need. Join us and help us drive impact through reimagining how we can deliver a people-centered healthcare experience and creating the solutions to do it. Explore career opportunities across our ministry locations and within our corporate headquarters.\n  \n \n \n  Ascension is a leading non-profit, faith-based national health system made up of over 150,000 associates and 2,600 sites of care, including more than 140 hospitals and 40 senior living communities in 19 states.\n  \n ", "techs": ["python", "r", "sql", "aws", "gcp", "tensorflow", "scikit-learn", "pytorch", "github"]}, "2589549892391303": {"terms": ["data science", "data analyst"], "salary_min": 100058.625, "salary_max": 126696.516, "title": "Senior Data Analyst", "company": "TailorCare", "desc": "About TailorCare \n  TailorCare is transforming the experience of specialty care. Our comprehensive care program takes a deeply personal, evidence-based approach to improving patient outcomes for joint, back, and muscle conditions. By combining a careful assessment of patients' symptoms, health histories, preferences, and goals with predictive data and latest evidence-based guidelines, we help patients choose\u2014and navigate\u2014the most effective treatment pathway for them, every step of the way. \n  TailorCare values the experiences and perspectives of individuals from all backgrounds. We are a highly collaborative, curious, and determined team passionate about scaling a high-growth start-up to improve the lives of those in pain. TailorCare is a remote-first company with hybrid offices in New York City and soon-to-be Nashville. \n \n  About the Role \n  We are looking for a high performing data analyst to be a core contributor in building our analytics platform from the ground up. You will be helping develop data products and key insights to support and scale a data-driven value-based care company. This role will be responsible for creating essential KPIs and analyses that help drive strong performance across a variety of domains. You will work closely with business stakeholders to understand the business problem and determine the best solution, while explaining difficult concepts to ensure transparency and explainability. \n  Primary Responsibilities \n \n Derive insights from descriptive analysis using multiple datasets including eligibility, medical claims, clinical, pharmacy claims, EMR data, etc. \n Create and maintain a suite of business intelligence reporting assets using Tableau \n Collaborate with the business stakeholders to develop strategic recommendations using multiple data sources \n Develop analytics data sets and enriched data models leveraging cloud computing technologies (DBT and Databricks) \n Apply statistical techniques to model healthcare utilization patterns and evaluate care outcomes \n Support a data-informed process for experimenting with new products or operational workflows to improve patient outcomes \n Design and implement metrics, work cross-functionally to create targets, and build reporting to track the performance of company initiatives \n Mentor and develop junior team members \n \n Minimum Qualifications \n \n Bachelor's Degree in relevant field (Preferably: Data Science, Healthcare Informatics, Statistics, Economics, Mathematics, Computer Science, etc.) or equivalent relevant work experience \n Minimum five years of recent experience in the design, development and delivery of data driven solutions and analytics \n Minimum three years working with healthcare data (eligibility, medical claims, pharmacy claims, EMR, etc.) \n Minimum five years of experience using SQL or Python, experience building data pipelines for analytics enrichments \n Strong communication skills and ability to present complex information concisely through written & verbal communication \n US work authorization \n \n Desired Qualifications \n \n Experience working in Databricks / data lake environment \n Experience using DBT to manage analytic transformations \n Experience building/scaling reporting suites leveraging BI platforms (e.g. Tableau) \n Experience with Python or R is nice to have \n \n Skills \n \n Healthcare Savy - Professional background in healthcare provider operations, independent medical groups, health plan operations, and/ or professional services \n Collaborative - Works cross-functionally with stakeholders to develop innovative and effective solutions \n Organized \u2013 Ability to manage and prioritize multiple projects simultaneously in a fast-paced environment \n Intellectual Curiosity - Demonstrated initiative staying current on industry practice outside of study and training, with a high and broad level of industry knowledge \n Scientific Approach \u2013 Ability to start with an open-ended question and design/execute an analysis to answer that question \n Leadership - Ability to manage projects/deliverables without significant direction, managing communication with stakeholders and mentoring junior team members \n \n \n  What's In It For You \n \n Meaningful work each day, we care deeply about our mission, our patients, and each-other. \n Work from anywhere in the US that best fits your lifestyle, or, for those that enjoy an in-person environment, join teammates in our hybrid hubs in NYC or Nashville. \n Rich PTO and holiday plans to ensure you have time away to rest and recharge \n We offer paid parental leave, support a healthy work-life integration, and offer work flexibility \u2013 we love to talk about our pets and families. \n Medical, dental, vision, life, disability, wellness resources, and an employer HSA contribution all from Day 1. \n We are committed to fair and equitable pay for all employees, and we help you achieve your future goals with an employer match 401k. \n An inclusive workplace where you can lean on your teammates, offer candid feedback, and bring your true self to work each day. \n \n TailorCare seeks to recruit and retain staff from diverse backgrounds and encourages qualified candidates to apply. TailorCare is an equal opportunity employer and does not discriminate on the basis of age, sex, gender identity/expression, sexual orientation, color, race, creed, national origin, ancestry, religion, marital status, political belief, physical or mental disability, pregnancy, military, or veteran status.", "cleaned_desc": " Collaborate with the business stakeholders to develop strategic recommendations using multiple data sources \n Develop analytics data sets and enriched data models leveraging cloud computing technologies (DBT and Databricks) \n Apply statistical techniques to model healthcare utilization patterns and evaluate care outcomes \n Support a data-informed process for experimenting with new products or operational workflows to improve patient outcomes \n Design and implement metrics, work cross-functionally to create targets, and build reporting to track the performance of company initiatives \n Mentor and develop junior team members \n \n Minimum Qualifications \n \n Bachelor's Degree in relevant field (Preferably: Data Science, Healthcare Informatics, Statistics, Economics, Mathematics, Computer Science, etc.) or equivalent relevant work experience   Minimum five years of recent experience in the design, development and delivery of data driven solutions and analytics \n Minimum three years working with healthcare data (eligibility, medical claims, pharmacy claims, EMR, etc.) \n Minimum five years of experience using SQL or Python, experience building data pipelines for analytics enrichments \n Strong communication skills and ability to present complex information concisely through written & verbal communication \n US work authorization \n \n Desired Qualifications \n \n Experience working in Databricks / data lake environment \n Experience using DBT to manage analytic transformations   Experience building/scaling reporting suites leveraging BI platforms (e.g. Tableau) \n Experience with Python or R is nice to have \n \n Skills \n \n Healthcare Savy - Professional background in healthcare provider operations, independent medical groups, health plan operations, and/ or professional services \n Collaborative - Works cross-functionally with stakeholders to develop innovative and effective solutions \n Organized \u2013 Ability to manage and prioritize multiple projects simultaneously in a fast-paced environment \n Intellectual Curiosity - Demonstrated initiative staying current on industry practice outside of study and training, with a high and broad level of industry knowledge \n Scientific Approach \u2013 Ability to start with an open-ended question and design/execute an analysis to answer that question ", "techs": ["dbt", "databricks", "sql", "python", "tableau", "databricks", "data lake", "bi platforms"]}, "f76a5fd190cc11e5": {"terms": ["data science"], "salary_min": 192800.0, "salary_max": 255000.0, "title": "VP, Data & AI Platform Innovation", "company": "McGraw Hill LLC.", "desc": "Overview: \n  \n  Build the Future \n \n \n   Could your creative thinking build the future? As a VP of Data & AI Platform Innovation at McGraw Hill, make a difference for learners and educators across the world. Our team needs individuals with new ideas who connect with people in innovative ways.\n  \n \n \n  What is this role about? \n \n \n   The VP of Data & AI Platform Innovation will lead the Enterprise Data & Analytics (D&A) Data Engineering and ML Engineering Teams for McGraw Hill and will report directly to the Head of Enterprise Data & Analytics (D&A). This role will initially focus on leading the design and delivery, cross-functionally, of data & AI platforms we need to deliver on our major digital transformation initiatives at McGraw Hill. As these D&A platforms go live, this role will lead the teams responsible for operations and support of our modernized D&A platforms. This role will empower our products to leverage data, AI, and information assets as a true competitive advantage for McGraw Hill.\n  \n \n \n  What can you expect from the position? \n \n \n  Modernized Data & AI Platform Architecture must be one of your \u201csuperpowers\u201d. \n  Provide strategic insights and direction to Chief Product Officer(s) to inform how future product strategy is delivered with modernized and/or net new data platforms AND source data systems (Oracle, SFDC, etc.); must provide our Chief Product Officer(s) with business cases/justification for your recommended path forward. \n  Responsible for partnering with cross-organizational business lines and functions to establish the framework for and execute the systems transformation, both data platforms and source data systems, ensuring cross-functional alignment with our IT (GTS) and Product Development (DPG) teams \n  Must drive design and delivery of technical solutions with a focus on business simplification by increasing operational efficiency and reducing manual processes, while supporting financial reporting requirements \n  Assess and define what metadata/data design requirements are needed to ensure product data & AI strategy can be delivered as part of broader product strategy; must call out gaps in data ownership for CPO(s) to ensure CPO(s) are able to work with appropriate senior leadership to assign data ownership and close gaps. \n  Efficiently assess enterprise systems, processes, operations, data, AI and financial reporting to document areas of impact for remediation \n  Utilize business process techniques to map which business processes can be improved through technology, and then define the correct KPIs to measure targeted improvement of end-to-end business processes as we progress with our digital transformation. \n  Assesses risk potentials and discover potential problems before they occur, and flag for CPO(s) to ensure CPO(s) can incorporate into broader communications and product strategy planning. \n  Identify and address resource constraints. \n  Contribute to business cases to secure funding. \n  Create and monitor project plans. \n  Write clear and well-structured business requirements and documentation. \n  Implement automation where there is opportunity. \n  Generate organizational confidence and buy-in by raising awareness, committing to accuracy, and operating with precision. \n \n \n \n  We are looking for someone with: \n \n \n  Extensive Knowledge of Data Platform (Data Lakes/Data Warehouses) Tools \u2013 At least 13+ years experience with modernized stack of Data platform capabilities across some combination of AWS, Databricks, Azure, etc. and how to best leverage to create delightful user experiences in a scalable and cost-efficient manner. \n  Extensive Knowledge of Self-Service BI (SSBI) Tools \u2013 At least 10+ years experience with modernized stack of SSBI tools (PowerBI, LookerBI, etc.) and how to best leverage to create delightful customer experiences. \n  System Knowledge \u2013 At least 8+ years of experience with quickly learning enterprise-wide systems (Oracle, SFDC, EDW) and their relationship; databases; modeling software. \n  Operational Leader- At least 10+ years of experience successfully in leading and implementing improvements. Experience in change management roles including in managing large complex cross-company transformation programs, while defining clear strategy to achieve vision, milestones, and measuring progress for each major milestone; must have strong financial acumen. \n  Communication \u2013 Must have 13+ years of experience interacting and persuading individuals in senior-level positions, with advanced writing and communication skills, as well as actively participating in meetings and group discussions with individuals from various functions across the organization (e.g., IT, Finance, Product). \n  Analytical \u2013 Must have 8+ years of experience synthesizing information from cross functional areas to draw insights and actionable recommendations (e.g., structure the problem, collect data, identify issues, and present results). Individual should have strong quantitative skills with a portion of their background familiar with business case analysis and corporate data analysis. \n \n \n  Preferred experience: \n \n \n   Multi-Cloud Data Platform Experience \u2013 Ideally, candidate has run multi-cloud environments, in addition to exposure to multiple cloud providers.\n  \n \n   Financial \u2013 Ideally, candidate will have 5+ years experience with or knowledge of corporate finance and accounting.\n  \n \n \n  Why work for us? \n \n \n   We have a collective passion for the work we do and a curiosity to find new solutions. If you share our determination, together we will drive learning forward.\n  \n \n \n  The pay range for this position is between $192,800 \u2013 $255,000 annually, however, base pay offered may vary depending on job-related knowledge, skills, experience, and location. An annual bonus plan may be provided as part of the compensation package, in addition to a full range of medical and/or other benefits, depending on the position offered. Click here to learn more about our benefit offerings.", "cleaned_desc": "   The VP of Data & AI Platform Innovation will lead the Enterprise Data & Analytics (D&A) Data Engineering and ML Engineering Teams for McGraw Hill and will report directly to the Head of Enterprise Data & Analytics (D&A). This role will initially focus on leading the design and delivery, cross-functionally, of data & AI platforms we need to deliver on our major digital transformation initiatives at McGraw Hill. As these D&A platforms go live, this role will lead the teams responsible for operations and support of our modernized D&A platforms. This role will empower our products to leverage data, AI, and information assets as a true competitive advantage for McGraw Hill.\n  \n \n \n  What can you expect from the position? \n \n \n  Modernized Data & AI Platform Architecture must be one of your \u201csuperpowers\u201d. \n  Provide strategic insights and direction to Chief Product Officer(s) to inform how future product strategy is delivered with modernized and/or net new data platforms AND source data systems (Oracle, SFDC, etc.); must provide our Chief Product Officer(s) with business cases/justification for your recommended path forward. \n  Responsible for partnering with cross-organizational business lines and functions to establish the framework for and execute the systems transformation, both data platforms and source data systems, ensuring cross-functional alignment with our IT (GTS) and Product Development (DPG) teams \n  Must drive design and delivery of technical solutions with a focus on business simplification by increasing operational efficiency and reducing manual processes, while supporting financial reporting requirements \n  Assess and define what metadata/data design requirements are needed to ensure product data & AI strategy can be delivered as part of broader product strategy; must call out gaps in data ownership for CPO(s) to ensure CPO(s) are able to work with appropriate senior leadership to assign data ownership and close gaps.    We are looking for someone with: \n \n \n  Extensive Knowledge of Data Platform (Data Lakes/Data Warehouses) Tools \u2013 At least 13+ years experience with modernized stack of Data platform capabilities across some combination of AWS, Databricks, Azure, etc. and how to best leverage to create delightful user experiences in a scalable and cost-efficient manner. \n  Extensive Knowledge of Self-Service BI (SSBI) Tools \u2013 At least 10+ years experience with modernized stack of SSBI tools (PowerBI, LookerBI, etc.) and how to best leverage to create delightful customer experiences. \n  System Knowledge \u2013 At least 8+ years of experience with quickly learning enterprise-wide systems (Oracle, SFDC, EDW) and their relationship; databases; modeling software. \n  Operational Leader- At least 10+ years of experience successfully in leading and implementing improvements. Experience in change management roles including in managing large complex cross-company transformation programs, while defining clear strategy to achieve vision, milestones, and measuring progress for each major milestone; must have strong financial acumen. \n  Communication \u2013 Must have 13+ years of experience interacting and persuading individuals in senior-level positions, with advanced writing and communication skills, as well as actively participating in meetings and group discussions with individuals from various functions across the organization (e.g., IT, Finance, Product). \n  Analytical \u2013 Must have 8+ years of experience synthesizing information from cross functional areas to draw insights and actionable recommendations (e.g., structure the problem, collect data, identify issues, and present results). Individual should have strong quantitative skills with a portion of their background familiar with business case analysis and corporate data analysis. \n \n \n  Preferred experience: ", "techs": ["data platform (data lakes/data warehouses) tools", "aws", "databricks", "azure,\nself-service bi (ssbi) tools", "powerbi", "lookerbi,\nsystem knowledge", "oracle", "sfdc", "edw,\noperational leader,\nchange management roles,\ncommunication", "it", "finance", "product,\nanalytical", "business case analysis", "corporate data analysis"]}, "a887c1bf53baa858": {"terms": ["data science"], "salary_min": 95414.29, "salary_max": 120815.74, "title": "Data Scientist - US/Canada", "company": "PHASTAR", "desc": "Overview: \n  \n  THE COMPANY \n \n \n \n  Phastar is a multiple award-winning global biometric Contract Research Organization (CRO) that is accredited as an outstanding company to work for by Best Companies. We partner with pharmaceutical, biotechnology and medical device organizations to provide the expertise and processes to manage and deliver on time, quality biostatistics, programming, data management and data science services. With offices across the UK, US, Germany, Denmark, Kenya, Australia, India, China and Japan, Phastar is the second largest specialized biometrics provider globally, and the largest in the UK.\n  \n \n \n  Our unique approach to data analysis, \u201cThe Phastar Discipline\u201d, has led us to build a reputation for outstanding quality. With this as our core focus, we\u2019re looking for talented individuals who share our passion for quality and technical expertise to join our team.\n  \n \n \n  WHY PHASTAR \n \n \n \n  Accredited as an outstanding company to work for, Phastar is committed to employee engagement, workplace satisfaction and ensuring a healthy work-life balance. We offer flexible working, part-time hours, involvement in developing company-wide initiatives, structured training and development plans, and a truly supportive, fun and friendly environment.\n  \n \n \n  What\u2019s more, when you join our team, Phastar will plant a tree in your honour, as one of our Environmental, Social and Governance (ESG) initiatives. So, not only would you get your dream job, you\u2019ll also be helping to save the planet!\n  \n \n \n  THE ROLE \n \n \n \n  Demand for our Functional Service Provision is growing, and we are looking for an experienced Senior Data Scientist to join our FSP team.\n  \n \n \n  This is a fantastic opportunity to work for a growing CRO that is recognized for its continuous learning and development opportunities, whilst also gaining direct experience of working within a pharmaceutical environment.\n   Responsibilities: \n  \n Collaborate with Payer and Epidemiology teams to maximise the value derived from large observational research data \n  Deliver analyses of data from EMR, claims and primary observational data required by TA RWE strategies \n  Support the development of IVS strategies and selection of optimised contact models for prioritised markets through analysis of RWD \n  Provide scientific guidance on the application of Real World Evidence and observational research data to address issues across the Oncology and Biopharmaceuticals business units \n  Provide technical input, options and directions to strategic decisions made by the client observational study teams on study design, data partner selection and best practices in RWE data utilization \n  Support technical teams to provide access to analytical tools and develop visual analytics to enable self-serving applications for end customers \n  Provide clear technical input, options, and direction to strategic decisions on RWE platform and capability build \n  Provide support for strategic decisions on client Medical Evidence and Observational Research external collaborations in the US and other markets \n  Assist in building a capability that becomes a source of sustained competitive advantage for the client in identifying, acquiring, integrating and mining diverse RW data from multiple geographic and healthcare system sources to support evidence generation and real-world studies \n  Evaluate and assess strengths and weaknesses of external RW data sources, and potential partners for advancing the data strategy for specific therapeutic areas \n  Maintain a strong insight into the capabilities of potential external partners in RWE, especially for US and emerging markets \n  Qualifications: \n  \n Experience in real-world evidence and familiarity with health economics/epidemiology \n  Expertise in methods development and application using statistical languages such as R/Matlab/SAS/SQL/Hadoop/Python \n  Preferred to have experience in advanced visualisation and visual analytics of routinely collected healthcare data \n  MSc or PhD in Data Science or related/advanced analytical degree field i.e. Biostatistics/Statistics, Epidemiology, Public Health \n  Experience working within a clinical trials environment (CRO, pharma or academia) \n  Excellent written and verbal communication skills \n \n \n \n  APPLY NOW \n \n \n \n  With the world\u2019s eyes focused on clinical trial data, this is a fantastic time to join an award-winning specialized biometric CRO that is renowned for its technical expertise, outstanding quality and cutting-edge data science techniques. We offer flexible working, part-time hours, structured training and development plans, continuous learning opportunities, and a competitive salary and benefits package. We\u2019re committed to ensuring our employees achieve a healthy work-life balance, within a supportive, fun and friendly working environment.\n  \n \n \n  Should you feel that you have the right skill set and motivations for this position, please apply! Please note that we are considering candidates located anywhere in the US or Canada as this role can be carried out remotely.\n  \n \n \n  Phastar is committed to the principles and practices of equal opportunities and to encouraging the establishment of a diverse workforce. It is our policy to employ individuals on the basis of their suitability for the work to be performed and their potential for development, regardless of age, sex, race, colour, nationality, ethnic or national origin, disability, marital status, pregnancy or maternity, sexual orientation, gender reassignment, religion, or belief. This includes creating a culture that fully reflects our commitment to equal opportunities for all. \n \n \n \n  Important notice to Employment businesses/ Agencies \n \n \n \n  Phastar does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact Phastar's Head of Talent Acquisition to obtain prior written authorization before referring any candidates to Phastar. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and Phastar. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of Phastar. Phastar shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.", "cleaned_desc": "  Assist in building a capability that becomes a source of sustained competitive advantage for the client in identifying, acquiring, integrating and mining diverse RW data from multiple geographic and healthcare system sources to support evidence generation and real-world studies \n  Evaluate and assess strengths and weaknesses of external RW data sources, and potential partners for advancing the data strategy for specific therapeutic areas \n  Maintain a strong insight into the capabilities of potential external partners in RWE, especially for US and emerging markets \n  Qualifications: \n  \n Experience in real-world evidence and familiarity with health economics/epidemiology \n  Expertise in methods development and application using statistical languages such as R/Matlab/SAS/SQL/Hadoop/Python \n  Preferred to have experience in advanced visualisation and visual analytics of routinely collected healthcare data \n  MSc or PhD in Data Science or related/advanced analytical degree field i.e. Biostatistics/Statistics, Epidemiology, Public Health \n  Experience working within a clinical trials environment (CRO, pharma or academia) \n  Excellent written and verbal communication skills \n \n \n \n  APPLY NOW ", "techs": ["r", "matlab", "sas", "sql", "hadoop", "python"]}, "4d17a04792a5ca82": {"terms": ["data science"], "salary_min": 73100.0, "salary_max": 166000.0, "title": "Data Scientist, Mid", "company": "Booz Allen Hamilton", "desc": "Job Description \n \n \n \n \n \n \n \n \n \n \n         Location: \n         \n \n         Arlington,VA,US \n         \n \n \n \n         Remote Work: \n         \n \n         Hybrid \n         \n \n \n \n         Job Number: \n         \n \n         R0179452\n         \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n         Data Scientist, Mid\n           The Opportunity:  \n As a data scientist, you\u2019re excited at the prospect of unlocking the secrets held by a data set, and you\u2019re fascinated by the possibilities presented by IoT, machine learning, and artificial intelligence. In an increasingly connected world, massive amounts of structured and unstructured data open new opportunities. As a data scientist, you can help turn these complex data sets into useful information to solve global challenges. Across private and public sectors from fraud detection to cancer research, to national intelligence, we need you to help find the answers in the data. \n \n  On our team, you\u2019ll use your leadership skills and data science expertise to create real-world impact. You\u2019ll work closely with clients to understand their questions and needs, and then dig into their data-rich environments to find the pieces of their information puzzle. You\u2019ll guide teammates and lead the development of algorithms and systems. You\u2019ll use the right combination of tools and frameworks to turn sets of disparate data points into objective answers to advise your clients as they make informed decisions. Ultimately, you\u2019ll provide a deep understanding of the data, what it all means, and how it can be used. Work with us as we use data science for good. \n \n  Join us. The world can\u2019t wait. \n \n  You Have: \n \n  Experience with data exploration, data cleaning, data analysis, data visualization, or data mining \n  Experience with statistical and general-purpose programming languages for data analysis \n  Knowledge of predictive data models, quantitative analyses, and visualization of targeted data sources \n  Ability to analyze structured and unstructured data sources \n  Secret clearance \n  Bachelor\u2019s degree \n \n \n  Nice If You Have: \n \n  Experience with the development of algorithms leveraging R, Python, or SQL and NoSQL \n  Experience with distributed data and computing tools, including MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL \n  Experience with visualization packages, including Plotly, Seaborn, or ggplot2 \n \n \n  Clearance: \n  Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required. \n \n  Create Your Career: \n  Grow With Us \n  Your growth matters to us\u2014that\u2019s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. \n \n  A Place Where You Belong \n  Diverse perspectives cultivate collective ingenuity. Booz Allen\u2019s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you\u2019ll develop your community in no time. \n \n  Support Your Well-Being \n  Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we\u2019ll support you as you pursue a balanced, fulfilling life\u2014at work and at home. \n \n  Your Candidate Journey \n  At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we\u2019ve compiled a list of resources so you\u2019ll know what to expect as we forge a connection with you during your journey as a candidate with us. \n \n  Compensation \n  At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen\u2019s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. \n  Salary at Booz Allen is determined by various factors, including but not limited to location, the individual\u2019s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $73,100.00 to $166,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen\u2019s total compensation package for employees.\n          \n  Work Model  Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. \n \n  If this position is listed as remote or hybrid, you\u2019ll periodically work from a Booz Allen or client site facility. \n  If this position is listed as onsite, you\u2019ll work with colleagues and clients in person, as needed for the specific role. \n \n \n  EEO Commitment \n  We\u2019re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change \u2013 no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.", "cleaned_desc": "  Experience with data exploration, data cleaning, data analysis, data visualization, or data mining \n  Experience with statistical and general-purpose programming languages for data analysis \n  Knowledge of predictive data models, quantitative analyses, and visualization of targeted data sources \n  Ability to analyze structured and unstructured data sources \n  Secret clearance \n  Bachelor\u2019s degree \n \n \n  Nice If You Have: \n \n  Experience with the development of algorithms leveraging R, Python, or SQL and NoSQL \n  Experience with distributed data and computing tools, including MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL \n  Experience with visualization packages, including Plotly, Seaborn, or ggplot2 \n \n \n  Clearance: \n  Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required. \n \n  Create Your Career: \n  Grow With Us ", "techs": ["data exploration", "data cleaning", "data analysis", "data visualization", "data mining", "statistical programming languages", "general-purpose programming languages", "predictive data models", "quantitative analyses", "targeted data sources", "structured data sources", "unstructured data sources", "r", "python", "sql", "nosql", "distributed data tools", "computing tools", "mapreduce", "hadoop", "hive", "emr", "kafka", "spark", "gurobi", "mysql", "visualization packages", "plotly", "seaborn", "ggplot2", "secret clearance", "bachelor's degree"]}, "1d45e38ae698ec1d": {"terms": ["data science", "data analyst"], "salary_min": 34123.504, "salary_max": 43207.957, "title": "Business Data Analyst Co-Op", "company": "Field Aerospace", "desc": "Field Aerospace is a leading aerospace company dedicated to providing cutting-edge aircraft modifications and support to the US military and allied nations. With over 75 years of industry experience, we take pride in our commitment to innovation, technical excellence, and delivering solutions that empower missions and support our valued clients. \n  General Duties: \n  We are seeking a dynamic and enthusiastic Business Data Analyst Intern to join our team. You will assist in supporting our data analysis efforts to help drive toward becoming a more data driven business. This internship is an excellent opportunity to gain practical experience in a dynamic aerospace company. \n  Essential Job Functions: \n \n  Act as a bridge between departments within the business to bring together data and analysis. \n  Support stakeholders of the business by turning requirements into reports and Dashboards. \n  Document purpose and description of reports to further organizational data literacy. \n  Perform data cleansing exercises to help standardize the data which exists in the system. \n  Capable of learning new systems and creating/implementing new system functionality. \n  Ability to create visually appealing and user-friendly reports and dashboards. \n  Providing frontline support for reporting tools (i.e. ReportsNow). \n \n  Skills and Experience: \n \n  Strong written and verbal communication skills, with attention to detail. \n  Familiarity with business systems. Experience with Microsoft Power BI (preferred) or Tableau. \n  Proficient in Microsoft Office suite (Word, Excel, PowerPoint). \n  Creative thinker with the ability to generate and implement innovative ideas. \n  Self-motivated, proactive, and able to work independently as well as collaboratively. \n  Strong organizational and time management skills, capable of handling multiple tasks efficiently. \n \n  Competencies: \n \n  Integrity \n  Excellence \n  Collaboration and Team Work \n  Communicating Orally and in Writing \n  Adaptability \n  Initiative \n  Organization and Planning \n  Education \n  Exercising Self-Control and Being Resilient \n  Interpersonal Skills \n \n \n  Education: \n \n  Currently pursuing a degree in Business, Information Systems, Computer Science, Data Science, or a related field. \n \n  Physical Requirements: \n  The physical demands described herein are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. \n  While performing the duties of this job, the employee is regularly required to talk and hear, stand and sit most of the time. The employee is periodically required to use hands to finger, handle, or feel objects, tools, or controls and to climb/ascend ladders using feet and legs to balance. The employee is occasionally required to walk; reach with hands and arms; stoop, kneel, crouch or crawl. \n  The employee may occasionally lift and move up to 10 pounds. Specific vision abilities required by this job include close vision, peripheral vision, depth perception and the ability to adjust focus. \n  The employee is subject to environmental conditions. Protection from weather conditions but not necessarily from temperature changes. \n  Additional Notes: \n  In order to comply with Export Control Laws and the NISPOM, we must secure all governmental approvals that are required to authorize our workforce to work on our defense and government programs. To ensure we comply with these regulations in a manner that does not violate our equal opportunity employment/non-discrimination compliance obligations, Field maintains the following recruitment policy: \n \n  All applicants, including applicants that may work remotely, must be eligible to secure a U.S. security clearance. \n \n  Field Aerospace is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability or other characteristics protected by law.", "cleaned_desc": "  Ability to create visually appealing and user-friendly reports and dashboards. \n  Providing frontline support for reporting tools (i.e. ReportsNow). \n \n  Skills and Experience: \n \n  Strong written and verbal communication skills, with attention to detail. \n  Familiarity with business systems. Experience with Microsoft Power BI (preferred) or Tableau. \n  Proficient in Microsoft Office suite (Word, Excel, PowerPoint). \n  Creative thinker with the ability to generate and implement innovative ideas. \n  Self-motivated, proactive, and able to work independently as well as collaboratively. ", "techs": ["reportsnow", "microsoft power bi", "tableau", "microsoft office suite (word", "excel", "powerpoint)"]}, "7ad064a04e9da853": {"terms": ["data science", "machine learning engineer"], "salary_min": 131310.0, "salary_max": 223290.0, "title": "Sr Data Scientist", "company": "ServiceNow", "desc": "Company Description \n  At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can\u2019t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you. \n With more than 7,700+ customers, we serve approximately 85% of the Fortune 500\u00ae, and we're proud to be one of FORTUNE 100 Best Companies to Work For\u00ae and World's Most Admired Companies\u2122. \n Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow. \n Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates. \n  Job Description \n  ServiceNow is changing the way people work. With a service-orientation toward the activities, tasks and processes that make up day-to-day work life, we help the modern enterprise operate faster and be more scalable than ever before. We\u2019re disruptive. We work hard but try not to take ourselves too seriously. We are highly adaptable and constantly evolving. We are passionate about our product, and we live for our customers. We have high expectations and a career at ServiceNow means challenging yourself to always be better. \n  Do you want to effect change? Do you want to be on the bridge providing insights to help shape the course of action we take? We are looking for a Data Rockstar! You will work with a data-centric group of analysts, data scientists, engineers, and capacity planners. The ideal candidate will have experience leveraging data mining technologies (SQL, Python, etc.), developing predictive models, and effective dashboards which provide a clear and transparent data driven story to multiple levels of the organization. The work will require strong partnerships across multiple departments developing operational metrics and providing insights which will help guide our Cloud and Infrastructure strategy. \n  In this role you will be member of the Cloud Capacity Analytics team responsible for capacity demand and supply planning, forecasting, cost measurement, and analytics for global infrastructure and cloud operations. You will help drive Cloud and Infrastructure strategy leveraging AI, Machine Learning and other advanced Analytics to inform our go-forward enterprise strategies. \n  What you get to do in this role: \n \n  Deliver effective Cloud Analytics and reporting across the Cloud business, leveraging Analytics and tools. Ensure metrics are focused on driving the achievement of goals and productivity for the Cloud business. \n  Collaborate daily with the capacity planning and operations team to develop predictive AI/ML-driven supply and demand models to forecast capacity demand precisely. \n  Work with stakeholders to understand detailed requirements and own your code from designing, implementing, testing, and delivering high-quality capacity planning and analytics solutions. \n  Generate insights and storylines through translation of metrics and a deep understanding of our internal processes, customers, and products \n  Implement organizational level dashboards including forecasting, planning, deployment, incident and problem management, and change management analytics across Cloud/Infrastructure & Operations \n  Identify and implements critical metrics vital to manage and monitor the performance of overall operations including: capacity, availability, budget, and inventory \n  Execute analytics projects to extract, manage, and analyze data from multiple applications, ensuring that deadlines are met. \n  Solve and present complex problems in an understandable way to stakeholders and business leaders. \n  Remain engaged and involved with industry best practices and policies. \n \n \n  Qualifications \n  To be successful in this role you have: \n \n  Bachelor's in Computer Science, Data Science, Management Information Systems, Finance, Statistics, Analytics or any other related field. An equivalent of the same in working experience is also acceptable for the position. \n  4+ years of working experience as a data scientist/analyst with 1+ of those years in designing and development data visualization and executive level dashboards. \n  An understanding of statistical regression, Bayesian forecasting models, econometric and causal data analytics, POC development, time series data correlation, and pattern detection \n  Must be able to work in fast paced environment and be able to adapt to changing requirements. \n  An understanding of cloud architecture, underlying markets and key market drivers, and global megatrends and the value of data & analytics in acting on external disruption and market trends. \n  Proficient working with large structured and unstructured data sets \n  Knowledge of SQL, Python and R \n  Knowledge of Tableau, Tableau Prep, Power BI and other BI analytics tools. \n  Knowledge of ServiceNow Platform desired \n  Knowledge of Performance Analytics desired. \n  Communication skills \n  Strong analytical skills \n \n \n  GCS23 \n  For positions in Washington (outside of the Seattle metro/Kirkland areas), we offer a base pay of $131,310 - $223,290, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location. For individuals who will be working in the Seattle metro/Kirkland areas, there is a pay enhancement for positions located in those geographical areas; please contact your recruiter for additional information. \n  Additional Information \n  ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law. \n At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office. \n If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance. \n For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government. \n Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site. \n   \n From Fortune. \u00a9 2022 Fortune Media IP Limited All rights reserved. Used under license. \n Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.", "cleaned_desc": "  Solve and present complex problems in an understandable way to stakeholders and business leaders. \n  Remain engaged and involved with industry best practices and policies. \n \n \n  Qualifications \n  To be successful in this role you have: \n \n  Bachelor's in Computer Science, Data Science, Management Information Systems, Finance, Statistics, Analytics or any other related field. An equivalent of the same in working experience is also acceptable for the position. \n  4+ years of working experience as a data scientist/analyst with 1+ of those years in designing and development data visualization and executive level dashboards.    An understanding of statistical regression, Bayesian forecasting models, econometric and causal data analytics, POC development, time series data correlation, and pattern detection \n  Must be able to work in fast paced environment and be able to adapt to changing requirements. \n  An understanding of cloud architecture, underlying markets and key market drivers, and global megatrends and the value of data & analytics in acting on external disruption and market trends. \n  Proficient working with large structured and unstructured data sets \n  Knowledge of SQL, Python and R \n  Knowledge of Tableau, Tableau Prep, Power BI and other BI analytics tools. \n  Knowledge of ServiceNow Platform desired \n  Knowledge of Performance Analytics desired. \n  Communication skills ", "techs": ["tableau", "tableau prep", "power bi", "sql", "python", "r", "servicenow platform", "performance analytics"]}, "64a8ff8cf593b253": {"terms": ["data science"], "salary_min": null, "salary_max": null, "title": "Medical Data Annotator/Quality Auditor", "company": "ClinDCast LLC", "desc": "Role: Medical Data Annotator/Quality Auditor \n Work Mode: 100% Remote \n Duration: Full-Time (40 hours/Week) \n Experience: 7+ Years of Experience \n Responsibilities: \n \u00b7 As a Data Annotator, you will be responsible for annotating and/or quality-reviewing clinical data for symptoms, diagnosis, treatment procedures, medications, adverse events, laboratory results etc. \n \u00b7 Apply your comprehensive knowledge in medical terminology, and coding procedures for data curation and database modelling. \n \u00b7 Annotation of clinical data- to make it easier for software, analytics & data science teams to improve the quality of health AI NLP services. \n \u00b7 You will work with a growing multidisciplinary team that works at the intersection of clinical knowledge and AI data labelling. \n Requirements and Qualifications: \n \u00b7 Must be detail-oriented for a data annotation project, in which you will annotate medical records. \n \u00b7 You must be with a clinical or healthcare background, who has great attention to detail, and is comfortable with highly competitive annotation tasks. \n \u00b7 Thoroughly familiar with general medical concepts and terminologies in any medical discipline. \n \u00b7 Basic working knowledge in one or more coding standards SNOMED-CT, ICD-10, CPT, LOINC, MULTUM ETC. \n \u00b7 Ability to perform text annotation tasks with consistency, quality, and speed. \n \u00b7 Candidates may have an educational background in Biotechnology, Biochemistry, Pharmacology, Computational linguistics, degree in Pharmacy etc. \n \u00b7 Medical practitioners or any medical degree holders are also encouraged to apply. \n \u00b7 Proven clinical data domain expertise preferably as a clinician, clinical coders, medical billing, and coding. \n \u00b7 Experience with Annotation process tools/software. \n \u00b7 Certification in AAPC (American Academy of Professional Coders) is a plus. \n \u00b7 Excellent command / highly proficient in spoken and written English. \n Skills Required: \n MEDICAL RECORDS, CPT, PROJECT, CLINICAL DATA, NLP, QUALITY, SNOMED, ADVERSE EVENTS, BASIC, DATA CURATION, LABELING, PROCEDURES, AI, TEAMS, DIAGNOSIS, MEDICAL TERMINOLOGY, LOINC, BIOTECHNOLOGY, SOFTWARE, IT, BIOCHEMISTRY, MEDICAL BILLING, DOMAIN, DATABASE MODELING, CODING STANDARDS, LABORATORY, COMPUTATIONAL LINGUISTICS, MEDICATIONS, ICD-10, ANALYTICS, PHARMACOLOGY, DATA SCIENCE, PHARMACY. \n Preferred Experience: \n \u00d8 NLP: 2 years \n \u00d8 ICD-10: 2 years \n \u00d8 CPT: 2 years \n \u00d8 Coding: 2 years \n \u00d8 MEDICAL BILLING: 2 years \n \u00d8 CLINICAL DATA: 2 years \n \u00d8 LOINC: 2 years \n \u00d8 MULTUM: 2 years \n \u00d8 Data Annotator: 2 years \n Job Type: Full-time \n Schedule: \n \n 8 hour shift \n \n Work Location: Remote", "cleaned_desc": " \u00b7 Annotation of clinical data- to make it easier for software, analytics & data science teams to improve the quality of health AI NLP services. \n \u00b7 You will work with a growing multidisciplinary team that works at the intersection of clinical knowledge and AI data labelling. \n Requirements and Qualifications: \n \u00b7 Must be detail-oriented for a data annotation project, in which you will annotate medical records. \n \u00b7 You must be with a clinical or healthcare background, who has great attention to detail, and is comfortable with highly competitive annotation tasks. \n \u00b7 Thoroughly familiar with general medical concepts and terminologies in any medical discipline. \n \u00b7 Basic working knowledge in one or more coding standards SNOMED-CT, ICD-10, CPT, LOINC, MULTUM ETC.   \u00b7 Ability to perform text annotation tasks with consistency, quality, and speed. \n \u00b7 Candidates may have an educational background in Biotechnology, Biochemistry, Pharmacology, Computational linguistics, degree in Pharmacy etc. \n \u00b7 Medical practitioners or any medical degree holders are also encouraged to apply. \n \u00b7 Proven clinical data domain expertise preferably as a clinician, clinical coders, medical billing, and coding. \n \u00b7 Experience with Annotation process tools/software. \n \u00b7 Certification in AAPC (American Academy of Professional Coders) is a plus. \n \u00b7 Excellent command / highly proficient in spoken and written English. ", "techs": ["annotation of clinical data", "health ai nlp services", "clinical knowledge", "ai data labeling", "data annotation project", "medical records", "clinical or healthcare background", "general medical concepts", "coding standards", "snomed-ct", "icd-10", "cpt", "loinc", "multum", "text annotation tasks", "biotechnology", "biochemistry", "pharmacology", "computational linguistics", "pharmacy", "medical practitioners", "clinical data domain expertise", "clinician", "clinical coders", "medical billing", "coding", "annotation process tools/software", "aapc certification", "spoken and written english"]}, "b1ee08ab9ca1c7ba": {"terms": ["data science", "data analyst"], "salary_min": 50.0, "salary_max": 80.0, "title": "Senior Data Analyst - International Project", "company": "Leading Edge Skills", "desc": "Leading Edge Skills (LES)  is a focused IT and Business training center, based in California, right in the heart of Silicon Valley, that prepares students in transitioning and launching a new career quickly. We feel pride in offering a unique learning experience through training programs that are aligned with the industry requirements and are affordable. \n We are looking for an expert level Senior Data Analyst who has a proven practical insight of the competence areas to cover at minimum following: \n Possess at least a Bachelor\u2019s degree such as Business Administration, or Information Technology \u2013 OR appropriate certifications to complete the tasks \uf0b7 \n Possess a minimum of 5 years of relevant experience such as engineering or performance improvement. \uf0b7 \n Possess a high level of proficiency in the use of HTML/XML coding. \uf0b7 Possess a high level of proficiency in the use of the Microsoft Office suite, including Microsoft SharePoint Designer. \uf0b7 Possess a high level of proficiency in working with business intelligence tools such as Tableau, Qlikview, and others. \n \uf0b7 Demonstrate knowledge and experience associated with successfully accomplishing the requirements in Data Administration, Management and Reporting. \n Provides performance improvement knowledge management support services for the operations, development, and oversight of the enterprise project repository and reporting system and other information technology systems used to manage and monitor enterprise efforts in performance improvement. The purpose of this support is to develop, maintain and manage automated solutions for the collection, configuration and display of performance improvement data within the repository system and to manage the relational database and other systems used to capture, track, and integrate data pertaining to Continuous Process Improvement program. The incumbant shall perform the following \n Gather and analyze information and data related to the development of performance improvement capabilities. Plan, schedule, and conduct studies to evaluate and recommend improvements in the identification of the data types and configuration to be developed in the Tableau Software (or equivalent interactive data visualization software). \n Analyze and evaluate the need for additional system interfaces with other systems. Pull data and reports out of multiple systems to include Tableau Software (or equivalent interactive data visualization software), Microsoft Office 365, Enterprise Task Management System , and others. Perform detailed analysis to validate quality of data/ metrics to resolve technical data problems. Identify causes of data error or omission, make recommendations and implement resolution when approved by the Government. Recommend the appropriate data to support analysis collected during performance improvement events, projects, meetings and problem solving teams that impact performance improvement goals. Data Administration \n Provides information to system specialists on data needed by project leads, project sponsors, performance improvement practitioners, and team members assigned to performance improvement projects and events. Develop queries using Tableau Software (or equivalent interactive data visualization software) reporting features. Extracts and compiles data/information and produce reports and briefings. Conducts functional system testing during implementation of system changes to ensure solutions meet strategy and innovation requirements. Develop internal procedures and guidelines for implementation of new methods. Writes instructions, develops and provide training, and provide guidance to Tableau Software (or equivalent interactive data visualization software) users on how to use application programs, data input, and business rules, track projects, run reports and develop measures. Manage, design, and maintain the Community SharePoint Site standards, content, styles, and guideline. Create libraries for standard context (text, graphics, audio, and video). Test and verify applications and webpage's prior to publishing on the web. Creates and test hyperlinks to connect pages of related information. Manage the incorporation of test, graphics, audio, video, scripts, applets or applications as needed for display of performance improvement information. Review and converts data into website formats (i.e. HTML (Hyper Text Markup Language), PDF (Adobe Portable Document Format), MS Word, Excel, Power Point, Access, and graphics (GIF, JPEG, BMP)). Any and all dashboards displaying data collected during projects shall be transferrable to systems. Establish procedures for controlling electronic master documents, posting of formal document changes and keeping auditable records of changes. Maintain the data structures and permission groups within the network share drive. Issue and maintain user system permission levels. Subject Matter Expertise Perform as subject matter expert to mentor Tableau Software (or equivalent interactive data visualization software) system users, super users, site administrators, and technical system specialists. Also, provides input to the Tableau Software (or equivalent interactive data visualization software) Configuration Control Board. Consult with customers to refine functional requirements, translate functional requirements into design specification, determine best approaches for implementation within the technical environment, and work with application developers to isolate and solve design problems encountered during testing and implementation stages. Identify and propose system software improvements to facilitate the efficiency and effectiveness of the Tableau Software (or equivalent interactive data visualization software) operations, and consolidates changes with activities and other associated organizations. \n Must have trained students in a classroom setting through virtual mode of delivery. \n Our experts/leaders are the most valuable and respected asset for us. We make sure they make use of their full potential to not only transform the skill set among the student body but make them feel comfortable when they come across unexpected and challenging scenarios they may face at the workplace. \n We\u2019re looking for humans with a passion, empathy, and accountability to prepare contents and train students for our training programs focused on various roles in Cyber Security Industry. \n SCOPE & GENERAL PURPOSE OF JOB : \n At Leading Edge Skills (LES), we refer to our instructors as trainers because they go above and beyond mere instruction. Our trainers are IT experts that possess engaging personalities, a strong passion for technology, and a knack for making the complicated simple. In addition to imparting training, the LES trainer role supports our organization\u2019s overall objective to continually improve the effectiveness of learning by creating IT-related videos and learning experiences that our learners can trust. Responsibilities include researching current trends and certifications in the IT industry, developing curriculum, creating instructional videos, product demonstrations, virtual labs, quizzes, and supplemental files designed to help individual learners, small businesses, enterprise teams, and government agencies achieve their IT-related goals. The LES trainer is also responsible for reviewing content created by other trainers to ensure that we always deliver our learners the highest quality content. \n MEASURES OF SUCCESS: \n \n Create accurate, concise, and engaging content on a regular basis \n Increase traffic to LES website ( www.leadingedgeskills.com ) by delivering timely and relevant content \n Increase usage of products and features such as virtual labs and practice exams \n Receive positive learner reviews \n Meet or exceed the Quality Control Standards set forth by the Learning Content team \n \n PRIMARY RESPONSIBILITIES: \n \n Stay up to date on current technologies, certifications, exams, and other IT industry news \n Develop course curriculum leveraging your real-world IT experience and exam-related objectives \n Design, develop, edit, and submit knowledge & skill-based instructional videos \n Create effective learning experiences utilizing quizzes, demonstrations, labs, and other tools at your disposal \n Collaborate with other instructors, illustrators, and other teams (e.g., Learning Content and Marketing) \n Provide timely & constructive peer review of content created by your colleagues \n Act as a brand ambassador for LES \n Participating in recruitment campaigns. \n \n SKILLS/ COMPETENCIES/EDUCATION: \n \n Education required: \n Relevant degree/certifications or demonstrated in-depth knowledge \n 5+ years\u2019 experience in Business/IT-related field/area of expertise. \n Sound Professional Training experience through virtual mode \n Skills/experience required: \n Independent thinker and self-starter \n Attention to detail \n Collaborative and comfortable working on a team \n Demonstrated knowledge in computer networking, operating systems (both Windows and Unix based operating systems), and virtualization (cloud and on-premise); \n Incident Response core skills including security event review, log analysis, host analysis, email analysis, and network analysis; \n Demonstrating knowledge of Windows or Linux forensic analysis (acquisition/evidence handling, rapid triage, and in-depth analysis to answer common analytic questions); \n \n Testing penetration and offensive security techniques; \n \n Having knowledge of malware analysis, Threat Hunting, Detection Engineering and reverse engineering \n Good understanding and experience of key topics in Cybersecurity as Vulnerability Management, Risk Management, Incident Response, Penetration Testing, Quality assurance \n Understanding of digital forensics, including one or more of the following a strong plus: \n File system, Registry, etc. artifacts commonly associated with malware or malicious activities. \n Experience with AWS Cloud services \n Be able to present a short lecture/demo (max 6 min) on the topic related to the course content of Cyber Security Analysis Program listed on our website at leadingedgeskills.com. \n Attitude required: \n Honesty, humility, and integrity \n Inclusive and respectful \n Strong work ethic \n Passion for learning \n Comfortable with autonomy \n Eager to add new skills and grow professionally \n Skills that are preferred, but not required: \n Curiosity \n Assertiveness \n A love of IT and/or technology \n Proficiency with Gmail, Google Docs, Slack, and internal LES tools. \n \n Job Types: Part-time, Contract \n Pay: $50.00 - $80.00 per hour \n Schedule: \n \n Day shift \n Evening shift \n On call \n Weekends as needed \n \n Work Location: Remote", "cleaned_desc": "Leading Edge Skills (LES)  is a focused IT and Business training center, based in California, right in the heart of Silicon Valley, that prepares students in transitioning and launching a new career quickly. We feel pride in offering a unique learning experience through training programs that are aligned with the industry requirements and are affordable. \n We are looking for an expert level Senior Data Analyst who has a proven practical insight of the competence areas to cover at minimum following: \n Possess at least a Bachelor\u2019s degree such as Business Administration, or Information Technology \u2013 OR appropriate certifications to complete the tasks \uf0b7 \n Possess a minimum of 5 years of relevant experience such as engineering or performance improvement. \uf0b7 \n Possess a high level of proficiency in the use of HTML/XML coding. \uf0b7 Possess a high level of proficiency in the use of the Microsoft Office suite, including Microsoft SharePoint Designer. \uf0b7 Possess a high level of proficiency in working with business intelligence tools such as Tableau, Qlikview, and others. \n \uf0b7 Demonstrate knowledge and experience associated with successfully accomplishing the requirements in Data Administration, Management and Reporting. \n Provides performance improvement knowledge management support services for the operations, development, and oversight of the enterprise project repository and reporting system and other information technology systems used to manage and monitor enterprise efforts in performance improvement. The purpose of this support is to develop, maintain and manage automated solutions for the collection, configuration and display of performance improvement data within the repository system and to manage the relational database and other systems used to capture, track, and integrate data pertaining to Continuous Process Improvement program. The incumbant shall perform the following \n Gather and analyze information and data related to the development of performance improvement capabilities. Plan, schedule, and conduct studies to evaluate and recommend improvements in the identification of the data types and configuration to be developed in the Tableau Software (or equivalent interactive data visualization software). \n Analyze and evaluate the need for additional system interfaces with other systems. Pull data and reports out of multiple systems to include Tableau Software (or equivalent interactive data visualization software), Microsoft Office 365, Enterprise Task Management System , and others. Perform detailed analysis to validate quality of data/ metrics to resolve technical data problems. Identify causes of data error or omission, make recommendations and implement resolution when approved by the Government. Recommend the appropriate data to support analysis collected during performance improvement events, projects, meetings and problem solving teams that impact performance improvement goals. Data Administration \n Provides information to system specialists on data needed by project leads, project sponsors, performance improvement practitioners, and team members assigned to performance improvement projects and events. Develop queries using Tableau Software (or equivalent interactive data visualization software) reporting features. Extracts and compiles data/information and produce reports and briefings. Conducts functional system testing during implementation of system changes to ensure solutions meet strategy and innovation requirements. Develop internal procedures and guidelines for implementation of new methods. Writes instructions, develops and provide training, and provide guidance to Tableau Software (or equivalent interactive data visualization software) users on how to use application programs, data input, and business rules, track projects, run reports and develop measures. Manage, design, and maintain the Community SharePoint Site standards, content, styles, and guideline. Create libraries for standard context (text, graphics, audio, and video). Test and verify applications and webpage's prior to publishing on the web. Creates and test hyperlinks to connect pages of related information. Manage the incorporation of test, graphics, audio, video, scripts, applets or applications as needed for display of performance improvement information. Review and converts data into website formats (i.e. HTML (Hyper Text Markup Language), PDF (Adobe Portable Document Format), MS Word, Excel, Power Point, Access, and graphics (GIF, JPEG, BMP)). Any and all dashboards displaying data collected during projects shall be transferrable to systems. Establish procedures for controlling electronic master documents, posting of formal document changes and keeping auditable records of changes. Maintain the data structures and permission groups within the network share drive. Issue and maintain user system permission levels. Subject Matter Expertise Perform as subject matter expert to mentor Tableau Software (or equivalent interactive data visualization software) system users, super users, site administrators, and technical system specialists. Also, provides input to the Tableau Software (or equivalent interactive data visualization software) Configuration Control Board. Consult with customers to refine functional requirements, translate functional requirements into design specification, determine best approaches for implementation within the technical environment, and work with application developers to isolate and solve design problems encountered during testing and implementation stages. Identify and propose system software improvements to facilitate the efficiency and effectiveness of the Tableau Software (or equivalent interactive data visualization software) operations, and consolidates changes with activities and other associated organizations. \n Must have trained students in a classroom setting through virtual mode of delivery. \n Our experts/leaders are the most valuable and respected asset for us. We make sure they make use of their full potential to not only transform the skill set among the student body but make them feel comfortable when they come across unexpected and challenging scenarios they may face at the workplace. \n We\u2019re looking for humans with a passion, empathy, and accountability to prepare contents and train students for our training programs focused on various roles in Cyber Security Industry. \n SCOPE & GENERAL PURPOSE OF JOB : \n At Leading Edge Skills (LES), we refer to our instructors as trainers because they go above and beyond mere instruction. Our trainers are IT experts that possess engaging personalities, a strong passion for technology, and a knack for making the complicated simple. In addition to imparting training, the LES trainer role supports our organization\u2019s overall objective to continually improve the effectiveness of learning by creating IT-related videos and learning experiences that our learners can trust. Responsibilities include researching current trends and certifications in the IT industry, developing curriculum, creating instructional videos, product demonstrations, virtual labs, quizzes, and supplemental files designed to help individual learners, small businesses, enterprise teams, and government agencies achieve their IT-related goals. The LES trainer is also responsible for reviewing content created by other trainers to ensure that we always deliver our learners the highest quality content. ", "techs": ["leading edge skills (les)", "business administration", "information technology", "html/xml coding", "microsoft office suite", "microsoft sharepoint designer", "tableau", "qlikview", "data administration", "management and reporting", "tableau software", "microsoft office 365", "enterprise task management system", "data visualization software", "community sharepoint site", "ms word", "excel", "power point", "access", "gif", "jpeg", "bmp", "virtual mode of delivery", "cyber security industry", "it experts", "curriculum development", "instructional videos", "product demonstrations", "virtual labs", "quizzes", "content review"]}, "183c65ab4c2eb8a9": {"terms": ["data science", "data engineer"], "salary_min": 108090.57, "salary_max": 130173.59, "title": "Data Engineer", "company": "Gridiron IT", "desc": "Gridiron IT is seeking a Data Engineer to support a federal program on a 100% remote basis. \n Role Description:The Data Engineer provides the ETL support to the data science and software engineering team members. Build, modify, support infrastructure for optimal extraction, transformation, and loading of data from variety of structure, unstructured data sources and multi-terabyte distributed file system. Candidate will formulate and rapidly prototype various approaches as well as effectively communicate the pros and cons of each. Provide data-driven approaches to tackle various business problems. The candidate will have the ability to contribute to a high-performing, motivated workgroup by applying interpersonal and collaboration skills to achieve project goals Architect for ML data pipeline with data acquisition and preprocessing functionalities that gather data from heterogenous data pool from the distributed file system, unstructured text extracted from multi-million images of medical records with varied OCR quality, their metadata from relational databases and custom annotations. \n Responsibilities: ? Provide current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments ? Manage/maintain structured, semi-structured, and unstructured data, structuring and wrangling data as appropriate for statistical analysis ? Implement data warehouse concepts and relational databases, big data management techniques and tools (e.g. Hadoop, MAPReduce) ? Communicate with technical and non-technical users and managers, and server administration, to include hardware and software support to existing servers. ? Provide software engineering support to operate, maintain and enhance systems that are integrated with and/or relied upon by the data engineering lifecycle ? Integrate, analyze, and visualize data and information in near real-time (within 24 hours) from multiple disparate data sources. ? Optimize data storage and access ? Proficiency with Python and Java, Oracle enterprise manager, SQL, AWS \n Qualifications: ? Masters degree in related field + 5 years experience; or PhD +1 year experience; or Bachelor\u2019s degree in related field + 7 years experience ? Minimum of 5 years experience conducting ETL tasks, performance engineering, run-time optimization, large data volume transfers ? Minimum 3 years experience with Regular Expressions, SQL (PostgreSQL), No-SQL (MongoDB) ? Minimum 1 year experience with Version control systems (Git) ? Preference to developer with experience working with healthcare data and Health IT Skills/Tools Utilized (at least 1-2 years exp in some of the following): ? Apache Hadoop (Cloudera) ? AWS Data Platforms (Redshift, S3, EMR/Hive) ? SQL ? Java ? Kafka ? Scala ? Kotlin ? Neo4j ? NiFi ? Flink ? Sqoop ? PostgreSQL ? EMR ? Apache Spark ? Python ? PHP ? Oracle ? Splunk ? BDD ? testing framework: Cucumber ? Knowledge of and experience using various NLP approaches, particularly: \n \n Pattern recognition/feature extraction \n Supervised, Unsupervised, and Semi-Supervised learning techniques \n Understanding of various language models (N-Gram, Skipgram, NLM, etc.) \n Chunking/Tokenization \n Semantic parsing \n \n Skills highly desired: ? Healthcare IT experience ? Statistical model building (particularly classification) \n Job Type: Full-time \n Pay: $108,090.57 - $130,173.59 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Education: \n \n Bachelor's (Required) \n \n Experience: \n \n ETL: 5 years (Required) \n SQL: 5 years (Required) \n Git: 5 years (Required) \n \n Security clearance: \n \n Secret (Required) \n \n Work Location: Remote", "cleaned_desc": "Gridiron IT is seeking a Data Engineer to support a federal program on a 100% remote basis. \n Role Description:The Data Engineer provides the ETL support to the data science and software engineering team members. Build, modify, support infrastructure for optimal extraction, transformation, and loading of data from variety of structure, unstructured data sources and multi-terabyte distributed file system. Candidate will formulate and rapidly prototype various approaches as well as effectively communicate the pros and cons of each. Provide data-driven approaches to tackle various business problems. The candidate will have the ability to contribute to a high-performing, motivated workgroup by applying interpersonal and collaboration skills to achieve project goals Architect for ML data pipeline with data acquisition and preprocessing functionalities that gather data from heterogenous data pool from the distributed file system, unstructured text extracted from multi-million images of medical records with varied OCR quality, their metadata from relational databases and custom annotations. \n Responsibilities: ? Provide current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments ? Manage/maintain structured, semi-structured, and unstructured data, structuring and wrangling data as appropriate for statistical analysis ? Implement data warehouse concepts and relational databases, big data management techniques and tools (e.g. Hadoop, MAPReduce) ? Communicate with technical and non-technical users and managers, and server administration, to include hardware and software support to existing servers. ? Provide software engineering support to operate, maintain and enhance systems that are integrated with and/or relied upon by the data engineering lifecycle ? Integrate, analyze, and visualize data and information in near real-time (within 24 hours) from multiple disparate data sources. ? Optimize data storage and access ? Proficiency with Python and Java, Oracle enterprise manager, SQL, AWS \n Qualifications: ? Masters degree in related field + 5 years experience; or PhD +1 year experience; or Bachelor\u2019s degree in related field + 7 years experience ? Minimum of 5 years experience conducting ETL tasks, performance engineering, run-time optimization, large data volume transfers ? Minimum 3 years experience with Regular Expressions, SQL (PostgreSQL), No-SQL (MongoDB) ? Minimum 1 year experience with Version control systems (Git) ? Preference to developer with experience working with healthcare data and Health IT Skills/Tools Utilized (at least 1-2 years exp in some of the following): ? Apache Hadoop (Cloudera) ? AWS Data Platforms (Redshift, S3, EMR/Hive) ? SQL ? Java ? Kafka ? Scala ? Kotlin ? Neo4j ? NiFi ? Flink ? Sqoop ? PostgreSQL ? EMR ? Apache Spark ? Python ? PHP ? Oracle ? Splunk ? BDD ? testing framework: Cucumber ? Knowledge of and experience using various NLP approaches, particularly: \n \n Pattern recognition/feature extraction ", "techs": ["cloudera", "redshift", "s3", "emr/hive", "sql", "java", "kafka", "scala", "kotlin", "neo4j", "nifi", "flink", "sqoop", "postgresql", "emr", "apache spark", "python", "php", "oracle", "splunk", "cucumber"]}, "6b9155510b969e11": {"terms": ["data science"], "salary_min": 109820.63, "salary_max": 139057.39, "title": "Digital Analytics Trainee - France and Germany", "company": "Neo Consulting", "desc": "Hello future NEO! Are you ready to #ELEVATE and #INNOVATE in Digital? We are in search of Talent in France, Germany, and the USA. \n You will provide support in defining, implementing, and configuring digital analytics tools to facilitate the collection, analysis, and presentation of digital data, with a focus on improving the performance and efficiency of clients' websites and mobile applications (native and hybrid). \n Your daily challenges will include: \n \n Provide support in Strategic Analytics, Technical Analytics, Cloud Engineering, and Data Science projects. \n  Analyze, interpret, and extract insights from quantitative and/or qualitative data. \n  Develop measurement models and propose actionable recommendations aligned with the business objectives of each project. \n  Apply analytical thinking and identify opportunities with significant cost-effectiveness for clients. \n  Create omni-channel data models and dataLayer structures. \n  Develop tagging guidelines. \n  Configure reports in various visualization tools, unifying data from different sources. \n  Prepare reports and present results, improvement proposals, and impact assessments. \n  Deliver presentations to executive and cross-functional teams, explaining channel performance and next steps. \n \n The recipe for success is in your hands: \n \n Graduates in Computer Engineering, Industrial Engineering, Marketing, Business Administration, Systems Engineering, or related fields. \n  Residents in France, Germany, and the USA. \n \n Hard Skills: \n \n Knowledge of Google Analytics, Adobe Analytics, or digital analytics tools. \n  Knowledge of SQL. \n  Desirable proficiency in Tag Manager or Tagging Tools. \n \n Soft Skills: \n \n Organizational skills. \n  Curiosity for self-learning and research. \n  Commitment and results orientation. \n  Strong communication skills. \n  Proficiency in technological tools. \n  Ability to manage priorities and requirements in a constantly changing environment. \n \n What we offer: \n \n Partnership with Google Cloud, Google Marketing Platform, Salesforce, AWS, and Hubspot. Certify with us! \n  Remote work with flexible hours. \n  Personalized mentoring with a bilingual consultant. \n  Access to the NEO library of success stories and specialized magazine subscriptions. \n  International network of contacts. \n  Participation in high-impact regional projects for the most important companies in Mexico, Chile, Colombia, and Peru. \n  Being part of a BCorp company. \n  Personal development call center - with professionals specializing in mental health and coaching. \n  Diverse team with a horizontal structure and a good organizational climate. \n \n \n At NEO, we promote diversity through equal opportunities and inclusion. \n \n \n About Neo Consulting: \n \n Neo Consulting is an international Marketing and Digital Strategy consultancy with over 20 years of experience in the market. Our purpose is to attract and develop individuals who see in Digital the opportunity to build a better world; that's why since 2018, we are certified as a BCorp Company. Throughout our history, we have developed projects in more than 15 industries, employing cutting-edge methodologies and technologies. \n We invite you to be part of the #NeoTalent! \n Learn more about us here: Neo Consulting (https://neoconsulting.ai/pe/)", "cleaned_desc": "Hello future NEO! Are you ready to #ELEVATE and #INNOVATE in Digital? We are in search of Talent in France, Germany, and the USA. \n You will provide support in defining, implementing, and configuring digital analytics tools to facilitate the collection, analysis, and presentation of digital data, with a focus on improving the performance and efficiency of clients' websites and mobile applications (native and hybrid). \n Your daily challenges will include: \n \n Provide support in Strategic Analytics, Technical Analytics, Cloud Engineering, and Data Science projects. \n  Analyze, interpret, and extract insights from quantitative and/or qualitative data. \n  Develop measurement models and propose actionable recommendations aligned with the business objectives of each project. \n  Apply analytical thinking and identify opportunities with significant cost-effectiveness for clients. \n  Create omni-channel data models and dataLayer structures. \n  Develop tagging guidelines. ", "techs": ["digital analytics tools", "websites", "mobile applications", "native", "hybrid", "strategic analytics", "technical analytics", "cloud engineering", "data science projects", "quantitative data", "qualitative data", "measurement models", "actionable recommendations", "business objectives", "analytical thinking", "cost-effectiveness", "omni-channel data models", "datalayer structures", "tagging guidelines"]}, "da6455e085ea3790": {"terms": ["data science"], "salary_min": null, "salary_max": null, "title": "Director of Analytics", "company": "TailorCare", "desc": "About TailorCare \n  TailorCare is transforming the experience of specialty care. Our comprehensive care program takes a deeply personal, evidence-based approach to improving patient outcomes for joint, back, and muscle conditions. By combining a careful assessment of patients' symptoms, health histories, preferences, and goals with predictive data and latest evidence-based guidelines, we help patients choose\u2014and navigate\u2014the most effective treatment pathway for them, every step of the way. \n  TailorCare values the experiences and perspectives of individuals from all backgrounds. We are a highly collaborative, curious, and determined team passionate about scaling a high-growth start-up to improve the lives of those in pain. TailorCare is a remote-first company with hybrid offices in New York City and soon-to-be Nashville. \n \n  About the Role \n  We are looking for an exceptional analytics leader to build a high-performance analytics team from the ground up. You will lead a team of analysts in developing data products and key insights to support and scale a data-driven value-based care company. This role will be responsible for partnering with and supporting leaders across the organization, offering strategic input on KPIs and analyses that help them drive strong performance in their respective domains. You will need to balance short-term needs to deliver insights with long-term thinking to develop a function capable of supporting a scaled company. There is ample opportunity for growth in this role. \n  Primary Responsibilities \n \n Identify and meet stakeholder analytic needs in the near term through the development of insights and data products including enriched data sets, dashboards and ad-hoc analyses \n Develop and own the analytics roadmap based on the prioritized needs of the organization \n Partners with Health Economics, Data Engineering and Data Science teams to develop data marts in Databricks that support the development of robust dashboards and reports to track organizational performance \n \"Hands-on\" operating style and approach with an eye toward the future, need a willingness to invest in people, technology, and organization development that will support a large company \n Detail oriented approach to data quality control and operating in accordance with Tailorcare's Security and Compliance program \n Proactively identifies analytic use cases throughout the organization and partners with cross-functional executives on value realization, i.e., problem-solving, opportunity-capture, data inputs, and workflows, etc \n \n Qualifications \n \n Bachelor's Degree in relevant field (Preferably: Data Science, Healthcare Informatics, Statistics, Economics, Mathematics, Computer Science, etc.) or equivalent related work experience \n Minimum seven years of recent experience in the design, development and delivery of data driven solutions and analytics \n Minimum three years of experience leading and managing a healthcare analytics team \n Deep healthcare analytics experience, including experience with identification and stratification for clinical programs, engagement metrics, and health insurance payment methodologies, preferably with exposure to value-based healthcare \n Proven and established professional expertise in applied statistical and actuarial analysis and concepts \n Meaningful experience both using and leading teams that use modern analytics tools and languages including SQL and Python, Tableau \n In-depth understanding and experience with nuances of healthcare claims data \n US work authorization \n \n Desired Technical Qualifications \n \n Experience working in Databricks / data lake environment \n Experience using DBT to manage analytic transformations \n Experience building/scaling reporting suites leveraging BI platforms (e.g. Tableau) \n \n Skills \n \n Healthcare - Exceptional professional background in healthcare provider operations, independent medical groups, health plan operations, and/ or professional services \n Strategic Thinker - Ability to understand and communicate on a broad array of technical and business issues \n Good Communication \u2013 Ability to succinctly communicate key insights with data \n Collaborative - Works cross-functionally with stakeholders from engineering, business development, clinical and marketing to develop innovative and effective solutions \n Organized \u2013 Expert in serving as a technical lead in support of large, complex projects \n Leadership - Ability to develop and communicate technical vision for projects and initiatives that can be understood by customers and management \n Intellectual Curiosity - Demonstrated initiative staying current on industry practice outside of study and training, with a high and broad level of industry knowledge \n \n \n  What's In It For You \n \n Meaningful work each day, we care deeply about our mission, our patients, and each-other. \n Work from anywhere in the US that best fits your lifestyle, or, for those that enjoy an in-person environment, join teammates in our hybrid hubs in NYC or Nashville. \n Rich PTO and holiday plans to ensure you have time away to rest and recharge \n We offer paid parental leave, support a healthy work-life integration, and offer work flexibility \u2013 we love to talk about our pets and families. \n Medical, dental, vision, life, disability, wellness resources, and an employer HSA contribution all from Day 1. \n We are committed to fair and equitable pay for all employees, and we help you achieve your future goals with an employer match 401k. \n An inclusive workplace where you can lean on your teammates, offer candid feedback, and bring your true self to work each day. \n \n TailorCare seeks to recruit and retain staff from diverse backgrounds and encourages qualified candidates to apply. TailorCare is an equal opportunity employer and does not discriminate on the basis of age, sex, gender identity/expression, sexual orientation, color, race, creed, national origin, ancestry, religion, marital status, political belief, physical or mental disability, pregnancy, military, or veteran status.", "cleaned_desc": " Partners with Health Economics, Data Engineering and Data Science teams to develop data marts in Databricks that support the development of robust dashboards and reports to track organizational performance \n \"Hands-on\" operating style and approach with an eye toward the future, need a willingness to invest in people, technology, and organization development that will support a large company \n Detail oriented approach to data quality control and operating in accordance with Tailorcare's Security and Compliance program \n Proactively identifies analytic use cases throughout the organization and partners with cross-functional executives on value realization, i.e., problem-solving, opportunity-capture, data inputs, and workflows, etc \n \n Qualifications \n \n Bachelor's Degree in relevant field (Preferably: Data Science, Healthcare Informatics, Statistics, Economics, Mathematics, Computer Science, etc.) or equivalent related work experience \n Minimum seven years of recent experience in the design, development and delivery of data driven solutions and analytics \n Minimum three years of experience leading and managing a healthcare analytics team   Deep healthcare analytics experience, including experience with identification and stratification for clinical programs, engagement metrics, and health insurance payment methodologies, preferably with exposure to value-based healthcare \n Proven and established professional expertise in applied statistical and actuarial analysis and concepts \n Meaningful experience both using and leading teams that use modern analytics tools and languages including SQL and Python, Tableau \n In-depth understanding and experience with nuances of healthcare claims data \n US work authorization \n \n Desired Technical Qualifications \n \n Experience working in Databricks / data lake environment \n Experience using DBT to manage analytic transformations   Experience building/scaling reporting suites leveraging BI platforms (e.g. Tableau) \n \n Skills \n \n Healthcare - Exceptional professional background in healthcare provider operations, independent medical groups, health plan operations, and/ or professional services \n Strategic Thinker - Ability to understand and communicate on a broad array of technical and business issues \n Good Communication \u2013 Ability to succinctly communicate key insights with data \n Collaborative - Works cross-functionally with stakeholders from engineering, business development, clinical and marketing to develop innovative and effective solutions \n Organized \u2013 Expert in serving as a technical lead in support of large, complex projects \n Leadership - Ability to develop and communicate technical vision for projects and initiatives that can be understood by customers and management ", "techs": ["databricks", "tableau", "sql", "python", "dbt"]}, "4d86803fd26f5cee": {"terms": ["data science"], "salary_min": null, "salary_max": null, "title": "Medical Data Annotator/Quality Auditor", "company": "ClinDCast LLC", "desc": "Role: Medical Data Annotator/Quality Auditor \n \n \n Work Mode: 100% Remote \n \n \n Duration: Full-Time (40 hours/Week) \n \n \n Experience: 7+ Years of Experience \n   \n \n \n \n Responsibilities: \n \n \n \n As a Data Annotator, you will be responsible for annotating and/or quality-reviewing clinical data for symptoms, diagnosis, treatment procedures, medications, adverse events, laboratory results etc. \n Apply your comprehensive knowledge in medical terminology, and coding procedures for data curation and database modelling. \n Annotation of clinical data- to make it easier for software, analytics & data science teams to improve the quality of health AI NLP services. \n You will work with a growing multidisciplinary team that works at the intersection of clinical knowledge and AI data labelling. \n \n \n \n Requirements and Qualifications: \n \n \n \n Must be detail-oriented for a data annotation project, in which you will annotate medical records. \n You must be with a clinical or healthcare background, who has great attention to detail, and is comfortable with highly competitive annotation tasks. \n Thoroughly familiar with general medical concepts and terminologies in any medical discipline. \n Basic working knowledge in one or more coding standards SNOMED-CT, ICD-10, CPT, LOINC, MULTUM ETC. \n Ability to perform text annotation tasks with consistency, quality, and speed. \n Candidates may have an educational background in Biotechnology, Biochemistry, Pharmacology, Computational linguistics, degree in Pharmacy etc. \n Medical practitioners or any medical degree holders are also encouraged to apply. \n Proven clinical data domain expertise preferably as a clinician, clinical coders, medical billing, and coding. \n Experience with Annotation process tools/software. \n Certification in AAPC (American Academy of Professional Coders) is a plus. \n Excellent command / highly proficient in spoken and written English. \n \n \n \n Skills Required: \n \n  MEDICAL RECORDS, CPT, PROJECT, CLINICAL DATA, NLP, QUALITY, SNOMED, ADVERSE EVENTS, BASIC, DATA CURATION, LABELING, PROCEDURES, AI, TEAMS, DIAGNOSIS, MEDICAL TERMINOLOGY, LOINC, BIOTECHNOLOGY, SOFTWARE, IT, BIOCHEMISTRY, MEDICAL BILLING, DOMAIN, DATABASE MODELING, CODING STANDARDS, LABORATORY, COMPUTATIONAL LINGUISTICS, MEDICATIONS, ICD-10, ANALYTICS, PHARMACOLOGY, DATA SCIENCE, PHARMACY.\n  \n \n Preferred Experience: \n \n  \u00d8 NLP: 2 years\n   \u00d8 ICD-10: 2 years\n   \u00d8 CPT: 2 years\n   \u00d8 Coding: 2 years\n   \u00d8 MEDICAL BILLING: 2 years\n   \u00d8 CLINICAL DATA: 2 years\n   \u00d8 LOINC: 2 years\n   \u00d8 MULTUM: 2 years\n   \u00d8 Data Annotator: 2 years\n  \n This is a remote position.", "cleaned_desc": " \n \n Responsibilities: \n \n \n \n As a Data Annotator, you will be responsible for annotating and/or quality-reviewing clinical data for symptoms, diagnosis, treatment procedures, medications, adverse events, laboratory results etc. \n Apply your comprehensive knowledge in medical terminology, and coding procedures for data curation and database modelling. \n Annotation of clinical data- to make it easier for software, analytics & data science teams to improve the quality of health AI NLP services. \n You will work with a growing multidisciplinary team that works at the intersection of clinical knowledge and AI data labelling. \n \n ", "techs": ["clinical data annotation", "quality-reviewing", "medical terminology", "coding procedures", "data curation", "database modelling", "ai nlp services", "ai data labelling."]}, "7808b987a7beb309": {"terms": ["data science", "machine learning engineer"], "salary_min": 99890.266, "salary_max": 126483.33, "title": "Software Engineer AI Applications", "company": "HP", "desc": "Locations include Spring, Texas (preferred location), and US remote. \n \n  The Team \n  We are a growing centralized team helping HP take advantage of new AI/ML technology, especially around Generative AI and large language models. We engage with business units to advise and prototype solutions, and we develop and run software applications for internal use. \n \n  The Role \n  As a Software Engineer you will work to rapidly develop AI-powered software applications, especially internal business applications powered by large language models. You will work closely with data scientists, machine learning engineers, and other software engineers, using the latest tools and technologies. \n \n  Skills and Profile \n \n Prior software engineering experience with business applications in a cloud environment. \n Full-stack ability and interest is a plus. We are hiring a dedicated Lead UI Software Engineer, but it is a plus if you can assist with front-end contributions when needed. \n In this team we value a start-up mindset and a sense of urgency to deliver to our internal customers. The ideal candidate will have experience from a fast-moving SaaS start-up in addition to experience from a large complex organization. \n Technologies you may use include Azure services, python, langchain, micro services, docker, CI/CD, React, Elastic Search, SQL and NoSQL. \n Plus for: Azure DevOps and platform, user & account administration, document search technologies, vector databases, and LLM-prompt engineering. \n The recent AI progress is disruptive, and our team is in the midst of it! As a consequence, day-to-day priorities, tasks, and team structure may change rapidly. We are looking for somebody who thrives in such an environment. The role is not a fit if you value \"business as usual\". \n Experience working in a distributed team with diverse backgrounds. Ability to engage in discussions in a respectful manner. \n Mastery in English is required. \n \n \n  Education and Length of Experience  Bachelor's degree in Computer Science or similar, or demonstrated competence, plus typically a minimum of 2-4 years of relevant experience or 1-3 years with a Master's degree. \n \n  About HP \n \n \n \n \n \n  You\u2019re out to reimagine and reinvent what\u2019s possible\u2014in your career as well as the world around you.\n  \n \n   So are we. We love taking on tough challenges, disrupting the status quo, and creating what\u2019s next. We\u2019re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.\n  \n \n \n \n   HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.\n  \n \n \n \n   Our history: HP\u2019s commitment to diversity, equity and inclusion \u2013 it's just who we are.\n  \n \n   From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you\u2019re more innovative and that helps grow our bottom line. Come to HP and thrive!", "cleaned_desc": "", "techs": ""}, "4e7da10cf10ee217": {"terms": ["data science"], "salary_min": 169374.0, "salary_max": 272290.0, "title": "Staff Data Scientist - Advertising", "company": "Demandbase", "desc": "Introduction to Demandbase: \n  Demandbase is the Smarter GTM\u2122 company for B2B brands. We help marketing and sales teams overcome the disruptive data and technology fragmentation that inhibits insight and forces them to spam their prospects. We do this by injecting Account Intelligence into every step of the buyer journey, wherever our clients interact with customers, and by helping them orchestrate every action across systems and channels - through advertising, account-based experience, and sales motions. The result? You spot opportunities earlier, engage with them more intelligently, and close deals faster. \n  As a company, we're as committed to growing careers as we are to building world-class technology. We invest heavily in people, our culture, and the community around us. We have offices in the San Francisco Bay Area, Seattle, and teams in the UK and India, and allow employees to work remotely.  We have also been   continuously recognized as one of the best places to work in the San Francisco Bay Area. \n  We're committed to attracting, developing, retaining, and promoting a diverse workforce. By ensuring that every Demandbase employee is able to bring a diversity of talents to work, we're increasingly capable of living out our mission to transform how B2B goes to market. We encourage people from historically underrepresented backgrounds and all walks of life to apply.  Come grow with us at Demandbase! \n  About the Role: \n  The compensation range for this role is: $169,374 - $272,290 \n  Are you a seasoned Data Scientist with a strong focus on AdTech, looking to take your career to the next level? We are seeking a Staff Data Scientist with extensive experience in AdTech to join our team. In this role, you will play a key role in driving innovation and optimizing advertising strategies, leveraging your deep knowledge of data science and AdTech to deliver impactful results for our customers. \n  Key Responsibilities: \n \n \n Product and Engineering:  Collaborate with our product and engineering teams to pinpoint the precise objectives of advertising campaigns and architect machine learning models that optimize these campaigns with precision. \n \n \n Account Ranking in B2B Buying Journey:  Spearhead the development of algorithms to rank accounts in the B2B buying journey, shaping the direction of targeted advertising campaigns. \n \n \n Intent Taxonomy:  Create and refine a browsable taxonomy of intent signals, enabling advertisers to gain a deeper understanding of user intent. \n \n \n What You'll Be Doing: \n \n Advanced Algorithm Development:  Drive the development of advanced machine learning algorithms that optimize advertising strategies and deliver immediate business impact on KPIs. \n Model Deployment:  End-to-end, fullstack ownership of ML models. Take charge of building, testing, and deploying custom ML/AI models and algorithms on extensive datasets, establishing rigorous processes for monitoring and analyzing their performance in production environments. \n Effective Communication:  Excel in effectively communicating complex data science methods, statistical findings, and algorithmic insights to technical and non-technical stakeholders. \n Championing Innovation:  Stay at the forefront of AdTech and data science, actively leading innovation initiatives within the organization. \n \n What We're Looking For: \n \n Experience:  A minimum of 5 years of robust data science experience in AdTech. \n Educational Background:  A degree in Statistics, Computer Science, Machine Learning, Mathematics, Computational Psychology, Operational Research, Physics, or a related field. \n Project Leadership:  A strong track record of leading greenfield projects from conceptualization to production release. \n Analytical Skills:  Proficiency in using analytical and database tools such as Jupyter notebooks, Hive, SQL, and No-SQL databases. \n Coding Expertise:  Demonstrated ability to write clean and high-performance code in Python. \n AI/ML Proficiency:  Proficiency in one or more of the following AI/ML technologies is advantageous: TensorFlow, scikit-learn, Spark MLlib, Bigquery Machine Learning, large-scale datasets, Natural Language Processing, and Graph algorithms. \n \n Nice to Have Skills: \n \n Team Leadership:  Previous experience in leading teams and managing complex projects. \n Cloud Competency:  Familiarity with Google Cloud or AWS cloud platforms. \n \n Desired Qualities: \n \n Ownership Mentality:  Willingness to take full ownership of end-to-end machine learning products you develop or contribute to. \n Self-Driven:  A highly self-motivated individual with the ability to operate with significant autonomy. \n Problem-Solving Acumen:  Proficiency in breaking down complex problems and delivering innovative solutions. \n Goal-Focused:  An ability to maintain a sharp focus on project goals without getting lost in the minutiae. \n Data Enthusiast:  A passion for data, metrics, and using them to drive the success of projects. \n \n As a Staff Data Scientist specializing in AdTech, you have the opportunity to make a significant impact in the ever-evolving landscape of data-driven advertising. Join our dynamic team today and take on a role that offers both challenges and rewards in the world of Advertising. Apply now to seize this exciting opportunity to contribute to the future of data-driven advertising. \n  Benefits: \n  Our benefits include options for up to 100% paid Medical and Vision premiums for employees, flexible PTO policy, no internal meeting Fridays, Modern Health mental wellness platform, and 11 paid holidays and 2 additional weeks where all Demandbase employees take off (the week of July 4th and the week of Thanksgiving). Plus 401(k), short-term/long-term disability, life insurance, and all those good things. \n  Our Commitment to Diversity, Equity, and Inclusion at Demandbase \n  At Demandbase, we believe in creating a workplace culture that values and celebrates diversity in all its forms. We recognize that everyone brings unique experiences, perspectives, and identities to the table, and we are committed to building a community where everyone feels valued, respected, and supported. Discrimination of any kind is not tolerated, and we strive to ensure that every individual has an equal opportunity to succeed and grow, regardless of their gender identity, sexual orientation, disability, race, ethnicity, background, marital status, genetic information, education level, veteran status, national origin, or any other protected status. We do not automatically disqualify applicants with criminal records and will consider each applicant on a case-by-case basis. \n  We recognize that not all candidates will have every skill or qualification listed in this job description. If you feel you have the level of experience to be successful in the role, we encourage you to apply! \n  We acknowledge that true diversity and inclusion require ongoing effort, and we are committed to doing the work required to make our workplace a safe and equitable space for all. Join us in building a community where we can learn from each other, celebrate our differences, and work together. \n \n \n  Personal information that you submit will be used by Demandbase for recruiting and other business purposes. Our Privacy Policy explains how we collect and use personal information.", "cleaned_desc": " Model Deployment:  End-to-end, fullstack ownership of ML models. Take charge of building, testing, and deploying custom ML/AI models and algorithms on extensive datasets, establishing rigorous processes for monitoring and analyzing their performance in production environments. \n Effective Communication:  Excel in effectively communicating complex data science methods, statistical findings, and algorithmic insights to technical and non-technical stakeholders. \n Championing Innovation:  Stay at the forefront of AdTech and data science, actively leading innovation initiatives within the organization. \n \n What We're Looking For: \n \n Experience:  A minimum of 5 years of robust data science experience in AdTech. \n Educational Background:  A degree in Statistics, Computer Science, Machine Learning, Mathematics, Computational Psychology, Operational Research, Physics, or a related field. \n Project Leadership:  A strong track record of leading greenfield projects from conceptualization to production release. \n Analytical Skills:  Proficiency in using analytical and database tools such as Jupyter notebooks, Hive, SQL, and No-SQL databases. \n Coding Expertise:  Demonstrated ability to write clean and high-performance code in Python.   AI/ML Proficiency:  Proficiency in one or more of the following AI/ML technologies is advantageous: TensorFlow, scikit-learn, Spark MLlib, Bigquery Machine Learning, large-scale datasets, Natural Language Processing, and Graph algorithms. \n \n Nice to Have Skills: \n \n Team Leadership:  Previous experience in leading teams and managing complex projects. \n Cloud Competency:  Familiarity with Google Cloud or AWS cloud platforms. \n \n Desired Qualities: \n \n Ownership Mentality:  Willingness to take full ownership of end-to-end machine learning products you develop or contribute to. \n Self-Driven:  A highly self-motivated individual with the ability to operate with significant autonomy. ", "techs": ["jupyter notebooks", "hive", "sql", "no-sql databases", "python", "tensorflow", "scikit-learn", "spark mllib", "bigquery machine learning", "natural language processing", "graph algorithms", "google cloud", "aws cloud platforms"]}, "5ac8ba544599dde2": {"terms": ["data science"], "salary_min": 136000.0, "salary_max": 219650.0, "title": "Director, Analytics and Data Strategy", "company": "Merkle", "desc": "About the job \n \n \n \n Alignment to Data Strategy Vision: \n \n  Senior-level client and team analytics leader and problem-solver. \n  Focus on data strategy \u2013 aligning data resources and collection and analytics methods to client goals and questions. \n  Team leadership \u2013 lead and mentor resources to scale team engagement and support small- and large-scale client loyalty engagements from initiation to data-driven loyalty. \n  Build strong client relationships \u2013 able to discuss data and analytics in client-accessible ways, empowering the client to socialize findings confidently. \n  Successful track record delivering actionable insights and recommendations to clients from user-level, behavioral data. \n  Create client growth by proactively identifying P&LS-addressable opportunities from data, analysis, and client context. \n  Orchestrate and delegate analysis efforts to offshore resources, ensuring that the solution addresses the problem/question appropriately and reliably. \n  Document and drive data requirements to execute data-driven marketing programs (targeting, personalization, and relevance). \n  Experienced to discuss and guide the choice of analytical approaches to achieve objectives and outcomes. \n  Support development of new capabilities (AI, Emotion). \n \n \n  Responsibilities: \n \n  Create client growth by proactively identifying and defining data-focused solutions supporting client objectives - build them into blueprints, measurement plans, pitches, and strategy decks. \n  Lead analytics and measurement for Strategic accounts, leveraging and integrating the LXM approach. \n  Develop and/or oversee measurement plans and strategies to provide program insights, address learning objectives and improve the effectiveness of the program. \n  Write and/or review briefs to overseas resources, defining and specifying requirements for Data Science projects (predictive analytics, ML, AI) and lead/oversee the delivery of solutions from the results. \n  Partner with external analytics stakeholders to confidently drive scalable solutions to address a client challenges (i.e., defining an approach, shaping the methods to apply to specific use case). \n  Provide thought leadership and act as the subject matter expert in the identification and deployment of data-driven solutions. \n  Translate unclear \u201cdraft\u201d learning objectives into clear, executable data analysis that addresses causality as well as connection. \n  Present findings to clients clearly and cogently, with a focus on delivering understanding that the client can confidently translate to their teams, peers and leadership. \n  Where needed, apply statistical concepts and techniques to analyze experiments and predict user behavior and behavioral drivers for accuracy and client alignment. \n \n \n \n \n \n \n Qualifications \n \n \n \n 8+ years\u2019 experience in data science or analytics roles delivering reporting, data-driven insights, predictive modeling, and recommendations to internal or external clients. \n  Record of excellence developing integrated long-term analytics programs for clients, based on goals and business model. \n  Master\u2019s (preferred) degree in a quantitative field (STEM or quantitative social sciences), or STEM Bachelor\u2019s degree with MBA. \n  Demonstrated strong presentation and storytelling with data skills. Be able to deliver clear value from effort while simplifying the complexity for business audience while partnering with other disciplines to deliver on recommendations. \n  Ability to confidently query, manipulate, extract and analyze data using Python, R and/or SQL to derive strong, supportable data-driven conclusions. \n  Experience with cloud-based big data platforms \u2013 e.g., Redshift, Big Query, Teradata. \n  Experience using Machine Learning and statistical data analysis tool/libraries such as NumPy, SciPy, scikit-learn, or equivalent to drive effective predictive modeling using Regression, Clustering, Scoring. \n  Proficient on querying and reducing data from large database repositories and/or flat files and combining those data to generate and extend dimensional analysis and modeling. \n  Knowledge of the digital marketing ecosystem and fluency with common ad tech platforms and technologies. \n  Measurement experience across multiple marketing domains (e.g., media and CRM). \n  Experimental design and offer testing \u2013 both theory and practice. \n  Experience and competence with Power BI or other front-end visualization tools. \n  Strong and demonstrated strategic thinking and business acumen. \n  Strong organizational skills \u2013 task prioritization, effective time management, meeting facilitation, conflict resolution, and risk identification/mitigation. \n \n \n \n \n \n Additional Information \n \n \n Employees from diverse or underrepresented backgrounds encouraged to apply.  Dentsu (the \"Company\") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying. \n  The anticipated salary range for this position is $136,000-$219,650. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. \n  #LI-MZ1 \n \n  About dentsu  Dentsu is the network designed for what\u2019s next, helping clients predict and plan for disruptive future opportunities in the sustainable economy. Taking a people-centered approach to business transformation, dentsu combines Japanese innovation with a diverse, global perspective to drive client growth and to shape society www.dentsu.com. \n  We are champions for meaningful progress and we strive to be a force for good\u2014for our people, for our clients, for the industry and for our society. We keep our people at the center, creating space for growth, understanding and learning so they can thrive. We embed diversity, in our mindset, in our solutions and in our teams to empower an inclusive, equitable and culturally fluent environment. Building this culture within our teams makes us better collaborators with each other and with our clients, driving better outcomes for all. \n \n       Dentsu (the \"Company\") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact your recruiter if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying.", "cleaned_desc": " 8+ years\u2019 experience in data science or analytics roles delivering reporting, data-driven insights, predictive modeling, and recommendations to internal or external clients. \n  Record of excellence developing integrated long-term analytics programs for clients, based on goals and business model. \n  Master\u2019s (preferred) degree in a quantitative field (STEM or quantitative social sciences), or STEM Bachelor\u2019s degree with MBA. \n  Demonstrated strong presentation and storytelling with data skills. Be able to deliver clear value from effort while simplifying the complexity for business audience while partnering with other disciplines to deliver on recommendations. \n  Ability to confidently query, manipulate, extract and analyze data using Python, R and/or SQL to derive strong, supportable data-driven conclusions. \n  Experience with cloud-based big data platforms \u2013 e.g., Redshift, Big Query, Teradata. \n  Experience using Machine Learning and statistical data analysis tool/libraries such as NumPy, SciPy, scikit-learn, or equivalent to drive effective predictive modeling using Regression, Clustering, Scoring. \n  Proficient on querying and reducing data from large database repositories and/or flat files and combining those data to generate and extend dimensional analysis and modeling. \n  Knowledge of the digital marketing ecosystem and fluency with common ad tech platforms and technologies. \n  Measurement experience across multiple marketing domains (e.g., media and CRM). \n  Experimental design and offer testing \u2013 both theory and practice. \n  Experience and competence with Power BI or other front-end visualization tools. \n  Strong and demonstrated strategic thinking and business acumen. ", "techs": ["python", "r", "sql", "redshift", "big query", "teradata", "numpy", "scipy", "scikit-learn", "power bi"]}, "0839a81932198ef3": {"terms": ["data science"], "salary_min": null, "salary_max": null, "title": "Sr SDoH and Population Insights Data Analyst - Remote", "company": "UnitedHealthcare", "desc": "At UnitedHealthcare, we\u2019re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start  Caring. Connecting. Growing together. \n \n Around here, we're always striving for better. A better way to think. A better way to care. A better way to succeed. As the Sr SDoH and Population Insights Data Analyst, you'll drive the improvements and enhancements of our data model. You\u2019ll partner with team members in IT and offshore. And you\u2019ll be part of making sure our data model is accurate and supportive of regulatory requirements. Not to mention, since UHC is part of the UnitedHealth Group family of businesses, you will also have the support and resources of a Fortune 5 industry leader. It's a chance to be a part of something transformative. Sound good? Join us today. \n \n You\u2019ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges. \n \n Primary Responsibilities: \n \n Onshore lead infrastructure analyst for the UHC Social Determinants of Health platform, responsible for prioritizing and defining work for offshore resources, mentoring them on data knowledge as well as technical skills, reviewing their results, and presenting to SDoH leadership for feedback and approval \n Performing in-depth data analysis on the enterprise data \n Monitoring the data quality and engaging with the stakeholders and reporting team to ensure right enhancements are documented, prioritized and road mapped \n Partnering with the data platform scrum team to implement enhancements and ensure quality data is delivered \n Functioning as a data SME for the tech team \n Conducting and managing outcomes of various studies that include analyzing, reviewing, forecasting, trending, and presenting information for operational and business planning \n Supporting short and long term operational/strategic business activities - by developing, enhancing, and maintaining operational information and models \n Developing and implementing effective/strategic business solutions through research and analysis of data and business processes \n Creating specifications to bring different data into a common structure, creating product specifications and models, developing data solutions to support analyses, performing analysis, interpreting results, developing actionable insights and presenting recommendations for use across the enterprise \n Serves as a key resource on complex and/or critical issues that arise around the SDoH platform \n \n \n You\u2019ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in. \n  Required Qualifications: \n \n Bachelor\u2019s degree in a technical field or 4+ years of equivalent professional experience \n 5+ years of experience writing code for various SQL applications; such as Oracle, Snowflake, MS SQL, Azure SQL, Hive, etc. \n Experience translating business requirements into capabilities and features \n Experience with data analysis and interpreting results \n Experience mentoring and partnering with other team members of various grade levels; with ability to motivate and inspire team members \n Experience reviewing work performed by others and provides recommendations for improvement \n Advanced understanding of Medicaid & Medicare business including member eligibility and claims data \n Intermediate or higher level of proficiency in data modeling & data warehousing \n Intermediate or higher level of proficiency with visualization tools like Tableau, Power BI \n Must be willing to adjust working hours to accommodate East Coast and offshore business hours as needed \n \n \n Preferred Qualifications: \n \n Hands-on experience with UHC applications (SMART for C&S, GPS for M&R, CMS warehouse, CDB, etc.) \n Proven solid communication skills with the ability to explain complex technical issues in the appropriate way for the current audience (technical or business) \n Proven self-reliant, self-driving, curious and a fast learner \n \n \n California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only:  The salary range for California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island or Washington residents is $85,000 to $167,300. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you\u2019ll find a far-reaching choice of benefits and incentives. \n \n \n All employees working remotely will be required to adhere to UnitedHealth Group\u2019s Telecommuter Policy \n \n \n At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone\u2013of every race, gender, sexuality, age, location and income\u2013deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes \u2014 an enterprise priority reflected in our mission. \n \n \n Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law. \n \n UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.", "cleaned_desc": " Monitoring the data quality and engaging with the stakeholders and reporting team to ensure right enhancements are documented, prioritized and road mapped \n Partnering with the data platform scrum team to implement enhancements and ensure quality data is delivered \n Functioning as a data SME for the tech team \n Conducting and managing outcomes of various studies that include analyzing, reviewing, forecasting, trending, and presenting information for operational and business planning \n Supporting short and long term operational/strategic business activities - by developing, enhancing, and maintaining operational information and models \n Developing and implementing effective/strategic business solutions through research and analysis of data and business processes \n Creating specifications to bring different data into a common structure, creating product specifications and models, developing data solutions to support analyses, performing analysis, interpreting results, developing actionable insights and presenting recommendations for use across the enterprise \n Serves as a key resource on complex and/or critical issues that arise around the SDoH platform \n \n   You\u2019ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in. \n  Required Qualifications: \n \n Bachelor\u2019s degree in a technical field or 4+ years of equivalent professional experience \n 5+ years of experience writing code for various SQL applications; such as Oracle, Snowflake, MS SQL, Azure SQL, Hive, etc. \n Experience translating business requirements into capabilities and features \n Experience with data analysis and interpreting results \n Experience mentoring and partnering with other team members of various grade levels; with ability to motivate and inspire team members \n Experience reviewing work performed by others and provides recommendations for improvement \n Advanced understanding of Medicaid & Medicare business including member eligibility and claims data   Intermediate or higher level of proficiency in data modeling & data warehousing \n Intermediate or higher level of proficiency with visualization tools like Tableau, Power BI \n Must be willing to adjust working hours to accommodate East Coast and offshore business hours as needed \n \n \n Preferred Qualifications: \n \n Hands-on experience with UHC applications (SMART for C&S, GPS for M&R, CMS warehouse, CDB, etc.) \n Proven solid communication skills with the ability to explain complex technical issues in the appropriate way for the current audience (technical or business) \n Proven self-reliant, self-driving, curious and a fast learner ", "techs": ["data quality monitoring", "stakeholder engagement", "reporting", "data platform", "enhancements", "data sme", "studies", "forecasting", "trending", "operational information", "models", "business solutions", "research", "analysis", "data solutions", "actionable insights", "recommendations", "sdoh platform", "sql applications", "oracle", "snowflake", "ms sql", "azure sql", "hive", "business requirements", "data analysis", "mentoring", "team collaboration", "work review", "medicaid", "medicare", "member eligibility", "claims data", "data modeling", "data warehousing", "visualization tools", "tableau", "power bi", "uhc applications", "communication skills"]}, "7c9aa7118a17b91d": {"terms": ["data science"], "salary_min": 105373.61, "salary_max": 133426.47, "title": "Data Governance SME", "company": "Savan Group", "desc": "Savan Group is seeking a subject matter expert to help develop and execute a data categorization strategy for a federal Chief Data Officer. We seek a highly organized individual with great attention to detail and strong client relationship skills. Responsibilities include establishing data dictionaries, identifying sensitive information, capturing, and managing business glossaries and controlled vocabularies in a Cabinet-level Department with federated data governance.  \n Role Responsibilities: \n \n  Design, capture, manage, and export data dictionaries, glossaries, and controlled vocabularies for major information systems. \n  Document mission programs\u2019 definitions of business, technical, and policy terms for an improved understanding of data assets, interoperability, and data sharing. \n  Configure the enterprise data governance tool to clearly describe the business applications, data assets, enterprise assets, and glossaries. \n  Plan for Department-level implementation of the data dictionaries with agency-level stakeholders. \n  Manage the governance of data dictionaries and controlled vocabularies.  \n Assist with the management of the client engagement. \n \n  Qualifications and Requirements: \n \n  Bachelor\u2019s degree in IT, engineering, or a data science discipline.  \n 8+ years of professional experience \n  U.S. Citizenship \n  3 years of experience in government consulting  \n 5 years of subject matter experience in knowledge modeling data governance. \n  Public trust or ability to obtain a Public Trust \n \n  Preferred Qualifications: \n \n  Experience with Controlled Unclassified Information (CUI) is highly preferred.  \n Experience managing IT projects for federal civilian agencies. \n \n  Work Location and Schedule: \n  Remote or Hybrid Schedule (Savan Group, HQ, Vienna, VA) \n  Application Process \n  Submit your application to the link on this page. Applicants selected for interviews will be notified by email or phone. \n  EOE, including disability and veterans. \n  Savan Group is an Equal Opportunity Employer and is committed to a workplace free of discrimination. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy), gender identity, sexual orientation, national origin, age (40 or older), marital status, disability, genetic information, status as a protected veteran, or any other applicable legally protected characteristics. \n  If you are an individual with a disability and would like to request a reasonable accommodation for the employment process, please email your request to humanresources@savangroup.com. For more information about our company, please visit our website at www.savangroup.com. \n   \n Kf8Z13q5Vw", "cleaned_desc": "", "techs": ""}, "3039de6c55cc31f7": {"terms": ["data science", "data analyst"], "salary_min": 89858.0, "salary_max": 112322.0, "title": "Senior Report, BI and Data Analyst", "company": "Stand for Children Leadership Center", "desc": "THE ROLE \n  Your role stands at the intersection of state-of-the-art data management and the effort to make positive changes in educational equity and racial justice in high schools nationwide.  As a key member of Stand\u2019s Technology team at a moment of tremendous need and opportunity , you will be responsible for leading and supporting data acquisition, integrity, analysis, and maintenance. As part of a major organizational strategic initiative, you will build a structure for campaigns and elections that drives decisions. Through your dedication and passionate approach to the role, you get the opportunity to work with colleagues who not only care deeply about justice and equity but who are bold and strategic and take responsibility for achieving substantial progress toward our mission every year. \n  This is a remote position reporting to the Chief Technology Officer.  \n THE ORGANIZATION \n  Stand for Children is a unique catalyst for  education equity  and  racial justice  to create a  brighter future for us all.  \n RESPONSIBILITIES: \n  Work closely with business owners, the business user community, the technology team, and other program, state, and national teams to: \n \n  Serve as a single-service, builder, provider and technical expert  for all data and reporting needs for the enterprise at Stand. Must be comfortable as a lead and only data person for the team. \n  Provide technical support in solving data and reporting problems  working with business units and/or other members of the tech team. \n  Write, develop/code, modify, and enhance reports using Tableau  including the interpretation of data, data visualization, dashboard, reporting, and information to support organization metrics. \n  Provide ad-hoc analysis and trending  to identify data anomalies and opportunities. \n  Lead & participate in project teams  to analyze/clarify business processes, business rules and define functional requirements for data and reporting needs. \n  Provide estimates on work effort  and proposed technical solutions to business problems. \n  Build and Support electoral data analytics , big data, BI and visualization to support campaign staff on engagement strategy and tactics. \n  Build and Support ETL , data transformation of big data from the Voter File to/from the marketing engagement system. \n  Lead and Serve  as a data gatekeeper to ensure data consistency, integrity, and quality.  \n Establish and maintain policies and procedures  for data ownership and governance. \n  Train business leaders & end users . \n  Work remotely in collaboration  with other technical team members and business community \n \n  QUALIFICATIONS: \n \n  Passionate commitment to Stand for Children\u2019s mission. \n  8-10 years of direct experience  in data analysis, BI, data manipulation including ETL and API and report writing. \n  Experience in Tableau is required . Proven ability to connect to/from other systems\u2019 data sources. Proficient in data prep, and transformation to build complex data visualization and dashboards in Tableau to \u201ctell the story\u201d. \n  Experience in pulling data from large enterprise systems . \n  Experience with Salesforce and/or CRM and ability to connect Salesforce database from Tableau. \n  Technical understanding of Database structure and data model . Experience working with SQL databases, data stores, data warehouses, and big data. \n  Proven technical skills  in Excel mastery, Query/SQL, Report writing, Report publishing, Data visualization (dashboard, metrics, graphs), and Ad-hoc analysis of data for BI for trends, issues, and opportunities. \n  Written, verbal, and presentation  communication skills to a wide audience. Experience effectively translating technical information and instructions to less technically-minded colleagues.  \n Proven ability to quickly adopt and apply new technologies  and Creative problem-solving. \n  Ability to  manage competing & time-sensitive priorities . \n  Exceptional customer service orientation  with strong interpersonal skills; a focus on rapport-building, collaboration, listening, asking questions, and delivering information in a timely and friendly manner. \n \n  STARTING SALARY RANGE:  $89,858- $112,322 annually commensurate with experience. Generous benefits. \n  Stand for Children consists of two separate entities. Stand for Children, Inc., a grassroots membership organization is exempt under section 501(c)(4) of the Internal Revenue Code, and Stand for Children Leadership Center, a leadership development organization is exempt under section 501(c)(3) \n  Stand for Children and Stand for Children Leadership Center provide equal employment opportunity (EEO)  to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression or marital status. We are committed to a diverse and culturally inclusive workplace in which our  differences broaden our awareness, enrich our daily experiences, and contribute to our collective strength .  We predominantly partner with low-income communities of color. Therefore, though race and other legally protected characteristics are never used to make final hiring decisions, we place a particular focus on recruiting staff members who share the backgrounds of the communities we serve. \n   \n   \n SKPd25oEyh", "cleaned_desc": "  Serve as a single-service, builder, provider and technical expert  for all data and reporting needs for the enterprise at Stand. Must be comfortable as a lead and only data person for the team. \n  Provide technical support in solving data and reporting problems  working with business units and/or other members of the tech team. \n  Write, develop/code, modify, and enhance reports using Tableau  including the interpretation of data, data visualization, dashboard, reporting, and information to support organization metrics. \n  Provide ad-hoc analysis and trending  to identify data anomalies and opportunities. \n  Lead & participate in project teams  to analyze/clarify business processes, business rules and define functional requirements for data and reporting needs. \n  Provide estimates on work effort  and proposed technical solutions to business problems. \n  Build and Support electoral data analytics , big data, BI and visualization to support campaign staff on engagement strategy and tactics. \n  Build and Support ETL , data transformation of big data from the Voter File to/from the marketing engagement system.    8-10 years of direct experience  in data analysis, BI, data manipulation including ETL and API and report writing. \n  Experience in Tableau is required . Proven ability to connect to/from other systems\u2019 data sources. Proficient in data prep, and transformation to build complex data visualization and dashboards in Tableau to \u201ctell the story\u201d. \n  Experience in pulling data from large enterprise systems . \n  Experience with Salesforce and/or CRM and ability to connect Salesforce database from Tableau. \n  Technical understanding of Database structure and data model . Experience working with SQL databases, data stores, data warehouses, and big data. \n  Proven technical skills  in Excel mastery, Query/SQL, Report writing, Report publishing, Data visualization (dashboard, metrics, graphs), and Ad-hoc analysis of data for BI for trends, issues, and opportunities. \n  Written, verbal, and presentation  communication skills to a wide audience. Experience effectively translating technical information and instructions to less technically-minded colleagues.  \n Proven ability to quickly adopt and apply new technologies  and Creative problem-solving. ", "techs": ["tableau", "etl", "api", "salesforce", "crm", "sql databases", "data stores", "data warehouses", "excel", "query/sql", "data visualization", "dashboard", "metrics", "graphs"]}, "dc86b0bb0b8f1b35": {"terms": ["data science", "data engineer", "machine learning engineer"], "salary_min": 95157.97, "salary_max": 120491.19, "title": "Data Engineer (REMOTE)", "company": "The RepTrak Company", "desc": "Data Engineer (REMOTE) -  Specializing in Data Products, Generative AI, and Data Management \n \n  Are you a skilled Data Engineer with a strong background in crafting data products, data management, and metadata design? Are you experienced in handling both structured and unstructured data, thriving in data-rich environments, and captivated by the potential of Generative AI to transform industries? Join our innovative team and play a pivotal role in shaping the future of data-driven product development enhanced by Generative AI technologies. \n \n  At  The RepTrak Company\u2122 , our purpose is to prove that saying and doing the right thing is good for business. This is at the core of everything we do. Our gold-standard Reputation model draws from a longitudinal media and perception data set that allows us to advise Fortune 500 clients on how to prioritize, implement, and prove the impact of doing and saying the right things. Addressing these challenges for today\u2019s corporations \u2013 life after COVID, understanding how to enact DE&I principles, ongoing high expectations around innovation and ethics \u2013is our everyday work. It\u2019s meaningful, and tied to what\u2019s happening now as the world pivots away from shareholder capitalism and toward stakeholder capitalism. \n \n  The  RepTrak Company\u2122  is the world\u2019s leading reputation data and insights company. We provide the only global platform for data-driven insights on Reputation, Brand, and ESG. Our proprietary RepTrak\u00ae model is the global standard for measuring and analyzing the sentiment of the world using proven data science models and machine learning techniques across industries and geographies. Subscribers to the RepTrak\u00ae Program use our predictive insights to protect business value, improve return on investment, and increase their positive impact on society. Established in 2004, The RepTrak Company owns the world\u2019s largest reputation benchmarking database of over 1 million company ratings per year used by CEOs, boards, and executives in more than 60 countries worldwide. \n \n  The Role \n  As a Data Engineer specializing in Data Products, Data Management, and Generative AI, you will be at the forefront of our efforts to revolutionize how data insights are harnessed to create transformative products. Your expertise in data management, metadata design, and the development of both structured and unstructured data will be pivotal in creating innovative data solutions that empower our clients to unlock new dimensions of value and insights. By leveraging your experience in data-rich environments and your keen interest in Generative AI, you will contribute to redefining how industries approach innovation. \n \n  Specific Responsibilities Include \n \n Collaborate closely with cross-functional teams, including project management, analytics, and data science, to understand business requirements and translate them into robust data engineering solutions. \n Apply advanced techniques in Generative AI to enhance data analysis, visualization, and predictive modeling. \n Develop solutions that handle both structured and unstructured data, extracting meaningful insights from diverse data sources.Implement best practices for data quality, data governance, and data privacy. \n Stay current with industry trends, especially in Generative AI and data management, and evaluate their applicability to our projects. \n Mentor and guide junior team members, fostering a culture of continuous learning and growth. \n Work collaboratively with analytics teams to provide the necessary data infrastructure for their analysis and modeling needs. \n Optimize and maintain data infrastructure, ensuring scalability, reliability, and security. \n Implement data management strategies, including metadata design and concepts, to ensure data accuracy, lineage, and traceability. \n Design and develop data pipelines that deliver actionable insights and support data-driven product initiatives. \n \n \n \n  Professional Qualifications \n \n Bachelor's or Master's degree in Computer Science, Data Science, or a related field. \n Proficiency in data pipeline development, ETL processes, and data warehousing. \n Experience with Generative AI frameworks and applications is a strong plus. \n Familiarity with handling both structured and unstructured data, extracting insights from diverse data sources. \n Excellent communication skills to collaborate effectively with technical and non-technical stakeholders. \n A passion for innovation, a curious mindset, and a deep interest in the potential of Generative AI. \n Familiarity with cloud platforms (e.g., AWS, GCP, Azure) and related data services. \n Strong understanding of data management principles, metadata design, and concepts. \n Self-starter that is comfortable with ambiguity and moves with urgency. \n Solid programming skills in languages like Python, SQL, and relevant Big Data technologies. \n Proven experience (3+ years) in Data Engineering, with a track record of developing data products or working in data insights-intensive environments. \n \n \n \n  Why Join Our Engineering Team: \n \n Cutting-Edge Projects:  Be part of pioneering initiatives that leverage data, metadata design, both structured and unstructured data, and Generative AI to create industry-leading products. \n Innovation Culture:  Immerse yourself in a dynamic and collaborative environment where your ideas and contributions are valued. \n Professional Growth:  Access ongoing learning opportunities, training, and mentorship to advance your skills and career. \n Impactful Work:  Contribute to solutions that drive real-world impact and transform industries. \n Diverse Team:  Join a team of diverse experts who are passionate about technology and innovation. \n \n \n \n  If you are a Data Engineer with a proven history of delivering data products, collaborating with analytics teams, expertise in data management, metadata design, and experience in handling both structured and unstructured data, and are excited about exploring the potential of Generative AI, we invite you to apply and be part of our journey to reshape the future! \n \n  Work Location \n  This position is fully remote. Ideal candidates are located within New England and able to work East Coast hours. \n \n  All candidates must have their primary residence in a country where RepTrak has an entity (Canada, Brazil, United States, United Kingdom, The Netherlands, Italy, Spain, Denmark, Australia). Any US based candidates are ideally located in one of RepTrak's core operating states (MA, CT, PA, NY, IL, GA, NH, or CA). \n \n  Some travel may be required. \n \n  Compensation \n  The role encompasses a compensation package including a competitive salary and an annual performance bonus plan. RepTrak offers a full benefits program including company holidays, paid vacation, and more. \n \n  The RepTrak Company is committed to diversity in the workplace and is an Equal Opportunity Employer. For more information, please visit our website at https://www.reptrak.com/careers/", "cleaned_desc": " \n Collaborate closely with cross-functional teams, including project management, analytics, and data science, to understand business requirements and translate them into robust data engineering solutions. \n Apply advanced techniques in Generative AI to enhance data analysis, visualization, and predictive modeling. \n Develop solutions that handle both structured and unstructured data, extracting meaningful insights from diverse data sources.Implement best practices for data quality, data governance, and data privacy. \n Stay current with industry trends, especially in Generative AI and data management, and evaluate their applicability to our projects. \n Mentor and guide junior team members, fostering a culture of continuous learning and growth. \n Work collaboratively with analytics teams to provide the necessary data infrastructure for their analysis and modeling needs. \n Optimize and maintain data infrastructure, ensuring scalability, reliability, and security. \n Implement data management strategies, including metadata design and concepts, to ensure data accuracy, lineage, and traceability. \n Design and develop data pipelines that deliver actionable insights and support data-driven product initiatives. \n \n   \n  Professional Qualifications \n \n Bachelor's or Master's degree in Computer Science, Data Science, or a related field. \n Proficiency in data pipeline development, ETL processes, and data warehousing. \n Experience with Generative AI frameworks and applications is a strong plus. \n Familiarity with handling both structured and unstructured data, extracting insights from diverse data sources. \n Excellent communication skills to collaborate effectively with technical and non-technical stakeholders. \n A passion for innovation, a curious mindset, and a deep interest in the potential of Generative AI. \n Familiarity with cloud platforms (e.g., AWS, GCP, Azure) and related data services. \n Strong understanding of data management principles, metadata design, and concepts. \n Self-starter that is comfortable with ambiguity and moves with urgency.   Solid programming skills in languages like Python, SQL, and relevant Big Data technologies. \n Proven experience (3+ years) in Data Engineering, with a track record of developing data products or working in data insights-intensive environments. \n \n \n \n  Why Join Our Engineering Team: \n \n Cutting-Edge Projects:  Be part of pioneering initiatives that leverage data, metadata design, both structured and unstructured data, and Generative AI to create industry-leading products. \n Innovation Culture:  Immerse yourself in a dynamic and collaborative environment where your ideas and contributions are valued. \n Professional Growth:  Access ongoing learning opportunities, training, and mentorship to advance your skills and career. \n Impactful Work:  Contribute to solutions that drive real-world impact and transform industries. \n Diverse Team:  Join a team of diverse experts who are passionate about technology and innovation. ", "techs": ["generative ai", "data analysis", "data visualization", "predictive modeling", "structured data", "unstructured data", "data quality", "data governance", "data privacy", "data management", "industry trends", "metadata design", "data accuracy", "data lineage", "data traceability", "data pipelines", "data-driven product initiatives", "data pipeline development", "etl processes", "data warehousing", "cloud platforms", "aws", "gcp", "azure", "data services", "programming skills", "python", "sql", "big data technologies", "data products", "data insights-intensive environments", "metadata design", "cutting-edge projects", "innovation culture", "professional growth", "impactful work", "diverse team."]}, "89e03009fcbaa993": {"terms": ["data science"], "salary_min": null, "salary_max": null, "title": "Adjunct Instructor of Business Analytics (Online) - Our Lady of the Lake University", "company": "Framingham State University", "desc": "Location:  San Antonio, TX \n  Category:  Faculty \n  Posted On:  Wed Apr 20 2022 \n  Job Description: \n \n \n The Department of Business invites applications from qualified individuals to teach courses in the Master of Science in Business Analytics program. All classes available are offered online. Courses in several topic areas may be available, including Data Analytics with Python, Data Science and Machine Learning, Dashboard, Scorecard and Visualization, and a Capstone Project in Business Analytics. \n \n \n Responsibilities: Prepare lectures, computer lab activities, assignments, and assessments, as appropriate to the course under the supervision of a lead profe... more...", "cleaned_desc": "", "techs": ""}, "7754227c067a4b84": {"terms": ["data science", "data engineer"], "salary_min": 120795.984, "salary_max": 152954.64, "title": "Sr. Data Engineer", "company": "CloudRay Inc", "desc": "Role :  Data Engineer \n \n \n Location:  Remote \n \n \n Duration:  Long Term Contract \n \n  Must have SSA Public Trust prior to starting; verifying with Manager \n \n \n Role Description : : \n  The Data Engineer provides the ETL support to the data science and software engineering team members. Build, modify, support infrastructure for optimal extraction, transformation, and loading of data from variety of structure, unstructured data sources and multi-terabyte distributed file system. Candidate will formulate and rapidly prototype various approaches as well as effectively communicate the pros and cons of each. Provide data-driven approaches to tackle various business problems. The candidate will have the ability to contribute to a high-performing, motivated workgroup by applying interpersonal and collaboration skills to achieve project goals Architect for Client data pipeline with data acquisition and preprocessing functionalities that gather data from heterogenous data pool from the distributed file system, unstructured text extracted from multi-million images of medical records with varied OCR quality, their metadata from relational databases and custom annotations. \n \n \n Responsibilities: \n  Provide current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments \n \n  Manage/maintain structured, semi-structured, and unstructured data, structuring and wrangling data as appropriate for statistical analysis \n \n  Implement data warehouse concepts and relational databases, big data management techniques and tools (e.g. Hadoop, MAPReduce) \n \n  Communicate with technical and non-technical users and managers, and server administration, to include hardware and software support to existing servers. \n \n  Provide software engineering support to operate, maintain and enhance systems that are integrated with and/or relied upon by the data engineering lifecycle \n \n  Integrate, analyze, and visualize data and information in near real-time (within 24 hours) from multiple disparate data sources. \n \n  Optimize data storage and access \n \n  Proficiency with Python and Java, Oracle enterprise manager, SQL, AWS \n \n \n Qualifications: \n  Masters degree in related field + 5 years experience; or PhD +1 year experience; or Bachelor's degree in related field + 7 years experience \n \n  Minimum of 5 years experience conducting ETL tasks, performance engineering, run-time optimization, large data volume transfers \n \n  Minimum 3 years experience with Regular Expressions, SQL (PostgreSQL), No-SQL (MongoDB) \n \n  Minimum 1 year experience with Version control systems (Git) \n \n  Preference to developer with experience working with healthcare data and Health IT", "cleaned_desc": "  Must have SSA Public Trust prior to starting; verifying with Manager \n \n \n Role Description : : \n  The Data Engineer provides the ETL support to the data science and software engineering team members. Build, modify, support infrastructure for optimal extraction, transformation, and loading of data from variety of structure, unstructured data sources and multi-terabyte distributed file system. Candidate will formulate and rapidly prototype various approaches as well as effectively communicate the pros and cons of each. Provide data-driven approaches to tackle various business problems. The candidate will have the ability to contribute to a high-performing, motivated workgroup by applying interpersonal and collaboration skills to achieve project goals Architect for Client data pipeline with data acquisition and preprocessing functionalities that gather data from heterogenous data pool from the distributed file system, unstructured text extracted from multi-million images of medical records with varied OCR quality, their metadata from relational databases and custom annotations. \n \n \n Responsibilities:    Provide current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments \n \n  Manage/maintain structured, semi-structured, and unstructured data, structuring and wrangling data as appropriate for statistical analysis \n \n  Implement data warehouse concepts and relational databases, big data management techniques and tools (e.g. Hadoop, MAPReduce) \n \n  Communicate with technical and non-technical users and managers, and server administration, to include hardware and software support to existing servers. \n    Provide software engineering support to operate, maintain and enhance systems that are integrated with and/or relied upon by the data engineering lifecycle \n \n  Integrate, analyze, and visualize data and information in near real-time (within 24 hours) from multiple disparate data sources. \n \n  Optimize data storage and access \n \n  Proficiency with Python and Java, Oracle enterprise manager, SQL, AWS \n ", "techs": ["hadoop", "mapreduce", "python", "java", "oracle enterprise manager", "sql", "aws"]}, "7c3fef22f0554849": {"terms": ["data science", "machine learning engineer"], "salary_min": 133068.69, "salary_max": 168494.6, "title": "Senior Data Scientist", "company": "Jerry", "desc": "We'd love to hear from you if you like: \n \n  Making a big impact with a Forbes Top Startup Employer \n  Working on products that have traction (40X revenue growth in 4 years | #1 rated app in the insurance comparison category) \n  Solving problems in a huge market ($2T market size) \n  Working closely with serial entrepreneurs and seasoned leaders who have scaled companies like Robinhood, Amazon, LinkedIn, Wayfair, SoFi, Microsoft, etc. \n \n \n  About the opportunity: \n  We are looking for a Senior Data Scientist to join our team! Helping everyday, hard working Americans save time and money on their cars and creating a world class experience is what drives every decision we make as a company. Since launching our mobile app in 2019, we have amassed over 4M customers, expanded our product offerings to multiple categories and scaled our team 10X. Our data team fuels all of our business and product decisions through delivering analytical insights and building advanced models. \n \n  Reporting to our VP of Business Operations and Analytics, you will leverage data to drive growth and retention for our core insurance product. You will perform analytical deep dives, develop and analyze experiments, build predictive models, and make recommendations that inform our product roadmap. Working with a brilliant team of product managers, product designers, software engineers, and key business leaders, you will play a big role in accelerating our growth and taking our customer experience to the next level. \n \n  How you will make an impact: \n \n  Partner closely with our product managers, software engineers, product designers, and key business leaders to drive user growth and retention for our core insurance product  \n Design, run, and analyze A/B experiments on new and existing features; extract key insights, share learnings and make recommendations on next steps \n  Build key reports, dashboards, and predictive models to monitor the performance of our insurance business, and communicate analytical outcomes to our teams \n  Transform and refine raw production data for analytical needs \n  Continually improve our data governance and data consistency standards within our database \n  Work with data engineering team on data tracking, integrity, and security as needed \n  Work with other data scientists to evolve, optimize and integrate machine learning models \n \n \n  Who you are: \n \n  Intellectually curious: You're not satisfied with surface level insights. You dive deep to understand how systems work, why people behave in certain ways and are intrinsically motivated to uncover root causes for issues or underlying reasons behind decisions. \n  Data-driven: You're extremely analytical and live in data. At the same time, you're confident enough to make decisions when the data is limited. \n  Creative problem-solver: No challenge is too complex, no issue is too hard, and \u201cno\u201d is not an acceptable answer. \n  Strong communicator: Able to drive alignment and communicate effectively to different audiences. \n \n \n  Ideal profile: \n \n  Bachelor\u2019s degree in Mathematics, Statistics, Economics, Computer Science or a related quantitative discipline \n  3+ years experience as a data scientist or product analyst in a consumer-facing web or mobile app environment \n  Experience designing and implementing A/B tests, and analyzing user experience \n  Hands-on experience with SQL (advanced proficiency) \n \n \n  Jerry is proud to be an Equal Employment Opportunity employer. We prohibit discrimination based on race, religion, color, national origin, sex, pregnancy, reproductive health decisions or related medical conditions, sexual orientation, gender identity, gender expression, age, veteran status, disability, genetic information, or other characteristics protected by applicable local, state or federal laws.  \n \n Jerry is committed to providing reasonable accommodations for individuals with disabilities in our job application process. If you need assistance or an accommodation due to a disability, please contact us at  recruiting@getjerry.com \n \n  About Jerry: \n  Jerry is America\u2019s first and only AllCar app. We are redefining and radically improving how people manage owning a car, one of their most expensive and time-consuming assets. \n \n  Backed by artificial intelligence and machine learning, Jerry simplifies and automates owning and maintaining a car while providing personalized services for all car owners' needs. We spend every day innovating and improving our AI-powered app to provide the best possible experience for our customers. From car insurance and financing to maintenance and safety, Jerry does it all. \n \n  We are the #1 rated and most downloaded app in our category with a 4.7 star rating in the App Store. We have more than 4 million customers \u2014 and we\u2019re just getting started. \n \n  Jerry was founded in 2017 by serial entrepreneurs and has raised more than $242 million in financing. \n \n  Join our team and work with passionate, curious and egoless people who love solving real-world problems. Help us build a revolutionary product that\u2019s disrupting a massive market.", "cleaned_desc": " \n \n  Ideal profile: \n \n  Bachelor\u2019s degree in Mathematics, Statistics, Economics, Computer Science or a related quantitative discipline \n  3+ years experience as a data scientist or product analyst in a consumer-facing web or mobile app environment \n  Experience designing and implementing A/B tests, and analyzing user experience \n  Hands-on experience with SQL (advanced proficiency) \n \n ", "techs": ["sql"]}, "b20ead97bc487449": {"terms": ["data science", "machine learning engineer", "mlops"], "salary_min": 195863.7, "salary_max": 248007.08, "title": "Staff Machine Learning Engineer, Financial Crimes", "company": "Afterpay", "desc": "Company Description \n \n \n \n     It all started with an idea at Block in 2013. Initially built to take the pain out of peer-to-peer payments, Cash App has gone from a simple product with a single purpose to a dynamic ecosystem, developing unique financial products, including Afterpay/Clearpay, to provide a better way to send, spend, invest, borrow and save to our 47 million monthly active customers. We want to redefine the world\u2019s relationship with money to make it more relatable, instantly available, and universally accessible.\n     \n  Today, Cash App has thousands of employees working globally across office and remote locations, with a culture geared toward innovation, collaboration and impact. We\u2019ve been a distributed team since day one, and many of our roles can be done remotely from the countries where Cash App operates. No matter the location, we tailor our experience to ensure our employees are creative, productive, and happy.\n     \n  Check out our locations, benefits, and more at cash.app/careers.\n    \n \n \n \n \n  Job Description \n \n \n  The Financial Crimes Technology Data team at Cash App finds and reports financial crimes activity on Cash App. We work globally with partners in business, engineering, counsel and product to guarantee we are providing a safe user experience for our customers while minimizing or eliminating bad activity on our platform. \n  We are leveraging Machine Learning as an integral part of our toolkit to fulfill our mission. As Cash App scales, we are monitoring billions of dollars in transactions across traditional payment and blockchain networks. We uncover and put an end to money laundering, fraud, and illegal activities before they impact our users. We are looking for senior and staff MLEs that can integrate vertically into the ML sub-team and be focused on building/enhancing tools, libraries, frameworks, developer environments, etc. for ML modeling workflows. \n  This is an IC role, but the Staff level has additional leadership responsibilities that include envisioning, owning, and driving strategic roadmaps & priorities to completion by collaborating with cross functional stakeholders. The role may have the opportunity to progress into a people manager role in the future. \n  You will: \n \n  Design and build services and tooling that support our ML and Data Science sub-teams \n  Be the MLOps lead and facilitate modelers on the team by unblocking access to the infrastructure/tools necessary for development & production work \n  Develop prototypes and partner with ML modelers to encourage adoption of new tools and technologies \n  Proactively identify new opportunities and future needs of our ML teams \n  Lead by example by applying ML and engineering best practices \n  Stay on top of changing ML infrastructure and libraries at Block and advocate/educate the team about new developments by sharing resources, demos and PoCs \n  Join a new, small, and growing team and have a significant impact on influencing team culture and direction \n \n \n \n \n \n  Qualifications \n \n \n \n  4-6+ years of combined Machine Learning and Engineering industry experience \n  A graduate degree in computer science, data science, operations research, applied math, stats, physics, or a related technical field \n  Familiarity with Linux/OS X command line, version control software (git), and general software development principles with a machine learning software development life-cycle orientation. \n  Experience working with product, business, and engineering to prioritize, scope, design, and deploy ML models \n  Familiarity with Python computing stack, MySQL, Snowflake, Airflow, Java/Go \n  Hosted models for inference on public clouds like GCP, AWS and/or built micro-services to facilitate event based triggering, feature generation, model inference and downstream actioning. \n \n \n \n \n \n  Additional Information \n \n \n  Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate\u2019s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.    Zone A: USD $202,500 - USD $247,500  Zone B: USD $192,400 - USD $235,200  Zone C: USD $182,300 - USD $222,800  Zone D: USD $172,200 - USD $210,400 \n \n  To find a location\u2019s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information. \n  Full-time employee benefits include the following: \n \n  Healthcare coverage (Medical, Vision and Dental insurance) \n  Health Savings Account and Flexible Spending Account \n  Retirement Plans including company match \n  Employee Stock Purchase Program \n  Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance \n  Paid parental and caregiving leave \n  Paid time off (including 12 paid holidays) \n  Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees) \n  Learning and Development resources \n  Paid Life insurance, AD&D, and disability benefits \n  Additional Perks such as WFH reimbursements and free access to caregiving, legal, and discounted resources \n \n  These benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans. \n  We\u2019re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, veteran status, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. \n  We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible.  Want to learn more about what we\u2019re doing to build a workplace that is fair and square? Check out our  I+D page . \n  Additionally, we consider qualified applicants with criminal histories for employment on our team, assessing candidates in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance. \n \n \n  Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.", "cleaned_desc": " \n \n \n \n  Qualifications \n \n \n \n  4-6+ years of combined Machine Learning and Engineering industry experience \n  A graduate degree in computer science, data science, operations research, applied math, stats, physics, or a related technical field \n  Familiarity with Linux/OS X command line, version control software (git), and general software development principles with a machine learning software development life-cycle orientation. \n  Experience working with product, business, and engineering to prioritize, scope, design, and deploy ML models \n  Familiarity with Python computing stack, MySQL, Snowflake, Airflow, Java/Go \n  Hosted models for inference on public clouds like GCP, AWS and/or built micro-services to facilitate event based triggering, feature generation, model inference and downstream actioning. \n ", "techs": ["linux/os x command line", "version control software (git)", "python computing stack", "mysql", "snowflake", "airflow", "java/go", "gcp", "aws"]}, "99f9a196e1771dfb": {"terms": ["data science", "machine learning engineer", "mlops"], "salary_min": 138801.78, "salary_max": 175754.0, "title": "Staff Machine Learning Engineer, Financial Crimes", "company": "Cash App", "desc": "Company Description\n   It all started with an idea at Block in 2013. Initially built to take the pain out of peer-to-peer payments, Cash App has gone from a simple product with a single purpose to a dynamic ecosystem, developing unique financial products, including Afterpay/Clearpay, to provide a better way to send, spend, invest, borrow and save to our 47 million monthly active customers. We want to redefine the world\u2019s relationship with money to make it more relatable, instantly available, and universally accessible.    Today, Cash App has thousands of employees working globally across office and remote locations, with a culture geared toward innovation, collaboration and impact. We\u2019ve been a distributed team since day one, and many of our roles can be done remotely from the countries where Cash App operates. No matter the location, we tailor our experience to ensure our employees are creative, productive, and happy.    Check out our locations, benefits, and more at cash.app/careers. \n \n \n \n Job Description\n   The Financial Crimes Technology Data team at Cash App finds and reports financial crimes activity on Cash App. We work globally with partners in business, engineering, counsel and product to guarantee we are providing a safe user experience for our customers while minimizing or eliminating bad activity on our platform. \n  We are leveraging Machine Learning as an integral part of our toolkit to fulfill our mission. As Cash App scales, we are monitoring billions of dollars in transactions across traditional payment and blockchain networks. We uncover and put an end to money laundering, fraud, and illegal activities before they impact our users. We are looking for senior and staff MLEs that can integrate vertically into the ML sub-team and be focused on building/enhancing tools, libraries, frameworks, developer environments, etc. for ML modeling workflows. \n  This is an IC role, but the Staff level has additional leadership responsibilities that include envisioning, owning, and driving strategic roadmaps & priorities to completion by collaborating with cross functional stakeholders. The role may have the opportunity to progress into a people manager role in the future. \n  You will: \n \n  Design and build services and tooling that support our ML and Data Science sub-teams \n  Be the MLOps lead and facilitate modelers on the team by unblocking access to the infrastructure/tools necessary for development & production work \n  Develop prototypes and partner with ML modelers to encourage adoption of new tools and technologies \n  Proactively identify new opportunities and future needs of our ML teams \n  Lead by example by applying ML and engineering best practices \n  Stay on top of changing ML infrastructure and libraries at Block and advocate/educate the team about new developments by sharing resources, demos and PoCs \n  Join a new, small, and growing team and have a significant impact on influencing team culture and direction \n \n \n \n \n Qualifications\n  \n \n  4-6+ years of combined Machine Learning and Engineering industry experience \n  A graduate degree in computer science, data science, operations research, applied math, stats, physics, or a related technical field \n  Familiarity with Linux/OS X command line, version control software (git), and general software development principles with a machine learning software development life-cycle orientation. \n  Experience working with product, business, and engineering to prioritize, scope, design, and deploy ML models \n  Familiarity with Python computing stack, MySQL, Snowflake, Airflow, Java/Go \n  Hosted models for inference on public clouds like GCP, AWS and/or built micro-services to facilitate event based triggering, feature generation, model inference and downstream actioning. \n \n  Additional Information\n   Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate\u2019s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.    Zone A: USD $202,500 - USD $247,500  Zone B: USD $192,400 - USD $235,200  Zone C: USD $182,300 - USD $222,800  Zone D: USD $172,200 - USD $210,400 \n \n  To find a location\u2019s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information. \n  Full-time employee benefits include the following: \n \n  Healthcare coverage (Medical, Vision and Dental insurance) \n  Health Savings Account and Flexible Spending Account \n  Retirement Plans including company match \n  Employee Stock Purchase Program \n  Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance \n  Paid parental and caregiving leave \n  Paid time off (including 12 paid holidays) \n  Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees) \n  Learning and Development resources \n  Paid Life insurance, AD&D, and disability benefits \n  Additional Perks such as WFH reimbursements and free access to caregiving, legal, and discounted resources \n \n  These benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans. \n  We\u2019re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, veteran status, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. \n  We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible.  Want to learn more about what we\u2019re doing to build a workplace that is fair and square? Check out our  I+D page . \n  Additionally, we consider qualified applicants with criminal histories for employment on our team, assessing candidates in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance. \n \n \n  Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.", "cleaned_desc": " Qualifications\n  \n \n  4-6+ years of combined Machine Learning and Engineering industry experience \n  A graduate degree in computer science, data science, operations research, applied math, stats, physics, or a related technical field \n  Familiarity with Linux/OS X command line, version control software (git), and general software development principles with a machine learning software development life-cycle orientation. \n  Experience working with product, business, and engineering to prioritize, scope, design, and deploy ML models \n  Familiarity with Python computing stack, MySQL, Snowflake, Airflow, Java/Go \n  Hosted models for inference on public clouds like GCP, AWS and/or built micro-services to facilitate event based triggering, feature generation, model inference and downstream actioning. \n \n  Additional Information", "techs": ["linux/os x command line", "git", "python", "mysql", "snowflake", "airflow", "java/go", "gcp", "aws"]}, "024c4eda91da7992": {"terms": ["data science", "machine learning engineer"], "salary_min": 184539.39, "salary_max": 233667.98, "title": "Senior Staff Machine Learning Engineer, Applied Science", "company": "Pinterest", "desc": "About Pinterest : \n  Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. \n  Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences. \n  Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our  PinFlex  landing page to learn more. \n \n  We are looking for a highly motivated and experienced Machine Learning Engineer to join our team and help us shape the future of machine learning at Pinterest. In this role, you will tackle new challenges in machine learning that will have a real impact on the way people discover and interact with the world around them. You will collaborate with a world-class team of research scientists and engineers to develop new machine learning algorithms, systems, and applications that will bring step-function impact to the business metrics (recent publications 1, 2, 3). You will also have the opportunity to work on a variety of exciting projects in the following areas: \n \n representation learning \n recommender systems \n graph neural network \n natural language processing (NLP) \n inclusive AI \n reinforcement learning \n user modeling \n \n You will also have the opportunity to mentor junior researchers and collaborate with external researchers on cutting-edge projects. \n  What you'll do: \n \n Lead cutting-edge research in machine learning and collaborate with other engineering teams to adopt the innovations into Pinterest problems \n Collect, analyze, and synthesize findings from data and build intelligent data-driven model \n Scope and independently solve moderately complex problems; write clean, efficient, and sustainable code \n Use machine learning, natural language processing, and graph analysis to solve modeling and ranking problems across growth, discovery, ads and search \n \n What we're looking for: \n \n Mastery of at least one systems languages (Java, C++, Python) or one ML framework (Pytorch, Tensorflow, MLFlow) \n Experience in research and in solving analytical problems \n Strong communicator and team player. Being able to find solutions for open-ended problems \n 8+ years working experience in the r&d or engineering teams that build large-scale ML-driven projects \n 3+ years experience leading cross-team engineering efforts that improves user experience in products \n MS/PhD in Computer Science, ML, NLP, Statistics, Information Sciences or related field \n \n Desired skills: \n \n Strong publication track record and industry experience in shipping machine learning solutions for large-scale challenges \n Cross-functional collaborator and strong communicator \n Comfortable solving ambiguous problems and adapting to a dynamic environment \n \n This position is not eligible for relocation assistance. \n  #LI-SA1 \n  #LI-REMOTE \n \n \n  At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. \n  Information regarding the culture at Pinterest and benefits available for this position can be found  here . \n \n  US based applicants only \n \n    $158,950\u2014$327,000 USD\n   \n \n \n  Our Commitment to Diversity: \n  Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.", "cleaned_desc": " natural language processing (NLP) \n inclusive AI \n reinforcement learning \n user modeling \n \n You will also have the opportunity to mentor junior researchers and collaborate with external researchers on cutting-edge projects. \n  What you'll do: \n \n Lead cutting-edge research in machine learning and collaborate with other engineering teams to adopt the innovations into Pinterest problems \n Collect, analyze, and synthesize findings from data and build intelligent data-driven model   Scope and independently solve moderately complex problems; write clean, efficient, and sustainable code \n Use machine learning, natural language processing, and graph analysis to solve modeling and ranking problems across growth, discovery, ads and search \n \n What we're looking for: \n \n Mastery of at least one systems languages (Java, C++, Python) or one ML framework (Pytorch, Tensorflow, MLFlow) \n Experience in research and in solving analytical problems \n Strong communicator and team player. Being able to find solutions for open-ended problems \n 8+ years working experience in the r&d or engineering teams that build large-scale ML-driven projects \n 3+ years experience leading cross-team engineering efforts that improves user experience in products ", "techs": ["natural language processing (nlp)", "inclusive ai", "reinforcement learning", "user modeling", "machine learning", "java", "c++", "python", "pytorch", "tensorflow", "mlflow"]}, "ba141259546f5748": {"terms": ["data science", "machine learning engineer"], "salary_min": 223253.78, "salary_max": 282689.03, "title": "Principal ML Engineer, ML Fairness and Inclusive AI - Advanced Technologies Group", "company": "Pinterest", "desc": "About Pinterest : \n  Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. \n  Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences. \n  Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our  PinFlex  landing page to learn more. \n \n  The Advanced Technologies Group (ATG) is Pinterest's advanced machine learning team. The goal of the team is to keep Pinterest at the forefront of machine learning technology across multiple use cases including recommendations, ranking, content understanding, and more. It is an applied team that works horizontally across the company on state of the art AI and ML and works on directly bringing that technology to the product in collaboration with product engineering teams. The team does publish its work in applied research conferences, but the main goal of the team is to have top line metric impact. \n  The goal of Pinterest is to inspire pinners to live the life they love. The product is powered by state of the art ML algorithms which are used to understand both the billions of visually rich items on the platform and the interests of our 400M+ monthly active users and recommend inspiring personalized content to them. What this means is that ML at Pinterest is not only multi-modal utilizing image, text, and graph as input, it needs to operate at a very large scale and often in a real time interactive user experience. \n  Pinterest is known for being a positive and inspirational place on the internet and we believe that inspiration starts with inclusion. The Inclusive AI team at Pinterest is a horizontal team that works across the company on both ML Fairness and user facing features which make our product more inclusive so we can better serve our diverse user base. This includes our launches of groundbreaking features such as user controllable skin tone and hair pattern filters and more recently body type. \n  We are looking for a  Tech Lead Architect who can who can work with the team to drive cross-team engineering efforts in the Inclusive AI / ML Fairness space,  work with senior leaders at the company to lead and set our technical strategy in this space, and also communicate and work with external groups regarding this work. You'll have the opportunity to work on various innovative projects of new product experiences, build large-scale low-latency systems and state-of-the-art machine learning models, and deliver great impact to our pinners and business metrics. \n  What you'll do: \n \n Lead projects that involve developing and deploying state of the art ML models in production systems across the company \n Collaborate with other engineering teams (infra. user modeling, content understanding) to leverage their platforms and signals and work with them to collaborate on the adoption and evaluation of Inclusive AI and ML Fairness \n Mentor junior engineers on the Inclusive AI team and across the company on Inclusive AI / ML Fairness and help them to built that into their workflow \n Work with the team and senior leaders at the company to define and drive technical strategy in this area \n Communicate externally about this work and with external groups including industry, advocacy, and government bodies regarding this work \n \n What we're looking for: \n \n Experience with state of the art ML Fairness techniques \n Familiarity with state of the art ML approaches like transformers, self supervised pre-training, generative modeling, LLMs, etc. \n Experience with large scale data processing (e.g. Hive, Scalding, Spark, Hadoop, Map-reduce) \n Hands-on experience building, deploying, and measuring ML Fairness at a very large scale. Successful candidates in this role need to be able to build bridge state of the art approaches to real world impact \n 8+ years working experience in the engineering teams that build large-scale ML-driven user-facing products \n 3+ years experience leading cross-team engineering efforts \n Strong execution skills in project management \n Strong communication skills to communicate strategy to senior management and outside of the company, externally \n Masters or PhD in Comp Sci or related fields \n Understanding of an object-oriented programming language (Java, C++, Python) \n \n Desired skills: \n \n Experience in working on, backend and ML systems for large-scale user-facing products, and have a good understanding of how they all work \n Experience in closely collaborating with other engineering teams to ship ML technologies to improve fairness and reduce bias for recommendation, content understanding, and ranking systems at scale \n \n This position is not eligible for relocation assistance. \n  #LI-SA1 \n  #LI-REMOTE \n \n \n  At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. \n  Information regarding the culture at Pinterest and benefits available for this position can be found  here . \n \n  US based applicants only \n \n    $187,000\u2014$385,000 USD\n   \n \n \n  Our Commitment to Diversity: \n  Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.", "cleaned_desc": " Familiarity with state of the art ML approaches like transformers, self supervised pre-training, generative modeling, LLMs, etc. \n Experience with large scale data processing (e.g. Hive, Scalding, Spark, Hadoop, Map-reduce) \n Hands-on experience building, deploying, and measuring ML Fairness at a very large scale. Successful candidates in this role need to be able to build bridge state of the art approaches to real world impact \n 8+ years working experience in the engineering teams that build large-scale ML-driven user-facing products \n 3+ years experience leading cross-team engineering efforts \n Strong execution skills in project management \n Strong communication skills to communicate strategy to senior management and outside of the company, externally \n Masters or PhD in Comp Sci or related fields \n Understanding of an object-oriented programming language (Java, C++, Python) \n ", "techs": ["transformers", "self supervised pre-training", "generative modeling", "llms", "hive", "scalding", "spark", "hadoop", "map-reduce", "ml fairness", "engineering teams", "user-facing products", "project management", "communication skills", "masters or phd in comp sci or related fields", "object-oriented programming language (java", "c++", "python)"]}, "78eaff2340fd9e6f": {"terms": ["data science"], "salary_min": 60.0, "salary_max": 70.0, "title": "ServiceNow Virtual Agent Analyst", "company": "Laiba Technologies", "desc": "Position: ServiceNow Virtual Agent Analyst \n Location: Phoenix, AZ - Remote, USA \n \u25cf Proven experience as a Business Analyst, with exposure to virtual agent or chatbot projects. \n \u25cf Familiarity with the ServiceNow platform and its Virtual Agent capabilities. \n \u25cf Strong analytical skills and the ability to translate complex business processes into clear and actionable virtual agent conversation designs. \n \u25cf Understanding of natural language processing (NLP) concepts and their application in designing effective virtual agent interactions. \n \u25cf Experience with process mapping, requirements gathering, and documentation. \n \u25cf Excellent communication skills, both written and verbal, with the ability to effectively liaise between technical and non-technical stakeholders. \n \u25cf Detail-oriented mindset with a focus on delivering high-quality solutions that meet business needs \n \u25cf Collaborate with business stakeholders to gather and understand requirements for virtual agent implementations across various business units. \n \u25cf Analyze existing business processes and user interactions to identify opportunities for automation and improved user experiences through virtual agents. \n \u25cf Translate business requirements into virtual agent conversational flows, intents, and decision trees using ServiceNow's Virtual Agent Designer. \n \u25cf Work closely with technical teams to ensure the accurate configuration and integration of virtual agent conversations within the ServiceNow platform. \n \u25cf Leverage your understanding of natural language processing (NLP) to design intuitive and user-friendly virtual agent responses that align with business objectives. \n \u25cf Conduct testing and quality assurance of virtual agent interactions, identifying and resolving any issues or discrepancies in functionality or accuracy. \n \u25cf Collaborate with UI/UX designers to ensure the virtual agent interface is visually appealing, intuitive, and aligned with the organization's branding. \n \u25cf Assist in user acceptance testing (UAT) by creating test scenarios, collecting feedback, and validating the virtual agent's performance against business requirements. \n \u25cf Continuously monitor and analyze virtual agent performance, suggesting iterative improvements to enhance effectiveness and user satisfaction. \n \u25cf Provide end-user training and support to ensure seamless adoption of virtual agent solutions across different business functions. \n \u25cf Stay informed about industry best practices, emerging trends, and advancements in virtual agent technology and business process automation. \n Job Types: Contract, Full-time \n Pay: $60.00 - $70.00 per hour \n Experience level: \n \n 10 years \n 9 years \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Experience: \n \n Virtual Agent: 4 years (Preferred) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "ccb2fcc65a559235": {"terms": ["data science"], "salary_min": 69781.45, "salary_max": 88358.875, "title": "Sr. Risk and Quality Analyst - Remote", "company": "Optum", "desc": "Opportunities with Optum in the Tri-State region  (formerly CareMount Medical, ProHEALTH New York and Riverside Medical Group). Come make a difference in the lives of people who turn to us for care at one of our hundreds of locations across New York, New Jersey and Connecticut. Work with state-of-the-art technology and brilliant co-workers who share your passion for helping people feel their best. Join a dynamic health care organization and discover the meaning behind \n   Caring. Connecting. Growing together. \n \n \n \n  The primary goal of the Sr. Risk & Quality Analyst is to use data science and analytics to support the improved health of our patients and the value-based performance of our physicians. This individual will report to the Manager of Risk Adjustment Analytics and will work with business leads to design, validate, produce, and distribute highly visible patient outreach and risk adjustment reporting. The Sr. Analyst is responsible for supporting patient outreach programs through development of Outreach and Risk Adjustment analytics and tracking with the aim to stand-up and evaluate programs through key performance indicators. The Analyst will also work closely with IT and Decision Support teams to institutionalize best-in-class analytics and scale reporting. The position requires solid analytic and organizational skills, initiative, ability to work independently, and a willingness to help develop a robust patient outreach reporting infrastructure. This position will require collaboration with data owners and functions across the organization. \n \n \n  You\u2019ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges. \n \n \n \n  Primary Responsibilities: \n \n \n Participating in the development of analytical tools and performance reports that will monitor organizational progress against key patient outreach metrics \n Providing required analyses in order to understand the performance of patient outreach programs \n Designing and constructing data tables, schemas, dashboards, and visualizations related to patient outreach program reporting and tracking \n Managing data from key third-party patient outreach vendors \n Developing, validating, and distributing member lists for internal patient outreach team and third-party vendors for various patient outreach initiatives \n Performing hands-on tasks, including data acquisition from disparate systems, data transformation and analysis, benchmarking, and communicating insights \n Performing data validation tests to ensure data completeness and accuracy \n Collaborating with IT, Decision Support, and external vendors to identify infrastructural improvement opportunities and develop new solutions that improve patient outreach performance \n Collaborating with Clinical Operations team, including providers, managers, staff and administration to analyze performance, design and implement work flow processes to achieve efficiency and effectiveness \n Performing ad-hoc analyses as assigned \n \n \n \n \n   You\u2019ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.\n  \n \n \n  Required Qualifications: \n \n \n Bachelor\u2019s degree in quantitative disciplines (mathematics, statistics, economics, etc.), or related field \n 2+ years of experience working in an analytics or consulting role in managed care, population health, or health insurance \n Experience working with large databases, e.g. health insurance claims databases, data warehousing, and/or database administration \n Applied experience with EMR data & clinical analytic processes \n Applied experience with patient outreach programs and customer relations management (CRM) platforms like Salesforce a decided advantage \n Demonstrated proficiency in SQL (SSMS, preferred) and Excel \n Experience with SSRS/SSIS packages, Python, R, Azure, Alteryx, or Visualization (Tableau/Power BI) \n Proven solid analytical and problem-solving skills \n Experience with healthcare analytics, informatics, and population health concepts \n Proven solid understanding of Medicare Advantage plans and value-based incentive programs \n Experience leading other analysts in developing capabilities \n Demonstrated solid communication skills, including an ability to communicate with staff at various levels, including both front line staff and senior management \n \n \n \n \n  Preferred Qualifications: \n \n \n Master\u2019s degree \n \n \n \n \n  California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only:  The salary range for California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island or Washington residents is $67,800 to $133,100. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you\u2019ll find a far-reaching choice of benefits and incentives.\n  \n \n \n \n All employees working remotely will be required to adhere to UnitedHealth Group\u2019s Telecommuter Policy. \n \n \n \n \n  At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone\u2013of every race, gender, sexuality, age,  \n location and income\u2013deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes \u2014 an enterprise priority reflected in our mission. \n \n \n \n \n  Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action  \n employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law. \n \n \n \n \n  UnitedHealth Group is a  \n drug -  \n free workplace. Candidates are required to pass a drug test before beginning employment.", "cleaned_desc": " \n  Required Qualifications: \n \n \n Bachelor\u2019s degree in quantitative disciplines (mathematics, statistics, economics, etc.), or related field \n 2+ years of experience working in an analytics or consulting role in managed care, population health, or health insurance \n Experience working with large databases, e.g. health insurance claims databases, data warehousing, and/or database administration \n Applied experience with EMR data & clinical analytic processes \n Applied experience with patient outreach programs and customer relations management (CRM) platforms like Salesforce a decided advantage \n Demonstrated proficiency in SQL (SSMS, preferred) and Excel \n Experience with SSRS/SSIS packages, Python, R, Azure, Alteryx, or Visualization (Tableau/Power BI) \n Proven solid analytical and problem-solving skills \n Experience with healthcare analytics, informatics, and population health concepts \n Proven solid understanding of Medicare Advantage plans and value-based incentive programs \n Experience leading other analysts in developing capabilities \n Demonstrated solid communication skills, including an ability to communicate with staff at various levels, including both front line staff and senior management ", "techs": ["salesforce", "sql (ssms)", "excel", "ssrs/ssis", "python", "r", "azure", "alteryx", "tableau/power bi"]}, "ec93f16b97d6d645": {"terms": ["data science", "machine learning engineer"], "salary_min": 226688.75, "salary_max": 287038.47, "title": "Principal ML Engineer, ML Fairness and Inclusive AI - Advanced Technologies Group", "company": "Pinterest", "desc": "About Pinterest :  \n Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. \n  Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences. \n  Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our  PinFlex  landing page to learn more.  \n \n The Advanced Technologies Group (ATG) is Pinterest\u2019s advanced machine learning team. The goal of the team is to keep Pinterest at the forefront of machine learning technology across multiple use cases including recommendations, ranking, content understanding, and more. It is an applied team that works horizontally across the company on state of the art AI and ML and works on directly bringing that technology to the product in collaboration with product engineering teams. The team does publish its work in applied research conferences, but the main goal of the team is to have top line metric impact. \n  The goal of Pinterest is to inspire pinners to live the life they love. The product is powered by state of the art ML algorithms which are used to understand both the billions of visually rich items on the platform and the interests of our 400M+ monthly active users and recommend inspiring personalized content to them. What this means is that ML at Pinterest is not only multi-modal utilizing image, text, and graph as input, it needs to operate at a very large scale and often in a real time interactive user experience. \n  Pinterest is known for being a positive and inspirational place on the internet and we believe that inspiration starts with inclusion. The Inclusive AI team at Pinterest is a horizontal team that works across the company on both ML Fairness and user facing features which make our product more inclusive so we can better serve our diverse user base. This includes our launches of groundbreaking features such as user controllable skin tone and hair pattern filters and more recently body type. \n  We are looking for a  Tech Lead Architect who can who can work with the team to drive cross-team engineering efforts in the Inclusive AI / ML Fairness space,  work with senior leaders at the company to lead and set our technical strategy in this space, and also communicate and work with external groups regarding this work. You'll have the opportunity to work on various innovative projects of new product experiences, build large-scale low-latency systems and state-of-the-art machine learning models, and deliver great impact to our pinners and business metrics. \n  What you'll do: \n \n Lead projects that involve developing and deploying state of the art ML models in production systems across the company \n Collaborate with other engineering teams (infra. user modeling, content understanding) to leverage their platforms and signals and work with them to collaborate on the adoption and evaluation of Inclusive AI and ML Fairness \n Mentor junior engineers on the Inclusive AI team and across the company on Inclusive AI / ML Fairness and help them to built that into their workflow \n Work with the team and senior leaders at the company to define and drive technical strategy in this area \n Communicate externally about this work and with external groups including industry, advocacy, and government bodies regarding this work \n \n What we're looking for: \n \n Experience with state of the art ML Fairness techniques \n Familiarity with state of the art ML approaches like transformers, self supervised pre-training, generative modeling, LLMs, etc. \n Experience with large scale data processing (e.g. Hive, Scalding, Spark, Hadoop, Map-reduce) \n Hands-on experience building, deploying, and measuring ML Fairness at a very large scale. Successful candidates in this role need to be able to build bridge state of the art approaches to real world impact \n 8+ years working experience in the engineering teams that build large-scale ML-driven user-facing products \n 3+ years experience leading cross-team engineering efforts \n Strong execution skills in project management \n Strong communication skills to communicate strategy to senior management and outside of the company, externally \n Masters or PhD in Comp Sci or related fields \n Understanding of an object-oriented programming language (Java, C++, Python)  \n \n Desired skills: \n \n Experience in working on, backend and ML systems for large-scale user-facing products, and have a good understanding of how they all work \n Experience in closely collaborating with other engineering teams to ship ML technologies to improve fairness and reduce bias for recommendation, content understanding, and ranking systems at scale \n \n This position is not eligible for relocation assistance. \n  #LI-SA1 \n  #LI-REMOTE \n \n \n  At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. \n  Information regarding the culture at Pinterest and benefits available for this position can be found  here . \n \n  US based applicants only \n \n    $187,000\u2014$385,000 USD\n   \n \n \n  Our Commitment to Diversity: \n  Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support. \n \n \n  Not Specified \n  0", "cleaned_desc": " Familiarity with state of the art ML approaches like transformers, self supervised pre-training, generative modeling, LLMs, etc. \n Experience with large scale data processing (e.g. Hive, Scalding, Spark, Hadoop, Map-reduce) \n Hands-on experience building, deploying, and measuring ML Fairness at a very large scale. Successful candidates in this role need to be able to build bridge state of the art approaches to real world impact \n 8+ years working experience in the engineering teams that build large-scale ML-driven user-facing products \n 3+ years experience leading cross-team engineering efforts \n Strong execution skills in project management \n Strong communication skills to communicate strategy to senior management and outside of the company, externally \n Masters or PhD in Comp Sci or related fields \n Understanding of an object-oriented programming language (Java, C++, Python)  \n ", "techs": ["transformers", "self supervised pre-training", "generative modeling", "llms", "hive", "scalding", "spark", "hadoop", "map-reduce", "ml fairness", "state of the art approaches", "large-scale ml-driven user-facing products", "cross-team engineering efforts", "project management", "communication skills", "masters", "phd", "comp sci", "object-oriented programming language", "java", "c++", "python"]}, "4a55dc10f741ca1c": {"terms": ["data science", "machine learning engineer"], "salary_min": 176000.6, "salary_max": 222855.97, "title": "Senior Machine Learning Engineer - Remote", "company": "Atlassian", "desc": "Working at Atlassian \n \n \n \n  Atlassians can choose where they work \u2013 whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company. \n \n \n \n  Your future team\n   \n \n    The Central AI org is part of the larger Atlassian Intelligence program. Its purpose is to accelerate AI innovation across all our products and platform, provide cohesive AI experiences and setup up an Atlassian AI infrastructure for the future.\n   \n \n    While it's anticipated that the majority of AI-driven programs will be developed and executed by federated product teams within Atlassian, the role of Central AI is important to this process. Central AI is tasked with constructing the underlying infrastructure and capacities, which are crucial for the seamless integration and optimal functionality of AI across various departments. By doing so, Central AI ensures that different teams within the organization do not work in silos but have access to a unified foundation that promotes efficiency and collaboration. Additionally, Central AI is responsible for developing some of the core shared experiences typical in the AI domain, such as search, knowledge discovery and conversation.\n   \n \n    As a Senior Machine Learning engineer, you will work on the development and implementation of the cutting edge machine learning algorithms, training sophisticated models, collaborating with product, engineering, and analytics teams, to build the AI functionalities into each Atlassian products and services. Your daily responsibilities will encompass a broad spectrum of tasks such as designing system and model architectures, conducting rigorous experimentation and model evaluations, and providing guidance to junior ML engineers. Your role is pivotal, stretching beyond these tasks, ensuring AI's transformative potential is realized across our offerings.\n   \n \n \n \n What you'll do \n \n \n  Masters in a quantitative subject (Statistics, Mathematics, Computer Science, Operations Research, or relevant work experience) \n  3+ years of related industry experience in the data science domain \n  Expertise in Python or Java with and the ability to write performant production-quality code, familiarity with SQL, knowledge of Spark and cloud data environments (e.g. AWS, Databricks) \n  Experience building and scaling machine learning models in business applications using large amounts of data \n  Explain data science concepts to diverse audiences, craft a compelling story \n  Focus on business practicality and the 80/20 rule; very high bar for output quality, but recognize the business benefit of \"having something now\" vs \"perfection sometime in the future\" \n  Agile development mindset, appreciating the benefit of constant iteration and improvement \n \n \n \n \n \n \n It's great, but not required, if you have \n \n \n  Experience working in a consumer or B2C space for a SaaS product provider, or the enterprise/B2B space \n  Experience in developing deep learning-based models and working on LLM-related applications \n  Excelling in solving ambiguous and complex problems, being able to navigate through uncertain situations, breaking down complex challenges into manageable components and developing innovative solutions \n \n \n \n \n \n \n Compensation \n \n \n    At Atlassian, we strive to design equitable and explainable compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience.\n   \n \n    In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:\n   \n \n    Zone A: $199,400 - $265,800\n   \n \n    Zone B: $179,400 - $239,200\n   \n \n    Zone C: $165,500 - $220,600\n   \n \n    This role may also be eligible for benefits, bonuses, commissions, and equity.\n   \n \n    Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.\n   \n \n    #LI-Remote\n   \n \n \n  Our perks & benefits \n \n \n \n  Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit \n    go.atlassian.com/perksandbenefits \n   to learn more.\n   \n \n \n  About Atlassian \n \n \n \n  At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.\n   \n \n \n  We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.\n   \n \n \n  To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.\n   \n \n \n  To learn more about our culture and hiring process, visit \n    go.atlassian.com/crh \n .", "cleaned_desc": " \n What you'll do \n \n \n  Masters in a quantitative subject (Statistics, Mathematics, Computer Science, Operations Research, or relevant work experience) \n  3+ years of related industry experience in the data science domain \n  Expertise in Python or Java with and the ability to write performant production-quality code, familiarity with SQL, knowledge of Spark and cloud data environments (e.g. AWS, Databricks) \n  Experience building and scaling machine learning models in business applications using large amounts of data \n  Explain data science concepts to diverse audiences, craft a compelling story \n  Focus on business practicality and the 80/20 rule; very high bar for output quality, but recognize the business benefit of \"having something now\" vs \"perfection sometime in the future\" \n  Agile development mindset, appreciating the benefit of constant iteration and improvement \n \n \n \n \n \n \n It's great, but not required, if you have \n \n \n  Experience working in a consumer or B2C space for a SaaS product provider, or the enterprise/B2B space ", "techs": ["python", "java", "sql", "spark", "aws", "databricks"]}, "235af1490eaa8fb0": {"terms": ["data science", "machine learning engineer"], "salary_min": 203778.53, "salary_max": 258029.03, "title": "Sr Staff Machine Learning Engineer, Search Relevance", "company": "Pinterest", "desc": "About Pinterest : \n  Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. \n  Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences. \n  Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our  PinFlex  landing page to learn more. \n \n  Pinterest helps more than 450 million people discover new ideas to help design their life. Our users come to Pinterest to explore ideas and run more than 6 billion search queries every month. Many of these queries represent exploratory search intent and are broad, which means that the search system should be able to deeply understand this intent, and then help people explore content, personalize results, harness visual signals effectively, and show the most engaging content up front. In addition, the team also owns query refinements and modules that help Pinners narrow their search intent from broad exploratory queries, to help them narrow their results down. All of this means that Pinterest search presents a unique challenge quite unlike other search systems and the opportunity to innovate on a product that only Pinterest can build.       \n We are looking for an exceptional staff machine learning engineer lead to drive and work on Search-related problems such as engagement and relevance modeling, query understanding, learned retrieval, and other NLP/Rec-Sys areas on Search. \n \n \n  What you will do: \n \n You will be responsible for leading the machine learning strategy and projects in Pinterest search, including indexing and document ranking, query and content understanding, personalization, ML based retrieval, shopping, videos, as well as infrastructure efficiency and scalability. In addition, the candidate will work on innovative applications of NLP and other techniques to drive query recommendations, autocompletions, and query based module generation and ranking.   \n Work closely with the other engineering teams in Pinterest to bring superior search experience to our users, such as content signal, research and infrastructure.   \n Bring cutting-edge research and industry knowledge into the team around NLP, information retrieval, machine learning, generative AI and related areas   \n Mentor and grow junior engineers on the team.   \n \n What we are looking for: \n \n 8-10 years of experience leading and working on a large-scale production search, recommendation or ads systems that are based on state-of-the-art machine learning and big data technology.   \n Applied machine learning experience is strongly preferred. Experience in related fields such as recommendation systems, natural language processing, generative AI is a bonus.   \n Ability to drive roadmap and directions of scalable production quality systems end-to-end.   \n A knack for product and impact on users of a consumer product. \n \n \n   \n This position is not eligible for relocation assistance. \n \n \n  #LI-HYBRID #LI-AK7 \n \n \n  At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. \n  Information regarding the culture at Pinterest and benefits available for this position can be found  here . \n \n  US based applicants only \n \n    $159,000\u2014$278,000 USD\n   \n \n \n  Our Commitment to Diversity: \n  Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.", "cleaned_desc": " What we are looking for: \n \n 8-10 years of experience leading and working on a large-scale production search, recommendation or ads systems that are based on state-of-the-art machine learning and big data technology.   \n Applied machine learning experience is strongly preferred. Experience in related fields such as recommendation systems, natural language processing, generative AI is a bonus.   \n Ability to drive roadmap and directions of scalable production quality systems end-to-end.   \n A knack for product and impact on users of a consumer product. \n \n ", "techs": ["state-of-the-art machine learning", "big data technology", "recommendation systems", "natural language processing", "generative ai"]}, "1952aab9b3a7ca2c": {"terms": ["data science"], "salary_min": 150000.0, "salary_max": 175000.0, "title": "Sr. Python Developer - Data Engineering", "company": "Ayar Labs", "desc": "Sr. Python Developer - Data Engineering\n  \n \n  Location: Santa Clara, CA or Remote\n  \n \n  We are seeking a Python Developer to design, build, and maintain the code necessary for the automated storage, processing, and analysis of measurement data and design information. You will be responsible for the entire lifecycle of data pipeline development. Projects typically begin with small, manually generated input datasets with a wide variety of formats, and end with highly automated and rigorously defined pipelines. In addition, you will drive the adoption of data format specifications from manufacturing suppliers, as well as adhere to data format specifications from customers.\n   \n  The Test Team is responsible for systems that collect measurements for the Design Teams (R&D, product development), Manufacturing and Operations (quality control screens), and Reliability Teams (product qualification, ongoing reliability testing). You will collaborate with Test Engineers to automate raw-to-parameteric data processing in hands-off pipelines and provide Python tools and frameworks for data producers and consumers.\n   \n  Responsibilities:\n  \n \n Developing, maintaining, and scaling data pipelines: Designing, developing, and maintaining ETL pipelines to extract data from various sources, transform it, and load it into a data warehouse or other data storage systems. \n Ensuring data quality and integrity: Ensuring the accuracy, consistency, and reliability of the data by developing and implementing data validation and testing strategies. \n Collaborating with product engineers, data scientists, and analysts: Understand their data needs and help them to access and analyze data effectively. \n Planning software projects: Developing schedules, communicating status effectively, and completing projects on time \n Maintaining a clean code base: releasing quality documentation, helping with Python code reviews, keeping commit history clean and well-structured, engaging in refactoring projects of existing code, willing to learn and own some parts of gitlab CI/CL fundamentals \n \n \n  Basic Requirements:\n  \n \n Bachelor\u2019s degree in Computer Science, Software Engineering, Applied Physics or Engineering \n 3 to 5 years of python development professional experience \n Expert in Python and Pandas \n Experience scoping, planning, and scheduling software projects \n Comfortable using git to manage contributions to a software project \n Excellent communication and collaboration skills - must be able to present plans, gather feedback, and proactively seek information relevant to assigned projects. \n \n \n  Preferred Qualifications:\n  \n \n Proficiency with SQL and relational database systems \n Domain-specific knowledge in electronics or photonics test and measurement, or background in electronic or photonic experiments \n Experience with MongoDB \n Familiarity with cloud computing platforms such as AWS, Azure, or Google Cloud Platform. \n \n \n \n \n    Pay Range is $150K to $175K\n    \n \n \n    NOTE TO RECRUITERS:\n    \n \n    Principals only. We are not accepting resumes from recruiters for this position. Remuneration for recruiting activities is only applicable subject to a signed and executed agreement between the parties. Please don\u2019t send candidates to Ayar Labs, and do not contact our managers.\n    \n \n \n \n \n \n About Ayar Labs: \n \n \n \n     At Ayar Labs we\u2019re about to revolutionize computing by moving data with light. We\u2019re unleashing processing power for artificial intelligence, high performance computing, cloud and telecommunications by removing the bottlenecks created by today\u2019s electrical I/O - making it possible to continue scaling computing system performance. Ayar Labs is the first to deliver an optical I/O solution that combines in-package optical I/O chiplets and multi-wavelength remote light sources to replace traditional electrical I/O. This silicon photonics-based I/O solution enables chips to communicate with each other from millimeters to kilometers, to deliver orders of magnitude improvements in latency, bandwidth density, and power consumption.\n      \n  With our strong collaborations with industry leaders and government, our deep ties to MIT and UC Berkeley, and our commitment to hiring the best engineers in photonics and electronics, joining our team gives you the opportunity to collaborate with renowned experts on challenging, paradigm-shifting work.\n      \n  We are passionate about delivering in-package optical I/O at scale, leveraging the strength of our patent portfolio and our team of leading interdisciplinary experts. We believe that deep cross-collaboration between teams facilitated by honest, open debate is the best way to drive innovation and achieve big wins. Join our team and experience the possibilities.\n      \n \n Resources: \n \n \n \n Executives from Intel and GLOBALFOUNDRIES share their thoughts on Ayar Labs and the promise of in-package optical I/O (video) \n Ayar Labs in the News and Recent announcements \n LinkedIn and Twitter \n \n \n \n \n          Ayar Labs is an Affirmative Action/Equal Opportunity Employer and is strongly committed to all policies which will afford equal opportunity employment to all qualified persons without regard to age, national origin, race, ethnicity, creed, gender, disability, veteran status, or any other characteristic protected by law.", "cleaned_desc": "Sr. Python Developer - Data Engineering\n  \n \n  Location: Santa Clara, CA or Remote\n  \n \n  We are seeking a Python Developer to design, build, and maintain the code necessary for the automated storage, processing, and analysis of measurement data and design information. You will be responsible for the entire lifecycle of data pipeline development. Projects typically begin with small, manually generated input datasets with a wide variety of formats, and end with highly automated and rigorously defined pipelines. In addition, you will drive the adoption of data format specifications from manufacturing suppliers, as well as adhere to data format specifications from customers.\n   \n  The Test Team is responsible for systems that collect measurements for the Design Teams (R&D, product development), Manufacturing and Operations (quality control screens), and Reliability Teams (product qualification, ongoing reliability testing). You will collaborate with Test Engineers to automate raw-to-parameteric data processing in hands-off pipelines and provide Python tools and frameworks for data producers and consumers.\n   \n  Responsibilities:\n  \n \n Developing, maintaining, and scaling data pipelines: Designing, developing, and maintaining ETL pipelines to extract data from various sources, transform it, and load it into a data warehouse or other data storage systems. \n Ensuring data quality and integrity: Ensuring the accuracy, consistency, and reliability of the data by developing and implementing data validation and testing strategies.   Collaborating with product engineers, data scientists, and analysts: Understand their data needs and help them to access and analyze data effectively. \n Planning software projects: Developing schedules, communicating status effectively, and completing projects on time \n Maintaining a clean code base: releasing quality documentation, helping with Python code reviews, keeping commit history clean and well-structured, engaging in refactoring projects of existing code, willing to learn and own some parts of gitlab CI/CL fundamentals \n \n \n  Basic Requirements:\n  \n \n Bachelor\u2019s degree in Computer Science, Software Engineering, Applied Physics or Engineering \n 3 to 5 years of python development professional experience \n Expert in Python and Pandas \n Experience scoping, planning, and scheduling software projects \n Comfortable using git to manage contributions to a software project \n Excellent communication and collaboration skills - must be able to present plans, gather feedback, and proactively seek information relevant to assigned projects. \n   \n  Preferred Qualifications:\n  \n \n Proficiency with SQL and relational database systems \n Domain-specific knowledge in electronics or photonics test and measurement, or background in electronic or photonic experiments \n Experience with MongoDB \n Familiarity with cloud computing platforms such as AWS, Azure, or Google Cloud Platform. \n \n \n \n \n    Pay Range is $150K to $175K\n    \n ", "techs": ["python", "pandas", "git", "sql", "mongodb", "aws", "azure", "google cloud platform"]}, "cdb73dbc3011b6a4": {"terms": ["data science", "machine learning engineer"], "salary_min": 105296.336, "salary_max": 133328.62, "title": "Machine Learning Lead Engineer - Startup Founder", "company": "Founding Teams", "desc": "Founding Teams  is a stealth AI Tech Incubator. We are supporting the next generation of AI startup founders with the resources they need including engineering, product, sales, marketing, and operations staff to create and launch their products with equity-only compensated talent. \n We are looking for lead AI engineers who have a startup idea they would like to launch on our upcoming platform/incubator. \n Please review our landing page  www.foundingteams.com \n The ideal candidate will have a passion for next-generation AI tech startups and working with great global startup talent. \n AI Startup Founder - Lead Machine Learning Engineer \n Job Description - \n - Build prototype AI/ ML models and tools to help us understand our customers and create personalized customer recommendations across multiple use cases and productize solutions to scale \n - Deeply understand customers, their behaviors and pain points, and develop a diversity of AI models addressing an array of customers\u2019 needs \n - Translate business needs into AI/ML problems and create innovative solutions to advance our business goals \n - Determine the types and amount of data needed and work with the data engineer to identify data sources and ingest into data lake \n - Structure, standardize, and annotate data into processable formats for ML; enrich data with necessary attributes to allow sophisticated personalization \n - Help shape the way our data science team does work - researching and making key decisions about what we build, how we build it, and which tools are best for solving our problems \n - Work alongside software and data engineers to implement data processing and visualization systems that make data readily available and simplify how insights are communicated \n - Evaluate the performance of AI models and make tradeoffs against quality metrics \n Investigate, and resolve performance issues in a timely manner \n Requirements \n - Bachelor\u2019s or Master\u2019s degree in Mathematics, ML, statistics, Computer Science, Software/Data Engineering, or a related field \n - Strong mathematical background in probability, statistics, and optimization algorithms. \n - Experience in building machine learning models and deploying them to production to make real decisions, then measuring the impact of these decisions. \n - Deep understanding of and have applied various machine learning techniques for solving real-world problems. \n - Expertise with advanced programming skills in Python, Java or any of the major languages to build robust algorithms \n - Proficient with SQL and can work \u201cfull stack\u201d to integrate solutions with our data ecosystem \n - Confident in taking ownership of projects from start to finish and enjoy the process of turning nebulous ideas into reality \n - Excellent communication skills \n - A self-starter who drives projects and builds strong relationships with stakeholders and teams to tackle large cross-functional efforts \n - Thrive with minimal guidance and process \n - Worked in both small teams/incubators and large corporations \n Job Type: Part-time \n Salary: $100.00 - $150,000.00 per year \n Application Question(s): \n \n Are you able to launch your AI startup on foundingteams.com within the next few months? \n \n Work Location: Remote", "cleaned_desc": " - Work alongside software and data engineers to implement data processing and visualization systems that make data readily available and simplify how insights are communicated \n - Evaluate the performance of AI models and make tradeoffs against quality metrics \n Investigate, and resolve performance issues in a timely manner \n Requirements \n - Bachelor\u2019s or Master\u2019s degree in Mathematics, ML, statistics, Computer Science, Software/Data Engineering, or a related field \n - Strong mathematical background in probability, statistics, and optimization algorithms.   - Experience in building machine learning models and deploying them to production to make real decisions, then measuring the impact of these decisions. \n - Deep understanding of and have applied various machine learning techniques for solving real-world problems. \n - Expertise with advanced programming skills in Python, Java or any of the major languages to build robust algorithms \n - Proficient with SQL and can work \u201cfull stack\u201d to integrate solutions with our data ecosystem \n - Confident in taking ownership of projects from start to finish and enjoy the process of turning nebulous ideas into reality \n - Excellent communication skills ", "techs": ["- python\n- java\n- sql"]}, "3822bf6896809814": {"terms": ["data science", "data engineer", "machine learning engineer"], "salary_min": 141596.64, "salary_max": 179292.89, "title": "Software Engineer - 2 (Cyber Data Collection/Python)", "company": "Akina, Inc.", "desc": "TS/SCI - Polygraph required \n  04-8092-SWE\n    **Potential, part-time telework depending on mission needs\n   \n \n Description: \n  The program is seeking a Software Engineer to support a highly visible cyber sensor and analytic modernization program. The candidate will work with a team to analyze and characterize large data sets, create data visualizations, and identify anomalous/interesting behavior. The successful candidate will have experience with analysis in the cyber domain and discovery operations, and have experience with algorithm design and development in cloud environments.\n   \n  The Software Engineer develops, maintains, and enhances complex and diverse software systems (e.g., processing-intensive analytics, novel algorithm development, manipulation of extremely large data sets, real-time systems, and business management information systems) based upon documented requirements. Works individually or as part of a team. Reviews and tests software components for adherence to the design requirements and documents test results. Resolves software problem reports. Utilizes software development and software design methodologies appropriate to the development environment. Provides specific input to the software components of system design to include hardware/software trade- offs, software reuse, use of Commercial Off-the-shelf (COTS)/Government Off-the-shelf (GOTS) in place of new development, and requirements analysis and synthesis from system level to individual software components.\n   \n \n Required Skills: \n  \u2022 Knowledge of cyber data collection, data flow and extraction techniques such as pcap, network logs, netflow\n    \u2022 Expertise with python (including relevant data science libraries) and data visualization tools (e.g., Kibana, Tableau)\n    \u2022 Experience with machine learning techniques on both structured and unstructured data (I.e. clustering, topic modeling)\n    \u2022 Familiarity with Spark or other distributed analytic platforms\n   \n \n Qualifications: \n  Fourteen (14) years experience as a SWE in programs and contracts of similar scope, type, and complexity is required. \n    Bachelor\u2019s degree in Computer Science or related discipline from an accredited college or university is required. \n    Four (4) years of additional SWE experience on projects with similar software processes may be substituted for a bachelor\u2019s degree.\n   \n \n Akina is a Woman Owned, Service Disabled, Veteran Owned, Small Business, looking for talented and ambitious individuals to join our team. We offer a generous compensation package that includes 24 days PTO accrued annually and 11 federal holidays. Our 401k is 100% vested on your start date and the company makes a direct contribution worth 10% of your salary. Akina covers 100% of healthcare costs for employees and 50% toward dependents. We offer educational assistance towards college classes and will cover costs associated with job related training and certifications \n  Akina is committed to excellence and creating innovative and flexible solutions for our clients. We are a small company with an open ear to our employees' needs in order to attract and retain quality talent that enables our customer's mission \n . \n  www.akina-inc.com/careers", "cleaned_desc": " \n Required Skills: \n  \u2022 Knowledge of cyber data collection, data flow and extraction techniques such as pcap, network logs, netflow\n    \u2022 Expertise with python (including relevant data science libraries) and data visualization tools (e.g., Kibana, Tableau)\n    \u2022 Experience with machine learning techniques on both structured and unstructured data (I.e. clustering, topic modeling)", "techs": ["pcap", "network logs", "netflow", "python", "data science libraries", "data visualization tools", "kibana", "tableau", "machine learning techniques", "structured data", "unstructured data", "clustering", "topic modeling"]}, "340740f6c447da8e": {"terms": ["data science", "machine learning engineer"], "salary_min": 190162.39, "salary_max": 240787.95, "title": "Sr. Staff Data Scientist, Ecosystem", "company": "Pinterest", "desc": "About Pinterest : \n  Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. \n  Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences. \n  Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our  PinFlex  landing page to learn more. \n \n  We are looking for a Senior Staff Data Scientist for Ecosystem. You will shape the future of people-facing and business-facing products we build at Pinterest. Your expertise in quantitative modeling, experimentation and algorithms will be utilized to solve some of the most complex engineering challenges at the company. You will collaborate on a wide array of product and business problems with a diverse set of cross-functional partners across Product, Engineering, Design, Research, Product Analytics, Data Engineering and others. The results of your work will influence and uplevel our product development teams while introducing greater scientific rigor into the real world products serving hundreds of millions of pinners, creators, advertisers and merchants around the world. \n  What you'll do: \n \n Build a deeper understanding of our Pinterest ecosystem. This person will work with Core and Monetization leaderships and will require a very senior person to effectively cut through the noise and influence across pods and orgs. Sample projects: \n \n Tradeoff between organic and ads \n Relationship between engagement metrics & monetization \n The value of repin behaviors to purchase decisions \n Levers to move enterprise metrics \n \n Develop best practices for instrumentation and experimentation and communicate those to product engineering teams to help us fulfill our mission - to bring everyone the inspiration to create a life they love \n Bring scientific rigor and statistical methods to the challenges of product creation, development and improvement with an appreciation for the behaviors of our Pinners \n Build and prototype analysis pipelines iteratively to provide insights at scale while developing comprehensive knowledge of data structures and metrics, advocating for changes where needed for product development \n Work cross-functionally to build and communicate key insights, and collaborate closely with product managers, engineers, designers, and researchers to help build the next experiences on Pinterest \n \n What we're looking for \n \n 10+ years of experience analyzing data in a fast-paced, data-driven environment with proven ability to apply scientific methods to solve real-world problems on web-scale data \n Extensive experience solving analytical problems using quantitative approaches including in the fields of Machine Learning, Statistical Modelling, Forecasting, Econometrics or other related fields \n Experience using machine learning and deep learning frameworks, such as PyTorch, TensorFlow or scikit-learn \n A scientifically rigorous approach to analysis and data, and a well-tuned sense of skepticism, attention to detail and commitment to high-quality, results-oriented output \n Ability to manipulate large data sets with high dimensionality and complexity; fluency in SQL (or other database languages) and a scripting language (Python or R) \n Excellent communication skills and ability to explain learnings to both technical and non-technical partners \n A team player who's able to partner with cross-functional leadership to quickly turn insights into actions \n \n \n \n  This position is not eligible for relocation assistance. \n \n \n  #LI-REMOTE \n  #LI-JT6 \n \n \n  At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. \n  Information regarding the culture at Pinterest and benefits available for this position can be found  here . \n \n  US based applicants only \n \n    $158,950\u2014$327,000 USD\n   \n \n \n  Our Commitment to Diversity: \n  Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.", "cleaned_desc": " Work cross-functionally to build and communicate key insights, and collaborate closely with product managers, engineers, designers, and researchers to help build the next experiences on Pinterest \n \n What we're looking for \n \n 10+ years of experience analyzing data in a fast-paced, data-driven environment with proven ability to apply scientific methods to solve real-world problems on web-scale data \n Extensive experience solving analytical problems using quantitative approaches including in the fields of Machine Learning, Statistical Modelling, Forecasting, Econometrics or other related fields \n Experience using machine learning and deep learning frameworks, such as PyTorch, TensorFlow or scikit-learn \n A scientifically rigorous approach to analysis and data, and a well-tuned sense of skepticism, attention to detail and commitment to high-quality, results-oriented output \n Ability to manipulate large data sets with high dimensionality and complexity; fluency in SQL (or other database languages) and a scripting language (Python or R) ", "techs": ["pytorch", "tensorflow", "scikit-learn", "sql", "python", "r"]}, "ad487903ec51acbe": {"terms": ["data science"], "salary_min": null, "salary_max": null, "title": "Digital Marketing Growth Manager - SEO Manager (Remote)", "company": "Lands' End", "desc": "People are the heart of our company, and our doors are open to everyone. As a customer, as an employee \u2013 we are committed to creating an inspiring culture that is welcoming, safe and inclusive for all. Our strength in work and life comes from the combination of our experiences, backgrounds and talents. It\u2019s what we do as people that makes Lands\u2019 End a great place to shop and a great place to work. \n  Lands' End is looking for a SEO Manager to help transform the owned plan and augment reach. You will effectively collaborate across multiple internal teams including product, creative, brand & PR, content, ecommerce, metrics and data science to deliver the Lands\u2019 End US SEO program to help grow brand awareness, new customers, and increase customer LTV. You are a creative, critical thinking leader and focused in your approach to plan development. You have thorough collaboration and technical SEO skills with a bias for action and the work ethic to advance the SEO team! \n  The SEO Manager is responsible for guiding the SEO team and accountable for the implementation of ecommerce SEO/content/link creating solutions for Lands' End (US). Demonstrated SEO plan ownership, project management and team guidance experience are essential, along with deep experience in online marketing. Staying on top of industry trends and search engine algorithms to inform new strategies is paramount. \n  Responsibilities \n \n Nurture SEO optimization strategies to augment Lands\u2019 End's target audience reach via an increase in search engine results on key nonbranded terms. Define measurable goals and track progress while translating the plan to tactical workstreams with the team. Partner cross functionally to bring strategies to life with a clear vision. \n Create meaningful internal relationships & consistent communication with cross-functional internal teams to promote awareness of programs, provide activity updates, and solicit feedback to deliver the right results for the business. \n Lead the SEO team with clear priorities, goals and workplans to ensure that they have the resources and support needed to be successful. \n Provide coaching, constructive feedback, development opportunities, and recognition to your team as appropriate. \n Identify opportunities to develop high potential staff through involvement in new projects, expanded accountabilities, promotions, and/or transfers. \n Support the SEO budget management to include forecasting and invoice oversight. Help to track monthly expense variances to optimize forecasting practices and enable spend accountability. Oversee any external providers. \n Work with Consumer Insights, Experience and Data Science to understand the consumer base, roadblocks and opportunities for a data-driven initiatives. \n Partner with Paid Media, Social, Brand, and PR teams to align workstreams and content to the holistic plan. \n Provide direction on content, for compelling and high-quality website content, including landing pages, blog posts and page descriptions. \n Update content and website links for maximum optimization and search engine rankings. \n Monitor daily performance metrics to understand SEO strategy performance. \n Perform keyword and technical SEO research, continuous site auditing, and recommend on-page optimization based on business goals for both existing functionality and new opportunities. \n Develop \u201cExecutive ready\u201d project deliverables and provide constant and clear communication of goals, status and results to all relevant stakeholders at various levels of the organization for key projects. \n Stay informed of trends and support the creation of competitive assessments to aid in developing near-mid-long-term strategies. \n Work closely with the content and development teams to create and optimize SEO-friendly content and landing pages. \n Conduct and present competitive analysis and industry research on a regular basis; analyze trends in the industry and competition; stay abreast of competitor strategies. \n Research and develop tactical strategies for content and tag development that are continuously enhanced. \n Explore external opportunities that we can utilize to improve our data and grow our audience reach. \n Work with the evergreen SEO team to improve our processes around scaling our strategies for SEO efforts like internal linking, achieving rich results, etc. \n Evangelizing and provide training for SEO across multiple departments, business groups and levels. \n Keep up-to-date with the latest SEO trends, strategies, and algorithm changes. \n Establish organization-wide SEO best practices, guidelines, and goals for other teams to adhere to. \n Partner with engineering and product to improve our SEO technology, identifying opportunities, requirements, and systems and reporting needed. \n \n Requirements \n \n Bachelor\u2019s degree in relevant field. \n 5-7+ years of progressive experience in Digital Marketing, eCommerce, or a related field. \n 5 years of experience in successfully managing a SEO team, developing, and overseeing SEO campaigns. \n Demonstrated success leading teams to hit financial performance goals. \n Demonstrated understanding of search engine algorithms and ranking methods. \n Experience with SEO including: Google Search Console, SEMRush, Conductor, MOZ, or aHrefs. \n Experience in keyword research and data mining. \n Demonstrated capability to complete analyses of other companies within the industry. \n Comfortable analyzing and synthesizing high volumes of search data. \n Familiarity with eCommerce websites or other CMS content management systems. \n Experience with other aspects of marketing, such as paid media, creative and promotion, is a plus. \n Ability to operate in ambiguity, adapting to a changing environment to pivot scope and focus with ease. \n Demonstrated experience working cross-functionally with internal teams and external partners, including stakeholders at all levels, to strengthen results. \n Superb organization and guidance skills, including the capacity to guide multiple projects and teams at once. \n Meaningful communication skills, both written and verbal, including the ability to summarize and escalate issues. \n A thorough business acumen and commercially passionate mindset. \n \n \n  Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities \n  The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor\u2019s legal duty to furnish information. 41 CFR 60-1.35(c)", "cleaned_desc": " \n Bachelor\u2019s degree in relevant field. \n 5-7+ years of progressive experience in Digital Marketing, eCommerce, or a related field. \n 5 years of experience in successfully managing a SEO team, developing, and overseeing SEO campaigns. \n Demonstrated success leading teams to hit financial performance goals. \n Demonstrated understanding of search engine algorithms and ranking methods. \n Experience with SEO including: Google Search Console, SEMRush, Conductor, MOZ, or aHrefs. \n Experience in keyword research and data mining. \n Demonstrated capability to complete analyses of other companies within the industry. \n Comfortable analyzing and synthesizing high volumes of search data.   Familiarity with eCommerce websites or other CMS content management systems. \n Experience with other aspects of marketing, such as paid media, creative and promotion, is a plus. \n Ability to operate in ambiguity, adapting to a changing environment to pivot scope and focus with ease. \n Demonstrated experience working cross-functionally with internal teams and external partners, including stakeholders at all levels, to strengthen results. \n Superb organization and guidance skills, including the capacity to guide multiple projects and teams at once. \n Meaningful communication skills, both written and verbal, including the ability to summarize and escalate issues. \n A thorough business acumen and commercially passionate mindset. \n \n \n  Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities ", "techs": ["google search console", "semrush", "conductor", "moz", "ahrefs"]}, "5b803123d8342369": {"terms": ["data science"], "salary_min": 73787.18, "salary_max": 93431.01, "title": "Statistical Programmer 2", "company": "Worldwide Clinical Trials", "desc": "Requisition Number \n 7030 \n \n \n  Employment Type \n : \n Regular \n \n \n \n \n \n \n \n \n  Who we are \n \n   \n Worldwide Clinical Trials (Worldwide), a leading global contract research organization (CRO), works in partnership with biotechnology and pharmaceutical companies to create customized solutions that advance new medications \u2013 from discovery to reality. Anchored in our company\u2019s scientific heritage, our dedicated therapeutic focus on cardiovascular, metabolic, neuroscience, oncology, and rare diseases, is applied to develop flexible plans and solve problems quickly for our customers. \n \n   \n Our talented team of 3,000+ professionals spans 60+ countries. We are united in cause with our customers to improve the lives of patients through new and innovative therapies. \n \n \n \n Why Worldwide \n \n   \n We believe everyone plays an important role in making a world of difference for patients and their caregivers. From our hands-on, accessible leaders, to our cohesive and supportive teams, we are committed to enabling professionals from all backgrounds and experiences to succeed. We prioritize cultivating a diverse and inclusive environment that continues to promote collaboration and creativity. We are proud to be a workplace where people thrive by being themselves and are inspired to do their best work every day. Join us! \n What Global Programming does at Worldwide \n The Worldwide team is an experienced and diverse group of Programmers who collaborate together as one, both via regularly scheduled group meetings to discuss issues, and impromptu one-on-one discussions between colleagues to discuss a particular topic. \n As the Lead Statistical Programmer on a project \u2013 you will be working directly with statisticians, sponsors, the wider study team and your own team of programmers, ensuring the data and output delivered for a study meets industry, regulatory submission and quality standards. Being an expert programmer remains a core to the role. As a Statistical Programmer at Worldwide, your analytical skills and ability to both program and understand / interpret data are the keys to success, and you will have the opportunity to continually grow your knowledge in SAS and CDISC across all phases and multiple therapy areas. \n What you will do \n \n Serve as a programming project lead on multiple complex studies (e.g. adaptive design, integrated safety/efficacy study) to distribute and oversee tasks for the programming team, communicate with internal and external clients, plan and execute delivery, and manage resources and competing project priorities effectively. Ensure budget and scope of project work remain aligned. \n Actively seek, propose and lead process improvement initiatives including development of standard SAS Macros, inhouse programming standards. \n Develop, test and execute SAS programs to produce and validate CDISC SDTM and ADaM datasets, tables, figures and listings (TFL). Develop and review specification for SDTM datasets and ADaM datasets for safety data, TFL shells and other specifications, e.g. patient profiles, OPS reports. \n Perform review and provide guidance on the development of the clinical database specification, data transfer agreement/specification, specification of tables, figures and listings (TFL) shells. Develop and review electronic data submission package (SDTM annotated CRF, define.xml, Study/Analysis Data Reviewer's Guide) for high complexity studies. \n \n What you will bring to the role \n \n Must be computer literate and numerate with a willingness to adapt to various computer systems. \n Hands-on expert level project statistical programmer experienced in providing programming leadership to projects. \n Statistical programming skills and knowledge across a broad range of applications together with key competencies in customer focus, delivering on commitments, building strong relationships, communicating and influencing, and embracing innovation and change. \n \n Your experience \n \n Educated to degree level or equivalent. \n Advanced SAS programming skills with expert knowledge in SAS/Macro, Proc Report and ODS. Good working knowledge in SAS/Graph, Proc SQL, SAS/STAT. \n \n We love knowing that someone is going to have a better life because of the work we do. \n \n   \n To view our other roles, check out our careers page at www.worldwide.com/careers! For more information on Worldwide, visit www.Worldwide.com or connect with us on LinkedIn. \n \n \n \n \n \n \n \n     Worldwide is an equal opportunity employer that is committed to enabling professionals from all backgrounds and experiences to succeed and, to that end, we prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and creativity. We provide equal employment opportunities to all employees and applicants regardless of race, color, ethnicity, ancestry, religion, national origin, gender, sex, gender identity or expression, sexual orientation, age, citizenship, marital or parental status, disability, military status, or other class protected by applicable law. Worldwide is committed to working with and providing reasonable accommodations to applicants with disabilities. We are proud to be an equal opportunity workplace where people thrive by being themselves and are inspired to do their best work every day.", "cleaned_desc": " Develop, test and execute SAS programs to produce and validate CDISC SDTM and ADaM datasets, tables, figures and listings (TFL). Develop and review specification for SDTM datasets and ADaM datasets for safety data, TFL shells and other specifications, e.g. patient profiles, OPS reports. \n Perform review and provide guidance on the development of the clinical database specification, data transfer agreement/specification, specification of tables, figures and listings (TFL) shells. Develop and review electronic data submission package (SDTM annotated CRF, define.xml, Study/Analysis Data Reviewer's Guide) for high complexity studies. \n \n What you will bring to the role \n \n Must be computer literate and numerate with a willingness to adapt to various computer systems. \n Hands-on expert level project statistical programmer experienced in providing programming leadership to projects. \n Statistical programming skills and knowledge across a broad range of applications together with key competencies in customer focus, delivering on commitments, building strong relationships, communicating and influencing, and embracing innovation and change. \n \n Your experience \n \n Educated to degree level or equivalent. ", "techs": ["sas programs", "cdisc sdtm", "adam datasets", "tfl", "sdtm datasets", "adam datasets", "safety data", "tfl shells", "patient profiles", "ops reports", "clinical database specification", "data transfer agreement/specification", "electronic data submission package", "sdtm annotated crf", "define.xml", "study/analysis data reviewer's guide"]}, "fcf5c62a32e8c563": {"terms": ["data science"], "salary_min": 110000.0, "salary_max": 110000.0, "title": "Auditor, Technical Trainer (Outpatient/Specialty - IRF/SNF)", "company": "Cotiviti", "desc": "Overview: \n  \n   This role is part of the training team for the Clinical Chart Validation solution. This position is responsible for improving the technical effectiveness of our teams by planning, developing, and delivering technical training, mentoring, and assessment. The individual will work collaboratively with subject matter experts in the Commercial & Government Audit Teams, Quality Assurance, Concept Development, and others to validate workflows and communication tools to enhance audit productivity, performance, and client satisfaction.\n  \n \n \n  For this role we would love to find someone with SNF and/or IRF coding & clinical knowledge. This is a remote position (US).\n   Responsibilities: \n  \n  Training, Development, and Mentoring. \n \n \n  Assess job-specific needs and develop technical training plans with clear business objectives, including working with subject matter experts, developing training materials, and developing appropriate assessments and measurements of success. \n  Select training/instructional methods and procedures appropriate for the situation when learning or teaching new skills. \n  Deliver specific training sessions, including using suitable delivery methods such as classroom, online, and webinar. Identify the development needs of others and coach, mentor, or otherwise assist others with improving their knowledge skills. \n  Provide support to the CCV audit team members; assist with orientation of new members as needed, mentor new team members after orientation. \n  Promote audit accuracy measures by training/educating and mentoring the auditor and providing documented and validated findings. \n  Encourage critical thinking and discussion among team members on concepts as needed. \n  Provide training on one or more of the following audit types: DRG & Clinical Validation, Short Stay, to include Milliman, InterQual, Readmissions,  Outpatient and Specialty Review Types (IRF, SNF, etc.). \n  Train Clinicians with coding certifications on coding principles. \n  Confer with management, and conducts surveys to identify training needs based on projected production processes, changes, and other factors. \n  Participate in weekly/monthly team meetings to share best practices initiatives and recommend audit vulnerabilities. \n  Support the Medical Director to ensure accurate assessments of improper payments are based on consistent application of clinical guidelines. \n \n \n  Assess customer/provider/stakeholder issues, complaints, and compliments. \n \n \n  Monitor/Assess performance of self, other individuals, &/or organizations to make improvements or recommend remediation or corrective action. \n  Work with the Quality Team to train audit team members on findings from quality review audits. \n  Develop testing and evaluation procedures. Evaluate instructor performance and the effectiveness of training programs, providing recommendations for improvements. Conduct or arrange for ongoing technical training and personal development classes for staff members. \n \n \n  Quality Assurance Controls. \n \n \n  Integrate healthcare auditing principles and uses objectivity in the performance of medical audit activities and reviews. \n  Draw on healthcare proficiency and industry knowledge to substantiate conclusions. \n  Perform work independently, review and interprets audit work of others. \n  Depending on nature and scope of the audit, may review medical records and apply in-depth knowledge of clinical criteria to determine medical necessity, appropriateness of setting, potential billing/coding issues, and quality concerns. \n  Demonstrate an understanding of complex contract specifications when performing medical record reviews. \n  Use healthcare expertise to determine approval or referral to the Medical Director. \n  Provide feedback on reviews to the Quality Assurance Manager as indicated in order to assist with the improvement of rationales sent to providers. \n \n \n  New Concepts and Processes. \n \n \n  Develop reasonable and effective recommendations for concept solutions that reflect an understanding of the client environment and risks inherent to our business and industry. \n  Suggest and or develop and implement new ideas, approaches, decision trees, and/or technological improvements that will support and optimize audit results. \n  Collaborate with Data Services in developing new reports. \n \n \n  Meets or Exceeds Standards/Guidelines for Productivity. \n \n \n  In addition to regular and predictable attendance, maintains production goals and quality standards set by the audit. \n  Performs QA audits against the expected level of quality and quantity (i.e. hit rate, # claims written, ID per hour). \n  Qualifications: \n  \n Associates Degree or equivalent relevant experience required. Bachelor\u2019s degree in Nursing, Healthcare Economics, Health Information Management, and/or Business, preferred, or 5 \u2013 7 years\u2019 relevant experience (experience in any of the following: claims auditing/quality assurance/recovery auditing). \n  Clinical /Nursing experience in an outpatient or specialty ( SNF or IRF preferred ) setting is a plus. \n  Coding certification required and maintained as a condition of employment. (CCS, CPC, etc.). Candidates who hold a CCDS will also be given consideration but will need to obtain an coding certification within 6 months. \n  5 to 7+ years of working with ICD-9/10CM, CPT, HCPCS, etc. \n  Experience with one or more of the following disciplines: SNF, IRF, APC, Home Health, ER, Diagnostics & Professional Service with a broad knowledge of medical claims billing/payment systems provider billing guidelines, payer reimbursement policies, medical necessity criteria and coding terminology. \n  Adherence to official coding guidelines, coding clinic determinations and CMS and other regulatory compliance guidelines and mandates. Requires expert coding knowledge - ICD-10, CPT, HCPCS codes. \n  Strong presentation skills. Comfortable in presenting/defending audit logic to client and key stakeholders (i.e. hospitals, physicians, validation contractors, auditing team, etc\u2026). \n  Independent thinker, logical, strategic, with a high focus and attention to detail. \n  Effective communication and presentation style (written and verbal) with proven ability to positively influence behavior and outcomes. \n  Knowledge of principles and methods for curriculum and training design, teaching and instruction for individuals and groups, and the measurement of training effects \n  Competent administrative and organizational skills, ability to multitask, set priorities, and meet deadlines. \n  Professional demeanor: Ability to creatively solve problems, deal with ambiguity, develop and implement policy and procedures, perform analysis and prepare reports, and foster team building. \n  High level of proficiency with all audit technology i.e., R3, CAT, etc. \n  Proficiency in Word, Access, Excel, PowerPoint and other applications. \n  Excellent written and verbal communication skills. \n \n \n  Work Environment \n \n \n  High-speed internet connection required \n  Alerts manager/team lead of systems issues or other issues impacting productivity. \n \n \n   Base compensation is $110,000.00. Specific offers are determined by various factors, such as experience, education, skills, certifications, and other business needs. This role is eligible for discretionary bonus consideration.\n  \n \n \n  Cotiviti offers team members a competitive benefits package to address a wide range of personal and family needs, including medical, dental, vision, disability, and life insurance coverage, 401(k) savings plans, paid family leave, 9 paid holidays per year, and 17-27 days of Paid Time Off (PTO) per year, depending on specific level and length of service with Cotiviti. For information about our benefits package, please refer to our Careers page.\n  \n \n \n  #LI-JB1\n  \n \n   #senior\n  \n \n   #LI-Remote\n  \n \n   #LI-JB1", "cleaned_desc": " Associates Degree or equivalent relevant experience required. Bachelor\u2019s degree in Nursing, Healthcare Economics, Health Information Management, and/or Business, preferred, or 5 \u2013 7 years\u2019 relevant experience (experience in any of the following: claims auditing/quality assurance/recovery auditing). \n  Clinical /Nursing experience in an outpatient or specialty ( SNF or IRF preferred ) setting is a plus. \n  Coding certification required and maintained as a condition of employment. (CCS, CPC, etc.). Candidates who hold a CCDS will also be given consideration but will need to obtain an coding certification within 6 months. \n  5 to 7+ years of working with ICD-9/10CM, CPT, HCPCS, etc. \n  Experience with one or more of the following disciplines: SNF, IRF, APC, Home Health, ER, Diagnostics & Professional Service with a broad knowledge of medical claims billing/payment systems provider billing guidelines, payer reimbursement policies, medical necessity criteria and coding terminology. \n  Adherence to official coding guidelines, coding clinic determinations and CMS and other regulatory compliance guidelines and mandates. Requires expert coding knowledge - ICD-10, CPT, HCPCS codes. \n  Strong presentation skills. Comfortable in presenting/defending audit logic to client and key stakeholders (i.e. hospitals, physicians, validation contractors, auditing team, etc\u2026). \n  Independent thinker, logical, strategic, with a high focus and attention to detail. \n  Effective communication and presentation style (written and verbal) with proven ability to positively influence behavior and outcomes. \n  Knowledge of principles and methods for curriculum and training design, teaching and instruction for individuals and groups, and the measurement of training effects \n  Competent administrative and organizational skills, ability to multitask, set priorities, and meet deadlines. \n  Professional demeanor: Ability to creatively solve problems, deal with ambiguity, develop and implement policy and procedures, perform analysis and prepare reports, and foster team building. \n  High level of proficiency with all audit technology i.e., R3, CAT, etc. \n  Proficiency in Word, Access, Excel, PowerPoint and other applications. \n  Excellent written and verbal communication skills. \n \n \n  Work Environment \n \n ", "techs": ["ccs", "cpc", "ccds", "icd-9/10cm", "cpt", "hcpcs", "snf", "irf", "apc", "icd-10", "r3", "cat", "word", "access", "excel", "powerpoint"]}, "0b984c170e7349b9": {"terms": ["data science", "data analyst"], "salary_min": 90000.0, "salary_max": 95000.0, "title": "Senior Actuarial Analyst", "company": "Accolade", "desc": "About Accolade \n Accolade (Nasdaq: ACCD) provides millions of people and their families with an exceptional healthcare experience that is personal, data driven and value based to help every person live their healthiest life. Accolade solutions combine virtual primary care, mental health support and expert medical opinion services with intelligent technology and best-in-class care navigation. Accolade\u2019s Personalized Healthcare approach puts humanity back in healthcare by building relationships that connect people and their families to the right care at the right time to improve outcomes, lower costs and deliver consumer satisfaction. Accolade consistently receives consumer satisfaction ratings over 90%. For more information, visit accolade.com. \n Role overview \n The  Sr   Actuarial Analyst  is responsible for supporting the reporting and savings calculations for various customers within a Market Segment (i.e. Enterprise, Strategic, Mid-Market, Health Plan, etc). This reporting will showcase Accolade\u2019s value & outcomes and will be communicated to their manager, their peers and other internal partners such as Customer Partnerships or Operations. They will create ad-hoc reporting as directed and analyze data to identify new variances or areas of opportunity. \n A day in the life\u2026 \n \n Support analytics & reports that produce key metrics, and highlight areas where Accolade creates value and identify new opportunities for value creation. \n Work with Product Management teams to develop & outline the requirements for reporting needs, support the construction of reports and validate that final reports meet needs and provide guidance on how to use those reports to key partners. \n Using analytic and technical skills, identify opportunities and design solutions to better automate Actuarial processes or to compare/contrast results across Accolade\u2019s book of business. \n Support customers with customized analysis, unique to their population or health programs and communicate those findings both internally and externally. \n Support monthly production analysis of Accolade\u2019s Savings results, including validating financial reconciliation, analyzing results for outlier patterns, and communicating summarized results to internal and external partners. \n Analyze healthcare spend and utilization patterns to uncover drivers of results with large and small populations. \n Evaluate the programs provided by Accolade\u2019s partners for the impact they have on cost and utilization. This would include telemedicine, second opinion services, cost transparency, etc. Create reporting and communicate results. \n \n What we are looking for\u2026 \n \n Bachelor\u2019s Degree in a field such as: Actuarial Science, Risk Management, Data Science, Mathematics, Statistics or related field. \n Associate in Society of Actuaries preferred. \n 3+ years of experience working with analyzing claims and healthcare data. \n Excellent Excel & PowerPoint skills. \n Good/Excellent SQL skills. \n Working knowledge of AWS Redshift. \n Good written and interpersonal communication skills. \n Strong technical skills with deep understanding of actuarial concepts and ability to apply them to work product. \n Independent worker: courage, decision quality, listening, drive for results, customer and client focus, integrity and ability to deal with ambiguity. \n Consistently demonstrate ability to influence and negotiate with internal partners. \n The ability to organize, manage and prioritize multiple projects at one time. \n Inquisitive, analytical, problem-solving in nature. \n Data-driven, detail-oriented and fact-based; focused on getting the best answer for customers and clients. \n Hands-on, action-oriented style. \n Proven ability to roll up your sleeves and make a contribution quickly. \n A team player capable of working effectively with individuals throughout the organization. \n \n Hiring Range \n $90400-$103300 \n Annual \n Actual compensation packages within that range are based on a wide array of factors unique to each candidate, including but not limited to skill set, years and depth of experience, certifications, and specific location. \n Benefits \n \n Comprehensive medical, dental, vision, life, and disability benefits, including access to Accolade Advocacy, Accolade Care, and Accolade EMO. \n HDHP medical plan with generous employer contributions towards an HSA \n 401(k) Retirement Plan with matching employer contributions \n Open Time Off \n Generous Holiday Schedule + 5 floating holidays \n 18 weeks of paid parental leave \n Subsidized commuter benefits programs \n Virtual access to coaching, self-care activities, and video-based therapy and psychiatry through Ginger \n 1 Volunteer days per year \n Employee Stock Purchase Plan (ESPP) w/ employee discount \n \n We strongly encourage you to be vaccinated against COVID-19. \n What is important to us... \n Creating an enduring company that is hyper-focused on our culture and making a meaningful impact in the lives of our employees, members and customers. The secret to our success is: \n We find joy and purpose in serving others \n Making a difference in our members\u2019 and customers\u2019 lives is what we do. Even when it\u2019s hard, we do the right thing for the right reasons. \n We are strong individually and together, we\u2019re powerful \n Trusting in our colleagues and embracing their different backgrounds and experiences enable us to solve tough problems in creative ways, having fun along the way. \n We roll up our sleeves and get stuff done \n Results motivate us. And we aren't afraid of the hard work or tough decisions needed to get us there. \n We\u2019re boldly and relentlessly reinventing healthcare \n We're curious and act big -- not afraid to knock down barriers or take calculated risks to change the world, one person at a time. \n Accolade is an Equal Opportunity and Affirmative Action Employer committed to advancing an inclusive environment for all qualified applicants and employees. We provide employment opportunities, without regard, to any legally protected status in accordance with applicable laws in the US. We are committed to help ensure you have a comfortable and positive interview experience. \n Accolade, Inc., PlushCare, Inc., and Accolade 2ndMD LLC will never ask you to pay to get a job. Anyone who does this is a scammer. Further, we will never send you a check and ask you to send on part of the money or buy gift cards with it. These are also scams. If you see or lose money to a job scam, report it to the Federal Trade Commission at ReportFraud.ftc.gov. You can also report it to your state attorney general. \n To review our policy around data use, visit our Accolade Privacy Policy Page. All your information will be kept confidential according to EEO guidelines. \n Accolade \n Job Type: Full-time \n Pay: $90,000.00 - $95,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n Life insurance \n Paid time off \n Tuition reimbursement \n Vision insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Supplemental pay types: \n \n Bonus opportunities \n \n Work Location: Remote", "cleaned_desc": " Associate in Society of Actuaries preferred. \n 3+ years of experience working with analyzing claims and healthcare data. \n Excellent Excel & PowerPoint skills. \n Good/Excellent SQL skills. \n Working knowledge of AWS Redshift. \n Good written and interpersonal communication skills. \n Strong technical skills with deep understanding of actuarial concepts and ability to apply them to work product. \n Independent worker: courage, decision quality, listening, drive for results, customer and client focus, integrity and ability to deal with ambiguity. \n Consistently demonstrate ability to influence and negotiate with internal partners. \n The ability to organize, manage and prioritize multiple projects at one time. \n Inquisitive, analytical, problem-solving in nature. \n Data-driven, detail-oriented and fact-based; focused on getting the best answer for customers and clients. \n Hands-on, action-oriented style. \n Proven ability to roll up your sleeves and make a contribution quickly. \n A team player capable of working effectively with individuals throughout the organization. \n \n Hiring Range ", "techs": ["associate in society of actuaries", "excel", "powerpoint", "sql", "aws redshift"]}, "a9667236ee6b2980": {"terms": ["data science", "data analyst"], "salary_min": 91785.0, "salary_max": 144034.0, "title": "Business Analyst/Lead Tester", "company": "Definitive Logic", "desc": "Definitive Logic is seeking a Business Analyst/Tester to support System and User Acceptance Testing (UAT) by verifying the broadest scope of functional, operational, interface and performance requirements creating actual end user experiences. The Business Analyst/Tester should be familiar with defect management system repositories and to be able to create complex test data records. The Business Analyst/Tester will be assigned to one large complex multi-year project. This role requires excellent written and verbal communication skills and client relationship building/management skills.\n  \n Roles and Responsibilities \n \n  Provide a range of subject matter and consulting expertise to support the development and execution of user testing sessions \n  Read, understand, and analyze business requirements; identify test scenarios; define test plans; create test cases and scripts; execute tests; identify & report defects and report outcome of test events \n  Manage multiple operations, ensuring quality standards and work performance \n  Develop and maintain plans to organize and oversee work efforts, providing risk management and ensuring quality management \n  Excel in oral and written communications (briefings, presentations, and strategic documentation)  \n Assist in day-to-day client relationships and communications \n  Prepare progress reports and client briefings \n  Prepare quality project deliverables \n  Help to plan and execute program within client governance structure \n \n  Required Qualifications \n \n  5+ years of professional experience in appropriate field \n  3+ years of supervising a team of testers \n  Experience on large transformation, system integration, and/or architecture contracts \n  Understanding of Acceptance Testing as part of Software Development Life Cycle and the differences between need for QA testing and UAT testing \n  Experience using Test Management tools \n  Strong analytical skills with ability to define, collect, and analyze data, establish facts, draw valid conclusions, and make logical decisions \n  Strong organization and prioritization skills \n  Excellent written, verbal and presentation skills \n  Ability to obtain a Secret clearance \n \n  Desired Qualifications \n \n  Experience performing on Federal government consulting projects \n  Experience in Federal Business System planning (financial management programs preferred) \n  COTS Business Systems experience \n  OneStream platform knowledge  \n Experience working with Jira and test management plug-ins (e.g. Xray, Zephyr) \n  Ability to work in a dynamic, results-driven environment \n  Ability to work effectively with diverse individuals across functional disciplines \n \n  Education \n \n  Bachelor\u2019s degree in Computer Science, Information Technology, Business, or related field OR Bachelor\u2019s degree in finance, accounting, or other field related to Federal financial management, with demonstrated software testing experience in ERP or CPM systems OR 7+ years of relevant experience \n \n \n  About Definitive Logic \n \n \n \n \n   Definitive Logic (DL) is a management and technology consulting firm known for delivering outcomes and ROI for agencies\u2019 most complex business challenges. DL delivers performance-based and outcome-driven technology consulting solutions that directly support the strategic intent of our Defense, Homeland Security, Emergency Management, Federal Civilian and Commercial clients. We\u2019re the preferred technology integration partner for Federal agencies to apply the best of data science, app dev, DevSecOps, cyber and cloud solutions to improve decision support, empower front-line employees and enhance back-office operations. We serve as trusted advisors providing objective, fact-based, vendor & technology-neutral consulting services.\n  \n \n \n \n   Definitive Logic is ultimately a team of problem solvers \u2014 thought leaders, domain experts, coders, data enthusiasts, and technophiles. Our exciting projects and learning and sharing culture have consistently resulted in validation as a Great Place to Work: 2023 Washington Post Top Workplaces (8-time winner) \\u007C 2023 Virginia Best Places to Work (10 years running, #1 midsize in 2019).\n  \n \n \n  Definitive Logic is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class.\n  \n \n \n  If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable, or limited in your ability, to use or access our Careers page: https://www.definitivelogic.com/careers/open-opportunities/ as a result of your disability. You can request a reasonable accommodation by sending an e-mail to Recruiting@DefinitiveLogic.com or via phone: 703-955-4186. In order to quickly respond to your request, please use the words \"Accommodation Request\" as your e-mail subject line.\n  \n \n \n  DL Benefits \n \n \n  Health \n \n \n  Dental \n \n \n  Vision \n \n \n  Life/AD&D:  Company paid \n  \n \n STD/LTD: Company paid\n  \n \n  Supplemental Plans:  TriCare Supplement, Pet Insurance through Nationwide, Legal Resources and hospital/accidental indemnity plans and Wellness initiatives.\n  \n \n \n \n  Compensation Benefits: \n \n \n  Competitive Base Salary \n \n \n  Annual performance based bonus \n \n \n  401(k) & Roth option: You are fully (100%) vested on day 1 and DL matches up to 5% \n \n \n  Spot Bonuses  \n \n \n Referral Bonuses \n \n \n \n \n  Additional Benefits: \n \n \n  Flexible Time Off (FTO):  Under our FTO plan, there is no cap in the amount of leave you choose to take, with proper coordination and prior approval.\n  \n \n  Volunteer Hours:  DL allocates up to 8 hours for you to use every year to volunteer for a 501c3 organization of your choice and DL will donate to that charity based on how many hours you volunteer.\n  \n \n  Cell Phone Reimbursement:  $80/month\n  \n \n  Location Specific Metro/Parking  \n \n \n Tuition Reimbursement  \n \n \n Training & Certifications", "cleaned_desc": "Definitive Logic is seeking a Business Analyst/Tester to support System and User Acceptance Testing (UAT) by verifying the broadest scope of functional, operational, interface and performance requirements creating actual end user experiences. The Business Analyst/Tester should be familiar with defect management system repositories and to be able to create complex test data records. The Business Analyst/Tester will be assigned to one large complex multi-year project. This role requires excellent written and verbal communication skills and client relationship building/management skills.\n  \n Roles and Responsibilities \n \n  Provide a range of subject matter and consulting expertise to support the development and execution of user testing sessions \n  Read, understand, and analyze business requirements; identify test scenarios; define test plans; create test cases and scripts; execute tests; identify & report defects and report outcome of test events \n  Manage multiple operations, ensuring quality standards and work performance \n  Develop and maintain plans to organize and oversee work efforts, providing risk management and ensuring quality management \n  Excel in oral and written communications (briefings, presentations, and strategic documentation)  \n Assist in day-to-day client relationships and communications \n  Prepare progress reports and client briefings \n  Prepare quality project deliverables \n  Help to plan and execute program within client governance structure \n \n  Required Qualifications \n \n  5+ years of professional experience in appropriate field \n  3+ years of supervising a team of testers \n  Experience on large transformation, system integration, and/or architecture contracts \n  Understanding of Acceptance Testing as part of Software Development Life Cycle and the differences between need for QA testing and UAT testing \n  Experience using Test Management tools \n  Strong analytical skills with ability to define, collect, and analyze data, establish facts, draw valid conclusions, and make logical decisions \n  Strong organization and prioritization skills \n  Excellent written, verbal and presentation skills    Ability to obtain a Secret clearance \n \n  Desired Qualifications \n \n  Experience performing on Federal government consulting projects \n  Experience in Federal Business System planning (financial management programs preferred) \n  COTS Business Systems experience \n  OneStream platform knowledge  \n Experience working with Jira and test management plug-ins (e.g. Xray, Zephyr) \n  Ability to work in a dynamic, results-driven environment \n  Ability to work effectively with diverse individuals across functional disciplines \n \n  Education \n \n  Bachelor\u2019s degree in Computer Science, Information Technology, Business, or related field OR Bachelor\u2019s degree in finance, accounting, or other field related to Federal financial management, with demonstrated software testing experience in ERP or CPM systems OR 7+ years of relevant experience \n \n \n  About Definitive Logic \n \n \n \n \n   Definitive Logic (DL) is a management and technology consulting firm known for delivering outcomes and ROI for agencies\u2019 most complex business challenges. DL delivers performance-based and outcome-driven technology consulting solutions that directly support the strategic intent of our Defense, Homeland Security, Emergency Management, Federal Civilian and Commercial clients. We\u2019re the preferred technology integration partner for Federal agencies to apply the best of data science, app dev, DevSecOps, cyber and cloud solutions to improve decision support, empower front-line employees and enhance back-office operations. We serve as trusted advisors providing objective, fact-based, vendor & technology-neutral consulting services.\n  ", "techs": ["business analyst/tester", "uat testing", "defect management system repositories", "test data records", "test management tools", "jira", "xray", "zephyr", "onestream platform"]}, "ed6299f14eeba35a": {"terms": ["data science", "data analyst"], "salary_min": 37181.715, "salary_max": 47080.34, "title": "Salesforce Business Analyst Co-Op", "company": "Field Aerospace", "desc": "Field Aerospace is a leading aerospace company dedicated to providing cutting-edge aircraft modifications and support to the US military and allied nations. With over 75 years of industry experience, we take pride in our commitment to innovation, technical excellence, and delivering solutions that empower missions and support our valued clients. \n  General Duties: \n  We are seeking a dynamic and enthusiastic Salesforce Intern to join our team. You will assist in supporting optimizing our use of Salesforce, developing dashboards, building reports, cleaning our data, develop training documentation all aimed at providing business incites based on data for our business development teams. This internship is an excellent opportunity to gain practical experience in Salesforce and a dynamic aerospace company. \n  Essential Job Functions: \n \n  Conduct research into how the system can be leveraged to provide greater value. \n  Support teams in troubleshooting issues with the system. \n  Create reporting and provide analysis of business development data. \n  Perform data cleansing exercises to help standardize the data which exists in the system. \n  Actively contribute to the value derived from the tool by creating documentation for end users and putting together trainings. \n \n  Skills and Experience: \n \n  Strong written and verbal communication skills, with attention to detail. \n  Familiarity with business systems (Salesforce is a bonus). \n  Proficient in Microsoft Office suite (Word, Excel, PowerPoint). \n  Creative thinker with the ability to generate and implement innovative ideas. \n  Self-motivated, proactive, and able to work independently as well as collaboratively. \n  Strong organizational and time management skills, capable of handling multiple tasks efficiently. \n \n  Competencies: \n \n  Integrity \n  Excellence \n  Collaboration and Team Work \n  Communicating Orally and in Writing \n  Adaptability \n  Initiative \n  Organization and Planning \n  Education \n  Exercising Self-Control and Being Resilient \n  Interpersonal Skills \n \n \n  Education: \n \n  Currently pursuing a degree in Business, Information Systems, Computer Science, Data Science, or a related field. \n \n  Physical Requirements: \n  The physical demands described herein are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. \n  While performing the duties of this job, the employee is regularly required to talk and hear, stand and sit most of the time. The employee is periodically required to use hands to finger, handle, or feel objects, tools, or controls and to climb/ascend ladders using feet and legs to balance. The employee is occasionally required to walk; reach with hands and arms; stoop, kneel, crouch or crawl. \n  The employee may occasionally lift and move up to 10 pounds. Specific vision abilities required by this job include close vision, peripheral vision, depth perception and the ability to adjust focus. \n  The employee is subject to environmental conditions. Protection from weather conditions but not necessarily from temperature changes. \n  Additional Notes: \n  In order to comply with Export Control Laws and the NISPOM, we must secure all governmental approvals that are required to authorize our workforce to work on our defense and government programs. To ensure we comply with these regulations in a manner that does not violate our equal opportunity employment/non-discrimination compliance obligations, Field maintains the following recruitment policy: \n \n  All applicants, including applicants that may work remotely, must be eligible to secure a U.S. security clearance. \n \n  Field Aerospace is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability or other characteristics protected by law.", "cleaned_desc": "  Actively contribute to the value derived from the tool by creating documentation for end users and putting together trainings. \n \n  Skills and Experience: \n \n  Strong written and verbal communication skills, with attention to detail. \n  Familiarity with business systems (Salesforce is a bonus). \n  Proficient in Microsoft Office suite (Word, Excel, PowerPoint). \n  Creative thinker with the ability to generate and implement innovative ideas. \n  Self-motivated, proactive, and able to work independently as well as collaboratively. ", "techs": ["salesforce", "microsoft office suite (word", "excel", "powerpoint)"]}, "bc3473f9d36e6c83": {"terms": ["data science", "machine learning engineer"], "salary_min": 128800.0, "salary_max": 193200.0, "title": "Senior Engineer, Software, Machine Learning", "company": "PacBio", "desc": "Job Title: Senior Engineer, Software, Machine Learning\n    Location: Remote, US or Europe\n  \n \n  Description:\n  \n \n   PacBio is dedicated to revolutionizing genomics with high-quality and long-read DNA sequencing. We are looking for a skilled Machine Learning Engineer to join our Platform Bioinformatics team and help us productize and improve cutting-edge deep-learning solutions. The ideal candidate will have a strong background in modern C++ programming, as well as experience with machine learning algorithms and tools.\n  \n \n  Responsibilities:\n  \n \n  Build, improve, and deploy machine learning models using TensorFlow and ONNXRuntime \n  Port existing models from Python to C++ for deployment on our sequencing instruments \n  Conduct data analysis and feature engineering to improve model performance \n  Collaborate with cross-functional teams to identify new use cases and implement ML solutions \n  Stay up-to-date with the latest advancements in machine learning and AI technology \n  Debug and troubleshoot issues with existing models \n  Write efficient and scalable code to support large-scale data processing on bare metal servers \n  Produce high quality documentation \n \n \n \n   Requirements:\n  \n \n  Typically requires a minimum of 4-8 years of related experience with a Bachelor's degree; or 2-6 years experience with Masters degree; or 0-5 years experience with a PhD \n  Degree in Computer Science, Mathematics, Statistics, or related field \n  Strong programming skills in Python and C++ and familiarity with TensorFlow and ONNXRuntime \n  Experience with deploying machine learning models on bare-metal linux servers \n  Daily experience with version control, git \n  Recognize the value of various testing approaches and how to apply them \n  Excellent verbal, written, and interpersonal communication skills \n  Able to motivate yourself and finish projects \n \n \n \n   Nice to have:\n  \n \n  Experience in developing machine learning models \n  Knowledge of statistics, linear algebra, and calculus \n \n \n \n   If you have a passion for ML and a desire to make a real impact on human health through genomics, we encourage you to apply.\n  \n \n  You may be required from time to time to visit and work at PacBio locations and for such times as the Company considers necessary for the proper performance of your duties.\n  \n \n  All listed tasks and responsibilities are deemed as essential functions to this position; however, business conditions may require reasonable accommodations for additional tasks and responsibilities.\n  \n \n  All qualified applicants will receive consideration for employment without regard to race, sex, color, religion, national origin, protected veteran status, or on the basis of disability, gender identity, and sexual orientation.\n  \n \n  #LI-Remote\n  \n \n   #LI-LV\n  \n \n \n  Salary Range: \n  $128,800.00 - $193,200.00\n  \n \n   Please be aware that, as a condition of employment, proof of COVID vaccination is required for all U.S.-based employees (subject to limited exceptions).\n  \n \n \n   To ensure the health and safety of all PacBio employees and our prospective candidates, we have instituted a virtual interview experience.\n   \n  To all Staffing and Recruiting Agencies: Our Careers Site is only for individuals seeking a job at PacBio. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. PacBio does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, PacBio employees or any other company location. PacBio is not responsible for any fees related to unsolicited resumes/applications.\n  \n \n  All qualified applicants will receive consideration for employment without regard to race, sex, color, religion, national origin, protected veteran status, or on the basis of disability, gender identity, and sexual orientation.\n  \n \n \n   If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at 650-521-8000, or \n   \n   http://careers@pacificbiosciences.com\n    for assistance.", "cleaned_desc": "Job Title: Senior Engineer, Software, Machine Learning\n    Location: Remote, US or Europe\n  \n \n  Description:\n  \n \n   PacBio is dedicated to revolutionizing genomics with high-quality and long-read DNA sequencing. We are looking for a skilled Machine Learning Engineer to join our Platform Bioinformatics team and help us productize and improve cutting-edge deep-learning solutions. The ideal candidate will have a strong background in modern C++ programming, as well as experience with machine learning algorithms and tools.\n  \n \n  Responsibilities:\n  \n \n  Build, improve, and deploy machine learning models using TensorFlow and ONNXRuntime \n  Port existing models from Python to C++ for deployment on our sequencing instruments \n  Conduct data analysis and feature engineering to improve model performance \n  Collaborate with cross-functional teams to identify new use cases and implement ML solutions    Stay up-to-date with the latest advancements in machine learning and AI technology \n  Debug and troubleshoot issues with existing models \n  Write efficient and scalable code to support large-scale data processing on bare metal servers \n  Produce high quality documentation \n \n \n \n   Requirements:\n  \n \n  Typically requires a minimum of 4-8 years of related experience with a Bachelor's degree; or 2-6 years experience with Masters degree; or 0-5 years experience with a PhD \n  Degree in Computer Science, Mathematics, Statistics, or related field \n  Strong programming skills in Python and C++ and familiarity with TensorFlow and ONNXRuntime \n  Experience with deploying machine learning models on bare-metal linux servers \n  Daily experience with version control, git \n  Recognize the value of various testing approaches and how to apply them \n  Excellent verbal, written, and interpersonal communication skills ", "techs": ["tensorflow", "onnxruntime", "python", "c++", "bare-metal linux servers", "version control (git)"]}, "89baafaff27ecc7b": {"terms": ["data science", "data engineer", "machine learning engineer"], "salary_min": 110000.0, "salary_max": 130000.0, "title": "Senior Data Engineer - Cloud Infrastructure and Data Lakes", "company": "Sports Info Solutions", "desc": "Title:  Senior Data Engineer - Cloud Infrastructure and Data Lakes \n  Department:  BIDS \n  Reports to:  Chief Data Scientist \n \n  About Sports Info Solutions \n  Pioneers in the Sports Data Industry \n  SIS was founded on the belief that decision making in sports could be improved and that we could help teams win more games through the use of better data, analytics, and technology. \n  That belief has been validated repeatedly since our founding in 2002 as we continue to revolutionize the way the game is played, both on and off the field. \n  Company overview \n  Our mission is to enrich and optimize the decision-making process for sports teams, sportsbooks, and sports fans. \n  We are proud to be a leader in collecting, analyzing and distributing the deepest data sets and insights to professional sports teams across the MLB, NBA and NFL. \n  We are now doubling down on what\u2019s made us successful by further advancing our data, technology, insights and partners as we drive forward the next innovations in Sports Data and Analytics. \n  Position overview \n  Sports Info Solutions (SIS) is looking for a new team member to fill a full time position in our BIDS department as Senior Data Engineer - Cloud Infrastructure and Data Lakes. \n  We seek a skilled and experienced Data Engineer to join our dynamic and growing team at Sports Info Solutions. As a Senior Data Engineer, you will be crucial in building and maintaining our cloud-based data infrastructure, designing and optimizing data lakes for structured and semi-structured data, and ensuring seamless data integration and preparation for machine learning training and inference. \n \n  This position is considered remote. \n \n  What you\u2019ll do as  Senior Data Engineer - Cloud Infrastructure and Data Lakes  on the team at SIS: \n \n  Includes (but is not limited to): \n \n \n \n  Design, develop, and maintain data pipelines for ingesting data from various sources into data lakes using proven distributed cloud technologies and other relevant tools. \n  Collaborate closely with the data science team to understand data requirements for machine learning training and inference and implement data transformations and preprocessing steps. \n  Perform complex ETL operations, data cleaning, and data enrichment to prepare data for machine learning models. \n  Continuously monitor and improve data pipelines and storage systems' efficiency, reliability, and scalability. \n  Collaborate with cross-functional teams to ensure smooth delivery of insights and predictions to clients, involving data quality assessment, transformation, and conversion processes. \n  Lead in migrating data from legacy on-premises SQL systems to cloud-based data lakes, optimizing data storage and access for scalability and performance. \n \n \n  Why work with SIS? \n  We believe in making sports better through data, analysis and insights. For that reason, we have an incredible team of technologists, scouts, analysts, and operators helping our partners win more games. \n  It is our ultimate vision to create an unparalleled platform of sporting data and insights, through best-in-class technology, products and partnerships. \n  We believe in a flexible, energetic, enjoyable working environment where we band together as teammates to do great things. We are committed to creating a diverse environment, working in a collaborative, team-centric environment. \n \n  Qualifications \n  If you possess the following, you are well on your way to making an impact at SIS: \n \n  Bachelor's degree in Computer Science, Engineering, a related field, or equivalent work experience; advanced degree is a plus. \n  Strong expertise in Apache Spark (preferred), Apache Flink, Apache Beam, or other distributed cloud compute processing system \n  Strong programming skills in languages such as Python (preferred), Java, Scala, Go. \n  Desire and ability to write maintainable, tested code. \n  Experience with data lake methodologies and cloud infrastructure technologies (e.g., AWS (preferred), Azure, GCP). \n  Proficiency in designing and implementing structured and semi-structured data pipelines using distributed cloud compute processing and related tools. \n  Proven experience with data migration from legacy on-premises SQL systems to cloud-based environments. \n  Familiarity with machine learning concepts and experience preparing data for training and inference purposes. Experience with feature stores is a plus. \n  Knowledge of graph databases is a plus. \n  Strong problem-solving skills and ability to work collaboratively in a fast-paced environment. \n  Excellent communication skills to convey complex concepts to technical and non-technical stakeholders. \n \n \n  EEO commitment \n  SIS provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, gender, national origin, age, veteran status, military status, disability, gender identity, sexual orientation, genetic information, or any other characteristic protected by law. In addition to federal law requirements, SIS complies with applicable state and local laws governing nondiscrimination in employment in every location where the company operates. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. \n  Abilities required \n  These physical demands are representative of the physical requirements necessary for an employee to successfully perform the essential functions of the job. Reasonable accommodation can be made to enable people with disabilities to perform the described essential functions of the job. \n  While performing the responsibilities of the job, the employee is required to talk and hear. The employee is often required to sit and use their hands and fingers, to handle or feel. The employee is occasionally required to stand, walk, reach with arms and hands, climb or balance, and to stoop, kneel, crouch or crawl. Vision abilities required by this job include close vision, including intensive computer usage. \n  Additional info \n  Sponsorship is not available for this position. Applicants must be currently authorized to work in the United States on a full-time basis. \n \n  Sports Info Solutions uses E-Verify and is an Equal Opportunity Employer. \n   \n   \n ndXdxN3La4", "cleaned_desc": "  Design, develop, and maintain data pipelines for ingesting data from various sources into data lakes using proven distributed cloud technologies and other relevant tools. \n  Collaborate closely with the data science team to understand data requirements for machine learning training and inference and implement data transformations and preprocessing steps. \n  Perform complex ETL operations, data cleaning, and data enrichment to prepare data for machine learning models. \n  Continuously monitor and improve data pipelines and storage systems' efficiency, reliability, and scalability. \n  Collaborate with cross-functional teams to ensure smooth delivery of insights and predictions to clients, involving data quality assessment, transformation, and conversion processes. \n  Lead in migrating data from legacy on-premises SQL systems to cloud-based data lakes, optimizing data storage and access for scalability and performance. \n \n \n  Why work with SIS? \n  We believe in making sports better through data, analysis and insights. For that reason, we have an incredible team of technologists, scouts, analysts, and operators helping our partners win more games. \n  It is our ultimate vision to create an unparalleled platform of sporting data and insights, through best-in-class technology, products and partnerships. \n  We believe in a flexible, energetic, enjoyable working environment where we band together as teammates to do great things. We are committed to creating a diverse environment, working in a collaborative, team-centric environment.   \n  Qualifications \n  If you possess the following, you are well on your way to making an impact at SIS: \n \n  Bachelor's degree in Computer Science, Engineering, a related field, or equivalent work experience; advanced degree is a plus. \n  Strong expertise in Apache Spark (preferred), Apache Flink, Apache Beam, or other distributed cloud compute processing system \n  Strong programming skills in languages such as Python (preferred), Java, Scala, Go. \n  Desire and ability to write maintainable, tested code. \n  Experience with data lake methodologies and cloud infrastructure technologies (e.g., AWS (preferred), Azure, GCP). \n  Proficiency in designing and implementing structured and semi-structured data pipelines using distributed cloud compute processing and related tools. \n  Proven experience with data migration from legacy on-premises SQL systems to cloud-based environments. \n  Familiarity with machine learning concepts and experience preparing data for training and inference purposes. Experience with feature stores is a plus. ", "techs": ["apache spark", "apache flink", "apache beam", "python", "java", "scala", "go", "aws", "azure", "gcp"]}, "28502ee624b77b4a": {"terms": ["data science"], "salary_min": 58.0, "salary_max": 67.0, "title": "Principal Statistical Programmer", "company": "Redbock", "desc": "Redbock is a specialized life science consulting firm that provides unique opportunities to candidates in the medical device, pharmaceutical, and biotechnology industries. We take the time to understand what both our clients and candidates are seeking, and work very hard to ensure we are providing everyone involved with an excellent level or service. We offer competitive pay, medical benefits, a 401k, as well as additional benefits to our Consultants. \n SENIOR/PRINCIPAL STATISTICAL PROGRAMMERS (x2) 6+ MONTHS RENEWABLE \n SUMMARY: A biopharmaceutical company in the Bay area needs the support of Senior Statistical Programmers for 6+ month renewable project. This is an organization focused on the treatment of rare genetic diseases and oncology therapeutics. The Senior Programmer must have a strong industry experience primarily on the Sponsor side and be able to independently create define.xml in the newest version of pinnacle 21 from scratch. Additionally, they should be able to develop SDTM and ADaM specifications (DDT) and program, develop and validate TFLs, have define package experience and ISS experience. They will also be responsible for programming activities for studies and submission activities, and be willing to work hands-on, supporting both the internal and CRO teams. \n QUALIFICATIONS & REQUIREMENTS: \n \n 12-15+ years of statistical programming experience in the pharma/biotech industries working for Sponsor companies \n Expertise with define.xml and able to create define.xml in the newest version of pinnacle 21 from scratch \n Must be able to independently develop SDTM and ADaM specifications (DDT) and program, develop and validate TFLs \n Must have worked on a submission within the last year or two \n Define package experience and ISS/ISE experience \n Experience leading and managing statistical programming activities for clinical trials and regulatory submissions \n Strong experience with regulatory submissions activities (BLA/NDA) \n \n DUTIES & RESPONSIBILITIES: \n \n Support statistical programming activities for clinical studies, ensure statistical programming deliverables are delivered within timelines \n Maintain statistical programming standards, processes, and SOPs \n Manage CRO and review deliverables for accuracy and compliance, performing perform acceptance check and validation \n Manage, review, and/or execute statistical programming deliverables for planned statistical analysis related to study monitoring, clinical study reports, data integrations, and regulatory query responses, covering multiple studies \n Generate TFLs to support ad hoc requests \n Review data management documents (CRF specification, data transfer agreements, DMP, annotated CRF), SDTM and ADaM specification, SAP, and TFL shells to provide statistical programming feedback \n \n LOCATION: Work will be performed remotely. Consultants can be located anywhere in the US \n Job Type: Contract \n Pay: $58.00 - $67.00 per hour \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n Life insurance \n Retirement plan \n Vision insurance \n \n Experience level: \n \n 11+ years \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Experience: \n \n Statistical programming: 10 years (Preferred) \n SDTM: 4 years (Preferred) \n regulatory submissions: 5 years (Preferred) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "c6188a4a51d90bcf": {"terms": ["data science"], "salary_min": 85648.4, "salary_max": 108449.96, "title": "Senior AI Governance Analyst", "company": "Altria", "desc": "Overview: \n  \n  Are you looking for an opportunity to join a Fortune 200 company where your expertise in data governance and its implication for Artificial Intelligence can shape the future use of AI in driving business strategy? If so, we want to speak with you! \n \n \n  We are currently recruiting for a \n   Senior AI Governance Analyst  to join our Advanced Analytics department in \n   Richmond, VA . or we are \n   open to remote work arrangements. \n \n \n  The Senior AI Governance Analyst will ensure the ethical, legal, and responsible use of artificial intelligence technologies within our organization. This role will play a critical role in defining and implementing policies, guidelines, and processes that align AI initiatives with regulatory requirements, industry best practices, and ethical standards. The ideal candidate is passionate about ensuring AI technologies benefit society while minimizing risks.\n  \n \n  What you'll be doing: \n \n \n Collaborate with cross-functional teams develop and communicate AI governance policies, including data privacy, bias mitigation, visibility, and accountability. \n Work with legal and compliance teams to ensure AI initiatives align with data protection laws and industry-specific regulations. \n Implement techniques to ensure fairness, equity, and transparency in AI decision-making processes. \n Collaborate with partners to develop risk mitigation strategies and action plans. \n Conduct training sessions and workshops to educate teams about AI governance principles and practices. \n Monitor changes in AI technologies and regulations to ensure ongoing compliance. \n Collaborate with AI developers, data scientists, legal teams, and business leaders to integrate AI governance practices throughout the development lifecycle. \n \n \n \n \n  What we want you to have: \n \n \n Bachelor's degree in Computer Science, Ethics, Law, or a related field. \n Proven experience (3+ years) in AI ethics, governance, compliance, or related roles. \n Familiarity with AI technologies, machine learning, and their societal implications. \n Strong understanding of data privacy regulations (e.g., GDPR, CCPA) and ethical AI frameworks. Intelligence  \n Excellent analytical skills to identify potential risks and propose effective solutions. \n Clear communication skills to convey complex ethical and technical concepts to various partners. \n Ability to collaborate with cross-functional teams. \n Knowledge of responsible AI principles, fairness metrics, and bias detection techniques. \n Previous experience in audit, compliance, or legal roles is advantageous. \n Certification in AI ethics or related fields is a plus. \n Limited domestic travel required \n \n \n \n  We recognize that our people are the reason we achieve our business goals. We believe in developing the leadership potential of our employees by providing them with opportunities for training, development, and advancement.\n  \n \n  In addition to the opportunity to apply and develop your skills toward key business objectives, we offer an excellent compensation package including a competitive base salary, comprehensive health/vision/dental insurance, participation in our incentive compensation and deferred profit-sharing programs, as well as a relocation assistance package.\n   Sponsorship: Immigration Sponsorship is not available for this role. Company Overview: Altria has a leading portfolio of tobacco products for U.S. tobacco consumers 21+. Our tobacco companies \u2013 which have been the undisputed market leaders in the U.S. tobacco industry for decades \u2013 include some of the most enduring names in American business. In combustibles, we own Philip Morris USA, the maker of Marlboro cigarettes and John Middleton, manufacturer of Black & Mild cigars. Our smoke-free portfolio includes ownership of U.S. Smokeless Tobacco Company, the maker of Copenhagen and Skoal, and Helix Innovations, the maker of on! oral nicotine pouches. Additionally, we have a majority-owned joint venture with JT Group, Horizon Innovations, for the U.S. marketing and commercialization of heated tobacco stick products. Through a separate agreement with Philip Morris International, we have the exclusive U.S. commercialization rights to the IQOS* Tobacco Heating System\u00ae and Marlboro HeatSticks\u00ae through April 2024. Our equity investments include Anheuser-Busch InBev SA/NV, the world\u2019s largest brewer and Cronos Group, a leading Canadian cannabinoid company. Each Altria company is an equal opportunity employer. We are committed to providing individuals with criminal records, including formerly incarcerated individuals and individuals with conviction records, a fair chance at employment. Learn more about Altria at www.altria.com and follow us on Twitter, Facebook and LinkedIn", "cleaned_desc": " \n  What we want you to have: \n \n \n Bachelor's degree in Computer Science, Ethics, Law, or a related field. \n Proven experience (3+ years) in AI ethics, governance, compliance, or related roles. \n Familiarity with AI technologies, machine learning, and their societal implications. \n Strong understanding of data privacy regulations (e.g., GDPR, CCPA) and ethical AI frameworks. Intelligence  \n Excellent analytical skills to identify potential risks and propose effective solutions.   Clear communication skills to convey complex ethical and technical concepts to various partners. \n Ability to collaborate with cross-functional teams. \n Knowledge of responsible AI principles, fairness metrics, and bias detection techniques. \n Previous experience in audit, compliance, or legal roles is advantageous. \n Certification in AI ethics or related fields is a plus. \n Limited domestic travel required \n \n \n ", "techs": ["bachelor's degree in computer science", "ethics", "law", "or a related field", "ai ethics", "governance", "compliance", "ai technologies", "machine learning", "data privacy regulations (e.g.", "gdpr", "ccpa)", "ethical ai frameworks", "analytical skills", "clear communication skills", "collaboration with cross-functional teams", "responsible ai principles", "fairness metrics", "bias detection techniques", "audit", "compliance", "legal roles", "certification in ai ethics or related fields", "limited domestic travel."]}, "9fee727bfd0e6df2": {"terms": ["data science"], "salary_min": null, "salary_max": null, "title": "Epidemiologist - US/Canada", "company": "PHASTAR", "desc": "Overview: \n  \n  THE COMPANY \n \n \n \n  Phastar is a multiple award-winning global biometric Contract Research Organization (CRO) that is accredited as an outstanding company to work for by Best Companies. We partner with pharmaceutical, biotechnology and medical device organizations to provide the expertise and processes to manage and deliver on time, quality biostatistics, programming, data management and data science services. With offices across the UK, US, Germany, Denmark, Kenya, Australia, India, China and Japan, Phastar is the second largest specialized biometrics provider globally, and the largest in the UK.\n  \n \n \n  Our unique approach to data analysis, \u201cThe Phastar Discipline\u201d, has led us to build a reputation for outstanding quality. With this as our core focus, we\u2019re looking for talented individuals who share our passion for quality and technical expertise to join our team.\n  \n \n \n  WHY PHASTAR \n \n \n \n  Accredited as an outstanding company to work for, Phastar is committed to employee engagement, workplace satisfaction and ensuring a healthy work-life balance. We offer flexible working, part-time hours, involvement in developing company-wide initiatives, structured training and development plans, and a truly supportive, fun and friendly environment.\n  \n \n \n  What\u2019s more, when you join our team, Phastar will plant a tree in your honour, as one of our Environmental, Social and Governance (ESG) initiatives. So, not only would you get your dream job, you\u2019ll also be helping to save the planet!\n  \n \n \n  THE ROLE \n \n \n \n  Demand for our Functional Service Provision is growing, and we are looking for an experienced Epidemiologist to join our FSP team.\n  \n \n \n  This is a fantastic opportunity to work for a growing CRO that is recognized for its continuous learning and development opportunities, whilst also gaining direct experience of working within a pharmaceutical environment.\n   Responsibilities: \n  \n Propose, design, initiate, and report epidemiologic study analyses for their assigned Registry \n  Coordinate work between Biostatistics and Statistical Programming resources for planning/implementation of registry analytical deliverables \n  Lead regularly scheduled team meetings with Biostatistics and Statistical Programming \n  Ensure project timelines are maintained \n  Review analysis plans, TLF shells, and TLFs. Prepare report summary and/or slides based on analytical project results (including SAB meeting slides) \n  Accountable to lead the project planning, management, and execution of epidemiology deliverables for their assigned Registry \n  Provide epidemiology support to Medical Affairs, Pharmacovigilance, Regulatory Affairs, and Clinical Development related to their assigned Registry \n  Partcicipate on cross-functional Registry Leadership Teams as appropriate \n  Lead sub-team meetings with Registry Biostatisticians and Programmers as appropriate \n  Provide epidemiology perspective into study protocols, case report forms (CRFs), and regulatory documents and other reports for their assigned Registry \n  May interact with authors, and in-house reviewers on scientific manuscripts, clinical conference abstracts, presentations, and posters based on data generated from their assigned Registry \n  May participate with Epidemiology department during interactions with key external stakeholders including Registry Scientific Advisory Boards and regulatory authorities \n  Qualifications: \n  \n A minimum of 3 years of hands-on experience, within the pharmaceutical industry, biotechnology, or consulting environment for Doctoral-level candidates, or 6 years of industry experience for Master\u2019s-level candidates \n  PhD / DSc / DrPH in Epidemiology, Biostatistics or related degree with 3 years of relevant experience, or MPH / MSc in Epidemiology, Biostatistics or related degree with 6 years of relevant experience \n  Thorough and up-to-date technical knowledge of epidemiology and biostatistics methods \n  Experience with study design, analysis planning and execution of analyses \n  Strong interpersonal skills and ability to work effectively in multidisciplinary teams \n  Record of high-quality publications in peer-reviewed journals  \n Experience working within a clinical trials environment (CRO, pharma or academia) \n  Excellent written and verbal communication skills \n \n \n \n  APPLY NOW \n \n \n \n  With the world\u2019s eyes focused on clinical trial data, this is a fantastic time to join an award-winning specialized biometric CRO that is renowned for its technical expertise, outstanding quality and cutting-edge data science techniques. We offer flexible working, part-time hours, structured training and development plans, continuous learning opportunities, and a competitive salary and benefits package. We\u2019re committed to ensuring our employees achieve a healthy work-life balance, within a supportive, fun and friendly working environment.\n  \n \n \n  Should you feel that you have the right skill set and motivations for this position, please apply! Please note that we are considering candidates located anywhere in the US or Canada as this role can be carried out remotely.\n  \n \n \n  Phastar is committed to the principles and practices of equal opportunities and to encouraging the establishment of a diverse workforce. It is our policy to employ individuals on the basis of their suitability for the work to be performed and their potential for development, regardless of age, sex, race, colour, nationality, ethnic or national origin, disability, marital status, pregnancy or maternity, sexual orientation, gender reassignment, religion, or belief. This includes creating a culture that fully reflects our commitment to equal opportunities for all. \n \n \n \n  Important notice to Employment businesses/ Agencies \n \n \n \n  Phastar does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact Phastar's Head of Talent Acquisition to obtain prior written authorization before referring any candidates to Phastar. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and Phastar. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of Phastar. Phastar shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.", "cleaned_desc": " \n \n  This is a fantastic opportunity to work for a growing CRO that is recognized for its continuous learning and development opportunities, whilst also gaining direct experience of working within a pharmaceutical environment.\n   Responsibilities: \n  \n Propose, design, initiate, and report epidemiologic study analyses for their assigned Registry \n  Coordinate work between Biostatistics and Statistical Programming resources for planning/implementation of registry analytical deliverables \n  Lead regularly scheduled team meetings with Biostatistics and Statistical Programming \n  Ensure project timelines are maintained \n  Review analysis plans, TLF shells, and TLFs. Prepare report summary and/or slides based on analytical project results (including SAB meeting slides) \n  Accountable to lead the project planning, management, and execution of epidemiology deliverables for their assigned Registry \n  Provide epidemiology support to Medical Affairs, Pharmacovigilance, Regulatory Affairs, and Clinical Development related to their assigned Registry \n  Partcicipate on cross-functional Registry Leadership Teams as appropriate \n  Lead sub-team meetings with Registry Biostatisticians and Programmers as appropriate \n  Provide epidemiology perspective into study protocols, case report forms (CRFs), and regulatory documents and other reports for their assigned Registry \n  May interact with authors, and in-house reviewers on scientific manuscripts, clinical conference abstracts, presentations, and posters based on data generated from their assigned Registry    May participate with Epidemiology department during interactions with key external stakeholders including Registry Scientific Advisory Boards and regulatory authorities \n  Qualifications: \n  \n A minimum of 3 years of hands-on experience, within the pharmaceutical industry, biotechnology, or consulting environment for Doctoral-level candidates, or 6 years of industry experience for Master\u2019s-level candidates \n  PhD / DSc / DrPH in Epidemiology, Biostatistics or related degree with 3 years of relevant experience, or MPH / MSc in Epidemiology, Biostatistics or related degree with 6 years of relevant experience \n  Thorough and up-to-date technical knowledge of epidemiology and biostatistics methods \n  Experience with study design, analysis planning and execution of analyses \n  Strong interpersonal skills and ability to work effectively in multidisciplinary teams \n  Record of high-quality publications in peer-reviewed journals  \n Experience working within a clinical trials environment (CRO, pharma or academia) \n  Excellent written and verbal communication skills \n \n \n \n  APPLY NOW \n ", "techs": ["none"]}, "c28c0ff5afc293b3": {"terms": ["data science", "machine learning engineer"], "salary_min": 226853.16, "salary_max": 287246.66, "title": "Principal Machine Learning Engineer - Advanced Technology Group (ATG)", "company": "Pinterest", "desc": "About Pinterest : \n  Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. \n  Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences. \n  Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our  PinFlex  landing page to learn more. \n \n  The Advanced Technologies Group (ATG) is Pinterest's advanced machine learning team. It keeps Pinterest at the forefront of machine learning technology across multiple application areas including recommendations, ranking, content understanding, and more. It is a high impact applied team that works horizontally across the company on state of the art AI and ML and works on directly bringing that technology to the product in collaboration with product engineering teams. The team publishes its work in applied research conferences, but the main contribution of the team's work is to drive top line metric impact across the company for our 400M+ monthly active users. \n  What you'll do: \n \n Lead projects that involve developing and deploying state of the art (and beyond) ML models in production systems across the company at scale for hundreds of millions of users. \n Help to define and drive forward looking ML strategy for the team and across the company. \n Collaborate with other engineering teams (infrastructure, user modeling, content understanding) to leverage their platforms and signals and work with them to collaborate on the adoption and evaluation of new technologies. \n Mentor junior engineers on the ATG and partner teams and help to uplevel ML talent across the company. \n \n What we're looking for: \n \n Experience with state of the art ML modeling techniques and approaches like transformers, self supervised pre-training, generative modeling, LLMs, etc. \n Experience with large scale data processing (e.g. Hive, Scalding, Spark, Hadoop, Map-reduce) \n Hands-on experience training and applying models at scale using deep learning frameworks like PyTorch or Tensorflow. Successful candidates in this role need to be able to build bridge state of the art approaches to real world impact. \n 8+ years working experience in the engineering teams that build large-scale ML-driven user-facing products \n 3+ years experience leading cross-team engineering efforts. \n Strong execution skills in project management \n Masters or PhD in Comp Sci or related fields \n Understanding of an object-oriented programming language (Java, C++, Python) \n \n Desired skills: \n \n Experience in working on, backend and ML systems for large-scale user-facing products, and have a good understanding of how they all work. \n Experience in closely collaborating with other engineering teams to ship new ML technologies to improve recommendation, content understanding, and ranking systems at scale. \n \n This position is not eligible for relocation assistance. \n  #LI-SA1 \n  #LI-REMOTE \n \n \n  At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. \n  Information regarding the culture at Pinterest and benefits available for this position can be found  here . \n \n  US based applicants only \n \n    $221,000\u2014$455,000 USD\n   \n \n \n  Our Commitment to Diversity: \n  Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.", "cleaned_desc": " Experience with large scale data processing (e.g. Hive, Scalding, Spark, Hadoop, Map-reduce) \n Hands-on experience training and applying models at scale using deep learning frameworks like PyTorch or Tensorflow. Successful candidates in this role need to be able to build bridge state of the art approaches to real world impact. \n 8+ years working experience in the engineering teams that build large-scale ML-driven user-facing products \n 3+ years experience leading cross-team engineering efforts. \n Strong execution skills in project management \n Masters or PhD in Comp Sci or related fields \n Understanding of an object-oriented programming language (Java, C++, Python) \n ", "techs": ["hive", "scalding", "spark", "hadoop", "map-reduce", "pytorch", "tensorflow", "java", "c++", "python"]}, "abfe65650b68c7bb": {"terms": ["data science"], "salary_min": 62683.22, "salary_max": 79370.92, "title": "CRM Analyst (Hubspot & Salesforce Expert) Contract", "company": "GoCheck Kids", "desc": "Overview: We are a dynamic and innovative company seeking a skilled Contract CRM Analyst with specialized expertise in HubSpot and Salesforce. Our organization is dedicated to providing top-notch sales and marketing solutions, and we are looking for an experienced professional to contribute to our success. As a Contract CRM Analyst, you will play a pivotal role in optimizing and leveraging our CRM platforms, enabling data-driven decision-making and enhancing our sales and marketing efforts. \n  Responsibilities: \n \n CRM Platform Management: \n    \n Customize and configure CRM modules, workflows, and processes to align with our sales and marketing objectives. \n Troubleshoot and resolve CRM-related issues, collaborating with internal teams and platform support as necessary. \n \n Sales and Marketing Dashboards: \n    \n Design, develop, and implement interactive and visually appealing sales and marketing dashboards using HubSpot and Salesforce reporting and analytics tools. \n Gather and analyze key performance metrics to provide insights on sales funnel efficiency, lead generation, conversion rates, and campaign performance. \n Identify trends, patterns, and opportunities for improvement based on data analysis, empowering stakeholders with actionable data. \n \n Data Analysis and Reporting: \n    \n Utilize SQL queries and data extraction techniques to perform in-depth data analysis and ad-hoc reporting, delivering meaningful findings to management. \n Regularly generate performance reports for sales and marketing teams, presenting clear and concise summaries of results and recommended actions. \n \n CRM Optimization: \n    \n Collaborate with cross-functional teams, including Sales, Marketing, and IT, to identify CRM process gaps and areas for improvement. \n Propose and implement enhancements to CRM workflows and automation to streamline processes and increase overall team productivity. \n \n Training and Support: \n    \n Provide training sessions and documentation to onboard new users on CRM functionalities and best practices. \n Offer ongoing support and guidance to CRM users, addressing inquiries and resolving issues promptly. \n   \n \n Qualifications: \n \n Bachelor's degree in Business Administration, Marketing, Data Science, or related field. \n Proven experience working with HubSpot and Salesforce as a CRM Analyst or similar role. \n Demonstrated expertise in building sales and marketing dashboards within HubSpot and Salesforce environments. \n Proficiency in SQL and data extraction techniques for data analysis and reporting. \n Strong analytical and problem-solving skills, with the ability to interpret complex datasets and draw actionable conclusions. \n Excellent communication skills, both verbal and written, with the ability to present technical information to non-technical stakeholders effectively. \n Proactive and self-motivated, with a keen attention to detail and the ability to work independently and manage multiple projects simultaneously. \n Experience in the sales and marketing domain, understanding key sales and marketing metrics and challenges, is highly desirable. \n \n This is a contract position, offering a unique opportunity to work with cutting-edge CRM platforms and make a significant impact on our sales and marketing operations. If you have a passion for data-driven decision-making and thrive in a fast-paced environment, we encourage you to apply and contribute to our company's success. \n \n  Project Overview: \n \n  Improvements to Hubspot Utilization \n \n  Sales: Outreach (Sales Professional Hub): \n \n \n Uploading Outreach Lists \n E-mails: sequences, individualized e-mails \n Calls: Logging calls \n All Other Activity: Tasks, etc. \n \n Marketing (Marketing Professional Hub): \n \n Uploading Outreach lists \n E-mails (drip campaigns) \n PPC Campaigns (with appropriate Landing Pages) \n Social Media \n Webinars (with appropriate landing pages) \n \n \n \n  SalesForce Utilization  \u2013 mainly require that integration between CRM\u2019s is seamless and deals  can be moved between platforms as needed (i.e., automatically transfer to SF at SQL stage, and  move back to HubSpot if closed lost, closed paused, etc.) \n  Sales \n \n Deal Stage Progression by Segment \n \n Marketing \n \n Tracking MQL to SQL to deal closing for purposes of tracking ROI on campaigns \n \n \n \n  Marketing Metrics Needed \n \n  Metrics Needed \n \n By segment and source, track ROI by looking at MQL, SQL, deal progression, closed won  ($$) \n \n \n  SOP\u2019s Required \n \n Ensure we have appropriate categories built out and appropriate integration between CRM's \n \n \n \n  Sales Metrics Needed \n \n Sales Rep activity (calls, e-mails, etc.) \n Sales Rep productivity (meetings scheduled) \n Aggregate information of the above \n E-mail Performance \n    \n Tracking performance of e-mail campaigns by segment (opens, replies, etc.) \n Tracking e-mail campaigns through to deal closing in Sales Force \n \n Deal Progression \n    \n Tracking progression through various stages relative to marketing/sales campaigns \n Tracking aging of deals by stage (days, last activity, etc.), particularly in SalesForce to determine how current the deal \n \n Deal Closing and Forecasting \n    \n Deals closed by segment \n Deals closed relative to rep and for team in aggregate", "cleaned_desc": " Collaborate with cross-functional teams, including Sales, Marketing, and IT, to identify CRM process gaps and areas for improvement. \n Propose and implement enhancements to CRM workflows and automation to streamline processes and increase overall team productivity. \n \n Training and Support: \n    \n Provide training sessions and documentation to onboard new users on CRM functionalities and best practices. \n Offer ongoing support and guidance to CRM users, addressing inquiries and resolving issues promptly. \n   \n \n Qualifications: \n \n Bachelor's degree in Business Administration, Marketing, Data Science, or related field. \n Proven experience working with HubSpot and Salesforce as a CRM Analyst or similar role. \n Demonstrated expertise in building sales and marketing dashboards within HubSpot and Salesforce environments. \n Proficiency in SQL and data extraction techniques for data analysis and reporting. \n Strong analytical and problem-solving skills, with the ability to interpret complex datasets and draw actionable conclusions. \n Excellent communication skills, both verbal and written, with the ability to present technical information to non-technical stakeholders effectively. \n Proactive and self-motivated, with a keen attention to detail and the ability to work independently and manage multiple projects simultaneously. \n Experience in the sales and marketing domain, understanding key sales and marketing metrics and challenges, is highly desirable. \n \n This is a contract position, offering a unique opportunity to work with cutting-edge CRM platforms and make a significant impact on our sales and marketing operations. If you have a passion for data-driven decision-making and thrive in a fast-paced environment, we encourage you to apply and contribute to our company's success. ", "techs": ["hubspot", "salesforce", "sql"]}, "ed7bb6d7406bec7f": {"terms": ["data science"], "salary_min": 158521.52, "salary_max": 200723.55, "title": "Cloud Data Architect - AWS Databases", "company": "DoiT International", "desc": "Location \n  Our Cloud Data Architect, GCP Databases, will be an integral part of our Customer Reliability Engineering team in North America. This role is based remotely with a strong preference for the West Coast; pacific or mountain timezones. \n  Who We Are \n  DoiT helps fast-growing, digital native companies globally to harness public cloud technology and services to drive business growth. A full-service provider of multi-cloud technology and expertise, DoiT combines the power of intelligent software with deep expertise in Kubernetes, artificial intelligence, and more to deliver the true promise of the cloud at peak efficiency - with ease, not cost. \n  An award-winning strategic partner of AWS, Google Cloud, and Microsoft Azure with $2B cloud spend under management, DoiT works alongside more than 3,000 customers in 70 countries. At DoiT, you'll join a growing team of committed, experienced, and collaborative \"Do'ers\" who are passionate about solving the most complex cloud challenges. \n  The Opportunity \n \n This role involves to work with customers to design, implement and troubleshoot scalable and secure Relational DBs and NoSQL solutions around Google Cloud \n Reporting to a staff cloud architect in our EMEA region, you will collaborate with team members globally, while serving our customers through online chat, helpdesk software, and video conference interactions \n We believe in life-work balance, in that order. We are fully remote and distributed globally, eliminating the need for on-call status, which allows our team to work standard business hours during the workweek \n This role involves supporting many customers simultaneously as they request assistance. Typical interactions include understanding existing architecture, desired future state, current challenges, and then making recommendations on how customers should implement best practices, while occasionally creating proof of concepts in our own controlled environment \n The successful candidate will be skilled in communication, documentation, and able to quickly learn new technologies and confidently instruct others based on newly-learned information \n Our client services team is made up of lifelong learners. Through customer interactions, DoiT-sponsored continuing education and certifications, plus allotted personal development time, this role guarantees the ability to satisfy the most voracious learning appetite \n This is an outstanding opportunity to help shape our future as we ourselves are growing at a rapid pace. Our team also contributes internally on various improvement or automation projects, or proposes their own. We've authored and maintained popular open-source tools, numerous automated Slack bots, a skills-based task routing system, integrations of various SaaS products, observability dashboards, and more \n \n Responsibilities \n \n Become a trusted advisor to our customers and their technical teams. \n Simultaneously support customers at various stages in their cloud journey, meeting them where they are, and helping them get to where they're going. \n Autonomously interact with customers online and over video conference calls. \n Develop and maintain documentation and architecture diagrams to support the design and implementation of cloud-based solutions. \n Be available and willing to support other client services team members in solving issues within their area of expertise (be a hero amongst your peers). \n Collaborate with other DoiT departments (Sales, Account Management, Marketing, Product, Engineering, IT), acting as the go-between technical expert between customer goals/objectives and DoiT's goals/objectives. \n Work with cross-functional teams to identify and prioritize opportunities for application modernization and cloud adoption. \n Stay up-to-date with the latest cloud technologies and best practices. \n \n Requirements \n \n The expertise to administer and troubleshoot mission-critical database systems on Google Cloud Platform (GCP) \n Ability to analyze needs and requirements to design and recommend appropriate database solutions by using GCP services \n DBA experience directly supporting at least one of the following: Cloud SQL for MySQL / Cloud SQL for PostgreSQL / Cloud SQL for SQL Server / Cloud Spanner, with particular strengths in: \n \n Deployment, Migrations and architectural best practices \n Management and Operations \n Monitoring and troubleshooting \n Security and encryption \n Optimization and performance tuning \n \n Understanding of Cloud SQL Proxy and connection management best practices \n Experience with Cloud SQL features and limitations \n Experience in identifying and optimizing application database code \n Experience performing Database migrations, including Database Migration Service and Cloud Datastream \n Knowledge of NoSQL databases on Google Cloud - Memorystore/Firestore/Bigtable \n \n Bonus Points \n \n Google Cloud certifications \n \n Are you a Doer? \n  Be your truest self. Work on your terms. Make a difference. \n  We are home to a global team of incredible talent who work remotely and have the flexibility to have a schedule that balances your work and home life. We embrace and support leveling up your skills professionally and personally. \n  What does being a Doer mean? We're all about being entrepreneurial, pursuing knowledge and having fun! \n  Sounds too good to be true? Check out our Glassdoor Page. \n  We thought so too, but we're here and happy we hit that 'apply' button. \n \n Unlimited PTO \n Flexible Working Options \n Health Insurance \n Parental Leave \n Employee Stock Option Plan \n Home Office Allowance \n Professional Development Stipend \n Peer Recognition Program \n \n #LI-Remote", "cleaned_desc": " Stay up-to-date with the latest cloud technologies and best practices. \n \n Requirements \n \n The expertise to administer and troubleshoot mission-critical database systems on Google Cloud Platform (GCP) \n Ability to analyze needs and requirements to design and recommend appropriate database solutions by using GCP services \n DBA experience directly supporting at least one of the following: Cloud SQL for MySQL / Cloud SQL for PostgreSQL / Cloud SQL for SQL Server / Cloud Spanner, with particular strengths in: \n \n Deployment, Migrations and architectural best practices \n Management and Operations \n Monitoring and troubleshooting \n Security and encryption   Optimization and performance tuning \n \n Understanding of Cloud SQL Proxy and connection management best practices \n Experience with Cloud SQL features and limitations \n Experience in identifying and optimizing application database code \n Experience performing Database migrations, including Database Migration Service and Cloud Datastream \n Knowledge of NoSQL databases on Google Cloud - Memorystore/Firestore/Bigtable \n \n Bonus Points \n \n Google Cloud certifications \n ", "techs": ["google cloud platform (gcp)", "cloud sql for mysql", "cloud sql for postgresql", "cloud sql for sql server", "cloud spanner", "cloud sql proxy", "cloud sql features and limitations", "database migration service", "cloud datastream", "nosql databases on google cloud - memorystore", "firestore", "bigtable", "google cloud certifications"]}, "b635f9f331217a95": {"terms": ["data science"], "salary_min": 105097.16, "salary_max": 133076.42, "title": "Azure BI Engineer", "company": "Cognizant Technology Solutions", "desc": "We are Cognizant Artificial Intelligence \n  Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. However, clients need new business models built from analyzing customers and business operations at every angle to really understand them. \n  With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks \n  * You must be legally authorized to work in United States without the need of employer sponsorship, now or at any time in the future * \n  This is a remote position open to any qualified applicant in the United States \n \n \n  Job Title: Azure BI Engineer \n  Experience: 10 to 14 years \n  Must Have Skills \n \n Databricks \n Azure Analysis Services \n MS Power BI \n \n Good To Have Skills \n \n Azure Storage \n Azure DW \n Azure Data Factory \n \n Responsibilities \n \n Coordinate and oversee MS-BI Development by offshore (India) team members and distribution schedule to ensure timely delivery to customers. \n Ensuring the highest quality for every project/request. Responsible for error resolution, follow up and performance metrics monitoring. \n Convert concepts to technical architecture, design, and implementation. \n Design and build BI solutions with an emphasis on performance, scalability, and high reliability. \n Design and Develop BI solutions using Microsoft  Power BI , MS SQL \n Develop data curation pipelines for reporting needs using  ADLS/ADF  and  Azure Databricks (Spark & SQL) \n Design and Build Model using  Azure Analysis Services (AAS) \n Manage creation of comprehensive workflows for the production and distribution of assigned reports, document reporting processes and procedures. \n Contribute to building a team of top-performing data technology professionals. \n Unit testing, Support System Testing and UAT \n Code optimization / tuning under guidance from Architects \n Perform peer reviews. \n Excellent problem-solving skills. \n Self-motivated, takes initiative to identify, communicate, and resolve potential issues \n \n Experience \n  Experience in MS-BI Development using Power BI MS SQL  Experience in data curation using - ADLS Data Bricks (Spark & SQL)  Exposure to Healthcare Payer BI implementation is an added plus \n  Salary and Other Compensation : \n  The annual salary for this position is depending on experience and other qualifications of the successful candidate. \n  This position is also eligible for Cognizant\u2019s discretionary annual incentive program, based on performance and subject to the terms of Cognizant\u2019s applicable plans. \n \n \n  Benefits : Cognizant offers the following benefits for this position, subject to applicable eligibility requirements: \n \n Medical/Dental/Vision/Life Insurance \n Paid holidays plus Paid Time Off \n 401(k) plan and contributions \n Long-term/Short-term Disability \n Paid Parental Leave \n Employee Stock Purchase Plan \n \n Disclaimer:  The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law. \n  Employee Status :  Full Time Employee \n  Shift :  Day Job \n  Travel :  No \n  Job Posting :  Sep 08 2023 \n \n \n  About Cognizant \n  Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.\n  \n  Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview. \n \n  Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law. \n  If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.", "cleaned_desc": " MS Power BI \n \n Good To Have Skills \n \n Azure Storage \n Azure DW \n Azure Data Factory \n \n Responsibilities \n \n Coordinate and oversee MS-BI Development by offshore (India) team members and distribution schedule to ensure timely delivery to customers. \n Ensuring the highest quality for every project/request. Responsible for error resolution, follow up and performance metrics monitoring. \n Convert concepts to technical architecture, design, and implementation.   Design and build BI solutions with an emphasis on performance, scalability, and high reliability. \n Design and Develop BI solutions using Microsoft  Power BI , MS SQL \n Develop data curation pipelines for reporting needs using  ADLS/ADF  and  Azure Databricks (Spark & SQL) \n Design and Build Model using  Azure Analysis Services (AAS) \n Manage creation of comprehensive workflows for the production and distribution of assigned reports, document reporting processes and procedures. \n Contribute to building a team of top-performing data technology professionals. \n Unit testing, Support System Testing and UAT \n Code optimization / tuning under guidance from Architects \n Perform peer reviews. \n Excellent problem-solving skills. \n Self-motivated, takes initiative to identify, communicate, and resolve potential issues \n \n Experience ", "techs": ["ms power bi", "azure storage", "azure dw", "azure data factory", "ms sql", "adls/adf", "azure databricks (spark & sql)", "azure analysis services (aas)"]}, "951a91dedc3860c5": {"terms": ["data science"], "salary_min": 128202.336, "salary_max": 162332.72, "title": "Senior Data Architect", "company": "Truist Bank", "desc": "The position is described below. If you want to apply, click the Apply Now button at the top or bottom of this page. After you click Apply Now and complete your application, you'll be invited to create a profile, which will let you see your application status and any communications. If you already have a profile with us, you can log in to check status. \n \n  Need Help? \n \n  If you have a disability and need assistance with the application, you can request a reasonable accommodation. Send an email to  Accessibility  (accommodation requests only; other inquiries won't receive a response). \n \n  Regular or Temporary:  Regular\n  \n  Language Fluency:  English (Required) \n \n  Work Shift:  1st shift (United States of America)\n  \n  Please review the following job description: \n \n \n  Define and document the technical architecture of the data warehouse, including the physical components and their functionality. \n  Derive infrastructure system specifications from business requirements, design solutions that support core organizational functions, and ensure their high availability and other performance goals. \n  Provide integrated infrastructure-related technical expertise across the organization, from conceptualization and project planning to the post-implementation support level. \n  Provide infrastructure system expertise, requirements and assistance to Systems and Database Administrators, other System Architects and application development teams. \n  Partner with line of business (LOB) teams, external vendors, and internal Data Services team to develop system specifications in compliance with corporate standards for architecture adherence and performance guidelines. \n  Provide technical resources to assist in the design, testing and implementation of software code and infrastructure to support Data Infrastructure and Governance activities. \n  Assess current technical architecture and estimate system capacity to meet near- and long-term processing requirements. Evaluate, select, test, and optimize hardware and software products. \n  Consult with end-users, clients and senior management to define infrastructure requirements for complex systems and infrastructure development. \n  Oversee development and implementation of end-to-end integration of infrastructure solutions. \n  Document the Bank's existing solution architecture and technology portfolio. Document and communicate needs for investing in infrastructure evolution, including analysis of cost reduction opportunities. \n  Serve as a liaison with Enterprise Architects to conduct research on emerging technologies, and recommend technologies that will increase operational efficiency, infrastructure flexibility and operational stability. \n  Instruct, direct and mentor other members of the team. \n \n \n  Requirements  \n \n Must have a Bachelor\u2019s degree in Computer Science, Computer Engineering, Electrical/Electronics Engineering or related technical field plus 8 years of progressive experience in data architect or analyst positions performing the following:\n    \n  Deriving technical specifications from business requirements and expressing complex technical concepts in terms that are understandable to multi-disciplinary teams, in verbal and written form \n  Using sophisticated analysis to exercise judgment and identify innovative data and technical solutions for banking and financial services with focus on master data management, and digital banking. \n  Applying knowledge of client-service models and customer orientation in service delivery. \n  Utilizing experience with: Enterprise and Data Architect tools: Unicom System Architect, Erwin Data Modeler; Big Data, Cloud and Streaming Platforms, including Apache Hadoop, Apache Kafka, Spark Streaming, and Adobe Analytics; Data Discovery, Wrangling, and Data Science tools: SAS Enterprise Guide, Anaconda Workbench; IBM DB2; IBM Netezza; MS SQLServer; Oracle; Informatica; Ab Initio; SQL; and Spark \n \n \n \n  In the alternative, employer will accept a Master\u2019s degree in Computer Science, Computer Engineering, Electrical/Electronics Engineering or related technical field plus 6 years of experience in data architect or analyst positions performing the aforementioned. \n \n \n  Position may be eligible to work remotely but is based out of and reports to Truist offices in Atlanta, GA. Must be available to travel to Atlanta, GA regularly for meetings and reviews with manager and project teams within 24-hours\u2019 notice. \n \n \n  General Description of Available Benefits for Eligible Employees of Truist Financial Corporation:  All regular teammates (not temporary or contingent workers) working 20 hours or more per week are eligible for benefits, though eligibility for specific benefits may be determined by the division of Truist offering the position. Truist offers medical, dental, vision, life insurance, disability, accidental death and dismemberment, tax-preferred savings accounts, and a 401k plan to teammates. Teammates also receive no less than 10 days of vacation (prorated based on date of hire and by full-time or part-time status) during their first year of employment, along with 10 sick days (also prorated), and paid holidays. For more details on Truist\u2019s generous benefit plans, please visit our Benefits site. Depending on the position and division, this job may also be eligible for Truist\u2019s defined benefit pension plan, restricted stock units, and/or a deferred compensation plan. As you advance through the hiring process, you will also learn more about the specific benefits available for any non-temporary position for which you apply, based on full-time or part-time status, position, and division of work. \n \n  Truist supports a diverse workforce and is an Equal Opportunity Employer that does not discriminate against individuals on the basis of race, gender, color, religion, citizenship or national origin, age, sexual orientation, gender identity, disability, veteran status or other classification protected by law. Truist is a Drug Free Workplace. \n \n  EEO is the Law Pay Transparency Nondiscrimination Provision E-Verify", "cleaned_desc": "  Provide technical resources to assist in the design, testing and implementation of software code and infrastructure to support Data Infrastructure and Governance activities. \n  Assess current technical architecture and estimate system capacity to meet near- and long-term processing requirements. Evaluate, select, test, and optimize hardware and software products. \n  Consult with end-users, clients and senior management to define infrastructure requirements for complex systems and infrastructure development. \n  Oversee development and implementation of end-to-end integration of infrastructure solutions. \n  Document the Bank's existing solution architecture and technology portfolio. Document and communicate needs for investing in infrastructure evolution, including analysis of cost reduction opportunities. \n  Serve as a liaison with Enterprise Architects to conduct research on emerging technologies, and recommend technologies that will increase operational efficiency, infrastructure flexibility and operational stability. \n  Instruct, direct and mentor other members of the team. \n \n \n  Requirements    \n Must have a Bachelor\u2019s degree in Computer Science, Computer Engineering, Electrical/Electronics Engineering or related technical field plus 8 years of progressive experience in data architect or analyst positions performing the following:\n    \n  Deriving technical specifications from business requirements and expressing complex technical concepts in terms that are understandable to multi-disciplinary teams, in verbal and written form \n  Using sophisticated analysis to exercise judgment and identify innovative data and technical solutions for banking and financial services with focus on master data management, and digital banking. \n  Applying knowledge of client-service models and customer orientation in service delivery. \n  Utilizing experience with: Enterprise and Data Architect tools: Unicom System Architect, Erwin Data Modeler; Big Data, Cloud and Streaming Platforms, including Apache Hadoop, Apache Kafka, Spark Streaming, and Adobe Analytics; Data Discovery, Wrangling, and Data Science tools: SAS Enterprise Guide, Anaconda Workbench; IBM DB2; IBM Netezza; MS SQLServer; Oracle; Informatica; Ab Initio; SQL; and Spark \n \n \n ", "techs": ["unicom system architect", "erwin data modeler", "apache hadoop", "apache kafka", "spark streaming", "adobe analytics", "sas enterprise guide", "anaconda workbench", "ibm db2", "ibm netezza", "ms sqlserver", "oracle", "informatica", "ab initio", "sql", "spark"]}, "c3419186309dea50": {"terms": ["data science"], "salary_min": 106639.66, "salary_max": 135029.56, "title": "Azure BI Engineer (Remote)", "company": "Cognizant Technology Solutions", "desc": "We are Cognizant Artificial Intelligence \n  Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. However, clients need new business models built from analyzing customers and business operations at every angle to really understand them. \n  With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks \n  * You must be legally authorized to work in United States without the need of employer sponsorship, now or at any time in the future * \n \n  This is a remote position open to any qualified applicant in the United States \n \n \n Job Title: Azure BI Engineer \n  Experience: 10 to 14 years \n \n Must Have Skills \n \n Databricks \n \n \n Azure Analysis Services \n \n \n MS Power BI \n \n Good To Have Skills \n \n Azure Storage \n \n \n Azure DW \n \n \n Azure Data Factory \n \n Responsibilities \n \n Coordinate and oversee MS-BI Development by offshore (India) team members and distribution schedule to ensure timely delivery to customers. \n Ensuring the highest quality for every project/request. Responsible for error resolution, follow up and performance metrics monitoring.  \n Convert concepts to technical architecture, design, and implementation. \n Design and build BI solutions with an emphasis on performance, scalability, and high reliability. \n Design and Develop BI solutions using Microsoft  Power BI , MS SQL \n Develop data curation pipelines for reporting needs using  ADLS/ADF  and  Azure Databricks (Spark & SQL) \n Design and Build Model using  Azure Analysis Services (AAS) \n Manage creation of comprehensive workflows for the production and distribution of assigned reports, document reporting processes and procedures. \n Contribute to building a team of top-performing data technology professionals. \n Unit testing, Support System Testing and UAT  \n Code optimization / tuning under guidance from Architects \n Perform peer reviews. \n Excellent problem-solving skills. \n Self-motivated, takes initiative to identify, communicate, and resolve potential issues \n \n Experience \n  Experience in MS-BI Development using Power BI MS SQL  Experience in data curation using - ADLS Data Bricks (Spark & SQL)  Exposure to Healthcare Payer BI implementation is an added plus \n \n  Salary and Other Compensation :\n  \n \n   The annual salary for this position is depending on experience and other qualifications of the successful candidate.\n  \n \n   This position is also eligible for Cognizant\u2019s discretionary annual incentive program, based on performance and subject to the terms of Cognizant\u2019s applicable plans.\n  \n \n \n \n  Benefits : Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:\n  \n \n Medical/Dental/Vision/Life Insurance \n Paid holidays plus Paid Time Off \n 401(k) plan and contributions \n Long-term/Short-term Disability \n Paid Parental Leave \n Employee Stock Purchase Plan \n \n \n Disclaimer:  The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.\n  \n \n \n \n  #LIMO1 #CB #Ind123 \n \n  Employee Status :  Full Time Employee \n  Shift :  Day Job \n  Travel :  No \n  Job Posting :  Sep 08 2023 \n \n \n  About Cognizant \n  Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.\n  \n  Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview. \n \n  Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law. \n  If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.", "cleaned_desc": " \n MS Power BI \n \n Good To Have Skills \n \n Azure Storage \n \n \n Azure DW \n \n \n Azure Data Factory \n \n Responsibilities \n \n Coordinate and oversee MS-BI Development by offshore (India) team members and distribution schedule to ensure timely delivery to customers. \n Ensuring the highest quality for every project/request. Responsible for error resolution, follow up and performance metrics monitoring.  \n Convert concepts to technical architecture, design, and implementation.   Design and build BI solutions with an emphasis on performance, scalability, and high reliability. \n Design and Develop BI solutions using Microsoft  Power BI , MS SQL \n Develop data curation pipelines for reporting needs using  ADLS/ADF  and  Azure Databricks (Spark & SQL) \n Design and Build Model using  Azure Analysis Services (AAS) \n Manage creation of comprehensive workflows for the production and distribution of assigned reports, document reporting processes and procedures. \n Contribute to building a team of top-performing data technology professionals. \n Unit testing, Support System Testing and UAT  \n Code optimization / tuning under guidance from Architects \n Perform peer reviews. \n Excellent problem-solving skills. \n Self-motivated, takes initiative to identify, communicate, and resolve potential issues \n \n Experience \n  Experience in MS-BI Development using Power BI MS SQL  Experience in data curation using - ADLS Data Bricks (Spark & SQL)  Exposure to Healthcare Payer BI implementation is an added plus \n \n  Salary and Other Compensation :\n  \n ", "techs": ["ms power bi", "azure storage", "azure dw", "azure data factory", "ms sql", "adls/adf", "azure databricks", "azure analysis services (aas)"]}, "42704497820d56ea": {"terms": ["data science", "machine learning engineer", "mlops"], "salary_min": 124785.82, "salary_max": 158006.66, "title": "Machine Learning Ops/DevOps Senior Engineer", "company": "Dell Technologies", "desc": "Machine Learning Ops/DevOps Senior Engineer \n  Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand new methodologies, tools, statistical methods and models. What\u2019s more, we are in collaboration with leading academics, industry experts and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data. \n  Our data scientists & engineers are helping to shape our marketing strategy for a digital world and at scale. We are looking for a DevOps Engineer to take care of our Machine Learning Models in production. \n  What you'll achieve: \n  This position will merge DevOps, software engineering, data science, and data engineering to deploy machine learning models that can optimize business and customer experiences at scale. You will use your technical knowledge and software development to build technology-centric solutions that accelerate the development of Artificial Intelligence and Machine Learning capabilities across the company. \n  You will take out the debate of what can be launched and when. The models that we build are essential ingredients to supply our customers & sales with relevant product recommendations and creative breakthroughs to our business. You & your team are critical for our success! \n  You will: \n \n  Develop high availability and highly scalable applications which will be used by both internal and external customers. \n  Participate in product development in all stages from planning and design to development, testing and documentation of ML solutions and data products. \n  Generate technical documentation as well as unit and functional tests. \n  Adhere to DevSecOps practices to protect underlying data/infrastructure assets. \n  Utilize a range of applicable technologies across the entire Model Lifecycle (e.g., data science packages, statistical and machine learning techniques, distributed computing, Big Data, CI/CD). \n \n  Take the first step towards your dream career  \n Every Dell Technologies team member brings something unique to the table. Here\u2019s what we are looking for with this role: \n  Essential Requirements: \n \n  Engineering Degree in Computer Science/Engineering and relevant professional experience in the field \n  Strong hands-on experience in Linux/Cloud environment and scripting languages as Python (including unittest or pytest) and Shell \n  Experience designing and developing APIs in Python (FastAPI, Flask or Django) \n  Excellent knowledge of software development and software testing methodologies along with configuration management practices in Linux-based environments. \n  Experience working with Docker, Kubernetes in a microservices architecture. \n \n  Desirable Requirements: \n \n  Experience in automating Continuous Integration, Continuous Delivery and Agile practices for highly scalable systems \n  Machine Learning or Artificial Intelligence experience \n \n  Who we are \n  We believe that each of us has the power to make an impact. That\u2019s why we put our team members at the center of everything we do. If you\u2019re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we\u2019re looking for you. \n  Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us. \n  Application closing date:  September 15th - 2023 \n  Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment.", "cleaned_desc": "  You will: \n \n  Develop high availability and highly scalable applications which will be used by both internal and external customers. \n  Participate in product development in all stages from planning and design to development, testing and documentation of ML solutions and data products. \n  Generate technical documentation as well as unit and functional tests. \n  Adhere to DevSecOps practices to protect underlying data/infrastructure assets.    Engineering Degree in Computer Science/Engineering and relevant professional experience in the field \n  Strong hands-on experience in Linux/Cloud environment and scripting languages as Python (including unittest or pytest) and Shell \n  Experience designing and developing APIs in Python (FastAPI, Flask or Django) \n  Excellent knowledge of software development and software testing methodologies along with configuration management practices in Linux-based environments. \n  Experience working with Docker, Kubernetes in a microservices architecture. \n ", "techs": ["fastapi", "flask", "django", "python", "shell", "linux", "docker", "kubernetes"]}, "c56d00a0112352e7": {"terms": ["data science", "mlops"], "salary_min": null, "salary_max": null, "title": "Director of Public Sector Research - Machine Learning", "company": "Clarifai", "desc": "About the Company \n  Clarifai is a leading, full-lifecycle deep-learning AI platform for computer vision, natural language processing, LLM's and audio recognition. We help organizations transform unstructured images, video, text, and audio data into structured data at a significantly faster and more accurate rate than humans would be able to do on their own. Founded in 2013 by Matt Zeiler, Ph.D. Clarifai has been a market leader in AI since winning the top five places in image classification at the 2013 ImageNet Challenge. Clarifai continues to grow with employees remotely based throughout the United States, Canada, Argentina, India and Estonia. \n  We have raised $100M in funding to date, with $60M coming from our most recent Series C, and are backed by industry leaders like Menlo Ventures, Union Square Ventures, Lux Capital, New Enterprise Associates, LDV Capital, Corazon Capital, Google Ventures, NVIDIA, Qualcomm and Osage. \n  Clarifai is proud to be an equal opportunity workplace dedicated to pursuing, hiring, and retaining a diverse workforce. \n  Your Impact \n  Note: We are interested in every qualified candidate who is eligible to work in the United States. However, we require an active Top Secret Clearance for this position. \n  As a leader within Clarifai, you will be the Director of the Public Sector Research team, working on AI/ML projects assisting US IC/DoD/Civilian agencies. You will work on cutting edge projects involving Computer Vision, LLM's, and NLP. \n  The Opportunity \n  As the Director of Public Sector Research, you will be in a key position to shape the future and growth of the company. \n \n You will lead multiple programs and manage a talented team of cleared research scientists. \n You will be expected to identify new research opportunities and help mitigate risk by prioritizing and making a cost/effectiveness analysis. \n You will lead your team to use and develop cutting edge technologies and SOTA research to solve real world problems empowering the usage of our platform. \n You will closely work with product, engineering, infra and solutions managers to optimize existing and new capabilities. \n You will be a key influencer on the roadmap, achieving our immediate goals and defining our long term strategy. \n \n Requirements \n \n 3+ years as an ML Research Manager \n Hands-on experience crafting and experimenting with neural networks, machine learning techniques and computer vision models \n Fluent with deep learning framework such as TensorFlow or PyTorch \n Fluent with Python and experience with data science and machine learning packages such as NumPy, Pandas, Scikit-Learn, and Matplotlib \n Masters in Computer Science or a related field \n Top Secret Clearance \n \n Great to Have \n \n PhD in machine learning-related field \n Experience with various machine learning hubs and frameworks like huggingface, detectron2, OpenMMLab (mmdetection, mmsegmentation, etc.) \n Experience with MLOps tools such as kubernetes and kubeflow \n Experience with NVIDIA's optimization tools such as TensorRT, Triton \n Experience with Go programming language \n Published research papers/patents/blogs \n Contribution to open-source projects", "cleaned_desc": " 3+ years as an ML Research Manager \n Hands-on experience crafting and experimenting with neural networks, machine learning techniques and computer vision models \n Fluent with deep learning framework such as TensorFlow or PyTorch \n Fluent with Python and experience with data science and machine learning packages such as NumPy, Pandas, Scikit-Learn, and Matplotlib \n Masters in Computer Science or a related field \n Top Secret Clearance   \n Great to Have \n \n PhD in machine learning-related field \n Experience with various machine learning hubs and frameworks like huggingface, detectron2, OpenMMLab (mmdetection, mmsegmentation, etc.) \n Experience with MLOps tools such as kubernetes and kubeflow ", "techs": ["tensorflow", "pytorch", "python", "numpy", "pandas", "scikit-learn", "matplotlib", "huggingface", "detectron2", "openmmlab (mmdetection", "mmsegmentation)", "kubernetes", "kubeflow"]}, "6a0049b41e65e3bc": {"terms": ["data science"], "salary_min": 60.0, "salary_max": 70.0, "title": "ServiceNow Virtual Agent Developer", "company": "Laiba Technologies", "desc": "Position: ServiceNow Virtual Agent Developer \n Location: Phoenix, AZ - Remote, USA \n \u25cf Proven experience as a ServiceNow Virtual Agent Developer, with a strong portfolio showcasing successful virtual agent implementations. \n \u25cf Proficiency in configuring and designing virtual agent conversations using ServiceNow's Virtual Agent Designer. \n \u25cf Solid understanding of natural language processing (NLP) principles and practical experience in creating conversational flows that deliver accurate responses. \n \u25cf Experience with ServiceNow platform development, including scripting (JavaScript, Glide), integrations, and APIs. \n \u25cf Strong problem-solving skills and the ability to troubleshoot technical issues related to virtual agent functionality and integration. \n \u25cf Strong teamwork and collaboration skills, coupled with the ability to work independently when required. \n \u25cf ServiceNow certification(s) in Virtual Agent Development is a plus. \n \u25cf Previous experience in developing chatbots or virtual agents for IT service management (ITSM) and customer service use cases is highly desirable. \n \u25cf Collaborate with cross-functional teams including analysts, UI/UX designers, and other developers to understand business requirements and design virtual agent solutions within the ServiceNow platform. \n \u25cf Design, configure, and develop virtual agent conversations using ServiceNow's Virtual Agent Designer, incorporating relevant skills, decision trees, intents, and natural language understanding (NLU) capabilities. \n \u25cf Leverage your expertise in NLP to create natural and user-friendly conversations that provide accurate and contextually relevant responses to user queries. \n \u25cf Integrate virtual agents with various ServiceNow modules and external systems, ensuring seamless data flow and process automation. \n \u25cf Develop and maintain custom integrations, APIs, and scripts to enhance the capabilities of virtual agents and enable interactions with external systems. \n \u25cf Continuously monitor and analyze virtual agent performance, making iterative improvements to enhance accuracy, user satisfaction, and efficiency. \n \u25cf Collaborate with UI/UX designers to ensure the virtual agent's interface is visually appealing, intuitive, and aligned with the organization's branding guidelines. \n \u25cf Stay up-to-date with industry trends, emerging technologies, and best practices related to virtual agents, NLP, and AI-driven solutions. \n \u25cf Participate in code reviews, knowledge sharing sessions, and provide technical guidance to junior developers as needed. \n Job Type: Contract \n Pay: $60.00 - $70.00 per hour \n Experience level: \n \n 10 years \n 9 years \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Experience: \n \n ServiceNow Virtual Agent: 3 years (Preferred) \n \n Work Location: Remote", "cleaned_desc": " \u25cf Strong teamwork and collaboration skills, coupled with the ability to work independently when required. \n \u25cf ServiceNow certification(s) in Virtual Agent Development is a plus. \n \u25cf Previous experience in developing chatbots or virtual agents for IT service management (ITSM) and customer service use cases is highly desirable. \n \u25cf Collaborate with cross-functional teams including analysts, UI/UX designers, and other developers to understand business requirements and design virtual agent solutions within the ServiceNow platform. \n \u25cf Design, configure, and develop virtual agent conversations using ServiceNow's Virtual Agent Designer, incorporating relevant skills, decision trees, intents, and natural language understanding (NLU) capabilities. \n \u25cf Leverage your expertise in NLP to create natural and user-friendly conversations that provide accurate and contextually relevant responses to user queries. \n \u25cf Integrate virtual agents with various ServiceNow modules and external systems, ensuring seamless data flow and process automation. ", "techs": ["servicenow certification(s) in virtual agent development", "\nprevious experience in developing chatbots or virtual agents for it service management (itsm) and customer service use cases", "\nservicenow's virtual agent designer", "\nnatural language understanding (nlu) capabilities", "\nnlp (natural language processing)", "\nintegration with various servicenow modules and external systems."]}, "6a00657c1c0f440a": {"terms": ["data science"], "salary_min": null, "salary_max": null, "title": "Technology Leadership Program", "company": "MultiPlan Inc.", "desc": "TECHNOLOGY LEADERSHIP PROGRAM (remote) \n \n \n  https://www.multiplan.us/technology-leadership-program/ \n \n \n  MultiPlan\u2019s Technology Leadership Rotation Program provides an opportunity for college graduates interested in launching a career in technology. The program is designed to provide and develop skills related to Information Technology and the different areas of MultiPlan they support. You will rotate through various fields within Technology services at MultiPlan while focusing on the development and application of leadership skills. \n \n   \n All rotations are 12 months long. During the course of this 3-year program, you will have the choice of the following Technology services (additional rotations may be added as technologies and processes evolve): \n \n \n \n Project Management/Business Analysis  \n Service Management/DevOps  \n Information Security/Risk Management  \n Database Administration (Oracle and SQL Server) \n Claims Processing/EDI \n Business Operations \n Data Science/Machine Learning \n Monitoring & Middleware \n Data Communication & Performance Monitoring \n Application Development\u2014Java Web Services \n Application Development\u2014Full Stack Java \n Application Development\u2014Microsoft.NET \n Application Development\u2014Out of Network Services \n \n If hired, during the course of your first three years you will be part of our Technology Leadership Program (TLP). You will spend these years rotating through various disciplines within technology as per your personal interests. We will teach you all that we can while giving you the hands-on experience you can\u2019t get in a classroom, much exposure to senior leadership, and many opportunities for leadership training. After those first three years you will choose which of these disciplines to join permanently. Our goal is to develop you into a future leader of our company within Technology! \n  LOCATION \n  This is a work-from-home position, however you must live within commuting distance to one of these locations for the occasional onsite meeting: New York City; Rockville, MD; Naperville, IL; Irving, TX; Bedford, MA. \n \n \n  PAY \n  All new Associates start at $85k with $10k sign-on bonus. Salary increases to $90k in second year and $95k in third year, with guaranteed minimum $105k after your third year. \n \n \n \n JOB REQUIREMENTS  \n \n We are currently only considering  undergrad seniors  who will graduate in spring of 2024 or December of 2023 with a Bachelor\u2019s Degree in Information Technology or related field. Start date is June/July 2024. \n Interest in a career in Information Technology. \n Unfortunately, we are currently unable to offer visa sponsorship for this position. That includes OPT/CPT. \n \n \n \n  BENEFITS \n \n \n  We realize that our employees are instrumental to our success, and we reward them accordingly with very competitive compensation and benefits packages, an incentive bonus program, as well as recognition and awards programs. Our work environment is friendly and supportive, and we offer flexible schedules whenever possible, as well as a wide range of live and web-based professional development and educational programs to prepare you for advancement opportunities. \n \n   \n Your benefits will include: \n \n \n  Medical, dental, and vision coverage (low copay & deductible)  \n Life insurance  \n Short- and long-term disability \n  401(k) + match \n  Generous Paid Time Off \n  Paid company holidays \n  Tuition reimbursement \n  Flexible Spending Account \n  Employee Assistance Program \n  Summer Hours \n \n \n  EEO STATEMENT \n  MultiPlan is an Equal Opportunity Employer and complies with all applicable laws and regulations. Qualified applicants will receive consideration for employment without regard to age, race, color, religion, gender, sexual orientation, gender identity, national origin, disability or protected veteran status. If you\u2019d like more information on your EEO rights under the law, please  click here .", "cleaned_desc": "", "techs": ""}, "754bcc25919aa46a": {"terms": ["data science", "machine learning engineer"], "salary_min": 150000.0, "salary_max": 180000.0, "title": "Principal AI Engineer", "company": "LivePerson", "desc": "LivePerson (NASDAQ: LPSN) is a global leader in trustworthy and equal AI for business. Hundreds of the world's leading brands \u2014 including HSBC, Chipotle, and Virgin Media \u2014 use our Conversational Cloud platform to engage with millions of consumers safely and responsibly. We power a billion conversational interactions every month, providing a uniquely rich data set and safety tools to unlock the power of Generative AI and Large Language Models for better business outcomes. \n  At LivePerson, we foster an inclusive workplace culture that encourages meaningful connection, collaboration, and innovation. Every mind is invited to ask questions and actively seek new ways to achieve success and reach their full potential. We operate as one with a growth mindset. This means spotting opportunities, solving ambiguities and seeking effective solutions to challenges that make things better. \n  Overview \n  We are seeking a Principal Prompt engineer to join our team to work with various Large Language Model (LLM) technologies and providers. The successful candidate will be responsible for designing and testing prompts and prompting methods for a variety of use cases and models, ensuring that the output is accurate, relevant, and high-quality. \n  If you are passionate about Natural Language Processing, Conversational AI, and have experience working with advanced language generation models, we encourage you to apply for this exciting opportunity to bring cutting-edge Generative AI technologies to customers around the world. \n \n \n  You will: \n \n Develop, implement, and test prompting strategies for a variety of products over various LLMs. \n Work with production engineers to deploy these prompts into production \n Collaborate with other engineers, data scientists, and analysts to improve the accuracy and quality of LLM responses \n Analyze internal and external feedback and behaviors to continuously improve the quality of prompts and language output \n Stay up-to-date with the latest developments in Generative AI specifically, and Natural Language Processing and Machine Learning in general. \n \n \n \n  You have: \n \n 8 years of industry experience \n Foundational knowledge of Natural Language Processing and Machine Learning \n Experience working with Large Language Models such as GPT-3/4, ChatGPT, Claude, or CoHere \n Proficiency in Python \n Excellent analytical and problem-solving skills \n Strong attention to detail and ability to manage multiple tasks simultaneously \n Effective communication skills and ability to collaborate with cross-functional teams \n Bachelor's degree \n \n \n \n  Preferred qualifications \n \n Master's or Ph.D. degree in Computer Science or related field \n Experience working with other Machine Learning models or approaches such as BERT or Reinforcement Learning \n Familiarity with deep learning frameworks such as TensorFlow or PyTorch \n Experience with large-scale data processing and analysis \n Familiarity with cloud computing platforms such as GCP, AWS, or Azure \n \n Benefits: \n \n he salary range for this role will be between $150,000 to $180,000. Final compensation will be determined by a variety of factors, including, but not limited to, your location, skills, experience, education, and/or certifications. During the phone screening, the recruiter will provide the location-specific salary range for this role. Regardless of your personal situation or where you are in the world, LivePerson offers comprehensive and great benefits programs to meet your needs: \n \n Health: medical, dental, vision and wellbeing.   \n Time away: Public holidays and discretionary PTO package for flexible days off with manager approval.   \n Financial: 401K, ESPP, Basic life and AD&D insurance, long-term and short-term disability   \n Family: parental leave, maternity support, fertility services.   \n Development: tuition reimbursement, native AI learning.   \n Additional: 24/7 access to professional counselors, voluntary insurance coverage, exclusive perks and discounts.   \n #LI-Remote   \n \n Why you'll love working here: \n  LivePerson is a hub for the ever-curious and proactive, offering a flexible work-life balance tailored to individual needs. With offices and WeWork locations worldwide, our flexible work policy provides our teams the freedom to work from their preferred environment. We're very proud to have earned recognition from Fast Company, Newsweek, and BuiltIn for being a top innovative, beloved, and remote-friendly workplace. \n  Belonging at LivePerson: \n  We are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants with criminal histories, consistent with applicable federal, state, and local law. \n  We are committed to the accessibility needs of applicants and employees. We provide reasonable accommodations to job applicants with physical or mental disabilities. Applicants with a disability who require reasonable accommodation for any part of the application or hiring process should inform their recruiting contact upon initial connection.", "cleaned_desc": "LivePerson (NASDAQ: LPSN) is a global leader in trustworthy and equal AI for business. Hundreds of the world's leading brands \u2014 including HSBC, Chipotle, and Virgin Media \u2014 use our Conversational Cloud platform to engage with millions of consumers safely and responsibly. We power a billion conversational interactions every month, providing a uniquely rich data set and safety tools to unlock the power of Generative AI and Large Language Models for better business outcomes. \n  At LivePerson, we foster an inclusive workplace culture that encourages meaningful connection, collaboration, and innovation. Every mind is invited to ask questions and actively seek new ways to achieve success and reach their full potential. We operate as one with a growth mindset. This means spotting opportunities, solving ambiguities and seeking effective solutions to challenges that make things better. \n  Overview \n  We are seeking a Principal Prompt engineer to join our team to work with various Large Language Model (LLM) technologies and providers. The successful candidate will be responsible for designing and testing prompts and prompting methods for a variety of use cases and models, ensuring that the output is accurate, relevant, and high-quality. \n  If you are passionate about Natural Language Processing, Conversational AI, and have experience working with advanced language generation models, we encourage you to apply for this exciting opportunity to bring cutting-edge Generative AI technologies to customers around the world. \n \n \n  You will: \n \n Develop, implement, and test prompting strategies for a variety of products over various LLMs.   Work with production engineers to deploy these prompts into production \n Collaborate with other engineers, data scientists, and analysts to improve the accuracy and quality of LLM responses \n Analyze internal and external feedback and behaviors to continuously improve the quality of prompts and language output \n Stay up-to-date with the latest developments in Generative AI specifically, and Natural Language Processing and Machine Learning in general. \n \n \n \n  You have: \n \n 8 years of industry experience   Foundational knowledge of Natural Language Processing and Machine Learning \n Experience working with Large Language Models such as GPT-3/4, ChatGPT, Claude, or CoHere \n Proficiency in Python \n Excellent analytical and problem-solving skills \n Strong attention to detail and ability to manage multiple tasks simultaneously \n Effective communication skills and ability to collaborate with cross-functional teams \n Bachelor's degree \n \n \n    Preferred qualifications \n \n Master's or Ph.D. degree in Computer Science or related field \n Experience working with other Machine Learning models or approaches such as BERT or Reinforcement Learning \n Familiarity with deep learning frameworks such as TensorFlow or PyTorch \n Experience with large-scale data processing and analysis \n Familiarity with cloud computing platforms such as GCP, AWS, or Azure \n \n Benefits: \n ", "techs": ["liveperson (nasdaq: lpsn)", "conversational cloud platform", "generative ai", "large language models", "natural language processing", "principal prompt engineer", "prompting strategies", "large language models (gpt-3/4", "chatgpt", "claude", "cohere)", "python", "generative ai technologies", "natural language processing", "machine learning", "gpt-3/4", "chatgpt", "claude", "cohere", "python", "deep learning frameworks (tensorflow", "pytorch)", "large-scale data processing and analysis", "cloud computing platforms (gcp", "aws", "azure)"]}, "c8886d38d8570385": {"terms": ["data science"], "salary_min": 149833.16, "salary_max": 189722.16, "title": "Cloud Data Architect - AWS Databases", "company": "DoiT International", "desc": "Location \n  Our Cloud Data Architect will be an integral part of our Cloud Reliability Engineering team in North America. \n  Who We Are  DoiT helps fast-growing, digital native companies globally to harness public cloud technology and services to drive business growth. A full-service provider of multi-cloud technology and expertise, DoiT combines the power of intelligent software with deep expertise in Kubernetes, artificial intelligence, and more to deliver the true promise of the cloud at peak efficiency - with ease, not cost. \n  An award-winning strategic partner of AWS, Google Cloud, and Microsoft Azure with $2B cloud spend under management, DoiT works alongside more than 3,000 customers in 70 countries. At DoiT, you'll join a growing team of committed, experienced, and collaborative \"Do'ers\" who are passionate about solving the most complex cloud challenges. \n  The Opportunity \n \n Work closely with clients to understand their business requirements and provide technical guidance and best practices \n Work with customers to design, implement and troubleshoot scalable and secure Relational DBs and NoSQL solutions around AWS Cloud \n Work closely with clients to understand their business requirements and provide technical guidance and best practices \n Collaborate with clients technical teams, including software engineers, cloud operations, data engineers, DBAs, project managers, and business stakeholders, to help them deliver their projects successfully on time and within budget \n Stay up-to-date with the latest AWS technologies and provide thought leadership on how to leverage them to benefit clients \n Provide technical support and troubleshooting for AWS-based services \n \n Responsibilities \n \n Collaborate with customers development teams, architects, operations teams and leadership to design and architect highly scalable, reliable, and secure cloud solutions on the AWS platform \n Identify and resolve performance and availability issues with customers \n Help customer monitor and optimize resource utilization to minimize costs \n Stay up-to-date with the latest AWS features and best practices \n The expertise to engineer, develop, and troubleshoot large production-grade distributed systems on Amazon Web Services (AWS), select the appropriate tools to tackle business problems at the right scale \n Designing, building and testing cloud native applications on AWS cloud infrastructure \n \n Qualifications \n \n Expertise to administer and troubleshoot mission-critical database systems on Amazon Web Services (AWS) \n Ability to analyze needs and requirements to design and recommend appropriate database solutions by using AWS services \n DBA experience directly supporting at least one of the following: RDS for MySQL / RDS for PostgreSQL / RDS for SQL Server / RDS Aurora, with particular strengths in: \n Deployment, Migrations and architectural best practices \n Management and Operations \n Monitoring and troubleshooting \n Security and encryption \n Optimization and performance tuning \n Experience with RDS features and limitations \n Experience in identifying and optimizing application database code \n Experience performing Database migrations, including AWS DMS \n \n Bonus Points \n \n AWS Certified Database - Specialty \n \n Are you a Do'er? \n  Be your truest self. Work on your terms. Make a difference. \n  We are home to a global team of incredible talent who work remotely and have the flexibility to have a schedule that balances your work and home life. We embrace and support leveling up your skills professionally and personally. \n  What does being a Do'er mean? We're all about being entrepreneurial, pursuing knowledge and having fun! \n  Sounds too good to be true? Check out our Glassdoor Page. \n  We thought so too, but we're here and happy we hit that 'apply' button. \n \n Unlimited PTO \n Flexible Working Options \n Health Insurance \n Parental Leave \n Employee Stock Option Plan \n Home Office Allowance \n Professional Development Stipend \n Peer Recognition Program \n \n #LI-Remote", "cleaned_desc": " Provide technical support and troubleshooting for AWS-based services \n \n Responsibilities \n \n Collaborate with customers development teams, architects, operations teams and leadership to design and architect highly scalable, reliable, and secure cloud solutions on the AWS platform \n Identify and resolve performance and availability issues with customers \n Help customer monitor and optimize resource utilization to minimize costs \n Stay up-to-date with the latest AWS features and best practices \n The expertise to engineer, develop, and troubleshoot large production-grade distributed systems on Amazon Web Services (AWS), select the appropriate tools to tackle business problems at the right scale \n Designing, building and testing cloud native applications on AWS cloud infrastructure \n   Qualifications \n \n Expertise to administer and troubleshoot mission-critical database systems on Amazon Web Services (AWS) \n Ability to analyze needs and requirements to design and recommend appropriate database solutions by using AWS services \n DBA experience directly supporting at least one of the following: RDS for MySQL / RDS for PostgreSQL / RDS for SQL Server / RDS Aurora, with particular strengths in: \n Deployment, Migrations and architectural best practices \n Management and Operations \n Monitoring and troubleshooting \n Security and encryption \n Optimization and performance tuning \n Experience with RDS features and limitations ", "techs": ["aws", "amazon web services", "rds", "mysql", "postgresql", "sql server", "aurora"]}, "b0a28a235289a06f": {"terms": ["data science"], "salary_min": 181212.9, "salary_max": 229455.9, "title": "Sr. Lead Product Marketing Manager, Generative AI", "company": "Pinterest", "desc": "About Pinterest : \n  Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. \n  Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences. \n  Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our  PinFlex  landing page to learn more. \n \n  You will lead our Generative AI go-to-market strategy, and partner across key cross-functional teams such as legal, comms, and marketing to own Pinterest's unique positioning on this emerging area in the market. You will influence our efforts to leverage Generative AI technology by providing key inputs to the product strategy to help more Pinners on their journeys from inspiration to action. \n  What you'll do: \n \n Shape our Generative AI product direction through market analysis, research and close collaboration with our Product, Engineering, and Design teams \n Assess the capabilities and value proposition of our Generative AI products through strategic, qualitative, and quantitative analysis \n Serve as an advocate and represent the voice of customers; ensure broad awareness of and empathy for customer pain points internally \n Drive cross-functional alignment throughout the go-to-market process, tracking success metrics accordingly \n Define the messaging, positioning, and launch plan for new Generative AI products \n Use data and insights to drive adoption of strategic priorities and influence our overall monetization strategy and consumer growth initiatives \n Educate and inspire internal and external teams on Pinterest's Generative AI solutions for consumers and advertisers \n Work closely with cross functional partners in product, engineering, data science, design, sales, marketing, finance, legal and other teams to develop and launch new Generative AI products \n \n What we're looking for: \n \n 8+ years of product marketing and/or product management experience at a fast growing ad tech company in: \n \n Machine Learning \n Consumer or advertising product development \n \n Deeply knowledgeable and curious about the Generative AI space \n Strong communicator at both executive and team levels \n A trusted collaborator with a gift for building relationships across teams and functions \n A passion for Pinterest! \n \n This position is not eligible for relocation assistance. \n  #LI-REMOTE  #LI-REX \n \n \n  At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. \n  Information regarding the culture at Pinterest and benefits available for this position can be found  here . \n \n  US based applicants only \n \n    $158,950\u2014$327,000 USD\n   \n \n \n  Our Commitment to Diversity: \n  Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.", "cleaned_desc": "", "techs": ""}, "698f0759473b8f72": {"terms": ["data science"], "salary_min": null, "salary_max": null, "title": "Regional Sales Manager", "company": "Everactive", "desc": "About the Role \n  We are looking for a dynamic and seasoned sales executive to accelerate our sales effort in North America with industrial enterprise customers, helping to execute and lead both direct and channel sales efforts. \n  We are open to candidates joining us in any of our three locations: Santa Clara, CA, Charlottesville, VA, or Ann Arbor, MI. We'll also consider candidates outside of these locations, provided that s/he is located near a major airport. \n  This position will require regular travel (approx. 50%-75%) in order to build and maintain customer and channel partner relationships. \n \n \n  Required Qualifications \n  Battle-tested: you have 10+ years of demonstrated ability, and proven success selling complex solution products into large (Fortune 500) industrial organizations with minimal supervision \n  Relentless: you have a reputation for going to extraordinary lengths to win, retain, and grow customers \n  Well-connected: vast network of contacts from years of selling to a range of industrial customers and channel partners \n  Entrepreneurial: you are comfortable in a small, VC-backed, technology start-up environment; able to be effective with minimal support and an evolving product set \n  Tech-savvy: you have experience with a broad range of technologies, especially SaaS (or other \"as-a-Service\"), Industrial IoT, and/or connected devices and infrastructure \n  Clear communicator: must be able to effectively work with product and business management teams from any location to clearly articulate, and support, customer needs; able to relay market insight in support of feature and product development \n \n \n  Additional Desired Qualifications  \n Industrial by nature: experience selling into large industrial companies, especially in relevant process-related verticals such as food and beverage, consumer processed goods, chemical processing, pharmaceuticals, and/or oil and gas \n  Inspiring leader: comfortable serving as a key evangelist for the company and product solutions throughout the customer base and industry \n  Not afraid of dirt: able to occasionally wear and maintain personal protective equipment including a hard hat, safety glasses, hearing protection, gloves, and coveralls! \n  Educated: BS or MS degree in business, engineering, computer science or data science related field (MBA preferred) \n   \n About our Values \n  Successful Everactive employees routinely exhibit our four core values: \n \n Pioneering ideas: to revolutionize computing, we look for the best ideas without being constrained by existing approaches. This clean slate, first principles approach expands our creative scope and helps us lead development of the next generation IIoT with field-changing technology. \n Rigorous, honest thinking: we approach all of our endeavors with intellectual honesty and rigor. This honesty leads us to careful self-evaluation - identifying and admitting what we do not know \u2013 and to making hard decisions when we believe they are right. \n Live well, work well: we believe that there are many things in life that are more important than work and that a fulfilling life outside of work makes it easier to do excellent work when you are focusing on the work part of life. Likewise, working well makes life more fulfilling. \n \n Everactive is an Equal Opportunity Employer. \n  We are committed to recruit, hire, and develop an exceptional team of individuals aligned by a common vision of the future of computing. We recognize that our team is strongest when we respect and embrace differences while cultivating a culture of inclusion across race, gender, age, religion, identity, orientation, beliefs, and life experiences that make us unique. Our diversity is what drives our greatest achievements. \n  #LI-Remote", "cleaned_desc": "", "techs": ""}, "0df48d321f51c62d": {"terms": ["data science"], "salary_min": 178992.14, "salary_max": 226643.92, "title": "Solution Architect, Principal", "company": "Infor", "desc": "General information \n       \n \n \n \n \n \n \n        Country \n        \n United States  \n \n \n \n        City \n        \n Remote Location  \n \n \n \n        Department \n        \n Development  \n \n \n \n        Job ID \n        \n 36419  \n \n \n \n \n \n \n \n \n \n       Description & Requirements \n       \n \n \n \n \n \n \n The Solution Architect is responsible for researching, documenting, and configuring the HC Golden Tenant as well as maintaining current data for Cloudsuite Healthcare. This role requires deep functional knowledge of Supply Management in the Healthcare Industry. It is highly collaborative with Solution Consulting, Strategy, Product Management, Consulting, and an offshore team supporting data automation and quality assurance. The golden tenant is the Healthcare prescriptive solution used in product demonstrations and in Infor Services deployments. \n  A Day in The Life Typically Includes: \n \n  Managing the build of new functionality and supporting ongoing configurations, for the Supply Management golden tenant.  \n Creating and maintaining Supply Management demo transactional data as necessary to support use cases demonstrated by Solution Consulting and Infor Services Business process walkthroughs with implementing clients. \n  Collaborating with Finance, HRT/Payroll and WFM Architects for the Golden Image as well as Industry Strategists, Solution Consulting, Product Management and Solutions and Innovation to define new or to modify existing builds.  \n Providing input to Product Management on new feature requirements and capabilities. \n  Collaborating with and mentoring the offshore resources supporting Supply Management capabilities and automated data and QA for the golden image.  \n Inventory control and migration of content from Test to Demo to Golden Tenant as required \n  Owning resolution for all RequestX tickets that are related to Supply Management in the golden tenant. \n \n  Basic Qualifications: \n \n  ERP lead consulting experience with Supply Management Infor ERP products or equivalent experience with a Healthcare provider \n  Supply Management functional practitioner knowledge in a Healthcare setting.  \n Experience leading and ability to collaborate and mentor functional skills with employees that have less functional depth.  \n Experience with complex sales or delivery engagements  \n \n Preferred Qualifications: \n \n  BA/BS Degree \n  MBA \n  \u201cBig 4\u201d Consulting or equivalent experience \n  Job Location: US Remote (Alpharetta, GA, Dallas, TX, St. Paul, MN)\n         \n \n \n \n \n \n \n \n         About Infor\n         \n \n \n  Infor is a global leader in business cloud software products for companies in industry specific markets. Infor builds complete industry suites in the cloud and efficiently deploys technology that puts the user experience first, leverages data science, and integrates easily into existing systems. Over 60,000 organizations worldwide rely on Infor to help overcome market disruptions and achieve business-wide digital transformation.\n         \n \n          For more information visit www.infor.com\n         \n \n \n \n \n \n \n \n         Our Values\n         \n \n \n  At Infor, we strive for an environment that is founded on a business philosophy called Principle Based Management\u2122 (PBM\u2122) and eight Guiding Principles: integrity, stewardship & compliance, transformation, principled entrepreneurship, knowledge, humility, respect, self-actualization. Increasing diversity is important to reflect our markets, customers, partners, and communities we serve in now and in the future.\n         \n \n \n  We have a relentless commitment to a culture based on PBM. Informed by the principles that allow a free and open society to flourish, PBM\u2122 prepares individuals to innovate, improve, and transform while fostering a healthy, growing organization that creates long-term value for its clients and supporters and fulfillment for its employees.\n         \n \n \n  Infor is an Equal Opportunity Employer. We are committed to creating a diverse and inclusive work environment. Infor does not discriminate against candidates or employees because of their sex, race, gender identity, disability, age, sexual orientation, religion, national origin, veteran status, or any other protected status under the law.\n          \n \n \n \n \n         At Infor we value your privacy that\u2019s why we created a policy that you can read here.\n         \n \n \n \n \n \n \n \n \n         This employer uses E-Verify. Please visit the following website for additional information: www.kochcareers.com/doc/Everify.pdf", "cleaned_desc": " \n  Managing the build of new functionality and supporting ongoing configurations, for the Supply Management golden tenant.  \n Creating and maintaining Supply Management demo transactional data as necessary to support use cases demonstrated by Solution Consulting and Infor Services Business process walkthroughs with implementing clients. \n  Collaborating with Finance, HRT/Payroll and WFM Architects for the Golden Image as well as Industry Strategists, Solution Consulting, Product Management and Solutions and Innovation to define new or to modify existing builds.  \n Providing input to Product Management on new feature requirements and capabilities. \n  Collaborating with and mentoring the offshore resources supporting Supply Management capabilities and automated data and QA for the golden image.  \n Inventory control and migration of content from Test to Demo to Golden Tenant as required \n  Owning resolution for all RequestX tickets that are related to Supply Management in the golden tenant. \n \n  Basic Qualifications: \n \n  ERP lead consulting experience with Supply Management Infor ERP products or equivalent experience with a Healthcare provider \n  Supply Management functional practitioner knowledge in a Healthcare setting.  \n Experience leading and ability to collaborate and mentor functional skills with employees that have less functional depth.  \n Experience with complex sales or delivery engagements  \n \n Preferred Qualifications: \n \n  BA/BS Degree \n  MBA \n  \u201cBig 4\u201d Consulting or equivalent experience \n  Job Location: US Remote (Alpharetta, GA, Dallas, TX, St. Paul, MN)\n         \n ", "techs": ["infor erp products", "healthcare provider", "complex sales", "erp lead consulting", "supply management", "\"big 4\" consulting", "ba/bs degree", "mba"]}, "b6f576db18fdf288": {"terms": ["data science", "machine learning engineer"], "salary_min": 107478.43, "salary_max": 136091.64, "title": "SOAR Cybersecurity Engineer", "company": "Amyx, Inc.", "desc": "Overview: \n  \n   Amyx is seeking a Zero Trust Security, Orchestration, Automation and Response (SOAR) Cyber Security Engineer (CSE) to support our TRANSCOM J6 SISO contract located at Scott AFB, IL\n  \n \n   In this role, the SOAR CSE will bring the background and experience collecting and analyzing data, and developing/implementing automated solutions that trigger worflows, alerts and other actions based on the results of data analysis. Data collected and analyzed will be retrieved from tools, applications and devices that comprise on-premises and cloud-based network environments. The ideal candidate will work collaboratively with our team and our customer to assess existing data collection and automation capabilities, examine other opportunities for automation, and increase the value of collected such that it can be used to further automation capabilities and provide additional insight into the overall security posture of network environments and applications running in those environments. With regards to Zero Trust, this position will require the candidate to become the resident expert of the \"Visibility & Analytics\" pillar of capabilities and activities that are a key part of overall zero trust efforts.\n   Responsibilities: \n  \n Determine and document the data that can be exported/extracted from the tools and applications that comprise a zero trust network architecture \n Conduct research on and become familiar with a variety of Security, Orchestration, Automation and Response (SOAR) solutions \n Understand how machine learning and artificial intelligence tools and processes can be used to enhance automated decision making \n Identify and implement opportunities to enrich data with the inclusion of relevant data from approved external sources \n Understand the input and output of in-place and planned Security Information and Event Management (SIEM) tools and how such data might be used to trigger automation actions and workflows \n Develop the actions and workflows that occur when data indicates conditions have been met that trigger the action or workflow \n Understand how raw data formats can be extracted from other tools, parsed and converted so it can be ingested and used by SOAR solutions \n Recommend SOAR solutions that provide the most benefit to the overall zero trust goals and objectives and enhance patch management/incident response capabilities \n Develop, test and deploy data filters and algorithms that correlate incidents and events with data from different data sources \n Develop, test and deploy data filters and algorithms that identify high-severity concerns identified through data analysis and prioritizes them for remediation/mitigation \n Develop and implement data mapping strategies to ensure ingested data is translated into the fields and expected formats required by SOAR tools and supports up-channel reporting \n Develop, test, and deploy data filters and algorithms that identify trends and anomalies within data sets \n Use SOAR tools to ingest and analyze user and device behavioral data to highlight behaviors that are outside of the norm \n Work with security analysts to identify points of interest that may be analyzed to enhance SIEM and incident response capabilities  \n Develop data-driven charts, graphs and infographics that illustrate the status of high-interest and high-severity subjects \n Examine/Explore new sources of useful raw data to include queries of endpoint agents, API calls, device and application logs, etc... \n \n \n \n  Technology used:\n  \n \n   Zero Trust, NiFi, Elastic, Splunk, Kafka\n   Qualifications: \n  \n   Required:\n  \n \n BS degree in one of the following: computer engineering, computer science, IT, cyber security, or a related field.  \n Relevant years of experience may be used in substitution for situations where the candidate does not have the required Bachelr's degree.  \n Possess a Certified Information Systems Security Professional (CISSP) or similar IA III certification and \n Possess DoD 8570/8140 CSSP Infrastructure Support certification \n Active Secret Clearance. \n At least 5 years of experience in Data science, cybersecurity, information technology, or related fields.  \n Expertise in large-scale data collection, transformation, correlation, and analysis. \n Experience developing, implementing and fine-tuning automation workflows/playbooks.  \n Knowledge of EO 14028, Federal, DoD, and other Zero Trust efforts and initiatives.  \n Experience identifying trends, anomalies, risk and issues of concern within data sets.  \n Excellent communication, collaboration, and problem-solving skills.  \n Ability to work independently and as part of a team \n Understanding of key IT/Cybersecurity concepts (network security, security operations & administration, incident response & recovery, identity and access management, vulnerability management, zero trust). \n Experience with SIEM, SOAR and endpoint protection tools. \n Understanding of Intrusion Detection and Prevention Systems (IDS, IPS) and Data Loss Prevention (DLP). \n Understanding of Unclassified and Classified Government networks. \n Experience supporting RMF/NIST/FedRAMP accreditation and assessment of systems. \n Experience implementing STIG requirements for Microsoft and Linux operating systems, services and applications. \n Familiarity with DevSecOps concepts and approaches. \n Experience with data technologies such as NiFi, Elastic, Splunk and Kafka. \n \n \n \n  Desired:\n  \n \n BS degree and at least 10 years cybersecurity experience  \n Demonstrated knowledge and understanding of the USTRANSCOM mission \n \n \n \n  Benefits include:\n  \n \n  Medical, Dental, and Vision Plans (PPO & HSA options available) \n  Flexible Spending Accounts (Health Care & Dependent Care FSA) \n  Health Savings Account (HSA) \n  401(k) with matching contributions \n  Roth \n  Qualified Transportation Expense with matching contributions \n  Short Term Disability \n  Long Term Disability \n  Life and Accidental Death & Dismemberment \n  Basic & Voluntary Life Insurance \n  Wellness Program \n  PTO \n  11 Holidays \n  Professional Development Reimbursement \n \n \n \n  Please contact talent@amyx.com with any questions!\n  \n \n \n  Amyx is an Equal Opportunity employer. Amyx is committed to providing equal employment opportunity to all job seekers. Every qualified applicant receives focused consideration for employment and no one is discriminated against on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status. In addition to federal law requirements, Amyx complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Equal Opportunity Employer- Minorities/Females/Veterans/Individuals with Disabilities/Sexual Orientation/Gender Identity. Amyx is an E-Verify employer.\n   \n  Amyx proudly and proactively takes affirmative action to advance employment of individuals who are minorities, women, protected veterans and individuals with disabilities.\n   \n \n \n \n Physical Demands \n \n  Employee needs to be able to sit at a workstation for extended periods; use hand(s) to handle or feel objects, tools, or controls; reach with hands and arms; talk and hear. Most positions require ability to work on desktop or laptop computer for extended periods of time reading, reviewing/analyzing information, and providing recommendations, summaries and/or reports in written format. Must be able to effectively communicate with others verbally and in writing. Employee may be required to occasionally lift and/or move moderate amounts of weight, typically less than 20 pounds. Regular and predictable attendance is essential.", "cleaned_desc": "Overview: \n  \n   Amyx is seeking a Zero Trust Security, Orchestration, Automation and Response (SOAR) Cyber Security Engineer (CSE) to support our TRANSCOM J6 SISO contract located at Scott AFB, IL\n  \n \n   In this role, the SOAR CSE will bring the background and experience collecting and analyzing data, and developing/implementing automated solutions that trigger worflows, alerts and other actions based on the results of data analysis. Data collected and analyzed will be retrieved from tools, applications and devices that comprise on-premises and cloud-based network environments. The ideal candidate will work collaboratively with our team and our customer to assess existing data collection and automation capabilities, examine other opportunities for automation, and increase the value of collected such that it can be used to further automation capabilities and provide additional insight into the overall security posture of network environments and applications running in those environments. With regards to Zero Trust, this position will require the candidate to become the resident expert of the \"Visibility & Analytics\" pillar of capabilities and activities that are a key part of overall zero trust efforts.\n   Responsibilities: \n  \n Determine and document the data that can be exported/extracted from the tools and applications that comprise a zero trust network architecture \n Conduct research on and become familiar with a variety of Security, Orchestration, Automation and Response (SOAR) solutions \n Understand how machine learning and artificial intelligence tools and processes can be used to enhance automated decision making \n Identify and implement opportunities to enrich data with the inclusion of relevant data from approved external sources \n Understand the input and output of in-place and planned Security Information and Event Management (SIEM) tools and how such data might be used to trigger automation actions and workflows \n Develop the actions and workflows that occur when data indicates conditions have been met that trigger the action or workflow \n Understand how raw data formats can be extracted from other tools, parsed and converted so it can be ingested and used by SOAR solutions \n Recommend SOAR solutions that provide the most benefit to the overall zero trust goals and objectives and enhance patch management/incident response capabilities \n Develop, test and deploy data filters and algorithms that correlate incidents and events with data from different data sources \n Develop, test and deploy data filters and algorithms that identify high-severity concerns identified through data analysis and prioritizes them for remediation/mitigation \n Develop and implement data mapping strategies to ensure ingested data is translated into the fields and expected formats required by SOAR tools and supports up-channel reporting \n Develop, test, and deploy data filters and algorithms that identify trends and anomalies within data sets   Use SOAR tools to ingest and analyze user and device behavioral data to highlight behaviors that are outside of the norm \n Work with security analysts to identify points of interest that may be analyzed to enhance SIEM and incident response capabilities  \n Develop data-driven charts, graphs and infographics that illustrate the status of high-interest and high-severity subjects \n Examine/Explore new sources of useful raw data to include queries of endpoint agents, API calls, device and application logs, etc... \n \n \n \n  Technology used:\n  \n \n   Zero Trust, NiFi, Elastic, Splunk, Kafka\n   Qualifications: \n  \n   Required:\n  \n \n BS degree in one of the following: computer engineering, computer science, IT, cyber security, or a related field.  \n Relevant years of experience may be used in substitution for situations where the candidate does not have the required Bachelr's degree.  \n Possess a Certified Information Systems Security Professional (CISSP) or similar IA III certification and \n Possess DoD 8570/8140 CSSP Infrastructure Support certification   Active Secret Clearance. \n At least 5 years of experience in Data science, cybersecurity, information technology, or related fields.  \n Expertise in large-scale data collection, transformation, correlation, and analysis. \n Experience developing, implementing and fine-tuning automation workflows/playbooks.  \n Knowledge of EO 14028, Federal, DoD, and other Zero Trust efforts and initiatives.  \n Experience identifying trends, anomalies, risk and issues of concern within data sets.  \n Excellent communication, collaboration, and problem-solving skills.  \n Ability to work independently and as part of a team \n Understanding of key IT/Cybersecurity concepts (network security, security operations & administration, incident response & recovery, identity and access management, vulnerability management, zero trust). \n Experience with SIEM, SOAR and endpoint protection tools. \n Understanding of Intrusion Detection and Prevention Systems (IDS, IPS) and Data Loss Prevention (DLP). \n Understanding of Unclassified and Classified Government networks. \n Experience supporting RMF/NIST/FedRAMP accreditation and assessment of systems. \n Experience implementing STIG requirements for Microsoft and Linux operating systems, services and applications. \n Familiarity with DevSecOps concepts and approaches. \n Experience with data technologies such as NiFi, Elastic, Splunk and Kafka. \n \n \n \n  Desired:", "techs": ["zero trust", "nifi", "elastic", "splunk", "kafka"]}, "17182aa90f7849f0": {"terms": ["data science"], "salary_min": 120000.0, "salary_max": 200000.0, "title": "Remote Bioinformatics Data Scientist", "company": "CyberCoders", "desc": "Remote Bioinformatics Data Scientist \n  \n  If you are a Bioinformatics Data Scientist with Python experience, please read on!\n   \n  -Remote in US w/ Bi-Monthly visits to Saint Louis | Full-Time/Permanent-\n   \n  We are focused on improving the early detection and prevention of colorectal cancer, the second leading cause of cancer-related deaths in the United States. Our company conducts research to better comprehend the molecular underpinnings of colorectal cancer formation and progression in addition to creating diagnostic tools!\n  \n  What You Need for this Position \n \n Ph.D. OR M.A. + 2 years industry experience OR B.A. + 4 yrs industry experience in informatics, or machine-learning or biostatistics or related discipline \n Bioinformatics \n Python/R \n DNA/RNA \n Statistics \n Machine Learning(plus skills) \n \n  What You Will Be Doing \n \n Create Pipelines for Automating Data Analysis: Data trending, data assembly, and graphical analysis. \n Statistical Modeling: Novel approaches for understanding how drugs act on biological systems and how the body responds to the drug \n Design and oversee test applications of clinical trial simulations and their respective drug candidates \n Develop and maintain quality reporting and CAPAs for regulatory and client audits. \n \n  What's In It for You \n \n Competitive base salary \n 401k  \n PTO \n Full benefits including medical, dental, and vision \n Opportunity for professional growth \n Awesome Glassdoor review \n Great company culture and work-life balance! \n Much more! \n \n \n   So, if you are a Remote Bioinformatics Data Scientist with experience, please apply today!\n  \n \n   Colorado employees will receive paid sick leave. For additional information about available benefits, please contact James Shramek\n  \n \n Applicants must be authorized to work in the U.S. \n  CyberCoders is proud to be an Equal Opportunity Employer \n \n  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.\n  \n \n Your Right to Work  \u2013 In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.", "cleaned_desc": "", "techs": ""}, "6045eea523ab5375": {"terms": ["data science"], "salary_min": 100000.0, "salary_max": 150000.0, "title": "Senior Systems Analyst - Cloud FinOps", "company": "CyberCoders", "desc": "Senior Systems Analyst - Cloud FinOps \n  \n  If you are a Senior Systems Analyst with Cloud FinOps experience, please read on!\n   \n  Headquartered in beautiful Austin TX with remote teams across the nation, we are a booming tech company with a proprietary FinOps platform! Due to growth and demand for our services, we are urgently looking to add several Senior Systems Analysts to our diverse and growing team!\n  \n  Top Reasons to Work with Us \n \n HUGE opportunity for growth! \n Competitive base salary + Equity + Benefits! \n Cutting-edge tech! \n Fully remote opportunity! \n \n  What You Will Be Doing \n \n Evolving a cutting-edge autonomous cloud cost optimization platform  \n Solving complex operational problems \n Working cross-organizationally with world-class teams in engineering, product, and data science \n \n  What You Need for this Position \n \n BS in Mathematics, Computer Science, or equivalent technical degree preferred \n 4+ years of professional experience in financial analysis and cost optimization, preferably with AWS and/or Azure costs \n Strong understanding of Reserved Instances \n Not afraid of math, spreadsheets, and problem solving \n \n  What's In It for You \n \n Competitive base salary ($100-150k DOE) \n Comprehensive benefits package (Medical, Dental, Vision) \n 401k \n Equity \n Unlimited PTO  \n Excellent work/life balance \n Small team culture with no corporate overhead, red tape, or unnecessary meetings \n Extremely well-funded and profitable start-up at critical stage of growth  \n Fun and innovative company culture with strong collaboration and personal ownership \n \n \n   So, if you are a Senior Systems Analyst with experience, please apply today!\n  \n \n   Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Noah Gjertsen-Illig\n  \n \n Applicants must be authorized to work in the U.S. \n  CyberCoders is proud to be an Equal Opportunity Employer \n \n  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.\n  \n \n Your Right to Work  \u2013 In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.", "cleaned_desc": "", "techs": ""}, "f870f79476915e99": {"terms": ["data science"], "salary_min": 150125.19, "salary_max": 190091.94, "title": "Developer Advocate", "company": "MongoDB", "desc": "The worldwide data management software market is massive (IDC forecasts it to be $137.6 billion by 2026!). At MongoDB we are transforming industries and empowering developers to build amazing apps that people use every day. We are the leading developer data platform and the first database provider to IPO in over 20 years. Join our team and be at the forefront of innovation and creativity. \n  We're seeking an AI Developer Advocate to join our Developer Relations team, driving enablement for MongoDB's artificial intelligence (AI) and machine learning (ML) capabilities. As developers increasingly choose MongoDB as their data platform, your role will involve directly engaging the community, creating inspiring resources and content, and collaborating with a global team of skilled Developer Advocates. From startups to Fortune 500 companies, you'll empower developers to integrate advanced data capabilities into their apps and solutions. If you're passionate about AI's potential and enjoy fostering a vibrant developer ecosystem, join us in shaping the future of development on MongoDB. \n  Our Developer Relations teams are distributed around the world and a majority of the group works remotely. This role can work from North America, South America, or Europe locations. Please check out our latest content and activity to see what our team is up to. \n  Candidate Profile \n  We are excited to welcome an influential person to our team who is already an established voice in AI and ML. From your experience over the past few years, we'll see you on videos (and probably at events before that) sharing technical insights, leading discussion, and inspiring others to effectively use AI/ML capabilities. More recently, you've posted content and talks building on your trusted voice as AI's adoption and future has accelerated. \n  At the foundation of your passion and expertise is a love to enable others. Our ideal candidate will have at least 5 years experience in a technical role, as a data scientist or developer, working directly within an enterprise or with customers. In addition to machine learning and AI-related content, the code you've shared and contributed will show your ability and enthusiasm to help others get started or solve innovative problems. \n  You should be an effective teacher with the ability to explain complex issues in a simple, understandable manner. In addition to an appreciation for key, AI-related capabilities of our platform, like vector search and aggregation pipelines, you will develop expertise in the core capabilities of MongoDB, document modeling, and emerging topics across our industry. \n  Though the expertise you bring will be unique, our team is built to work together to share knowledge and help each other grow. You will work with others at all levels, supporting peer reviews for examples, demonstrations, sessions, and other content. You should have a genuine passion for enabling other developers to be more productive and successful. \n  Position Expectations \n \n Leverage your strong foundation in ML/AI and database technologies to become a recognized authority on MongoDB within the organization and the wider community. \n Maintain and continually develop proficiency in one or more complementary programming languages to effectively communicate with developers and contribute to product improvements. \n Data engineering experience and familiarity with popular LLM development frameworks (Langchain / LlamaIndex) \n Work closely with product and growth marketing teams, offering expert counsel on the most effective ways to engage and enable the developer community, both online and in-person. \n Partner with our marketing teams to co-create compelling, authentic messaging and strategies that resonate with our target audiences. \n Partner with our Community team to support growth of our MongoDB user groups, Champions, and influencer programs across the region \n \n Requirements \n \n Expertise building solutions with machine learning technologies and a cloud provider (e.g. AWS, GCP, Azure) \n Expertise in one of more machine/deep learning workflows with integrations to databases \n Prolific author of content (tutorials, how to guides, readme) to help developers learn \n Shared code and content to with the intent to support/enable other developers \n Great track record of presenting at events and/or via YouTube, Webinars, or Live Streams. \n 10% travel \n \n To drive the personal growth and business impact of our employees, we're committed to developing a supportive and enriching culture for everyone. From employee affinity groups, to fertility assistance and a generous parental leave policy, we value our employees' wellbeing and want to support them along every step of their professional and personal journeys. Learn more about what it's like to work at MongoDB, and help us make an impact on the world! \n  MongoDB is committed to providing any necessary accommodations for individuals with disabilities within our application and interview process. To request an accommodation due to a disability, please inform your recruiter. \n  MongoDB is an equal opportunities employer.", "cleaned_desc": "", "techs": ""}, "3660972e017d75b2": {"terms": ["data analyst"], "salary_min": 68000.0, "salary_max": 80000.0, "title": "Entry Level Business Analyst", "company": "GlobalSoftSolution", "desc": "Responsibilities: \n \n Utilizing Waterfall methodologies to effectively assist with the planning and implementation of business and software development plans. \n Involve in formulating the requirements for Functional Requirement Documentation (FRD) and translating them to Technical Design Documentation. \n Involvement in defining terms, conducting a stakeholder analysis, eliciting business needs, conducting business process modeling, and facilitating JAD sessions. \n Involve in designing and developing Use cases and activity diagrams to analyze and translate business requirements into system specifications. \n \n Requirement: \n \n Master\u2019s degree or bachelor\u2019s degree in Business, Management, Science, Technology, Engineering, or any relevant. \n Knowledge of software development methodologies such as SDLC, Agile/Scrum, and Waterfall. \n Knowledge in writing Business Requirements Document (BRD), Functional Requirements Document (FRD) and SRS. \n Knowledge of performing Cost/benefit analysis, GAP analysis, and risk analysis \n \n Job Type: Full-time \n Pay: $68,000.00 - $80,000.00 per year \n Benefits: \n \n 401(k) \n Green card sponsorship \n Paid sick time \n Paid time off \n Visa sponsorship \n \n Compensation package: \n \n Performance bonus \n Yearly bonus \n \n Experience level: \n \n No experience needed \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Education: \n \n Bachelor's (Preferred) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "9ef4525babacd3aa": {"terms": ["data analyst"], "salary_min": 28.18, "salary_max": 45.09, "title": "Data Analyst - Remote", "company": "CEDARS-SINAI", "desc": "Grow your career at Cedars-Sinai! \n  The Enterprise Information Services (EIS) team at Cedars-Sinai understands that true clinical transformation and the optimization of a clinical information systems implementation is fueled through the alignment of the right people, processes, and technologies. \n  Why work here? \n  Cedars-Sinai Medical Center has been ranked the #1 hospital in California by U.S. News & World Report, 2023\u20112024. When you join our team, you\u2019ll gain access to our groundbreaking biomedical research facilities and advanced medical education programs. We offer learning programs, tuition reimbursement and performance-improvement projects so you can achieve certifications and degrees while gaining the knowledge and experience needed to advance your career. \n  We take pride in hiring the best, most hard-working employees. Our dedicated doctors, nurses and staff reflect the culturally and ethnically diverse community we serve. They are proof of our dedication to creating a dynamic, inclusive environment that fuels innovation and the gold standard of patient care we strive for. \n  What will you be doing: \n  The Data Analyst supports Cedars-Sinai\u2019s Enterprise Data Quality Management program. The goal is to improve data integrity and information usability for internal decision making and for the increasing external data demands. This is accomplished by applying data quality management principles in cross functional settings specifically across the administrative and financial information supply chain as well as clinical progression of care. The Enterprise Data Quality Analyst will interact with various business data owners, data production system staff, data analysts, and decision makers to develop and implement proactive data quality practices. \n \n Performs data and process validations for recurring and ad hoc data loads and analytics for internal and external customers. \n Ensures adherence to data quality standards. \n Resolves data quality incidents reported to Data Quality & Governance (DQG) from data users across Cedars-Sinai. \n Uses data quality tools to proactively identify data integrity issues. \n Develops and implements proactive data quality monitoring procedures and practices. \n Monitors, maintains, and extends the effectiveness of DQG\u2019s process and help documentation. \n Innovates and provides collaborative leadership for DQG in documentation and support including advocating for excellence in quality and customer service, increased automation and efficiency, and proactive business process improvements. \n \n \n \n Experience Requirements: \n  One (1)+ years of relevant work experience. Role of a Data Analyst preferred. \n  Demonstrated understanding in data/information quality principles and practices. \n  Knowledgeable in database development cycle including requirements gathering, modeling, coding, and testing. \n  Display intellectual curiosity and ability to articulate business context relevant to the job. \n  Strong problem-solving skills and persistence to solve difficult problems. \n  Ability to work in teams, ability to follow directions, ability to manage completion of multiple tasks within specified timeframes. \n \n \n  Educational/Certification Requirements: \n  Bachelor\u2019s degree in Information Science, Computer Science, a quantitative discipline in science, Health Administration, Public Health, or related healthcare field. (preferred) \n  Master\u2019s degree preferred. \n  Jobs-Indeed \n LI-Remote \n \n \n \n Working Title:  Data Analyst - Remote\n  \n Department:  Data Quality & Goverance\n  \n Business Entity:  Cedars-Sinai Medical Center\n  \n Job Category:  Information Technology\n  \n Job Specialty:  Business Intelligence/Reporting\n  \n Position Type:  Full-time\n  \n Shift Length:  8 hour shift\n  \n Shift Type:  Day\n  \n Base Pay: $28.18 - $45.09", "cleaned_desc": "", "techs": ""}, "6462c9016b8fb9b2": {"terms": ["data analyst"], "salary_min": 51000.0, "salary_max": 51000.0, "title": "Business Data Analyst", "company": "VSP Global", "desc": "The Business Data Analyst supports a business team\u2019s ability to leverage data in support of business monitoring and decision management. Applies a range of analytics and data management techniques to generate accurate data and relevant insights, including the creation of visualizations and dashboards to optimize business decisions and outcomes.\n  \n \n   Engage with business teams to clarify needs and define business requirements for data analysis in support of business optimization and decision-making\n  \n \n \n   Perform data exploration and analysis to identify and interpret trends and patterns in datasets\n  \n \n \n   Create dashboards, visualizations and reports to support business performance monitoring to support stakeholders\u2019 ability to be data driven\n  \n \n \n   Work collaboratively with the enterprise analytics & information management team to articulate business needs\n  \n \n \n   Contribute to data inventory and stewardship efforts, including contributions to metadata management, data profiling, data quality remediation and data cleansing\n  \n \n \n   Perform research within the business area to determine the appropriate definition and logic for cataloging and modeling data within the analytics ecosystem\n  \n \n \n   Perform user acceptance testing to confirm data output and capabilities meet business expectations and align with business processes\n  \n \n \n   Troubleshoot data discrepancies and inconsistencies and partner with the enterprise analytics & Information management team to detect issues and automate resolutions\n  \n \n \n   Maintain high standards and adhere to all enterprise policies and guidelines regarding data security and privacy, data management, data quality, and data governance; regard data as an enterprise asset that is to be protected and used both strategically and appropriately\n  \n \n \n   Job Specifications\n  \n \n \n  Typically has the following skills or abilities: \n \n \n \n   Bachelor\u2019s degree in statistics, analytics, mathematics, economics, computer science or related functional area; or equivalent experience\n  \n \n \n   2+ years of demonstrated data analytics experience including experience querying and analyzing operational and/or financial data, generating insights, and communicating those effectively with colleagues and leaders\n  \n \n \n   Experience building data queries using SQL in order to join, filter, cleanse, and aggregate data; understanding of how GUI-driven applications apply SQL concepts to do the same\n  \n \n \n   Experience creating data visualizations/dashboards that highlight trends and patterns with the ability to monitor business performance against KPIs\n  \n \n \n   Experience using data exploration and analysis platforms such as Cognos Analytics, Tableau, PowerBI, or similar\n  \n \n \n   Familiarity with data modeling concepts and relational database structures\n  \n \n \n   Ability to analyze and document business requirements, inclusive of needs and expectations\n  \n \n \n   Effective written and verbal communication, presentation, and analytical skills\n  \n \n \n   Expert-level skills in Excel\n  \n \n \n   Preferred Skills\n  \n \n \n   Experience providing analytical and reporting support\n  \n \n \n   SQL and Salesforce experience\n  \n \n \n   #LI-REMOTE\n  \n \n   #LI-VISIONCARE\n  \n \n \n   Compensation range for the role is listed below. Applicable salary ranges may differ across markets.\n     Actual pay will be determined based on experience and other job-related factors permitted by law. As a part of the compensation package, this role may include eligible bonuses, equity and commissions. For more information regarding VSP Vision benefits, please \n   \n   click here\n   .\n  \n \n \n   Salary Range:\n   51000\n   85500\n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n                        VSP Vision is an equal opportunity employer and gives consideration for employment to qualified applicants without regard to age, gender, race, color, religion, sex, national origin, gender identity, sexual orientation, disability or protected veteran status. We maintain a drug-free workplace and perform pre-employment substance abuse testing.\n                       \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n   Notice to Candidates: Fraud Alert - Fake Job Opportunity Solicitations Used to Collect Fees/Personal Information.\n  \n \n  We have been made aware that fake job opportunities are being offered by individuals posing as VSP Vision and affiliate recruiters. \n   \n   Click here \n   to learn about our application process and what to watch for regarding false job opportunities.\n  \n \n \n   As a regular part of doing business, VSP Vision (\u201cVSP\u201d) collects many different types of personal information, including protected health information, about our audiences, including members, doctors, clients, brokers, business partners, and employees. VSP Vision employees will have access to this sensitive personal information and are subject to follow Information Security and Privacy Policies.", "cleaned_desc": "The Business Data Analyst supports a business team\u2019s ability to leverage data in support of business monitoring and decision management. Applies a range of analytics and data management techniques to generate accurate data and relevant insights, including the creation of visualizations and dashboards to optimize business decisions and outcomes.\n  \n \n   Engage with business teams to clarify needs and define business requirements for data analysis in support of business optimization and decision-making\n  \n \n \n   Perform data exploration and analysis to identify and interpret trends and patterns in datasets\n  \n \n \n   Create dashboards, visualizations and reports to support business performance monitoring to support stakeholders\u2019 ability to be data driven\n  \n \n \n   Work collaboratively with the enterprise analytics & information management team to articulate business needs\n  \n \n \n   Contribute to data inventory and stewardship efforts, including contributions to metadata management, data profiling, data quality remediation and data cleansing\n  \n \n \n   Perform research within the business area to determine the appropriate definition and logic for cataloging and modeling data within the analytics ecosystem\n  \n \n \n   Perform user acceptance testing to confirm data output and capabilities meet business expectations and align with business processes\n  \n \n \n   Troubleshoot data discrepancies and inconsistencies and partner with the enterprise analytics & Information management team to detect issues and automate resolutions\n  \n   \n   Maintain high standards and adhere to all enterprise policies and guidelines regarding data security and privacy, data management, data quality, and data governance; regard data as an enterprise asset that is to be protected and used both strategically and appropriately\n  \n \n \n   Job Specifications\n  \n \n \n  Typically has the following skills or abilities: \n \n \n \n   Bachelor\u2019s degree in statistics, analytics, mathematics, economics, computer science or related functional area; or equivalent experience\n  \n \n \n   2+ years of demonstrated data analytics experience including experience querying and analyzing operational and/or financial data, generating insights, and communicating those effectively with colleagues and leaders\n  \n \n \n   Experience building data queries using SQL in order to join, filter, cleanse, and aggregate data; understanding of how GUI-driven applications apply SQL concepts to do the same\n  \n \n \n   Experience creating data visualizations/dashboards that highlight trends and patterns with the ability to monitor business performance against KPIs\n  \n \n \n   Experience using data exploration and analysis platforms such as Cognos Analytics, Tableau, PowerBI, or similar\n  \n \n \n   Familiarity with data modeling concepts and relational database structures   \n \n \n   Ability to analyze and document business requirements, inclusive of needs and expectations\n  \n \n \n   Effective written and verbal communication, presentation, and analytical skills\n  \n \n \n   Expert-level skills in Excel\n  \n \n \n   Preferred Skills\n  \n \n \n   Experience providing analytical and reporting support\n  \n \n \n   SQL and Salesforce experience\n  \n \n \n   #LI-REMOTE\n  \n \n   #LI-VISIONCARE\n  \n \n ", "techs": ["data analytics", "visualizations", "dashboards", "sql", "cognos analytics", "tableau", "powerbi", "data exploration and analysis platforms", "data modeling", "relational database structures", "excel", "salesforce"]}, "6aa70479e5ec1901": {"terms": ["data analyst"], "salary_min": 54273.87, "salary_max": 68722.81, "title": "Junior Data Analyst", "company": "Cognize tech solutions", "desc": "Apply Statistical and Machine Learning methods to specific business problems and data. \uf0b7 Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, cross-lingual alignment/mapping, etc. \uf0b7 Using tools such as Tableau, Looker and GoogleData Studio to aggregate data from across our software tools to build and develop dashboards for both client-facing and internal purposes across the company that feature relevant KPIs and metrics \uf0b7 Create data dashboards and other data visualization tools to track progress to inform continuous quality improvement of the Congregate Settings Investigation and Response Unit. \uf0b7 Design, develop, evaluate, and release highly innovative models elevate the customer experience and track impact over time \n Job Types: Part-time, Full-time \n Pay: $65,000.00 - $80,000.00 per hour \n Schedule: \n \n 8 hour shift \n \n Education: \n \n Bachelor's (Required) \n \n Experience: \n \n SQL: 1 year (Required) \n Data analysis skills: 1 year (Required) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "61706b294ed23303": {"terms": ["data analyst"], "salary_min": 65000.0, "salary_max": 75000.0, "title": "Entry Level Business Analyst", "company": "BACloudSystems", "desc": "Responsibilities: \n \n Creating and maintaining required documentation for projects including business requirements, design documents, and test scripts. \n Conduct business process analysis through input from management, managers, supervisors, and subject matter experts. \n Facilitate JAD sessions for creating design documents and system specifications for applications. \n Involvement in managing functional and non-functional Business Requirements and new requested requirements. \n \n Requirements: \n \n Master\u2019s degree or bachelor\u2019s degree in Business, Management, Science, Technology, Engineering, or any relevant. \n Good understanding of business requirements gathering, business process mapping, functional design, and documentation. \n Knowledge in creating BRD, FRS documents, URS, and CR documents for system application development. \n Knowledge of the full SDLC methodologies, frameworks, and analysis to compare As-Is with To-Be business processes. \n \n Job Type: Full-time \n Pay: $65,000.00 - $75,000.00 per year \n Benefits: \n \n 401(k) \n Green card sponsorship \n Health insurance \n Health savings account \n Visa sponsorship \n \n Compensation package: \n \n Bonus opportunities \n Hourly pay \n Performance bonus \n \n Experience level: \n \n No experience needed \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Education: \n \n Bachelor's (Preferred) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "2c596bc52bc893b4": {"terms": ["data analyst"], "salary_min": 42305.98, "salary_max": 53568.8, "title": "Part Time Insights and Data Entry Analyst", "company": "UTZ Brands, Inc", "desc": "Utz Quality Foods is seeking a  Part Time Insights and Data Entry Analyst  to join our team. Working remotely and reporting to the Midwest Sales Director, this position will be responsible for supporting the team on insight and reporting needs internally and externally. This position will perform a variety of support needs under the direction of the Midwest Director of Sales to prioritize projects, critical data/customer administration, pricing, execution reports and more. Responsibilities will also include supporting national support for selling story development, blue planner support, category insights and IRI to support our overarching selling story. This person will also serve as a customer Liaison for key customers, much of the work that a broker has done previously for several national customers. \n  Key Responsibilities \n \n Assume broker data entry, submissions and all customer needs for Target, Sav-A-Lot, and Hy-Vee. \n Perform various support functions using essential systems: IRI, Blue Planner, Power Bi and all other tools that support data and insight refinement. \n Supports account plans by assisting team in entering and reviewing Blue Planner plans supporting specific and identified teammates as neede. \n Creates, runs, and shares weekly/monthly internal and IRI reports to the team on sales and share trends for the category, sub-categories, and brands. \n Creates shelf and display POG's upon request to support the sales teams space efforts. \n \n Spreading Joy Through Food! \n  Here at Utz, we have a passion for food that our associates enjoy creating, our consumers love, and our customers choose. Our renowned family of brands includes Utz, Golden Flake, Good Health, TGI Friday's, On the Border, Bachman, and Zapp's. We provide a supportive, caring and inclusive environment that offers opportunities for career growth and advancement. We believe it is important to help take care of our associates by providing benefits, resources, and programs that ensure they live their healthiest lives. Our benefit package includes medical, dental, and vision plans, an employer match 401k, profit sharing plan, employee stock purchase plan and much more. Join OneUtzTeam and help us spread joy through food for generations to come!  \n An Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.  Requirements: \n  \n General understanding of the business operations related to the snack food \n Performs other duties as assigned that will impact and support the overall outcome of the company and department \n Strong background in Excel and PPT. \n Experience with essential systems: IRI, Blue Planner, Power BI and other outside tools. \n Time management and organizational", "cleaned_desc": "", "techs": ""}, "b2114372e83a6a1f": {"terms": ["data analyst"], "salary_min": null, "salary_max": null, "title": "Financial Data Analyst", "company": "Cape Fox Federal Integrators", "desc": "Cape Fox is seeking a highly qualified Data Analyst to join our team in support of a government customer. Must be available to participate in phone calls and virtual meetings during work hours, with adequate technological resources (such as internet bandwidth) to do so. A data analyst gathers, cleans, and studies data sets to help solve problems. Be proficient in using financial systems and software applications (e.g., Microsoft Excel, Word, PowerPoint, SharePoint) for presentation of analysis and preparation of supporting documentation. Provide comprehensive financial analysis support services for the planning, execution, and monitoring of various Federal projects and support to the project management team. \n This position is contingent upon award. \n Core Duties: \n Ensuring project plans follow financial regulations and support evidenced based project performance. \n Support the project management team in responding to financial inquiries from stakeholders. \n (Tracks fund execution, including commitments, obligations, unobligated balances, and expenditures for programs and provide reports as necessary. \n Analyze project financial data, including expenditures, revenue, and variances against budgeted amounts. \n Provide financial insights and recommendations to optimize project resource allocation. \n Identify potential schedule delays and recommend corrective actions to ensure timely obligation and drawdown of project allocations. \n Collaborate with project managers to forecast project financial outcomes based on current performance. \n Monitor project financial health and flag any financial risks or discrepancies to relevant stakeholders. \n Assist in the preparation of financial reports for submission to stakeholders. \n Support internal and external audits by providing financial data and documentation as needed. \n Participate in project review meetings to discuss financial performance and strategies for improvement. \n Populates budget and financial information into executive level presentations. \n Develop tracking and reporting mechanisms to facilitate project management and project tracking. \n JOB REQUIREMENTS \n \n Possess a bachelor\u2019s degree in a business, finance, accounting, or related field and 4 years of relevant experience OR possess 10+ years of relevant experience in lieu of a degree \n Ability to analyze and interpret financial data, to interact effectively with others in a broad range of functional and technical backgrounds, and to handle pressures in a deadline-driven environment. \n Ability to use a high degree of professional judgment and ingenuity in interpreting guidelines for application in day-to-day issues and analyses. \n Ability to perform problem solving of a precedent-setting nature. \n Must be able to pass a Tier 1 Background Investigation \n Remote Position \n \n Job Types: Full-time, Contract \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": " Populates budget and financial information into executive level presentations. \n Develop tracking and reporting mechanisms to facilitate project management and project tracking. \n JOB REQUIREMENTS \n \n Possess a bachelor\u2019s degree in a business, finance, accounting, or related field and 4 years of relevant experience OR possess 10+ years of relevant experience in lieu of a degree \n Ability to analyze and interpret financial data, to interact effectively with others in a broad range of functional and technical backgrounds, and to handle pressures in a deadline-driven environment. \n Ability to use a high degree of professional judgment and ingenuity in interpreting guidelines for application in day-to-day issues and analyses. ", "techs": ["executive level presentations", "tracking and reporting mechanisms", "project management", "project tracking", "bachelor\u2019s degree", "business", "finance", "accounting", "relevant experience", "financial data analysis", "deadline-driven environment", "professional judgment", "application guidelines"]}, "e23f5f833069bb2f": {"terms": ["data analyst"], "salary_min": 20.0, "salary_max": 30.0, "title": "Data Analyst - Claims Accounts Receivable - eviCore - Remote", "company": "The Cigna Group", "desc": "Do you want to Drive Growth and Improve Lives? EviCore, a division of The Cigna Group is hiring Claims Associate Analyst \u2013 for the Accounts Receivable, Research and Reconciliation team. the Claim Associate Analyst will be responsible for compiling and analyzing claims data and perform related duties including tracking trends, noting patterns, and detecting any irregularities as it relates to aged Accounts receivable and claims processing trends. The reporting analyst will also serve as a liaison between internal departments and clients to communicate data interpretation and trends. \n \n  Create and maintain reports, dashboards, and other data visualization tools to communicate findings to internal and external stakeholders. \n \n  Create and maintain reports to track key performance indicators (KPI) and trends \n \n  Develop and implement data quality checks to ensure accuracy and consistency of date \n \n  Support Claim Manager in identifying opportunities for process improvement and automation to increase efficiency and effectiveness of reporting \n \n  Support special projects, JOC calls, and provider analytics \n \n  Support and streamline provider escalated inquiries based on data trends \n \n  Responsible for internal and external claims reporting \n \n  Verifies data and processing requirements as it relates to departmental policies and SOPS \n \n  Stay current with best practices and industry standard trends in reporting and analytics \n \n \n What you\u2019ll need to succeed: \n  High school or GED required, Associate degree or equivalent experience in computer science, information systems or a related field preferred \n \n  2+ years of experience in data analytics and reporting required \n \n  2+ years of experience with advanced functions of Microsoft EXCEL required \n \n  2+ years of experience with data analysis tools required \n \n  Proficient with PCM, MCRS, and Data Warehouses strongly preferred \n \n  Highly proficient with all Microsoft Office applications required \n \n  Previous experience and advanced development with third party systems \n \n  A high level of accuracy and attention to detail and have a dedicated approach to work \n \n  If you will be working at home occasionally or permanently, the internet connection must be obtained through a cable broadband or fiber optic internet service provider with speeds of at least 10Mbps download/5Mbps upload. \n \n  For this position, we anticipate offering an hourly rate of 20 - 30 USD / hourly, depending on relevant factors, including experience and geographic location. \n \n  This role is also anticipated to be eligible to participate in an annual bonus plan. \n \n  We want you to be healthy, balanced, and feel secure. That\u2019s why you\u2019ll enjoy a comprehensive range of benefits, with a focus on supporting your whole health. Starting on day one of your employment, you\u2019ll be offered several health-related benefits including medical, vision, dental, and well-being and behavioral health programs. We also offer 401(k) with company match, company paid life insurance, tuition reimbursement, a minimum of 18 days of paid time off per year and paid holidays. For more details on our employee benefits programs, visit Life at Cigna Group . \n \n  About Evernorth Health Services \n Evernorth Health Services, a division of The Cigna Group, creates pharmacy, care and benefit solutions to improve health and increase vitality. We relentlessly innovate to make the prediction, prevention and treatment of illness and disease more accessible to millions of people. Join us in driving growth and improving lives. \n \n  Qualified applicants will be considered without regard to race, color, age, disability, sex, childbirth (including pregnancy) or related medical conditions including but not limited to lactation, sexual orientation, gender identity or expression, veteran or military status, religion, national origin, ancestry, marital or familial status, genetic information, status with regard to public assistance, citizenship status or any other characteristic protected by applicable equal employment opportunity laws. \n \n  If you require reasonable accommodation in completing the online application process, please email: SeeYourself@cigna.com for support. Do not email SeeYourself@cigna.com for an update on your application or to provide your resume as you will not receive a response. \n \n  The Cigna Group has a tobacco-free policy and reserves the right not to hire tobacco/nicotine users in states where that is legally permissible. Candidates in such states who use tobacco/nicotine will not be considered for employment unless they enter a qualifying smoking cessation program prior to the start of their employment. These states include: Alabama, Alaska, Arizona, Arkansas, Delaware, Florida, Georgia, Hawaii, Idaho, Iowa, Kansas, Maryland, Massachusetts, Michigan, Nebraska, Ohio, Pennsylvania, Texas, Utah, Vermont, and Washington State.", "cleaned_desc": " \n What you\u2019ll need to succeed: \n  High school or GED required, Associate degree or equivalent experience in computer science, information systems or a related field preferred \n \n  2+ years of experience in data analytics and reporting required \n \n  2+ years of experience with advanced functions of Microsoft EXCEL required \n \n  2+ years of experience with data analysis tools required \n ", "techs": ["microsoft excel", "data analysis tools"]}, "20af79b180c7a436": {"terms": ["data analyst"], "salary_min": 54210.094, "salary_max": 68642.06, "title": "Customer Insights Analyst", "company": "Overstock.com Inc.", "desc": "We Are Bed Bath & Beyond: \n \n  At Bed Bath & Beyond, we believe that everyone should \u201cBe You!\u201d. Bed Bath & Beyond is a community that upholds a culture of understanding, acceptance, and respect. We believe a person\u2019s individuality is at the core of diversity, and those traits, beliefs, and characteristics should be valued and embraced. Living by this ethos is essential to the success of our business. Our goal is to foster a more diverse environment where every employee visibly demonstrates inclusive behaviors and respect for individuals. \n \n  This position is eligible for remote work. Currently we can hire employees in the following states: Arizona, California, Connecticut, Florida, Idaho, Kansas, Maine, Massachusetts, Michigan, Minnesota, New Hampshire, New Jersey, New York, North Carolina, Ohio, Pennsylvania, Rhode Island, South Dakota, Texas, Utah, Washington, Wisconsin, Wyoming. This role will have occasional on site expectations. \n \n  Job Responsibilities \n \n  Create, program, and troubleshoot surveys \n  Analyze and interpret survey comments, data and/or prospective investments \n  Prepare ongoing reporting and help design new reports \n  Analyze and report on secondary research \n  Develop and design reports using survey data and/or research reports \n  Meet with departments to disseminate research results \n  Meet and work with third-party vendors to acquire and analyze data \n  Utilize secondary research to provide competitive intelligence \n  Assist with recruiting projects to invite customers onsite for user testing, etc. \n  May prepare surveys to go onsite using Site Intercept and Qualtrics \n  May assist with the company panel: panel management, implementing research activities and engaging with panel participant \n  Perform other duties as required and assigned by manager and upper management \n  Follow legal policies as directed \n \n \n  Job Requirements \n \n  Basic knowledge of statistical analyses (mean, percentages, significance, etc.) \n  Strong logic/critical thinking ability: ability to anticipate the business' needs with only basic direction. Ability to anticipate how/why people respond the way they do to research \n  Technical aptitude: ability to quickly learn new tools, software, computer platforms \n  Excellent communication, writing and presentation skills, with an ability to articulate actionable insights derived from analysis \n  Self-starter: ability to produce high-quality work with minimal supervision, effectively manages time \n  Excellent collaboration skills \n  Works well under pressure, is agile/flexible in fast-changing/undefined situations \n  Ability to conduct competitive analysis of the industry \n  Attend meetings and present data and reports to other teams and departments \n  Manage research projects from start to finish (presenting findings to invested parties) \n \n \n  Skills \n \n  Experience in survey research and analysis: questionnaire design, survey programming, implementation, data collection, data and verbatim comment analyses, and reporting \n  Working knowledge of data management tools (SPSS, SQL, SAS, or R) \n  Familiarity with text mining software \n  Basic HTML/CSS skills \n  Strong Microsoft Office skills (especially Excel, PowerPoint and Word) \n \n \n  Education \n \n  Bachelor's degree required, preferably in Economics, Marketing, Finance, Statistics, or related field \n \n \n  Who We Are: \n \n  We\u2019re a passionate group of collaborative problem solvers and creative innovators, working on cutting-edge technology. From building award-winning retail applications (with amazing AR functionality) to creating leading blockchain and machine learning technologies, each of us embodies a unique value and contributes a diverse perspective to the team. \n \n  What We Offer: \n \n  401k (6% match) \n  Flexible Schedules \n  Onsite Health Clinic \n  Tuition Reimbursement, Leadership Development Program, & Mentorship Program \n  Onsite Fitness Center \n  Employee Resource Groups (LatinX, Black Employee Network, LGBTQIA+, Women\u2019s Network, Women In Tech) \n  And More\u2026 \n \n \n Benefits vary based on position, tenure, location, and employee election \n \n \n  Physical Requirements: \n  This position requires you to sit, stand and perform general office functions. You may also be required to lift up to 25 pounds occasionally. Bending, stooping and reaching are also frequently required. \n \n  Equal Employment Opportunity: \n  It is our policy to provide equal employment opportunity for all applicants and associates. This policy includes our commitment to ensure that all employment decisions are made without regard to race, color, religion, gender, national origin, disability, pregnancy, veteran status (including Vietnam era veterans), age, sexual orientation, gender identity, or any other non-job-related characteristic protected by law. \n \n  Back to Bed Bath & Beyond Careers", "cleaned_desc": "  Excellent collaboration skills \n  Works well under pressure, is agile/flexible in fast-changing/undefined situations \n  Ability to conduct competitive analysis of the industry \n  Attend meetings and present data and reports to other teams and departments \n  Manage research projects from start to finish (presenting findings to invested parties) \n \n \n  Skills \n \n  Experience in survey research and analysis: questionnaire design, survey programming, implementation, data collection, data and verbatim comment analyses, and reporting \n  Working knowledge of data management tools (SPSS, SQL, SAS, or R) \n  Familiarity with text mining software \n  Basic HTML/CSS skills \n  Strong Microsoft Office skills (especially Excel, PowerPoint and Word) \n ", "techs": ["spss", "sql", "sas", "r", "html/css", "microsoft office"]}, "4b1710bd5ae53bec": {"terms": ["data analyst"], "salary_min": 45.0, "salary_max": 55.0, "title": "Data Analyst - Remote", "company": "Acara Solutions", "desc": "Data Analyst \n Location: Remote \n Day Shift - 6.00AM-4.00PM (Mon-Thurs) 4/10 hrs/Week. \n \n High School Diploma or GED \n Minimum of years experience in Agile Scrum or Scaled Agile Framework (SAFe) \n Minimum of years experience in Meeting Facilitation \n \n Job Types: Temporary, Contract \n Salary: $45.00 - $55.00 per hour \n Experience level: \n \n 5 years \n \n Schedule: \n \n 10 hour shift \n \n Experience: \n \n Data Analyst in (Agile Scrum or Scaled Agile Framework): 5 years (Required) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "670498d4fbc3d34d": {"terms": ["data analyst"], "salary_min": null, "salary_max": null, "title": "Virtual Health Data Analyst", "company": "INTECON", "desc": "The telehealth Data Analyst (Operations Research Analyst) known as a VMC Decision Science Analyst, elicits, analyzes, validates, and documents virtual healthcare data and information to support the business needs of stakeholders including the DHA and MTFs. The Decision Science Analyst will develop metrics driven by information requirements and supported by data to include statistical analysis, research design and decision support to improve decision making and business practices at all levels of the MHS. \n \n  The contractor shall provide healthcare analysis in support of senior level working groups involved in a variety of work associated with VMC and DHA trategic business and organizational planning, corporate metrics and benchmarking, patient satisfaction and project implementation matters.  \n The contractor shall identify, develop, and execute \"think tank\" studies on a variety of health systems  \n The contractor shall coordinate with VMC leadership staff offices for input on key study topics and develops data requirements and analysis for study.  \n The contractor shall assess current best business practices and industry literature to identify future health systems requirements and to improve current operations.  \n The contractor shall provide recommendations on policies and programs to address major health strategy and policy issues impacting on the MHS, develop alternatives, and recommend courses of action for the VMC.  \n The contractor shall assist in the communication of DHA policies to the field by providing explanations of the analysis upon which the policies are based.  \n The contractor shall attend meetings as necessary to represent VMC and DHA interests.  \n The contractor shall implement Congress, DoD, and DHA staff approved concept plans.  \n The contractor shall prepare Director approved documents for approval of concepts at the appropriate approval level. \n  The contractor shall review healthcare industry standards to identify benchmarks and best practices for incorporation into VMC and DHA enterprise operations. \n  The contractor shall retrieve corporate performance data from a variety of MHS data query tools to include the Military Health System (MHS) MDR \u2013 Medical Data Repository, MHS Mart (M2) (MDR, M2, the TRICARE Operations Center (TOC), and other MHS systems. \n  The contractor shall develop, maintain, and enhance decision support tools and metrics used for senior level decision making.  \n The contractor shall apply statistical analysis to operational data to provide decision support and program analysis. \n  The contractor shall participate in health policy planning and project implementation efforts.  \n The contractor shall evaluate, assess, and analyze all resource, workload, and customer satisfaction data.  \n The contractor shall provide alternatives and recommend courses of action after validating the estimates, projections, forecasts, and conclusions of the studies.  \n The contractor shall participate on multifunctional and MHS teams to oversee the accomplishment of long-term implementation plans.  \n The contractor shall coordinate with MHS, DHA, and MTFs to facilitate initiatives and program analysis. \n  The contractor shall furnish technical assistance, recommend priorities, and target dates, coordinate work to avoid delay and overlapping assignments, review decisions, reports, correspondence, etc.  \n The contractor shall apply knowledge and understanding of all MHS components to solve difficult and obscure problems in performance planning, develop new approaches for use by the field, and to negotiate with other MHS organizations on multiple areas of collaboration. \n  The contractor shall formulate and present recommendations and implementation strategies for significant changes to standard policies and procedures. \n  The contractor shall develop business models and solutions for healthcare performance planning, realignments, resource management, and overall decision support. \n  The contractor shall perform data extraction and manipulation in Statistical Analysis Systems (SAS) (commercial software product) and Business Objects using multiple MHS data systems to include the M2, MDR, Expense Accounting System version 4 (EASIV), Defense Medical Human Resources System - Internet (DMHRS-I), and others. \n \n  Qualifications \n \n  A bachelor's degree in business, project management or clinical environment is required.  \n Comprehensive knowledge of the missions, organizations, structure, programs, data systems, and requirements of healthcare delivery systems for the military departments and the MHS to apply and develop practices, concepts and standards that lead to emerging strategy for the success and progression of VH in the MHS. \n  Expert knowledge required to research present and future objectives of the DHA, plan for delivery of healthcare to eligible beneficiaries, recognize and respond to trends in healthcare delivery performance, and anticipate future directions for strategic objectives. \n \n Expert knowledge of analytical, quantitative, and qualitative evaluation methods to select and apply appropriate methods and techniques \n  Experience \n A minimum of six (6) years of experience working in a clinical setting is required. Prior military service and virtual healthcare experience is desirable but not required.", "cleaned_desc": "", "techs": ""}, "8f30c8856c802e57": {"terms": ["data analyst"], "salary_min": 64408.816, "salary_max": 81555.914, "title": "Data Analyst III - Healthcare - Remote", "company": "Qlarant", "desc": "Qlarant is a not-for-profit corporation that partners with public and private sectors to create high quality, safe, and efficient delivery of health care and human services programs. We have multiple lines of business including utilization review, managed care organization quality review, and quality assurance for programs serving individuals with developmental disabilities. Qlarant is also a national leader in fighting fraud, waste and abuse for large organizations across the country. In addition, our Foundation provides grant opportunities to those with programs for under-served communities.\n  \n \n   Are you seeking to begin or grow your career in healthcare data analysis? Qlarant has the perfect opportunity for a motivated candidate to join our PPI MEDIC data analytics team! This position requires strong analytical skills, knowledge of SAS and SQL (preferred) and/or other statistical programming (R, Python) and database languages and a willingness to learn and contribute.\n  \n \n \n \n   This is a remote position but could be located in our Easton or Baltimore, MD offices if preferred. We offer opportunities for advancement, a collaborative and inclusive work environment and a very competitive benefits package.\n  \n \n \n \n   In this role you will be a part of a dedicated and talented data analytics team focused on making a positive difference in the future of our nation\u2019s healthcare programs. The PPI MEDIC supports the Centers for Medicare and Medicaid\u2019s fraud, waste and abuse monitoring efforts in the Medicare Parts C and D programs. The primary goal of the PPI MEDIC is to analyze Parts C and Part D data, conduct audits of plans, provide outreach and education to plan sponsors, and ensure compliance with regulatory requirements of Medicare Parts C and D.\n  \n \n \n \n   The Data Analyst III is an entry level professional position that performs study design, data analysis, and report preparation. Studies originate from preliminary data analysis (trends), literature review, experience and expertise of the team, and mandated projects. Data analysis, including data preparation and presentation of findings is performed in conjunction with other analysts. Reports are drafted by teams with leadership of Scientists.\n  \n  Essential Duties and Responsibilities  include the following. Other duties may be assigned. \n \n Trend data to identify potential opportunities (e.g., variances, significant outliers, percentile ranked groups) for quality improvement or focused investigations. \n Aid in design data analysis strategies to identify potential areas for quality improvement or focused investigation. \n Analyze data, draw conclusions, and summarize into quality indicator values \n Develop tabular and graphical presentations of data, which clearly and concisely illustrate current levels of care. \n Populate tabular and graphical presentation of data. \n Contribute to the development of interventions (i.e., develop educational materials for doctors and nurses) which will improve healthcare processes and outcomes. \n Facilitate design re-measurement strategies (after intervention) of healthcare processes and outcomes to effectively quantify impact of interventions for improvement. \n Analyze re-measurement data and summarize into quality indicator values. \n Support development of reports concerning all of the above. \n May mentor junior Data Analysts in technical aspects of their work. \n Assist in preparing findings for publishing in peer reviewed journals. \n Familiar with commonly used concepts, practices and procedures, relying on instructions and pre-established guidelines to perform the functions of the job. \n \n Supervisory Responsibilities \n  This job has no supervisory responsibilities. \n \n \n  Required Skills\n  \n To perform the job successfully, an individual should demonstrate the following competencies: \n \n Analytical  - Synthesizes complex or diverse information; Collects and researches data; Uses intuition and experience to complement data. \n Problem Solving  - Identifies and resolves problems in a timely manner; Gathers and analyzes information skillfully; Develops alternative solutions. \n Judgment  - Exhibits sound and accurate judgment; Supports and explains reasoning for decisions. \n \n   Other Skills and Abilities \n \n To perform this job successfully, an individual should have fluency in SAS or other statistical programming software and MS Office. \n Ability to work independently and in teams. \n Must possess familiarity with statistical and epidemiological methodologies, and automation techniques for analytic tasks. \n Ability to work with highly sensitive information while preserving the confidentiality of the information. \n Working knowledge of healthcare systems, Medicare/Medicaid preferred, healthcare databases and coding systems. \n \n \n \n  Required Experience\n  \n \n Bachelor's degree (BA or BS) in Statistics, Biostatistics, Epidemiology, Public Health or related discipline (e.g., Economics, Mathematics, Engineering, Computer Science) required. \n Minimum of 2 years of hands-on experience with SAS and SQL. Other statistical programming work experience, or demonstrated combination of education and experience (Masters degree and project/internship experience using SAS and SQL) will be considered. \n 6 months experience with health related analytic research and quality improvement methodology (ISO, CQI, TQM, Six Sigma, Lean, Etc.) preferred. \n Intermediate or better ability to utilize Microsoft Office to include Excel, Word and Outlook. \n \n  Qlarant is an Equal Opportunity Employer of Minorities, Females, Protected Veterans, and Individuals with Disabilities.", "cleaned_desc": "  Required Skills\n  \n To perform the job successfully, an individual should demonstrate the following competencies: \n \n Analytical  - Synthesizes complex or diverse information; Collects and researches data; Uses intuition and experience to complement data. \n Problem Solving  - Identifies and resolves problems in a timely manner; Gathers and analyzes information skillfully; Develops alternative solutions. \n Judgment  - Exhibits sound and accurate judgment; Supports and explains reasoning for decisions. \n \n   Other Skills and Abilities \n \n To perform this job successfully, an individual should have fluency in SAS or other statistical programming software and MS Office. \n Ability to work independently and in teams. \n Must possess familiarity with statistical and epidemiological methodologies, and automation techniques for analytic tasks.   Ability to work with highly sensitive information while preserving the confidentiality of the information. \n Working knowledge of healthcare systems, Medicare/Medicaid preferred, healthcare databases and coding systems. \n \n \n \n  Required Experience\n  \n \n Bachelor's degree (BA or BS) in Statistics, Biostatistics, Epidemiology, Public Health or related discipline (e.g., Economics, Mathematics, Engineering, Computer Science) required. \n Minimum of 2 years of hands-on experience with SAS and SQL. Other statistical programming work experience, or demonstrated combination of education and experience (Masters degree and project/internship experience using SAS and SQL) will be considered. \n 6 months experience with health related analytic research and quality improvement methodology (ISO, CQI, TQM, Six Sigma, Lean, Etc.) preferred. \n Intermediate or better ability to utilize Microsoft Office to include Excel, Word and Outlook. \n ", "techs": ["sas", "ms office", "sql", "iso", "cqi", "tqm", "six sigma", "lean", "excel", "word", "outlook"]}, "a0e3b3cd11eed5de": {"terms": ["data analyst"], "salary_min": 84885.53, "salary_max": 107484.0, "title": "Geospatial Business Analyst", "company": "PredictX", "desc": "About PredictX\n   \n \n \n  PredictX is a market leader in data fusion, analysis, prediction and automation of critical decision making for businesses. With our integrative AI technology, companies can make tactical decisions to improve their strategies, policies, and forecasts. A principal use of PredictX is to measure, forecast and execute policies surrounding \u2018Employee Generated Spend\u2019 (EGS), allowing large global corporations and organisations to make decisions around spend management, supplier management, sustainability and governance for activity that has historically been opaque and impossible to effectively manage.\n   \n \n \n  PredictX is a well-funded, profitable, pre-IPO SaaS business. Our clients include the largest and most sophisticated international companies in technology, pharmaceuticals, financial services and manufacturing who have become enthusiastic and referenceable advocates for us. We have an extremely strong focus on data and truly believe this should be the core of how every business makes decisions - our goal is to enable this for each and every one of our clients.\n   \n \n    Our Culture\n   \n \n \n \n    We take pride in creating a work environment that promotes invention, independence and transparency. Our social and 'open door' approach allows everyone to show initiative, express creativity and collaborate across the business. Our team consists of valuable and knowledgeable industry experts who seek to push the boundaries of technology, data analytics and AI.\n   \n \n \n  PredictX wants inventors. We are looking for people with ideas and solutions to improve the way we do things. If you are a problem-solver, creator and playbook designer looking to expand your knowledge and responsibilities, talk to us!\n   \n \n \n \n Role Description \n \n \n \n  We have an exciting opportunity for a Business Analyst with an interest and knowledge working with geospatial data to join our team and help propel us to the next level. As a PredictX Business Analyst, you will be working within a cross functional team, playing a key role in the requirements gathering and analysis of our Geospatial and Insurance Modelling product.\n       \n \n \n \n      We are constantly looking to enhance our products, improving the efficiency and increasing the value it unlocks for our customers. We are strong proponents of new technologies and leveraging facets of recent technological advancements to expand our product offering is a key pillar within our focus. The successful candidate will be a strong advocate within this space and will help take this product to the next level.\n      \n \n \n  You will have a strong handle of the value proposition offered by our product and be able to break this down from business requirements to high-level user stories and prioritising the team backlog to streamline the execution of program priorities. Furthermore, you will have the capacity to diligently undertake detailed requirements gathering and feature analysis to help ensure our products and solutions fully address our customers' needs and pain points.\n      \n \n \n \n \n \n \n Accountabilities \n \n \n  Work with business stakeholders to understand and document business needs, processes, requirements, and problems. Develop detailed functional and technical specifications. \n  Analyse business and technical requirements of new projects or enhancement requests and translate into tangible technical specifications and user stories \n  Conduct process analysis and documentation of workflows, data flows, systems integrations and traceability \n  Facilitate workshops and meetings, to elicit and validate requirements with users and project teams \n  Develop user stories, use cases, process flows, data models, and functional specifications \n  Collaborate with the QA team to develop test cases and conduct UAT testing to validate solutions meet business objectives \n  Create and maintain detailed project documentation \n  Identify and mitigate risks and issues to ensure on-time and on-budget delivery \n  Keep abreast with Agile/Scrum best practices and new trends \n  Help motivate, guide and push the development team and acknowledge exemplary efforts, behaviour, skills or accomplishments. \n \n \n \n \n \n \n Requirements, Qualifications & Key Skills \n \n \n  Excellent communicator, team player and influencer \n  Demonstrable planning, scheduling, problem solving and risk mitigation skills \n  Expertise eliciting, analysing and documenting business and technical requirements using standard BA techniques \n  Degree in Information Systems, Business Administration, or related field \n  Knowledge of Agile process and principles \n   Outstanding presentation skills \n  Excellent organisational and time management skills \n  An analytical mindset with a want to solve problems \n  A creative thinker with the ability to help define and deliver our PredictX Vision \n  Detail oriented with the ability to multitask on different projects concurrently", "cleaned_desc": " \n Accountabilities \n \n \n  Work with business stakeholders to understand and document business needs, processes, requirements, and problems. Develop detailed functional and technical specifications. \n  Analyse business and technical requirements of new projects or enhancement requests and translate into tangible technical specifications and user stories \n  Conduct process analysis and documentation of workflows, data flows, systems integrations and traceability \n  Facilitate workshops and meetings, to elicit and validate requirements with users and project teams \n  Develop user stories, use cases, process flows, data models, and functional specifications \n  Collaborate with the QA team to develop test cases and conduct UAT testing to validate solutions meet business objectives \n  Create and maintain detailed project documentation \n  Identify and mitigate risks and issues to ensure on-time and on-budget delivery \n  Keep abreast with Agile/Scrum best practices and new trends \n  Help motivate, guide and push the development team and acknowledge exemplary efforts, behaviour, skills or accomplishments. \n ", "techs": ["none"]}, "1e9446532d84af46": {"terms": ["data analyst"], "salary_min": 60000.0, "salary_max": 100000.0, "title": "IT - Business Analyst II - III (Remote)", "company": "The Cincinnati Insurance Companies", "desc": "US-Remote\n   \n \n \n \n  Description \n \n  Make a difference with a career in insurance \n  At The Cincinnati Insurance Companies, we put people first and apply the Golden Rule to our daily operations. To put this into action, we\u2019re looking for extraordinary people to join our talented team. Our service-oriented, ethical, knowledgeable, caring associates are the heart of our vision to be the best company serving independent agents. We help protect families and businesses as they work to prevent or recover from a loss. Share your talents to help us reach for continued success as we bring value to the communities we serve and demonstrate that Actions Speak Louder in Person\u00ae.    If you\u2019re ready to build productive relationships, collaborate within a diverse team, embrace challenges and develop your skills, then Cincinnati may be the place for you. We offer career opportunities where you can contribute and grow.      Start your journey with us \n  The Cincinnati Insurance Companies' IT department is currently seeking a business analyst to act as the client relationship liaison between a business unit and the IT department; and work with IT service or project teams to collect, clarify and translate business requirements into documentation and conceptual design (using appropriate tools and models) from which technology solutions can be developed. \n  The pay range for this position is $60,000  - $100,000  annually.  The pay determination is based on the applicant\u2019s education, experience, location, knowledge, skills, and abilities. Eligible associates may also receive an annual cash bonus and stock incentives based on company and individual performance. \n  Be Ready To : \n \n capture, analyze and document business and user requirements \n facilitate meetings with IT and business unit management and staff accordingly \n understand the business domain, its processes, systems, data and the inherent linkages among them and is able to execute on project plans by creating deliverables to meet project needs \n participate in or direct business process analysis in support of requirements identification \n partner with test team and assist in the preparation of the testing strategy and testing execution plans to ensure test cases map to requirements \n provide assistance with how-to requirements in terms of reporting documentation \n contribute to the development of IT Business Analysis & Testing Services team best practices via research and mentoring \n be an active participant in all phases of the project lifecycle and work with several parties to ensure that all business requirements are understood, documented and met \n define (or redefine) business needs for new and/or enhanced products, services or optimized business processes \n build and cultivate strong relationships with stakeholders; consult with stakeholders to understand goals and objectives \n \n  Be Equipped With : \n \n three years or more of business analysis experience supporting medium-sized initiatives \n detail oriented, highly organized and able to manage multiple project tasks concurrently \n ability to be flexible and able to deal with frequently-changing priorities in a fast-paced environment \n strong analytical and problem-solving skills \n excellent communication skills \n adept and effective meeting facilitator \n clear and effective writer and documenter with experience writing technical documentation \n experience gathering requirements \n advanced knowledge of and experience with business modeling tools, such as process and workflow modeling \n ability to create test scripts and perform software tests \n knowledge of writing test cases and test plans \n ability to oversee/direct target user group profiling \n experienced and skilled user of office productivity tools, including advanced knowledge of Microsoft\u00ae Word, Excel and PowerPoint \n explicit insurance knowledge and experience preferred \n \n You've Earned : \n \n university degree/college diploma or equivalent business experience \n \n \n Enhance your talents \n \n \n Providing outstanding service and developing strong relationships with our independent agents are hallmarks of our company. Whether you have experience from another carrier or you\u2019re new to the insurance industry, we promote a lifelong learning approach. Cincinnati provides you with the tools and training to be successful and to become a trusted, respected insurance professional \u2013 all while enjoying a meaningful career. \n \n \n Enjoy benefits and amenities \n \n \n Your commitment to providing strong service, sharing best practices, and creating solutions that impact lives is appreciated. To increase the well-being and satisfaction of our associates, we offer a variety of benefits and amenities. \n  Embrace a diverse team \n \n \n As a relationship-based organization, we welcome and value a diverse workforce. We provide equal employment opportunity to all qualified persons without regard to race; creed; color; sex, including sexual orientation, gender identity and transgender status; religion; national origin; age; disability; military service; veteran status; pregnancy; AIDS/HIV or genetic information; or any other basis prohibited by law. .", "cleaned_desc": " build and cultivate strong relationships with stakeholders; consult with stakeholders to understand goals and objectives \n \n  Be Equipped With : \n \n three years or more of business analysis experience supporting medium-sized initiatives \n detail oriented, highly organized and able to manage multiple project tasks concurrently \n ability to be flexible and able to deal with frequently-changing priorities in a fast-paced environment \n strong analytical and problem-solving skills \n excellent communication skills \n adept and effective meeting facilitator \n clear and effective writer and documenter with experience writing technical documentation   experience gathering requirements \n advanced knowledge of and experience with business modeling tools, such as process and workflow modeling \n ability to create test scripts and perform software tests \n knowledge of writing test cases and test plans \n ability to oversee/direct target user group profiling \n experienced and skilled user of office productivity tools, including advanced knowledge of Microsoft\u00ae Word, Excel and PowerPoint \n explicit insurance knowledge and experience preferred \n \n You've Earned : \n \n university degree/college diploma or equivalent business experience ", "techs": ["business analysis experience", "business modeling tools", "process and workflow modeling", "test scripts", "software tests", "test cases", "test plans", "microsoft word", "excel", "powerpoint", "insurance knowledge"]}, "c22aa700b237d90d": {"terms": ["data analyst"], "salary_min": 85000.0, "salary_max": 95000.0, "title": "Mergers & Acquisitions Analyst", "company": "Perigon Wealth Management", "desc": "POSITION OVERVIEW \n The M&A Analyst at Perigon Wealth Management will support the COO in all phases of the M&A process. Work is generally independent and collaborative in nature. The Analyst will contribute to moderately complex aspects of an opportunity. The M&A Analyst will create deliverables related to valuation, re-cast P&L, Financial Model, and Letter of Intent. The role will require organization and coordination of team members (both internal and of potential acquisitions). \n COMPANY OVERVIEW \n Perigon Wealth Management is a rapidly growing independent Registered Investment Advisor (RIA) based in San Francisco, CA. We manage approximately $5.5 billion in client assets as a fee-only fiduciary. In addition to our headquarters in San Francisco, we have offices nationwide including Hawaii, New York, New Jersey, Connecticut, Massachusetts, Delaware, Georgia and Florida with plans for significant near-term growth and expansion into new markets. \n ESSENTIAL DUTIES AND RESPONSIBILITIES \n The essential functions include, but are not limited to the following: \n \n Compile and analyze data required for merger, acquisition, and divestiture projects. \n Manage project data site. \n Perform integrated revenue/expense analysis. \n Perform financial forecasting and modeling. \n Prepare required documentation for the M&A Project. \n Organize and maintain key logistical elements around process management and team coordination. \n Develop and manage relationships with advisors, executives and team members of a potential acquisition. \n Conduct due diligence and use information to develop integrated revenue/expense analysis, projections, reports, and presentations regarding the impact of a merger/acquisition on the organization. \n Support all aspects of the deal process including initial contact, contract structure, due diligence, approval, contract negotiation, post\u2013closing transaction, and communication (internal and external) \n \n Minimum Qualifications (Knowledge, Skills, and Abilities) \n \n Excellent time management skills and the ability to work towards meeting multiple deadlines. \n Client relationship skills \n Ability to compile and organize findings for management presentations. \n Excellent communication skills with all levels of the organization \n Recognize and respects the importance and reasons for adhering to policies and procedures. \n Operate with a clear understanding of when to act independently and when to escalate for others\u2019 involvement. \n Experience with financial modeling and/or financial management of small to medium-sized companies. \n Advanced skills with Microsoft Excel and/or Powerpoint. \n Take ownership of initiatives and continues to move forward in the face of challenges. \n Bachelor's degree in business, finance, accounting or equivalent. \n \n Physical Demands and Work Environment \n The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this position. Reasonable accommodations may be made to enable individuals with disabilities to perform the functions. While performing the duties of this position, the employee is regularly required to talk or hear. The employee frequently is required to use hands or fingers, handle or feel objects, tools, or controls. The employee is occasionally required to stand; walk; sit; and reach with hands and arms. The employee must occasionally lift and/or move up to 25 pounds. Specific vision abilities required by this position include close vision, distance vision, and the ability to adjust focus. The noise level in the work environment is usually low to moderate. \n Note \n This job description in no way states or implies that these are the only duties to be performed by the employee(s) of this position. Employees will be required to follow any other job-related instructions and to perform any other job-related duties requested by any person authorized to give instructions or assignments. All duties and responsibilities are essential functions and requirements and are subject to possible modification to reasonably accommodate individuals with disabilities. To perform this job successfully, the employee(s) will possess the skills, aptitudes, and abilities to perform each duty proficiently. Some requirements may exclude individuals who pose a direct threat or significant risk to the health or safety of themselves or others. The requirements listed in this document are the minimum levels of knowledge, skills, or abilities. This document does not create an employment contract, implied or otherwise, other than an at will relationship. \n Job Type: Full-time \n Pay: $85,000.00 - $95,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n Paid time off \n Vision insurance \n \n Experience: \n \n Mergers & acquisitions: 2 years (Preferred) \n \n Work Location: Remote", "cleaned_desc": " \n Excellent time management skills and the ability to work towards meeting multiple deadlines. \n Client relationship skills \n Ability to compile and organize findings for management presentations. \n Excellent communication skills with all levels of the organization \n Recognize and respects the importance and reasons for adhering to policies and procedures. \n Operate with a clear understanding of when to act independently and when to escalate for others\u2019 involvement. \n Experience with financial modeling and/or financial management of small to medium-sized companies. \n Advanced skills with Microsoft Excel and/or Powerpoint. ", "techs": ["financial modeling", "financial management", "microsoft excel", "microsoft powerpoint"]}, "ab6b0118a091d000": {"terms": ["data analyst"], "salary_min": 69059.7, "salary_max": 87444.97, "title": "Data Analyst", "company": "Wasabi Technologies", "desc": "At Wasabi, we\u2019re a proven collection of pioneers, visionaries and disruptive doers. We see things differently than our competitors, and we make our mark in the industry by challenging the norm and delivering the unexpected and improbable. We\u2019re a fast-growing company taking the Cloud Storage industry by storm and recognized as one of the best places to work in Boston.\n  \n \n \n \n   Wasabi hot cloud storage is a new class and category of cloud storage, breaking all traditional barriers and boundaries of storage with a disruptive value proposition of being 1/5th the cost of AWS S3, faster than the competition, with no fees for egress or API request and delivered as a single-tier solution. Cloud storage has never been so simple, so fast and so inexpensive. It\u2019s all part of our vision to make cloud storage the next great global utility, just like electricity.\n  \n \n \n  Role Description: Data Analyst \n \n \n \n \n  Role Purpose: \n \n \n \n \n   As a Data Analyst, you will play a pivotal role in helping us understand our data, optimize our operations, and make data-driven decisions. This integral role will work closely with the CFO and the Vice President of Finance Operations, and they will report to the Manager of Data Analysis.\n  \n \n \n \n   If you're a curious, analytical thinker who loves diving into data to uncover hidden insights and enjoys working in a fast-paced, innovative environment, then this role is perfect for you!\n  \n Responsibilities: \n \n  Drive improvement of business processes, leveraging Excel, SFDC & Tableau to drive growth and scale \n  Partner with our Finance team to ensure the compensation process is in alignment with company goals and running smoothly with accurate data and analysis. \n  Investigate, clean, and wrangle large datasets from various sources to ensure data accuracy and consistency. \n  Analyze data to identify trends, patterns, and anomalies, and translate these findings into actionable insights that will drive business decisions. \n  Create and maintain interactive dashboards and reports to visualize data and provide easy-to-understand insights for different teams within the organization. \n  Collaborate with Finance, Sales, Marketing and R&D leadership to gather requirements and provide data support for various projects and initiatives. \n  Develop and implement forecasting models to predict future trends, demand, and resource allocation. \n  Promote a data-driven decision-making culture within the company by providing data training and support to non-technical team members. \n \n  Requirements: \n \n  Bachelor\u2019s degree (BS or BA) in analytics, or related field. \n  2+ years in a highly analytical role; preferably in a startup or a SaaS company. Solid math and finance skills are a plus. \n  Excellent analytical and problem-solving skills, combined with strong business judgment and ability to present analysis in a clear and compelling manner. \n  Use of business intelligence tools to conduct analysis, Tableau and Excel experience a plus. \n  A passion for technology and a desire to stay updated on the latest trends in data analytics. \n  Knowledge of SaaS metrics such as churn, retention, and customer lifetime value is a definite plus. \n  Outstanding organizational and communication (verbal, written, and presentation) skills \n  Experience working in a high-growth, performance and deadline driven environment. \n  Results oriented, attention to detail, ability to prioritize multiple objectives and projects and continually bringing new ideas to the table. \n  Ability to manage own workload, work efficiently, and meet deadlines. \n \n \n   Wasabi Technologies is an Equal Opportunity Employer. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.", "cleaned_desc": "  Investigate, clean, and wrangle large datasets from various sources to ensure data accuracy and consistency. \n  Analyze data to identify trends, patterns, and anomalies, and translate these findings into actionable insights that will drive business decisions. \n  Create and maintain interactive dashboards and reports to visualize data and provide easy-to-understand insights for different teams within the organization. \n  Collaborate with Finance, Sales, Marketing and R&D leadership to gather requirements and provide data support for various projects and initiatives. \n  Develop and implement forecasting models to predict future trends, demand, and resource allocation. \n  Promote a data-driven decision-making culture within the company by providing data training and support to non-technical team members. \n \n  Requirements: \n \n  Bachelor\u2019s degree (BS or BA) in analytics, or related field.    2+ years in a highly analytical role; preferably in a startup or a SaaS company. Solid math and finance skills are a plus. \n  Excellent analytical and problem-solving skills, combined with strong business judgment and ability to present analysis in a clear and compelling manner. \n  Use of business intelligence tools to conduct analysis, Tableau and Excel experience a plus. \n  A passion for technology and a desire to stay updated on the latest trends in data analytics. \n  Knowledge of SaaS metrics such as churn, retention, and customer lifetime value is a definite plus. \n  Outstanding organizational and communication (verbal, written, and presentation) skills \n  Experience working in a high-growth, performance and deadline driven environment. \n  Results oriented, attention to detail, ability to prioritize multiple objectives and projects and continually bringing new ideas to the table. \n  Ability to manage own workload, work efficiently, and meet deadlines. \n ", "techs": ["tableau", "excel"]}, "97b3d939cfb3b24b": {"terms": ["data analyst"], "salary_min": 100000.0, "salary_max": 110000.0, "title": "Technical Data Analyst (Remote)", "company": "Digital Strategy LLC", "desc": "Digital Strategy is seeking a Technical Data Analyst with strong analytical and database development skills to provide support to federal clients in performing data analysis and developing reports and dashboards in various business intelligence platforms. The candidate will work with federal and contractor teams to understand reporting needs and deliver data analytics products to meet and exceed client expectations. Specific duties will include but are not limited to the following: \n \n Works with clients and contractor Agile teams to analyze raw data from structured and unstructured data sources and organize them in the central data warehouse. \n Interprets business and functional requirements to design and develop reports and dashboards. \n Develops reports and charts with key indicators for a variety of stakeholders. \n Utilizes various structured and unstructured data sources to extract, analyze, and summarize data into custom reports and dashboards. \n Supports data governance efforts including design and development to data architecture, data standardization, and data quality. \n \n Position Requirements: \n \n Bachelor\u2019s degree in Information Systems, Finance, Economics, Business, or similar majors. \n Ten (10) years of total work experience, including requirements gathering and data analysis. \n Two (2) years of experience may be substituted for Master\u2019s degree. \n At least four (4) years of experience in developing and delivering business/functional requirements. \n At least four (4) years of experience with data analysis and data visualization projects. \n Experience developing reports and dashboards utilizing business intelligence tools, e.g. Microsoft Power BI, Cognos ReportNet, Tableau, or others. \n Experience using SQL to extract and analyze data from SQL Server databases. \n Experience with database design/modeling to support efforts in managing the data in the warehouse. \n Experience extracting data from unstructured data sources, e.g. Excel, Word, or others. \n Strong attention to detail and ability to grasp and document new information rapidly. \n Strong verbal and written communication skills. \n \n Min. Citizenship Status Required:  U.S. Citizen \n Digital Strategy is a small woman-owned, information technology and management consulting firm with several Federal Government projects. Due to the nature of our contracts and federal guidelines, US Citizenship is required. \n In addition to the flexibility of working remotely, we offer a generous benefits package that includes Health and Life insurance, 401K plan, and Paid-Time-Off, plus 11 paid Federal holidays! \n Find out what makes us unique by visiting our website at: www.ds-llc.com. \n Job Type: Full-time \n Pay: $100,000.00 - $110,000.00 per year \n Benefits: \n \n 401(k) \n 401(k) matching \n AD&D insurance \n Dental insurance \n Health insurance \n Life insurance \n Paid holidays \n Paid time off \n Vision insurance \n Work from home \n \n Experience level: \n \n 10 years \n \n Schedule: \n \n Monday to Friday \n \n Application Question(s): \n \n Are you legally authorized to work in the US for any employer without restrictions? \n \n Education: \n \n Bachelor's (Required) \n \n Experience: \n \n total overall work: 10 years (Required) \n data analysis: 4 years (Required) \n database development: 2 years (Required) \n data visualization: 4 years (Required) \n reports/BI development: 4 years (Required) \n \n Work Location: Remote", "cleaned_desc": "Digital Strategy is seeking a Technical Data Analyst with strong analytical and database development skills to provide support to federal clients in performing data analysis and developing reports and dashboards in various business intelligence platforms. The candidate will work with federal and contractor teams to understand reporting needs and deliver data analytics products to meet and exceed client expectations. Specific duties will include but are not limited to the following: \n \n Works with clients and contractor Agile teams to analyze raw data from structured and unstructured data sources and organize them in the central data warehouse. \n Interprets business and functional requirements to design and develop reports and dashboards. \n Develops reports and charts with key indicators for a variety of stakeholders. \n Utilizes various structured and unstructured data sources to extract, analyze, and summarize data into custom reports and dashboards. \n Supports data governance efforts including design and development to data architecture, data standardization, and data quality. \n \n Position Requirements: \n \n Bachelor\u2019s degree in Information Systems, Finance, Economics, Business, or similar majors. \n Ten (10) years of total work experience, including requirements gathering and data analysis. \n Two (2) years of experience may be substituted for Master\u2019s degree.   At least four (4) years of experience in developing and delivering business/functional requirements. \n At least four (4) years of experience with data analysis and data visualization projects. \n Experience developing reports and dashboards utilizing business intelligence tools, e.g. Microsoft Power BI, Cognos ReportNet, Tableau, or others. \n Experience using SQL to extract and analyze data from SQL Server databases. \n Experience with database design/modeling to support efforts in managing the data in the warehouse. \n Experience extracting data from unstructured data sources, e.g. Excel, Word, or others. \n Strong attention to detail and ability to grasp and document new information rapidly. \n Strong verbal and written communication skills. \n \n Min. Citizenship Status Required:  U.S. Citizen \n Digital Strategy is a small woman-owned, information technology and management consulting firm with several Federal Government projects. Due to the nature of our contracts and federal guidelines, US Citizenship is required. \n In addition to the flexibility of working remotely, we offer a generous benefits package that includes Health and Life insurance, 401K plan, and Paid-Time-Off, plus 11 paid Federal holidays! \n Find out what makes us unique by visiting our website at: www.ds-llc.com. ", "techs": ["business intelligence platforms", "microsoft power bi", "cognos reportnet", "tableau", "sql server databases", "excel", "word"]}, "f93e29a5d4330e7c": {"terms": ["data analyst"], "salary_min": 60181.33, "salary_max": 76202.97, "title": "Customer Insights Analyst", "company": "Bed Bath & Beyond", "desc": "We Are Bed Bath & Beyond:\n  \n \n \n   At Bed Bath & Beyond, we believe that everyone should \u201cBe You!\u201d. Bed Bath & Beyond is a community that upholds a culture of understanding, acceptance, and respect. We believe a person\u2019s individuality is at the core of diversity, and those traits, beliefs, and characteristics should be valued and embraced. Living by this ethos is essential to the success of our business. Our goal is to foster a more diverse environment where every employee visibly demonstrates inclusive behaviors and respect for individuals.\n  \n \n \n   This position is eligible for remote work. Currently we can hire employees in the following states: Arizona, California, Connecticut, Florida, Idaho, Kansas, Maine, Massachusetts, Michigan, Minnesota, New Hampshire, New Jersey, New York, North Carolina, Ohio, Pennsylvania, Rhode Island, South Dakota, Texas, Utah, Washington, Wisconsin, Wyoming. This role will have occasional on site expectations.\n  \n \n \n   Job Responsibilities\n  \n \n  Create, program, and troubleshoot surveys \n  Analyze and interpret survey comments, data and/or prospective investments \n  Prepare ongoing reporting and help design new reports \n  Analyze and report on secondary research \n  Develop and design reports using survey data and/or research reports \n  Meet with departments to disseminate research results \n  Meet and work with third-party vendors to acquire and analyze data \n  Utilize secondary research to provide competitive intelligence \n  Assist with recruiting projects to invite customers onsite for user testing, etc. \n  May prepare surveys to go onsite using Site Intercept and Qualtrics \n  May assist with the company panel: panel management, implementing research activities and engaging with panel participant \n  Perform other duties as required and assigned by manager and upper management \n  Follow legal policies as directed \n \n \n \n  Job Requirements\n  \n \n  Basic knowledge of statistical analyses (mean, percentages, significance, etc.) \n  Strong logic/critical thinking ability: ability to anticipate the business' needs with only basic direction. Ability to anticipate how/why people respond the way they do to research \n  Technical aptitude: ability to quickly learn new tools, software, computer platforms \n  Excellent communication, writing and presentation skills, with an ability to articulate actionable insights derived from analysis \n  Self-starter: ability to produce high-quality work with minimal supervision, effectively manages time \n  Excellent collaboration skills \n  Works well under pressure, is agile/flexible in fast-changing/undefined situations \n  Ability to conduct competitive analysis of the industry \n  Attend meetings and present data and reports to other teams and departments \n  Manage research projects from start to finish (presenting findings to invested parties) \n \n \n \n  Skills\n  \n \n  Experience in survey research and analysis: questionnaire design, survey programming, implementation, data collection, data and verbatim comment analyses, and reporting \n  Working knowledge of data management tools (SPSS, SQL, SAS, or R) \n  Familiarity with text mining software \n  Basic HTML/CSS skills \n  Strong Microsoft Office skills (especially Excel, PowerPoint and Word) \n \n \n \n  Education\n  \n \n  Bachelor's degree required, preferably in Economics, Marketing, Finance, Statistics, or related field \n \n \n \n  Who We Are:\n  \n \n \n   We\u2019re a passionate group of collaborative problem solvers and creative innovators, working on cutting-edge technology. From building award-winning retail applications (with amazing AR functionality) to creating leading blockchain and machine learning technologies, each of us embodies a unique value and contributes a diverse perspective to the team.\n  \n \n \n   What We Offer:\n  \n \n \n \n     401k (6% match)\n    \n \n \n     Flexible Schedules\n    \n \n \n     Onsite Health Clinic\n    \n \n \n     Tuition Reimbursement, Leadership Development Program, & Mentorship Program\n    \n \n \n     Onsite Fitness Center\n    \n \n \n     Employee Resource Groups (LatinX, Black Employee Network, LGBTQIA+, Women\u2019s Network, Women In Tech)\n    \n \n \n     And More\u2026\n    \n \n \n Benefits vary based on position, tenure, location, and employee election \n \n \n \n   Physical Requirements:\n  \n \n  This position requires you to sit, stand and perform general office functions. You may also be required to lift up to 25 pounds occasionally. Bending, stooping and reaching are also frequently required.\n  \n \n \n   Equal Employment Opportunity:\n  \n \n  It is our policy to provide equal employment opportunity for all applicants and associates. This policy includes our commitment to ensure that all employment decisions are made without regard to race, color, religion, gender, national origin, disability, pregnancy, veteran status (including Vietnam era veterans), age, sexual orientation, gender identity, or any other non-job-related characteristic protected by law.\n  \n \n \n \n    Back to Bed Bath & Beyond Careers", "cleaned_desc": "  May assist with the company panel: panel management, implementing research activities and engaging with panel participant \n  Perform other duties as required and assigned by manager and upper management \n  Follow legal policies as directed \n \n \n \n  Job Requirements\n  \n \n  Basic knowledge of statistical analyses (mean, percentages, significance, etc.) \n  Strong logic/critical thinking ability: ability to anticipate the business' needs with only basic direction. Ability to anticipate how/why people respond the way they do to research \n  Technical aptitude: ability to quickly learn new tools, software, computer platforms \n  Excellent communication, writing and presentation skills, with an ability to articulate actionable insights derived from analysis \n  Self-starter: ability to produce high-quality work with minimal supervision, effectively manages time \n  Excellent collaboration skills \n  Works well under pressure, is agile/flexible in fast-changing/undefined situations \n  Ability to conduct competitive analysis of the industry \n  Attend meetings and present data and reports to other teams and departments \n  Manage research projects from start to finish (presenting findings to invested parties) \n \n \n \n  Skills\n  \n    Experience in survey research and analysis: questionnaire design, survey programming, implementation, data collection, data and verbatim comment analyses, and reporting \n  Working knowledge of data management tools (SPSS, SQL, SAS, or R) \n  Familiarity with text mining software \n  Basic HTML/CSS skills \n  Strong Microsoft Office skills (especially Excel, PowerPoint and Word) \n \n \n \n  Education\n  \n \n  Bachelor's degree required, preferably in Economics, Marketing, Finance, Statistics, or related field \n \n \n \n  Who We Are:\n  \n \n \n   We\u2019re a passionate group of collaborative problem solvers and creative innovators, working on cutting-edge technology. From building award-winning retail applications (with amazing AR functionality) to creating leading blockchain and machine learning technologies, each of us embodies a unique value and contributes a diverse perspective to the team.\n  \n \n \n   What We Offer:\n  ", "techs": ["panel management", "implementing research activities", "engaging with panel participants", "statistical analyses", "logic/critical thinking ability", "technical aptitude", "communication skills", "writing skills", "presentation skills", "self-starter", "collaboration skills", "competitive analysis", "meeting attendance", "data and report presentation", "research project management", "survey research", "questionnaire design", "survey programming", "data collection", "data analysis", "reporting", "data management tools (spss", "sql", "sas", "r)", "text mining software", "html/css skills", "microsoft office skills", "bachelor's degree in economics", "marketing", "finance", "statistics or related field."]}, "f65f0af454dd5a31": {"terms": ["data analyst"], "salary_min": 58300.0, "salary_max": 133000.0, "title": "MUMPS Data Analyst", "company": "Booz Allen Hamilton", "desc": "Job Description \n \n \n \n \n \n \n \n \n \n \n         Location: \n         \n \n         Melbourne,FL,US \n         \n \n \n \n         Remote Work: \n         \n \n         Yes \n         \n \n \n \n         Job Number: \n         \n \n         R0179409\n         \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n         MUMPS Data Analyst\n           The Opportunity: \n  Are you looking to join a team of experts in digital project management and shift the way the world works? Are you driven to help our team modernize and deliver leading-edge technical solutions to our U.S. Department of Veterans Affairs client? \n \n  As a MUMPS Data Analyst, you\u2019ll be responsible for analyzing user business problems to be solved with automated systems. You\u2019ll work directly with client stakeholders to formulate and define information system scope and objectives through research, analysis, testing, and fact finding with a deep understanding of business systems and client requirements. You will provide problem definition, evaluation of requirements, and assist with the implementation of systems to meet business, user, and functional requirements. We are looking for someone with excellent written and verbal communication expertise, who as a functional expert can prepare communications and make presentations on recommendations on system enhancements or alternatives. \n \n  Join us. The world can\u2019t wait. \n \n  You Have: \n \n  10+ years of experience with systems, software, process, and data analysis \n  Experience with MUMPS software development \n  Experience with working in VistA environments and related Fileman utilities for MUMPS applications \n  Experience with system, data, and requirements analyses, development of technical integrations, and design of proof of concept (POC) solutions \n  Knowledge of software development \n  Knowledge of software and systems design and architectural patterns \n  Knowledge of architecture styles and APIs \n  Ability to communicate directly and clearly with external clients, project stakeholders, and cross-functional team members, including delivering oral and written presentations \n  Ability to obtain and maintain a Public Trust or Suitability/Fitness determination based on client requirements \n  Bachelor\u2019s Degree in EE, Computer Engineering, or IT \n \n \n  Nice If You Have: \n \n  Knowledge of Agile frameworks \n  SAFe Certification \n \n \n  Vetting: \n  Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client. \n \n  Create Your Career: \n \n  Grow With Us \n  Your growth matters to us\u2014that\u2019s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms. \n \n  A Place Where You Belong \n  Diverse perspectives cultivate collective ingenuity. Booz Allen\u2019s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you\u2019ll develop your community in no time. \n \n  Support Your Well-Being \n  Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we\u2019ll support you as you pursue a balanced, fulfilling life\u2014at work and at home. \n \n  Your Candidate Journey \n  At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we\u2019ve compiled a list of resources so you\u2019ll know what to expect as we forge a connection with you during your journey as a candidate with us. \n \n  Compensation \n  At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen\u2019s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page. \n  Salary at Booz Allen is determined by various factors, including but not limited to location, the individual\u2019s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $58,300.00 to $133,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen\u2019s total compensation package for employees.\n          \n  Work Model  Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. \n \n  If this position is listed as remote or hybrid, you\u2019ll periodically work from a Booz Allen or client site facility. \n  If this position is listed as onsite, you\u2019ll work with colleagues and clients in person, as needed for the specific role. \n \n \n  EEO Commitment \n  We\u2019re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change \u2013 no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.", "cleaned_desc": " \n \n \n \n \n \n \n \n         MUMPS Data Analyst\n           The Opportunity: \n  Are you looking to join a team of experts in digital project management and shift the way the world works? Are you driven to help our team modernize and deliver leading-edge technical solutions to our U.S. Department of Veterans Affairs client? \n \n  As a MUMPS Data Analyst, you\u2019ll be responsible for analyzing user business problems to be solved with automated systems. You\u2019ll work directly with client stakeholders to formulate and define information system scope and objectives through research, analysis, testing, and fact finding with a deep understanding of business systems and client requirements. You will provide problem definition, evaluation of requirements, and assist with the implementation of systems to meet business, user, and functional requirements. We are looking for someone with excellent written and verbal communication expertise, who as a functional expert can prepare communications and make presentations on recommendations on system enhancements or alternatives. \n \n  Join us. The world can\u2019t wait. \n \n  You Have: \n \n  10+ years of experience with systems, software, process, and data analysis \n  Experience with MUMPS software development \n  Experience with working in VistA environments and related Fileman utilities for MUMPS applications    Experience with system, data, and requirements analyses, development of technical integrations, and design of proof of concept (POC) solutions \n  Knowledge of software development \n  Knowledge of software and systems design and architectural patterns \n  Knowledge of architecture styles and APIs \n  Ability to communicate directly and clearly with external clients, project stakeholders, and cross-functional team members, including delivering oral and written presentations \n  Ability to obtain and maintain a Public Trust or Suitability/Fitness determination based on client requirements \n  Bachelor\u2019s Degree in EE, Computer Engineering, or IT \n \n \n  Nice If You Have: \n \n  Knowledge of Agile frameworks \n  SAFe Certification \n \n \n  Vetting: \n  Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client. \n \n  Create Your Career: \n \n  Grow With Us ", "techs": ["mumps data analyst", "mumps software development", "vista environments", "fileman utilities", "system analysis", "software development", "systems design", "architectural patterns", "agile frameworks", "safe certification"]}, "f10c5541e43c089f": {"terms": ["data analyst"], "salary_min": 54600.0, "salary_max": 98700.0, "title": "Business Analyst", "company": "Leidos", "desc": "Description   \n The Leidos Defense Group currently has an opening for a Defense Property Accountability System (DPAS) Business Analyst. This role will support the DPAS program through Material Management, Warehouse Management, Property Accountability or Maintenance and Utilization applications, processes, and associated interfaces. Duties includes both management and monitoring of the existing applications and processes, as well as partnering with the development teams to guide future business direction, enhancements, and optimization. \n \n  The analyst will work with stakeholders to gather and document key functional and technical requirements, assist in development of the business case, and collaborate with internal development teams to deliver technology solutions that meet the business need and deliver value. \n \n  This role will be located with our DPAS team in Mechanicsburg, PA in a Hybrid/Remote capacity requiring on-site and off-site support with expectation of one or more days in the office as needed and up-to 25% travel! \n \n  Primary Responsibilities \n \n  Assists and supports the development and implementation for new and significant improvements to major processes through collaboration with colleagues. \n  Develop a thorough understanding of business processes, best practices, and can support technology & documentation. \n  Work and collaborate closely with key stakeholders to document the key business goals and objectives for a new or existing process. \n  Prepares detailed requirement specifications in the form of user stories or functional requirement documents for the development teams. \n  Participation in the agile software development process as a proxy for the process owner role to help facilitate progress, answer any questions, and provide more detailed requirements as necessary. \n  Collaborate with development teams to produce any required documentation or communications that may be necessary in support of the project including project status, project roadmap, release plan, etc. \n  Work closely with Q/A Analyst to assist with developing test cases/scenarios and detailed acceptance criteria. \n  Support project delivery by working with the user community to document and coordinate user acceptance testing and training plans. \n  Assist the Help Desk with problem and incident management. \n  Assist with end user training as necessary. \n \n \n  Basic Qualifications \n \n  Bachelors\u2019 Degree in Information Technology, Logistics, or related field with two or more years of relevant experience. Additional years of relevant experience, training, and/or certifications may be considered in lieu of degree requirement. \n  US Citizen: Currently possessing or ability to successfully obtain and maintain a DoD Secret Security Clearance upon hire. \n  Working knowledge of Windows based Desktop/Laptop Computers. \n  Experience with Microsoft Office products to include Microsoft Word, Excel, PowerPoint, and Outlook. \n  Demonstrated problem-solving skills. \n  Demonstrated critical skills such as collaboration, strong written and verbal communication skills, and analytical capability. Self-motivated, with the ability to prioritize, meet deadlines, and manage changing priorities. \n  Ability to be flexible and work hard, in a fast-paced environment with changing priorities. \n \n \n  Preferred Qualifications \n \n  Experience with Department of Defense Supply Systems, DLMS processing or MILS processing. \n  DPAS experience \n \n \n  LMS2023 \n \n  Pay Range:  Pay Range $54,600.00 - $98,700.00\n  \n  The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law. \n  #Remote", "cleaned_desc": "  US Citizen: Currently possessing or ability to successfully obtain and maintain a DoD Secret Security Clearance upon hire. \n  Working knowledge of Windows based Desktop/Laptop Computers. \n  Experience with Microsoft Office products to include Microsoft Word, Excel, PowerPoint, and Outlook. \n  Demonstrated problem-solving skills. \n  Demonstrated critical skills such as collaboration, strong written and verbal communication skills, and analytical capability. Self-motivated, with the ability to prioritize, meet deadlines, and manage changing priorities. \n  Ability to be flexible and work hard, in a fast-paced environment with changing priorities. \n \n ", "techs": ["microsoft office products", "microsoft word", "microsoft excel", "microsoft powerpoint", "microsoft outlook"]}, "69ab5c7aadbcd0f0": {"terms": ["data analyst"], "salary_min": 70000.0, "salary_max": 100000.0, "title": "Wage and Hour Data Analyst (CA, CO, MN, PA, TX, VA, WA)", "company": "Berger Consulting Group", "desc": "Description \n Berger Consulting Group provides statistical and economic consulting services, as well as expert testimony in litigation matters involving labor and employment, class actions, and other commercial disputes. With over a decade of experience serving clients, we have focused and sharpened our expertise in statistics, data management, and data analysis, specializing in the litigation process. \n We are seeking to fill the role of Data Analyst, which places heavy emphasis on the use of complex data analysis and statistics. This role includes strong emphases on data wrangling and cleaning, data formatting and normalization, and analytics. The ideal candidate has excellent verbal and written communication skills, exceptional empirical skills, and passion for rigorous data analysis in the fast-paced and constantly changing litigation context. Analysts work as part of a team on challenging and complex projects in a work environment that promotes a company culture that believes in both doing good work and having time to experience the other important parts of life. \n Job Requirements \n \n Conduct independent data and statistical analyses utilizing Excel and Stata. \n Normalize data sets including but not limited to timekeeping and payroll data. \n Construct, manage, and manipulate small to large databases and perform data cleaning and analysis. \n Run Quality Assurance checks on the data. \n Prepare notes of findings, including salient examples from source documents for client use. \n Identify and clearly communicate trends, outliers, potential issues, shortcomings, and additional needs based on document review and analysis. \n Communicate with manager on a regular basis. \n \n Required Skills and Experience \n \n 2+ years of intensive experience working with large (~1M rows), complex, and potentially messy datasets. \n Expertise in Microsoft Excel and Stata (2+ years of regular Stata usage in the workplace). \n Expertise and use in work settings in complex equation building in Excel; including but not limited to nested logic, lookups (vlookup, hlookup, xlookup), match index, find, date/time functions, left/mid/right, sumifs, averageifs, shortcuts, sorting, pivot tables, etc. \n Excellent quantitative and analytical skills. \n Excellent verbal and written communication skills. \n Ability to work independently. \n Ability to manage caseload, including proactive planning, anticipating obstacles, and monitoring the progress of many cooccurring projects. \n Willingness to work on multiple projects with tight deadlines, including last minute requests and changes. \n Patience and attention to detail, with sharp focus on accuracy and problem solving. \n \n Remote Work \n Berger Consulting Group does not have a central office and all employees work remotely. Since employees are remote, they need to choose a permanent home office for their workplace upon hiring. Changes to your home office should be discussed with your manager prior to any relocation. \n This position is open to candidates in California, Colorado, Minnesota, Pennsylvania, Texas, Virginia, and Washington. \n Job Type: Full-time \n Pay: $70,000.00 - $100,000.00 per year \n Benefits: \n \n 401(k) \n 401(k) 3% Match \n Dental insurance \n Disability insurance \n Health insurance \n Paid holidays \n Paid time off \n Vision insurance \n Work from home \n \n Compensation package: \n \n Retention bonus \n Signing bonus \n \n Experience level: \n \n 1 year \n 2 years \n 3 years \n \n Schedule: \n \n 8 hour shift \n Day shift \n Monday to Friday \n No nights \n No weekends \n \n Application Question(s): \n \n Will you now or in the future require sponsorship for employment status (e.g. H1B visa status)? (Required for consideration) \n Are you located in CA, CO, MN, PA, TX, VA, or WA? (Required for consideration) \n Please describe your use of Excel, Stata, R, and/or Python for cleaning large, messy, datasets. (Required for consideration) \n \n Work Location: Remote", "cleaned_desc": "Description \n Berger Consulting Group provides statistical and economic consulting services, as well as expert testimony in litigation matters involving labor and employment, class actions, and other commercial disputes. With over a decade of experience serving clients, we have focused and sharpened our expertise in statistics, data management, and data analysis, specializing in the litigation process. \n We are seeking to fill the role of Data Analyst, which places heavy emphasis on the use of complex data analysis and statistics. This role includes strong emphases on data wrangling and cleaning, data formatting and normalization, and analytics. The ideal candidate has excellent verbal and written communication skills, exceptional empirical skills, and passion for rigorous data analysis in the fast-paced and constantly changing litigation context. Analysts work as part of a team on challenging and complex projects in a work environment that promotes a company culture that believes in both doing good work and having time to experience the other important parts of life. \n Job Requirements \n \n Conduct independent data and statistical analyses utilizing Excel and Stata. \n Normalize data sets including but not limited to timekeeping and payroll data. \n Construct, manage, and manipulate small to large databases and perform data cleaning and analysis. \n Run Quality Assurance checks on the data. \n Prepare notes of findings, including salient examples from source documents for client use. \n Identify and clearly communicate trends, outliers, potential issues, shortcomings, and additional needs based on document review and analysis. \n Communicate with manager on a regular basis. \n   Required Skills and Experience \n \n 2+ years of intensive experience working with large (~1M rows), complex, and potentially messy datasets. \n Expertise in Microsoft Excel and Stata (2+ years of regular Stata usage in the workplace). \n Expertise and use in work settings in complex equation building in Excel; including but not limited to nested logic, lookups (vlookup, hlookup, xlookup), match index, find, date/time functions, left/mid/right, sumifs, averageifs, shortcuts, sorting, pivot tables, etc. \n Excellent quantitative and analytical skills. \n Excellent verbal and written communication skills. \n Ability to work independently. \n Ability to manage caseload, including proactive planning, anticipating obstacles, and monitoring the progress of many cooccurring projects. \n Willingness to work on multiple projects with tight deadlines, including last minute requests and changes. \n Patience and attention to detail, with sharp focus on accuracy and problem solving. \n \n Remote Work ", "techs": ["excel", "stata", "microsoft excel"]}, "d81b0af4f1184e2e": {"terms": ["data analyst"], "salary_min": 20.0, "salary_max": 23.0, "title": "Claims Analyst I", "company": "ALLCARE HEALTH", "desc": "Claims Analyst I with AllCare Health in Grants Pass, Oregon \n PLEASE NOTE:  this is a telecommute/remote position - however, due to our pay practices, qualified candidates MUST reside within one of the following states: OR, WA, CA, NV, AK, ID, MT, CO, UT, NM, TX, KS, IN, TN, SC, VA, GA and NY. While candidates need to live or relocate to one of the aforementioned states, working remotely will require reliable broadband internet and personal cell phone service. Telecommute/remote work may include working day-to-day operations during AllCare Health\u2019s standard business hours (Pacific Standard Time). \n Summary \n This position is responsible for working closely with the Claims Processing Manager and the Claims and Encounter Data Analyst along with other staff to review, monitor, audit and adjudicate claims. \n Essential Duties \n 1. Accurately and efficiently adjudicates all claims applying benefits and pricing according to company policy, contract language and Medicare/Medicaid. \n 2. Evaluates pending claims to identify errors and takes appropriate action. \n 3. Take calls from Billing offices or Providers who have questions regarding claims status or processing. \n 4. Respectfully provides communication to billing offices and providers who may need guidance and education on billing processes. \n Education & Experience \n \n Associate's degree (A. A.) or equivalent from two-year College or technical school; or one to three years related experience and/or training; or equivalent combination of education and experience is required. \n Knowledge of medical terminology, ICD-10CM, ICD10PCS, Current Procedural Terminology, Healthcare Common Procedure Coding Systems, CMS1500 and UB04 and other claim forms is required. \n EZ-Cap experience preferred. \n PACE knowledge is preferred. \n \n Certificates, Licenses and/or Registrations \n \n A Certified Professional Coder certificate is required or to be obtained within 12 months of hire date. \n \n Company Overview \n AllCare offers competitive wages, an excellent benefit package that includes a 401k retirement, wellness program, and telecommute and flexible work schedule programs. \n Since 2016, AllCare Health has been a Certified B Corp\u00ae. As a Certified B Corp\u00ae, AllCare Health considers its impact on society and the environment during the business decision-making process, and has long recognized the real value in social, economic, and environmental concerns of its stakeholders, including its employees, customers, and community members. \n Grants Pass is located in Southern Oregon on the Rogue River and is surrounded by mountains and breathtaking views. The community is ideal for families and outdoor enthusiasts, with a temperate climate and easy access to outdoor recreation, wineries, outdoor concerts, the Ashland Shakespeare Festival, and much more. \n AllCare Health: https://www.allcarehealth.com/ \n B Lab: https://bcorporation.net/about-b-corps \n Job Type: Full-time \n Pay: $20.00 - $23.00 per hour \n Benefits: \n \n 401(k) \n Dental insurance \n Employee assistance program \n Flexible schedule \n Health insurance \n Life insurance \n Paid time off \n Professional development assistance \n Referral program \n Tuition reimbursement \n Vision insurance \n \n Schedule: \n \n Monday to Friday \n \n Work setting: \n \n Remote \n \n Education: \n \n Associate (Preferred) \n \n Experience: \n \n EZ-Cap: 1 year (Preferred) \n claims: 1 year (Preferred) \n medical coding: 1 year (Preferred) \n \n License/Certification: \n \n Certified Professional Coder (preferred) (Preferred) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "8407437ddb8967da": {"terms": ["data analyst"], "salary_min": 97214.53, "salary_max": 123095.25, "title": "Data Analyst", "company": "Evergreen Nephrology", "desc": "WHO YOU ARE \n  You are devoted, compassionate, and passionate about creating an exceptional employee experience. You're excited to be part of building a team from the ground up, and love to help set up employees for success by supporting their onboarding, performance management, and engagement. You thrive in innovative and evolving environments with high rates of change. You are driven by process improvements.  Does this sound like you?  If so, we should talk. \n  WHO WE ARE \n  Evergreen Nephrology partners with nephrologists to transform kidney care through a value-based, person-centered, holistic, and comprehensive approach to kidney care. We believe patients living with kidney disease deserve the best care. We are committed to improving patient outcomes and improving quality of life by delaying disease progression, shifting care to the home, and accelerating kidney transplants. \n  We help nephrologists focus on the right patients at the right time across the full care spectrum. We do this by providing them with the best-in-class interdisciplinary clinical resources, analytical insight and tools, and services to patients. \n  We listen to the needs of our patients, our employees, and our client partners, continually working to push beyond the status quo in which the care system manages patients today. \n  YOUR ROLE \n  As a Data Analyst, you will be a key contributor in healthcare data validation and issue resolution to ensure the data within Evergreen Nephrology's population health cloud data platform is of the highest quality for care management and decision making. You will trace and validate data and its relationships across data domains from multiple sources, including but not limited to: Electronic Medical Record systems (EMRs), Health Information Exchanges (HIEs), lab vendors, and healthcare payers. \n  The ideal candidate will demonstrate data analysis and problem-solving skills and be able to quickly identify and elevate data issues for resolution. Ultimately, you will work as a key member of the data & interoperability team to ensure high-quality data solutions that successfully support Evergreen Nephrology's business, nephrology partners, and our patients. \n  PRIMARY FUNCTIONS \n \n Analyze and understand the clinical and administrative data sources and how they integrate and flow into the enterprise healthcare data warehouse to meet business requirements \n Conduct data profiling of incoming clinical data sources to fully understand the structure, content, interrelationships, and quality of the data \n Create and execute clinical data validation routines and interrogate data to ensure proper data integration \n Assist with clinical data stewardship by actively monitoring the data quality as it moves across the health cloud platform \n Elevate data quality issues and gaps and communicate clear recommendations for remediation with the integration team \n Create test cases for user acceptance testing aligned to the user stories and pre-defined acceptance criteria \n Participate in conducting quality analysis and testing of new source data coming into the data warehouse \n Assist in defining best practice processes and procedures to support all data validation and data quality reporting related activities \n Support a work environment in which people are able to perform to the best of their ability, holding yourself accountable for productive results \n \n YOU'RE GOOD AT \n \n You reviewed the Who You Are section of this job posting and immediately felt the need to read on. That makes you a match for our innovative culture. \n You accept that things change quickly in a startup environment and are willing to pivot rapidly on priorities. \n Six (6) years of experience working in the IT data field. \n Four (4) years of experience providing healthcare data analysis, data validation in a value-based care provider setting. \n Two (2) years of working in a cloud-based environment, Azure/Snowflake a strong plus. \n Full understanding of all data warehouse concepts. \n Excellent SQL skills and full understanding of querying relational databases. \n Experience with Innovaccer Health Cloud Platform a strong plus. \n Full understanding of healthcare payer data such as member eligibility and claims. \n Working knowledge of Electronic Medical Record Systems such as Epic, Allscripts or eClinicalWorks. \n Working knowledge of clinical code sets such as CPT, ICD-10, LOINC, SNOWMED and RxNorm. \n Demonstrated analytical, written, problem solving, and interpersonal collaboration skills. \n Proven understanding of healthcare privacy and security practices. \n Accept that things change quickly in a startup environment and are willing to pivot quickly on priorities. \n Demonstrated team player. \n Bachelor of Science in Computer Science or similar. \n \n WE'RE GOOD AT \n \n You will benefit from Evergreen Nephrology's exceptional total rewards package, including competitive base pay with bonuses, paid time off starting at four weeks for full-time employees, 12 paid holidays per year, reimbursement for continuing medical education, 401k with match, health, dental, and vision insurance. \n We are proud to offer family-friendly policies that support paid parental leave and flexible work arrangements. \n We commit to a robust training and development program that starts with onboarding and continues throughout your career with Evergreen Nephrology \n As an inclusive and diverse team, you will collaborate with like-minded healthcare professionals who, like you, understand the importance and value of Evergreen Nephrology's high-quality, value-based care model \n \n \n \n  Common characteristics of the people who comprise Evergreen Nephrology: \n  Smart, detail-oriented, mission-driven, entrepreneurial, and operates with urgency", "cleaned_desc": " \n Analyze and understand the clinical and administrative data sources and how they integrate and flow into the enterprise healthcare data warehouse to meet business requirements \n Conduct data profiling of incoming clinical data sources to fully understand the structure, content, interrelationships, and quality of the data \n Create and execute clinical data validation routines and interrogate data to ensure proper data integration \n Assist with clinical data stewardship by actively monitoring the data quality as it moves across the health cloud platform \n Elevate data quality issues and gaps and communicate clear recommendations for remediation with the integration team \n Create test cases for user acceptance testing aligned to the user stories and pre-defined acceptance criteria \n Participate in conducting quality analysis and testing of new source data coming into the data warehouse \n Assist in defining best practice processes and procedures to support all data validation and data quality reporting related activities \n Support a work environment in which people are able to perform to the best of their ability, holding yourself accountable for productive results   Experience with Innovaccer Health Cloud Platform a strong plus. \n Full understanding of healthcare payer data such as member eligibility and claims. \n Working knowledge of Electronic Medical Record Systems such as Epic, Allscripts or eClinicalWorks. \n Working knowledge of clinical code sets such as CPT, ICD-10, LOINC, SNOWMED and RxNorm. \n Demonstrated analytical, written, problem solving, and interpersonal collaboration skills. \n Proven understanding of healthcare privacy and security practices. \n Accept that things change quickly in a startup environment and are willing to pivot quickly on priorities. \n Demonstrated team player. \n Bachelor of Science in Computer Science or similar. \n ", "techs": ["innovaccer health cloud platform", "epic", "allscripts", "eclinicalworks", "cpt", "icd-10", "loinc", "snowmed", "rxnorm."]}, "1abd18ff9f84ad70": {"terms": ["data analyst"], "salary_min": 85000.0, "salary_max": 85000.0, "title": "Jr. Business Analyst", "company": "Oran Inc", "desc": "Senior Business Analyst \n \n  We are seeking an experienced Senior Business Analyst with a strong background in SharePoint Collabspace, LMS365, and PowerApps environments to join our team. The ideal candidate will play a pivotal role in bridging the gap between business needs and technology solutions, ensuring the effective utilization of these platforms in alignment with our organizational goals. If you are a skilled analyst with expertise in these Microsoft 365 technologies, we encourage you to apply for this senior-level position. \n Responsibilities \n Collaborate with stakeholders to understand business needs and objectives related to SharePoint Collabspace, LMS365, and PowerApps environments. \n Elicit, document, and analyze business requirements, ensuring they are clear, concise, and aligned with best practices. \n Evaluate the current M365 environment for usability, security and compliance including SharePoint, Collabspace/Collabmail, Teams, PowerApps, and LMS365. \n Work closely with technical teams to translate business requirements into technical specifications. \n Document detailed system requirements, data models, workflows, and use cases. \n Create and maintain comprehensive documentation for SharePoint Collabspace, LMS365, and PowerApps projects \n Identify opportunities for process improvement within the SharePoint Collabspace, LMS365, and PowerApps environments. \n Develop and execute test plans and test cases to validate the functionality of SharePoint Collabspace, LMS365, and PowerApps solutions. \n Provide training and support to end-users, helping them maximize the value of SharePoint Collabspace, LMS365, and PowerApps applications. \n Assist in resolving user issues and inquiries. \n Monitor the performance and usage of SharePoint Collabspace, LMS365, and PowerApps environments. \n Ongoing collaboration and training of IT staff and eLearning Training Specialist on the redevelopment and implementation of newly developed and redeveloped PowerApps and features \n Procure and manage licensing for Collabspace, LMS365 and other add-ons as recommended and accepted by FMCS. \n \n \n Qualifications: \n  Bachelor's degree in Business Administration, Information Technology, or a related field. \n 5+ years of experience working directly with small to medium governmental agencies \n Completed background investigation at the Tier 2 moderate risk level (formerly MBI) with a favorable adjudication. \n 5+ years of experience as a Business Analyst, with a focus on SharePoint Collabspace, LMS365, and PowerApps environments. \n Strong knowledge of Microsoft 365 technologies and platforms. \n Proficiency in requirements gathering, analysis, and documentation. \n Excellent communication and interpersonal skills to facilitate effective collaboration. \n Ability to translate business needs into technical specifications. \n Experience with process improvement and workflow optimization. \n Strong problem-solving and analytical skills. \n Ability to work in a dynamic, fast-paced environment. \n Relevant certifications in business analysis or Microsoft 365 technologies (e.g., Power Platform certifications) preferred. \n \n \n Desired Qualifications: \n  Experience with Agile development \n Experience with cloud-based SharePoint solutions", "cleaned_desc": " Experience with process improvement and workflow optimization. \n Strong problem-solving and analytical skills. \n Ability to work in a dynamic, fast-paced environment. \n Relevant certifications in business analysis or Microsoft 365 technologies (e.g., Power Platform certifications) preferred. \n \n \n Desired Qualifications: ", "techs": ["none"]}, "9f8c7352b73c5278": {"terms": ["data analyst"], "salary_min": 60000.0, "salary_max": 65000.0, "title": "Analyst, Master Data Management", "company": "Sol de Janeiro", "desc": "Launched in 2015, SOL DE JANEIRO is an award-winning, fast-growing premium global lifestyle beauty brand inspired by the beauty rituals and attitude of the beaches of Rio. Our brand purpose is to spark endless self-celebration and joy through the power and warmth of the Brazilian spirit. This philosophy extends from our products to our corporate culture. We are leaders, permanently inquisitive, category obsessed, incredibly diverse, self-demanding, true team players, and we are all Brazilian in Spirit. \n  Headquartered in New York and launched as a digitally native brand, we now have teams in Rio, Paris, London and Italy supporting the expansion across the globe. \n \n  We are looking for someone who loves Operations, gets excited about problem solving, and is passionate about building relationships. \n  The position will report to  Manager, Inventory Control & Master Data  and will partner with various stakeholders to support our Global Master Data Management needs, which include but are not limited to; building best practices and implementing new procedures, policies, and management of all internal Master Data objects. \n  Overall Responsibilities: \n \n Assist in the development of Master Data rules and standard practices. \n    \n Data Enrichment flows \n Departmental data attributes responsibilities \n \n Maintain, refine, and update Active Master Data attributes and fields in Net Suite \n    \n Build more efficient ways to carry out said data updates. \n \n Own BOM creation and maintenance in Net Suite \n    \n Ensure all BOM's Active Dates are maintained and or ended based on Production Planning Requirements \n \n Own and develop Automated data flows between Net Suite and 3rd Party Systems \n Run Daily, Weekly and Monthly reporting to ensure all Master Data attributes fields are populated in Net Suite \n \n Job Requirements/Qualifications: \n \n Bachelor's degree in Business, Supply Chain or related field preferred. \n We will also consider students with a graduation date of May / June '23, to start after graduation. \n Must be comfortable working a hybrid schedule (remote & in our New York City office, when needed). \n 1-2 years of experience in Operations and Supply Chain preferred \n Advanced Excel skills required. \n Strong attention to detail \u2013 must analyze many pieces of data and shipping routes \n Solid written and verbal communication skills \n Ability to operate effectively and efficiently on an independent level \n A strong business sense and experience operating in small to medium-sized business environment \n Net Suite experience is preferred \n Beauty Industry experience is preferred \n \n #LI-Hybrid \n \n  Salary Range \n \n    $60,000\u2014$65,000 USD\n   \n \n \n  In addition to base salary, this position is eligible for participation in a highly competitive bonus program with the possibility for overachievement based on performance and company results. \n  The information contained herein is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee, as these may change or new ones may be assigned at any time. \n  Sol de Janeiro is committed to diversity and inclusion in the workplace. We are an equal opportunity employer and do not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. \n \n \n  https://soldejaneiro.com/pages/privacy-policy-sol-de-janeiro", "cleaned_desc": "", "techs": ""}, "686f85c85c0ce1cf": {"terms": ["data analyst"], "salary_min": 70000.0, "salary_max": 82000.0, "title": "Clinical Data Analyst", "company": "Medical Components, Inc.", "desc": "Clinical Data Analyst \n  Remote position only accepting candidates residing in the following States: AZ, CT, FL, GA, MD, MN, NJ, NC, OH, PA, TN, TX, UT, VA, and WV. \n  Summary:  Will be responsible for performing statistical analysis on epidemiological data and assisting in the creation and maintenance of clinical data repositories. \n  Essential Duties and Responsibilities  include the following. Other duties may be assigned as needed: \n \n  Apply statistical techniques, including but not limited to logistical regression, cox proportional hazards, t-testing, Poisson testing, and chi square testing, to analyze clinical data and extract insights. \n  Create and/or maintain clinical data repositories for safety data, outcome data, and quantification of clinical evidence. \n  Develop references, graphics, tables, and data listings, to support clinical evaluation documentation and publication manuscripts. \n  Clean and transform data for analysis purposes, apply knowledge in data appraisal and analysis methods, and identify and resolve data discrepancies. \n  Stay current with new statistical techniques, epidemiological trends, and best practices in analyzing clinical evidence. \n  Support product development, clinical trials, and post market clinical activities under the direction of the Clinical Data Manager and/or the department head \n  Assist the activities of our Contract Research Organizations (CROs) under the direction of the Clinical Data Manager and/or the department head. \n  Maintain familiarity with applicable regulations and standards that apply to Clinical Affairs responsibilities. \n  Ensures documents are produced in accordance with procedures, internal and external guidelines, and regulations. \n  Collaborate with internal (project owners, Regulatory Affairs, Quality, Compliance, Engineers, etc.) and external (clinical consultants, subject matter experts, regulatory agencies) stakeholders to ensure comprehensive analysis of clinical evidence. \n  Assist in the execution of PMCF activities and data review. \n  Cultivate a strong internal culture designed around collaboration, feedback, motivation, and accountability. \n  Domestic and International travel may be required. \n \n  Education and/or Experience \n  Bachelor's degree (B.S.) from a four-year college or university in Statistics, Health Informatics, Epidemiology, or a related science or health-related field. Experience working in a regulated environment and familiarity with GDPR and HIPAA is preferred. \n  About Us -  Medcomp\u00ae develops, manufactures, markets, and supports cutting-edge vascular access devices and accessories to meet the clinical needs of the medical industry, particularly in the fields of interventional medicine and dialysis. Our company's engineering and applications expertise provides superior products whose progressive designs accommodate advances in medicine and whose quality anticipates the requirements of our professional clients and the patients they serve. Currently one of the world's largest manufacturers of dialysis and centrally terminating venous catheters, Medcomp is, and always has been, on the cutting edge of new vascular access device technologies. \n  Our offerings have our Team Members well-being in mind. \n \n  Competitive salary \n  Health, Dental, Vision coverage \n  FSA \n  Life Insurance \n  401k \n  Paid Holidays \n  Generous PTO \n  Opportunities for job growth and advancement \n \n  What you will get from us: \n  VISION & MISSION STATEMENT \n  To be the leading solution provider for every customer we serve. \n  To drive sustainable growth of our business through the continued investment in our people, technology and quality while delivering world class products and services. \n  We offer meaningful rewarding work to our team members by providing a culture that is built on and supported by practicing our core values, and by providing our team members with the tools and resources they need to succeed. \n  We believe in our  Core Values \n  Respect, Integrity, Safety, Collaboration, Commitment and Passion \n  We are committed to ensuring equal employment opportunities for all job applicants and employees. Employment decisions are based upon job-related reasons regardless of an applicant's race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, marital status, genetic information, protected veteran status, or any other status protected by law. \n   \n LB0agPtl9X", "cleaned_desc": "Clinical Data Analyst \n  Remote position only accepting candidates residing in the following States: AZ, CT, FL, GA, MD, MN, NJ, NC, OH, PA, TN, TX, UT, VA, and WV. \n  Summary:  Will be responsible for performing statistical analysis on epidemiological data and assisting in the creation and maintenance of clinical data repositories. \n  Essential Duties and Responsibilities  include the following. Other duties may be assigned as needed: \n \n  Apply statistical techniques, including but not limited to logistical regression, cox proportional hazards, t-testing, Poisson testing, and chi square testing, to analyze clinical data and extract insights. \n  Create and/or maintain clinical data repositories for safety data, outcome data, and quantification of clinical evidence. \n  Develop references, graphics, tables, and data listings, to support clinical evaluation documentation and publication manuscripts. ", "techs": ["clinical data analyst", "statistical analysis", "epidemiological data", "clinical data repositories", "logistical regression", "cox proportional hazards", "t-testing", "poisson testing", "chi square testing", "references", "graphics", "tables", "data listings"]}, "a22e4e05fcfd8f19": {"terms": ["data analyst"], "salary_min": 50.0, "salary_max": 60.0, "title": "Business Analyst (Healthcare)", "company": "IDC", "desc": "Title: Business Analyst (Healthcare) \n Location: 100% Remote (prefer candidates local to Bedford MA, NYC, Irving TX, Naperville IL, or Omaha NE) \n Duration: 6-month Temp-to-Perm \n Supplier Notes: \n This is a heavily customer facing role and requires strong communication skills, strong understanding of healthcare claims processes, and a strong technical communication skill. \n Requirements: \n 2+ years\u2019 experience in analysis and design on software development projects in analyst role (preferably within a Scrum Agile team). \n Knowledge of Healthcare industry and understanding all major workflows associated with healthcare claim processing. \n Ability to effectively present information and respond to questions from internal and external customers. \n Excellent verbal and written communication skills. \n Experience in SAFe Agile preferred. \n TDD/BDD Development preferred. \n Job Types: Contract, Temporary \n Salary: $50.00 - $60.00 per hour \n Benefits: \n \n Dental insurance \n Health insurance \n Life insurance \n Vision insurance \n \n Experience level: \n \n 2 years \n \n Schedule: \n \n 8 hour shift \n \n Experience: \n \n SQL: 1 year (Preferred) \n \n Work Location: Remote", "cleaned_desc": " 2+ years\u2019 experience in analysis and design on software development projects in analyst role (preferably within a Scrum Agile team). \n Knowledge of Healthcare industry and understanding all major workflows associated with healthcare claim processing. \n Ability to effectively present information and respond to questions from internal and external customers. \n Excellent verbal and written communication skills. \n Experience in SAFe Agile preferred. \n TDD/BDD Development preferred. ", "techs": ["scrum agile", "healthcare industry", "safe agile", "tdd/bdd development"]}, "8b2c4cdc42711753": {"terms": ["data analyst"], "salary_min": 70000.0, "salary_max": 110000.0, "title": "Jr. Business Analyst W2 (Remote)", "company": "Kani Solution", "desc": "Expertise in working with stakeholders and collaborating with software development teams to transform requirements into system development. \n \n Build, own, and maintain a large and complex system of data pipelines, transforming raw data all the way into aggregated tables that feed into dashboards. \n \n Maintain and iterate on your dashboards to continue to drive increased value through additional business insights. \n Interface with cross-functional stakeholders to thought partner on data-informed strategic decision-making. \n Analytical mindset and a knack for problem-solving, along with project management skills. \n \n We consider aspirants eligible to work in United States \n We are an E-verified company , we can help you with H1B sponsorship \n Entry levels with non - IT background can also apply for this position \n No prior programming knowledge is required for this role. \n Placement Assistance, Mentorship support , Workshops offered on Business Analyst Roles. \n Domain preparation , hands-on preparation & interview preparation for Entry Level roles and guidance on the certification is provided. \n Contract:  W2 Position , 1099. \n Talent Acquisition Specialist Anil.k@kanisol.com (609) 644-4005 \n Job Type: Contract \n Salary: $70,000.00 - $110,000.00 per year \n Benefits: \n \n 401(k) \n \n Compensation package: \n \n 1099 contract \n \n Experience level: \n \n 3 years \n \n Schedule: \n \n 8 hour shift \n \n Experience: \n \n SQL: 1 year (Preferred) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "15c74ed7d1390961": {"terms": ["data analyst"], "salary_min": 67506.2, "salary_max": 85477.89, "title": "Provider Operations Data Analyst - Remote", "company": "UnitedHealthcare", "desc": "At UnitedHealthcare, we\u2019re simplifying the health care experience, creating healthier communities and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable. Ready to make a difference? Join us to start  Caring. Connecting. Growing together. \n \n \n \n The Provider Operations Data Analyst will leverage their deep knowledge of Provider Operations, Analytics, and business savvy to develop insights into Provider Experience based on health plan operational processes. \n \n We\u2019re looking for someone with a high degree of analytic and consultative experience, and well-developed communication, data storytelling / visualization, and technical skills. \n \n This role is responsible for generating data-driven insights to support executive decision making to improve provider experience as well as health plan business and administrative processes. \n \n By understanding the capabilities, strategies, financial and operational goals of stakeholders, successful individuals in this role will identify opportunities to leverage data visualization and core analytic solutions to drive higher efficiency and improve customer experience across UHG. \n \n \n \n You\u2019ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges. \n \n \n \n   \n \n \n Primary Responsibilities: \n \n \n \n \n Work with vast amounts of data from multiple sources to deliver high-impact actionable insights to senior leadership and business operations; enable data-driven decision-making across the enterprise to improve the efficiency and effectiveness of UHC Operations, while focusing on constituent experience improvement \n Perform exploratory data analysis from structured, unstructured, and diverse datasets; apply knowledge of statistics, data presentation and visualization, simulation, and advanced mathematics to recognize patterns, identify opportunities, pose business questions, and make valuable discoveries leading to prototype development and product improvement \n Helping source, clean, prepare and verify integrity of data to be used for analysis \n Integrate data sources and create new metrics to accurately measure business and administrative processes \n Discover and communicate actionable insights to decision makers. Make presentations to sr. executives and decision-makers \n Provide expert advisory services for strategy, metrics, product development, and analytics vision to senior leadership \n Partner with various functions to produce innovative and transformative data-driven solutions which directly contribute to achieving enterprise goals and vision \n \n \n \n \n You\u2019ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in. \n \n \n Required Qualifications: \n \n \n \n Undergraduate degree in a quantitative field \n 2+ years of experience in data analysis, data-driven investigations, data storytelling, visualizations, and analytic insight creation for executive consumption \n Experience handling terabyte size datasets, diving into data to discover hidden patterns, using data visualization tools, advanced proficiency in SQL \n Advanced proficiency in Excel \n Advanced proficiency in R and/or Python for data analysis \n Demonstrated business focus; must excel at connecting business requirements to analytic objectives and to measurable business benefit \n Proven advanced skills in data visualization tools such as Tableau / PowerBI / D3JS etc \n \n \n \n  Preferred Qualification: \n \n Master\u2019s degree or higher in a highly quantitative field. \n 3+ years of experience in data analysis, data-driven investigations, data storytelling, visualizations, and analytic insight creation for executive consumption. \n Experience in any of the following: medical and/ or Rx claims, clinical outcomes, Customer Service Analytics, Provider behavioral analytics. \n \n \n \n \n  California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only:  The salary range for California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island or Washington residents is $67,800 to $133,100. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you\u2019ll find a far-reaching choice of benefits and incentives.\n   \n \n \n \n   \n \n \n \n All employees working remotely will be required to adhere to UnitedHealth Group\u2019s Telecommuter Policy \n \n \n \n \n At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone\u2013of every race, gender, sexuality, age,  location  and income\u2013deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized  groups  and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering  equitable  care that addresses health disparities and improves health outcomes \u2014 an enterprise priority reflected in our mission . \n \n \n \n \n \n \n \n Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action  employer  and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law . \n \n \n \n UnitedHealth Group is a  drug -  free workplace. Candidates  are required to  pass a drug test before beginning employment .", "cleaned_desc": " \n \n \n You\u2019ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in. \n \n \n Required Qualifications: \n \n \n \n Undergraduate degree in a quantitative field \n 2+ years of experience in data analysis, data-driven investigations, data storytelling, visualizations, and analytic insight creation for executive consumption \n Experience handling terabyte size datasets, diving into data to discover hidden patterns, using data visualization tools, advanced proficiency in SQL \n Advanced proficiency in Excel \n Advanced proficiency in R and/or Python for data analysis \n Demonstrated business focus; must excel at connecting business requirements to analytic objectives and to measurable business benefit \n Proven advanced skills in data visualization tools such as Tableau / PowerBI / D3JS etc ", "techs": ["tableau", "powerbi", "d3js"]}, "0e6234e2a6f1badc": {"terms": ["data analyst"], "salary_min": 70.0, "salary_max": 75.0, "title": "Guidewire Business Analyst", "company": "TekValue IT Solutions", "desc": "Guidewire Business Analyst \n Remote \n 12+ Months \n Required Skills: \n \n Requirements definition for projects and multi-year strategic Guidewire initiatives; identify and translate business needs into clearly defined requirements, and create documentation inclusive of business use cases, process flows, data flows, traceability matrices and report mock-ups \n \n \n Plan, facilitate, and conduct requirements gathering sessions, meetings, and presentations \n \n \n Schedule review sessions for completed business/functional requirements with key business users focused on gaining consensus and final business approval \n \n \n Collaborate with development and testing teams to provide subject matter expertise, ensure that out of the box functionality is leveraged and assist in troubleshooting and resolving issues \n \n \n 5-7 years of business analysis experience on Guidewire Policy Center or Billing Center projects \n \n \n Mandatory experience with Guidewire Policy Center or Billing Center. Preferred to have GW certifications. \n \n \n Experienced with Agile Scrum methodology and business analysis methodologies, working knowledge of business analysis documentation standards \n \n \n Hands-on experience writing user stories is required. \n \n \n Experience operating and interfacing with business management, including negotiation and presentation skills. \n \n \n Proven track record of creating clear, concise deliverables that reflect a deep understanding of business needs and the out of the box software functionality. \n \n \n Excellent problem-solving and analytical skills. \n \n \n Excellent communication skills, both written and verbal; understands communication channels and escalates appropriately. \n \n \n Must be proficient with process modeling tools (e.g., Visio); experience with other visualization tools is beneficial. \n \n \n Experience supervising small teams. \n \n \n Strong initiative; able to self-manage; comfortable with ambiguity and able to work through it to get the right things done. \n \n \n Must be able to see tasks through to completion without significant guidance. \n \n Job Type: Contract \n Salary: $70.00 - $75.00 per hour \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n \n Experience: \n \n SQL: 1 year (Preferred) \n \n Work Location: Remote", "cleaned_desc": " Hands-on experience writing user stories is required. \n \n \n Experience operating and interfacing with business management, including negotiation and presentation skills. \n \n \n Proven track record of creating clear, concise deliverables that reflect a deep understanding of business needs and the out of the box software functionality. \n \n \n Excellent problem-solving and analytical skills. \n \n \n Excellent communication skills, both written and verbal; understands communication channels and escalates appropriately. ", "techs": ["none"]}, "f86b65080cef177c": {"terms": ["data analyst"], "salary_min": 0.0, "salary_max": 34.0, "title": "Business Analyst II", "company": "RPO International", "desc": "Note: This is not open to C2C! \n Job Description: Reviews, analyzes, and evaluates business systems and user needs. Formulates systems to parallel overall business strategies. Writes detailed description of user needs, program functions, and steps required to develop or modify computer programs. Relies on experience and judgment to plan and accomplish goals. Performs a variety of complicated tasks. May report directly to a project lead or manager. Basic Requirements: \n \n 2+ years of business analyst experience \n Bachelor\u2019s degree or Master\u2019s degree \n Familiar with relational database concepts, and client-server concepts. \n A wide degree of creativity and latitude is expected. \n Data analytics reports experience \n Proven good data analytics skills \n Advanced excel skills good communication \n Can take initiative and not wait \n SQL/writing experience \n Only candidates available and ready to work directly as NJAI/Genesis10 employees will be considered for this position. \n \n Compensation : $34.00 per hour \n If you have the described qualifications and are interested in this exciting opportunity, apply today! \n Job Types: Full-time, Contract \n Pay: Up to $34.00 per hour \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n Paid time off \n Vision insurance \n \n Experience level: \n \n 2 years \n \n Schedule: \n \n 8 hour shift \n \n Experience: \n \n business analyst: 2 years (Preferred) \n SQL: 2 years (Preferred) \n \n License/Certification: \n \n US Citizen or Greencard Holder (Preferred) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "85dbe3ba839fcf2e": {"terms": ["data analyst"], "salary_min": 90036.68, "salary_max": 114006.49, "title": "Database Analyst IV", "company": "Black Knight", "desc": "Position:\n   Database Analyst IV\n  \n \n   Job Description:\n  \n \n   Black Knight is the premier provider of integrated technology, services, data and analytics that lenders and servicers look to first to help successfully manage the entire loan life cycle. Our deep understanding of regulatory and compliance issues complements the knowledge, technology and solutions we offer to help our clients achieve their business goals. Black Knight offers leading software systems; data and analytics offerings; and information solutions that facilitate and automate many of the business processes across the mortgage life cycle.\n  \n \n \n   JOB FAMILY DESCRIPTION\n  \n \n \n   Designing, modeling, developing and supporting Database Management Systems (DBMS).\n  \n \n  RESPONSIBILITIES\n  \n \n \n \n     Analyzes data requirements, application, and processing architectures, data dictionaries, and database schema(s)\n    \n \n \n     Designs, develops, amends, optimizes, and certifies database schema design to meet system(s) requirements\n    \n \n \n     Gathers, analyzes, and normalizes relevant information related to, and from business processes, functions, and operations to evaluate data credibility and determine relevance and meaning\n    \n \n \n     Develops database and warehousing designs across multiple platforms and computing environments\n    \n \n \n     Develops an overall data architecture that supports the information needs of the business in a flexible but secure environment\n    \n \n \n     Performs other related duties as assigned\n    \n \n \n  MINIMUM REQUIREMENTS\n  \n \n \n \n     Bachelor\u2019s degree in Computer Science, Information Systems or the equivalent combination of education, training, or work experience.\n    \n \n \n     Requires ten (10) years of work experience in DBA, DBMS design and support and relevant computing environments\n    \n \n \n     Expertise in SQL or PL/SQL (or comparable language)\n    \n \n \n     Strong experience in database architecture, data modeling and schema design\n    \n \n \n     Familiar with data standards/procedures and data governance\n    \n \n \n  PREFERRED REQUIREMENTS\n  \n \n \n \n     Experience in orchestrating the coordination of data related activities to ensure on-time delivery of data solutions to support business capability requirements including data activity planning, risk mitigation, issue resolution and design negotiation\n    \n \n \n     Ability to design effective management of reference data\n    \n \n \n     Experience in IBM DB2 administration and/or DB2 Application Development\n    \n \n \n     Experience in large scale OLTP and DSS database deployments\n    \n \n \n     Experience in utilizing the performance tuning tools\n    \n \n \n     Experience in large scale database and data migrations across disparate platforms\n    \n \n \n     Experience in data platform construction supporting business-to-business integrations\n    \n \n \n \n   LOCATION: Qualified candidates may work remotely 100% of the time.\n  \n \n \n  The target pay range for this position is typically $106,080 - $145,600 \n \n \n \n   Black Knight is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, and protected veteran or military family status. Our employees\u2019 diversity is our strength, and when we embrace our differences, it makes us better and brighter. Black Knight\u2019s commitment to inclusion is at the core of who we are, and motivates us in how we do business each and every day.\n  \n \n \n   Black Knight carefully considers multiple factors to determine compensation, including a candidate\u2019s education, training, specialty, experience, and work location. The base salary (exempt) or hourly rate (non-exempt) is just one component of the total rewards package offered to our employees, including potential bonus or commission eligibility, insurance (medical/dental/vision/life/disability), matching 401(k) plan and matching employee stock purchase plan\n  \n \n \n   EEO Statement:\n  \n \n \n  Black Knight is an Equal  \n Opportunity/Affirmative \n  Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, and protected veteran or military family status. Our employees\u2019 diversity is our strength, and when we embrace our differences, it makes us better and brighter. Black Knight\u2019s commitment to inclusion is at the core of who we are, and motivates us in how we do business each and every day. \n \n \n \n   Location:\n   Remote\n  \n \n   Time Type:\n   Full time", "cleaned_desc": " \n \n     Requires ten (10) years of work experience in DBA, DBMS design and support and relevant computing environments\n    \n \n \n     Expertise in SQL or PL/SQL (or comparable language)\n    \n \n \n     Strong experience in database architecture, data modeling and schema design\n    \n \n \n     Familiar with data standards/procedures and data governance\n    \n \n \n  PREFERRED REQUIREMENTS\n  \n \n \n \n     Experience in orchestrating the coordination of data related activities to ensure on-time delivery of data solutions to support business capability requirements including data activity planning, risk mitigation, issue resolution and design negotiation\n    \n \n       Ability to design effective management of reference data\n    \n \n \n     Experience in IBM DB2 administration and/or DB2 Application Development\n    \n \n \n     Experience in large scale OLTP and DSS database deployments\n    \n \n \n     Experience in utilizing the performance tuning tools\n    \n \n \n     Experience in large scale database and data migrations across disparate platforms\n    \n \n \n     Experience in data platform construction supporting business-to-business integrations\n    \n \n \n \n   LOCATION: Qualified candidates may work remotely 100% of the time.\n  ", "techs": ["dba", "dbms", "sql", "pl/sql", "database architecture", "data modeling", "schema design", "data standards", "data governance", "orchestrating", "data activity planning", "risk mitigation", "issue resolution", "design negotiation", "management of reference data", "ibm db2 administration", "db2 application development", "oltp", "dss", "performance tuning tools", "database migrations", "data platform construction", "business-to-business integrations"]}, "1281436ae579643e": {"terms": ["data analyst"], "salary_min": 100000.0, "salary_max": 110000.0, "title": "Agile Business Analyst (Remote)", "company": "Digital Strategy LLC", "desc": "Digital Strategy is seeking an Agile Business Analyst with strong analytical and problem-solving skills to provide support to federal clients in performing data analysis and developing reports and dashboards in various business intelligence platforms. The candidate will work with federal and contractor teams to understand reporting needs and deliver data analytics products to meet and exceed client expectations. Specific duties will include but are not limited to the following: \n \n Manage the Product Backlog:  Work with the Product Manager and clients to create and refine work items in the backlog, including features and user stories with acceptance criteria. \n Requirements Gathering:  Collaborate with stakeholders to understand and document the business needs, processes, design (wireframes/mockups), and acceptance criteria for data analytics products. \n Product Development Support:  Work closely with developers to explain the product design, address their questions, and provide clarifications throughout the development process. \n Testing and Quality Assurance:  Develop test plans and test cases to ensure that the developed product meets the specified requirements. Conduct testing and troubleshoot issues, working with developers to rectify any defects. \n Product Rollout:  Participate in the rollout of new and enhanced products, including providing demo and support to end-users on how to effectively use the new products. Support the creation of training documents to aid in system adoption. \n \n Position Requirements: \n \n Bachelor\u2019s degree Computer Science, Information Technology, Management Information Systems, Business, or similar majors. \n Ten (10) years of total work experience, including requirements gathering and data analysis. \n Two (2) years of experience may be substituted for Master\u2019s degree. \n At least four (4) years of experience in developing and delivering business/functional requirements, with at least 2 years in an Agile environment. \n At least four (4) years of experience with data analysis and data visualization projects. \n Experience developing reports and dashboards utilizing business intelligence tools, e.g. Microsoft Power BI, Cognos ReportNet, Tableau, or others. \n Ability to analyze complex systems, identify problems, and propose effective solutions. \n Excellent verbal and written communication skills to interact with diverse teams and explain technical concepts to non-technical stakeholders. \n Critical thinking and problem-solving abilities to address issues and challenges in system design and implementation. \n Basic project management skills to manage timelines, prioritize tasks, and ensure timely delivery of projects. \n \n Min. Citizenship Status Required: \n U.S. Citizen \n Salary: \n $100-110k/annually \n Digital Strategy is a small woman-owned, information technology and management consulting firm with several Federal Government projects. Due to the nature of our contracts and federal guidelines, US Citizenship is required. \n In addition to the flexibility of working remotely, we offer a generous benefits package that includes Health and Life insurance, 401K plan, and Paid-Time-Off, plus 11 paid Federal holidays! \n Find out what makes us unique by visiting our website at: www.ds-llc.com. \n Job Type: Full-time \n Pay: $100,000.00 - $110,000.00 per year \n Benefits: \n \n 401(k) \n 401(k) matching \n Dental insurance \n Health insurance \n Life insurance \n Paid time off \n Vision insurance \n \n Compensation package: \n \n Yearly pay \n \n Experience level: \n \n 10 years \n \n Schedule: \n \n Monday to Friday \n \n Application Question(s): \n \n Are you legally authorized to work in the US for any employer without restrictions? \n \n Education: \n \n Bachelor's (Required) \n \n Experience: \n \n Total Overall Work: 10 years (Required) \n Data Visualization: 4 years (Required) \n Data Analysis and Reporting: 4 years (Required) \n Requirements Gathering: 4 years (Required) \n \n Work Location: Remote", "cleaned_desc": "Digital Strategy is seeking an Agile Business Analyst with strong analytical and problem-solving skills to provide support to federal clients in performing data analysis and developing reports and dashboards in various business intelligence platforms. The candidate will work with federal and contractor teams to understand reporting needs and deliver data analytics products to meet and exceed client expectations. Specific duties will include but are not limited to the following: \n \n Manage the Product Backlog:  Work with the Product Manager and clients to create and refine work items in the backlog, including features and user stories with acceptance criteria. \n Requirements Gathering:  Collaborate with stakeholders to understand and document the business needs, processes, design (wireframes/mockups), and acceptance criteria for data analytics products. \n Product Development Support:  Work closely with developers to explain the product design, address their questions, and provide clarifications throughout the development process. \n Testing and Quality Assurance:  Develop test plans and test cases to ensure that the developed product meets the specified requirements. Conduct testing and troubleshoot issues, working with developers to rectify any defects. \n Product Rollout:  Participate in the rollout of new and enhanced products, including providing demo and support to end-users on how to effectively use the new products. Support the creation of training documents to aid in system adoption. \n \n Position Requirements: \n \n Bachelor\u2019s degree Computer Science, Information Technology, Management Information Systems, Business, or similar majors. \n Ten (10) years of total work experience, including requirements gathering and data analysis. \n Two (2) years of experience may be substituted for Master\u2019s degree.   At least four (4) years of experience in developing and delivering business/functional requirements, with at least 2 years in an Agile environment. \n At least four (4) years of experience with data analysis and data visualization projects. \n Experience developing reports and dashboards utilizing business intelligence tools, e.g. Microsoft Power BI, Cognos ReportNet, Tableau, or others. \n Ability to analyze complex systems, identify problems, and propose effective solutions. \n Excellent verbal and written communication skills to interact with diverse teams and explain technical concepts to non-technical stakeholders. \n Critical thinking and problem-solving abilities to address issues and challenges in system design and implementation. \n Basic project management skills to manage timelines, prioritize tasks, and ensure timely delivery of projects. \n \n Min. Citizenship Status Required: \n U.S. Citizen \n Salary: \n $100-110k/annually \n Digital Strategy is a small woman-owned, information technology and management consulting firm with several Federal Government projects. Due to the nature of our contracts and federal guidelines, US Citizenship is required. ", "techs": ["agile business analyst", "data analysis", "reports", "dashboards", "business intelligence platforms", "product backlog", "product manager", "requirements gathering", "design", "wireframes/mockups", "acceptance criteria", "product development support", "developers", "testing and quality assurance", "test plans", "test cases", "product rollout", "end-users", "training documents", "computer science", "information technology", "management information systems", "business", "requirements gathering", "data analysis", "agile environment", "data visualization projects", "business intelligence tools", "microsoft power bi", "cognos reportnet", "tableau", "data analysis", "data visualization projects", "systems analysis", "verbal and written communication skills", "critical thinking", "problem-solving abilities", "project management skills", "u.s. citizen", "digital strategy"]}, "10db8d2e44202aa4": {"terms": ["data analyst"], "salary_min": 71935.54, "salary_max": 91086.43, "title": "Business Intelligence Analyst II", "company": "Mindbody", "desc": "We're revolutionizing the fitness & wellness industry, and we're looking for talented people to help us do it. Mindbody + ClassPass bring together the best of both sides of the market: Mindbody is the industry's most trusted all-in-one technology platform; ClassPass is one of the most popular apps for fitness & self-care enthusiasts. Together we're partnering with more than 70,000 fitness studios, gyms, salons, and spas around the world. We're not just another tech company\u2014we're far and away the leader of our industry. So join the team, work with mission-led people, and enjoy amazing benefits. Let's see what we can accomplish together! \n \n  About the right team member \n  Mindbody is seeking a highly motivated Business Intelligence Analyst to support our team's mission of creating and maintaining a world-class data environment. Our Business Intelligence team sits at the center of everything data-related at Mindbody, and works cross-functionally with stakeholders to understand business needs, build data models that support tracking critical metrics, and provide analytical insights that drive business strategy. This opportunity is an excellent fit for a candidate that is passionate about sitting on a centralized data team, is a systems thinker, and has a strong desire to contribute to the operational and strategic initiatives at the company. This role is predominantly focused on the Mindbody business, though sits on the centralized Business Intelligence team whose responsibilities span both Mindbody and ClassPass. We are open to candidates located anywhere in the US, but with a preference for candidates located in or near our offices in New York, NY, Missoula, MT, or San Luis Obispo, CA \n  About the role \n \n Write, QA, and maintain/update the SQL logic underpinning our core data models which power all business KPI reporting at the company \n Collaborate with business stakeholders to understand their reporting needs; leverage a combination of data insights and business acumen to advocate and build business cases for strategic initiatives \n Create and maintain reports, dashboards, and data sources that effectively communicate and surface business insights for our most critical KPI's and can be easily understood by teams \n Monitor performance trends and identify areas of improvement; conduct ad hoc analysis and provide recommendations to teams for improving business processes and strategy \n Work closely with our technical engineering, product, and business teams throughout the development of new features or changes to third party business systems to anticipate and communicate data needs \n Create and manage a roadmap with primary business stakeholders to define goals, prioritize work, and communicate expected timelines and deliverables \n Develop BI best practices around our data warehouse and ETL pipelines, data architecture, documentation, and data visualization tools (e.g., Tableau, Looker) \n \n Skills and experience \n \n Bachelor's degree, quantitative field preferred \n 3+ years of experience in Business Intelligence or similar role that involves developing data models and database as a nascent or rapidly scaling business; proven ability to transform messy or unstructured data into robust, well-organized data models \n Advanced SQL skills required (PostgreSQL, Snowflake) \n Previous Financial experience preferred \n Familiarity with BI tools and platforms (Tableau, Looker, Fivetran, ETL pipelines, dbt, Excel) \n Has a strong interest in the business-side implications of work; is able to build positive working relationships with teams to \"keep up\" with what's going on and feels comfortable collaborating with stakeholders of all seniority levels to understand data needs \n Ability to present complex analysis in a clear and understandable manner to both technical and non-technical stakeholders \n Strong problem-solving skills; has a logical mindset and can identify business challenges and propose data-driven solutions \n Detail-oriented; loves finding \"edge cases\" and ensures data accuracy by troubleshooting, testing, and carefully reviewing output \n  Familiarity with road mapping or sprint cycles; is able to communicate priorities and meet deadlines \n \n Pay transparency \n  It is Mindbody's intent to pay all Team Members competitive wages and salaries that are motivational, fair and equitable. The goal of Mindbody's compensation program is to be transparent, attract potential employees, meet the needs of all current employees, and encourage Team Members to stay with our organization. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location. The base salary range for this position in the United States is  $74,100  to  $95,900 . The total compensation package for this position may also include performance bonus, stock, benefits and/or other applicable incentive compensation plans. \n \n \n  Have we piqued your curiosity? \n  Sound like the role for you? We'd love to hear from you! Even if you're not 100% sure about potential fit, we still encourage you to apply. We're looking for the right person, not the perfect series of checkboxes. \n  Mindbody is an Equal Opportunity Employer. We highly value diversity at our company and encourage people of all different backgrounds, experiences, abilities and perspectives to apply. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or other protected characteristics. \n  California Applicants Privacy Notice | UK/EU Applicants Privacy Notice", "cleaned_desc": " Develop BI best practices around our data warehouse and ETL pipelines, data architecture, documentation, and data visualization tools (e.g., Tableau, Looker) \n \n Skills and experience \n \n Bachelor's degree, quantitative field preferred \n 3+ years of experience in Business Intelligence or similar role that involves developing data models and database as a nascent or rapidly scaling business; proven ability to transform messy or unstructured data into robust, well-organized data models   Advanced SQL skills required (PostgreSQL, Snowflake) \n Previous Financial experience preferred \n Familiarity with BI tools and platforms (Tableau, Looker, Fivetran, ETL pipelines, dbt, Excel) \n Has a strong interest in the business-side implications of work; is able to build positive working relationships with teams to \"keep up\" with what's going on and feels comfortable collaborating with stakeholders of all seniority levels to understand data needs \n Ability to present complex analysis in a clear and understandable manner to both technical and non-technical stakeholders \n Strong problem-solving skills; has a logical mindset and can identify business challenges and propose data-driven solutions ", "techs": ["tableau", "looker", "postgresql", "snowflake", "fivetran", "etl pipelines", "dbt", "excel"]}, "97e2b2768b58b233": {"terms": ["data analyst"], "salary_min": 66000.0, "salary_max": 80000.0, "title": "IT Business Analyst", "company": "Techflairs", "desc": "Responsibilities: - Develops requirements for information systems from a project\u2019s inception to conclusion \n \u25cf Gather requirements from end users and document in a pre-defined format for the development team to build solutions. \n \u25cf Resource will develop and document test cases, testing plans, and procedures in an agile environment. \n \u25cf Resource will design and execute IT software tests and evaluate results to ensure compliance with applicable regulations. \n \u25cf Resources will design and prepare all needed test data. And will also review test results and evaluate for conformance to design. \n \u25cf Develop and maintain automated regression and integration test plans for validation. \n \u25cf Resources will communicate test results and feedback with development teams. \n \u25cf Perform database queries to identify and validate test data. \n \u25cf Database testing using SQL queries. \n \u25cf Assist with data setup for test automation and perform validation of test results. \n \u25cf Review data flow across multiple systems \n \u25cf Report and track defects and work closely with Developers in defect resolution. \n \u25cf Analyze the resolution of software defects to serve as input to plans for retesting. \n \u25cf Assist Test Analysts\\Engineers with the development and maintenance of the test framework. \n Education Qualification: \n \u25cf A Bachelor's Degree from an accredited college or university with a major in Computer Science, Information Systems, Engineering, Business, or other related scientific or technical discipline is required. \n General Experience: \n \u25cf 8 years of experience as an analyst/tester on software projects in supporting requirement analysis, application test script development, and execution. \n 5 years of experience in mapping the business and technical requirements into test cases and generating test scripts. \n \u25cf 5 years of practical experience in using relational databases and generating SQL scripts is required. \n \u25cf 5 years of experience in using MS Office (Word, Excel, PowerPoint, Visio) \n \u25cf Strong ability to communicate effectively with technical and non-technical teams and users. \n \u25cf Should be able to triage change requests, and incidents during all phases of design, development, testing, and training. \n \u25cf Practical knowledge in manual and automated testing, 508 compliance, performance, UAT, Unit, regression, and integration testing. \n \u25cf Experience with automated testing and automated testing tools. \n Preferred Qualifications: \n \u25cf Master's Degree from an accredited college or university with a major in Computer Science, Information Systems, Engineering, Business, or other related scientific or technical discipline. \n \u25cf Experience in testing Web application, Mobile applications in a cross-platform and browser environment. \n \u25cf Experience in testing Java, JavaScript, jQuery, AngularJS, JMeter, and GitHub is required. \n \u25cf Should be able to triage change requests, and incidents during all phases of design, development, testing, and training.Analyze and interpret complex data sets to identify trends, patterns, and insights - Develop and maintain data models and databases for efficient data storage and retrieval - Collaborate with cross-functional teams to gather requirements and define analytics objectives - Create visualizations and reports using tools such as Visio to effectively communicate findings - Provide recommendations based on data analysis to drive business decisions and improve processes - Stay up-to-date with industry trends and advancements in analytics techniques \n Requirements: - Bachelor's degree in a related field such as Computer Science or Data Analytics - Proven experience in data analysis and visualization using tools like Visio - Strong analytical and problem-solving skills with the ability to think critically - Proficiency in SQL for data extraction and manipulation - Knowledge of statistical analysis techniques and predictive modeling is a plus - Excellent communication skills to effectively present findings to stakeholders - Ability to work independently as well as collaboratively in a team environment \n Note: Experience with vaticinate software is preferred but not required. \n This is an exciting opportunity for an IT Analyst who is passionate about leveraging data to drive business outcomes. Join our team and contribute to the success of our organization through your expertise in analytics. Apply now! \n Job Types: Contract, Full-time \n Pay: $66,000.00 - $80,000.00 per year \n Experience level: \n \n 8 years \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Experience: \n \n relational databases and generating SQL scripts is required: 5 years (Required) \n knowledge in manual and automated testing, 508 compliance: 5 years (Required) \n Gather requirements from end users and document: 8 years (Required) \n \n Work Location: Remote", "cleaned_desc": " \u25cf Review data flow across multiple systems \n \u25cf Report and track defects and work closely with Developers in defect resolution. \n \u25cf Analyze the resolution of software defects to serve as input to plans for retesting. \n \u25cf Assist Test Analysts\\Engineers with the development and maintenance of the test framework. \n Education Qualification: \n \u25cf A Bachelor's Degree from an accredited college or university with a major in Computer Science, Information Systems, Engineering, Business, or other related scientific or technical discipline is required. \n General Experience: \n \u25cf 8 years of experience as an analyst/tester on software projects in supporting requirement analysis, application test script development, and execution. \n 5 years of experience in mapping the business and technical requirements into test cases and generating test scripts. \n \u25cf 5 years of practical experience in using relational databases and generating SQL scripts is required.   \u25cf 5 years of experience in using MS Office (Word, Excel, PowerPoint, Visio) \n \u25cf Strong ability to communicate effectively with technical and non-technical teams and users. \n \u25cf Should be able to triage change requests, and incidents during all phases of design, development, testing, and training. \n \u25cf Practical knowledge in manual and automated testing, 508 compliance, performance, UAT, Unit, regression, and integration testing. \n \u25cf Experience with automated testing and automated testing tools. \n Preferred Qualifications: \n \u25cf Master's Degree from an accredited college or university with a major in Computer Science, Information Systems, Engineering, Business, or other related scientific or technical discipline. \n \u25cf Experience in testing Web application, Mobile applications in a cross-platform and browser environment. \n \u25cf Experience in testing Java, JavaScript, jQuery, AngularJS, JMeter, and GitHub is required. \n \u25cf Should be able to triage change requests, and incidents during all phases of design, development, testing, and training.Analyze and interpret complex data sets to identify trends, patterns, and insights - Develop and maintain data models and databases for efficient data storage and retrieval - Collaborate with cross-functional teams to gather requirements and define analytics objectives - Create visualizations and reports using tools such as Visio to effectively communicate findings - Provide recommendations based on data analysis to drive business decisions and improve processes - Stay up-to-date with industry trends and advancements in analytics techniques   Requirements: - Bachelor's degree in a related field such as Computer Science or Data Analytics - Proven experience in data analysis and visualization using tools like Visio - Strong analytical and problem-solving skills with the ability to think critically - Proficiency in SQL for data extraction and manipulation - Knowledge of statistical analysis techniques and predictive modeling is a plus - Excellent communication skills to effectively present findings to stakeholders - Ability to work independently as well as collaboratively in a team environment \n Note: Experience with vaticinate software is preferred but not required. \n This is an exciting opportunity for an IT Analyst who is passionate about leveraging data to drive business outcomes. Join our team and contribute to the success of our organization through your expertise in analytics. Apply now! \n Job Types: Contract, Full-time \n Pay: $66,000.00 - $80,000.00 per year \n Experience level: \n \n 8 years \n \n Schedule: ", "techs": ["ms office (word", "excel", "powerpoint", "visio)", "relational databases", "sql scripts", "automated testing tools", "java", "javascript", "jquery", "angularjs", "jmeter", "github", "visio", "computer science", "data analytics", "sql", "statistical analysis techniques"]}, "000e18d9dfaceafb": {"terms": ["data analyst"], "salary_min": 55000.0, "salary_max": 65000.0, "title": "COMMERCIAL ANALYST", "company": "Kampgrounds Of America", "desc": "KOA, INC. DEI STATEMENT \n At KOA, we believe the outdoors is fun and for everyone. We are committed to having a diverse, equitable and inclusive environment where all are treated with dignity and respect. We strive to: \n \n \n intentionally create a sense of community and belonging for our guests, employees and franchise partners \n continually educate ourselves and advance our understanding about DEI \n sustain a culture that promotes diversity of thought and experiences \n ensure everyone has the ability to experience the outdoors and that our facilities are accessible to all \n drive change in our company and industry through action and implementation \n  ABOUT KAMPGROUNDS OF AMERICA, INC. \n Kampgrounds of America, Inc. (KOA) is the world\u2019s largest network of privately owned campgrounds and the leader in outdoor hospitality. KOA has 500+ locations across the United States and Canada including a mix of franchised and company-owned parks. KOA has approximately 90 employees at its corporate headquarters in Billings, Montana, and 1200+ across its location. Founded in 1962, the mission of KOA is \u201cconnecting people to the outdoors and each other,\u201d and those who represent the brand share the values of being family-oriented, passionate, entrepreneurial, customer-focused and progressive. \n REPORTS TO \n Commercial Strategy, Revenue Technology Manager \n \n  GENERAL DUTIES \n The Commercial Analyst is responsible for examining campground revenue, operations, expenses, and competition to find ways to improve the properties\u2019 financial standing. At Kampgrounds of America, Inc. we strive to ensure each campground is profitable and a leader within its market. The Commercial Analyst will supply critical analysis of rates, occupancy, pricing, margins, yield strategies, and expenses to find more ways to make each campground profitable. In addition to analysis, this role will support with building and maintaining analysis tools to ensure resources are optimized to support campground and department needs. \n SPECIFIC DUTIES \n \n \n Gather and analyze data from proprietary campground management system \n Interpret gathered data to identify patterns and trends and uncover revenue opportunities \n Produce property-level and company-wide business reporting of all relevant performance KPIs on a daily, weekly, monthly, quarterly, and yearly basis \n Submit reports to relevant departments including but not limited to operations, marketing, revenue management, and business development \n Leverage available data to assist in setting revenue goals and strategy \n Monitor demand generators and competitor strategy, reviewing pricing and availability to help predict demand in markets \n Work with revenue and technology teams to establish new data gathering and analysis techniques within the organization \n Perform any special projects needed to provide business decision makers visibility to key data \n Identify, propose, and implement potential new opportunities to improve processes and tools to drive revenue \n Ensure relevant revenue and pricing issues are investigated and analyzed \n Assist in the maintenance of any software programs which contribute to the management of revenue \n  EXPECTED RESULTS \n \n \n In collaboration with operations, marketing, revenue management, and business development, strive to meet and exceed registration revenue, camper nights and additional KPI goals for each property in your influence \n Align to and execute all commercial strategy processes \n Delivery of projects within defined budget in a timely manner as defined by manager \n Development of a collaborative and team focused style while working closely with Marketing, Revenue, and Business Development Departments \n JOB QUALIFICAITONS \n A bachelor\u2019s degree, preferably in hospitality, hotel mgmt., or a highly analytical field. MBA a plus. \n 1-3 years\u2019 experience as an analyst, data scientist, or revenue related role \n Proficiency in mathematics with the skill to translate complex mathematical information into understandable reports \n Works well independently as well as in a team environment. \n Deep and broad quantitative and analytical skills \n Highly computer literate with a strong command of Microsoft Excel \n Business Intelligence (BI) tool experience a plus, especially with Microsoft Power BI \n Professional in Business Analysis, Agile Analysis, and Business Analysis Professionals Certifications a plus \n Strong oral and written communication skills \n Report writing and presentation skills \n Has a valid Driver\u2019s License \n  PHYSICAL REQUIREMENTS \n \n \n Ability to sit for extended periods \n Able to lift 25 pounds \n Able to travel up to 30% of the time \n \n Benefits Offered: \n \n \n Medical Insurance \n Dental Insurance \n Vision Insurance \n Health Savings Account \n Flexible Spending Account \n Dependent Care Account \n Paid Life Insurance \n Paid Long Term Disability \n Voluntary Life Insurance \n Voluntary Short-Term Disability \n Voluntary Critical Illness Insurance \n Voluntary Accident Insurance \n Paid Time Off \n Paid Parental Leave \n Employee Assistance Program \n 401K Retirement Plan \n 401K Company Contributions", "cleaned_desc": "  EXPECTED RESULTS \n \n \n In collaboration with operations, marketing, revenue management, and business development, strive to meet and exceed registration revenue, camper nights and additional KPI goals for each property in your influence \n Align to and execute all commercial strategy processes \n Delivery of projects within defined budget in a timely manner as defined by manager \n Development of a collaborative and team focused style while working closely with Marketing, Revenue, and Business Development Departments \n JOB QUALIFICAITONS \n A bachelor\u2019s degree, preferably in hospitality, hotel mgmt., or a highly analytical field. MBA a plus. \n 1-3 years\u2019 experience as an analyst, data scientist, or revenue related role \n Proficiency in mathematics with the skill to translate complex mathematical information into understandable reports \n Works well independently as well as in a team environment. \n Deep and broad quantitative and analytical skills \n Highly computer literate with a strong command of Microsoft Excel \n Business Intelligence (BI) tool experience a plus, especially with Microsoft Power BI ", "techs": ["microsoft excel", "microsoft power bi"]}, "d90c563ea2e9a438": {"terms": ["data analyst"], "salary_min": 77030.85, "salary_max": 97538.23, "title": "Business Analyst", "company": "Sapio Sciences LLC", "desc": "About Sapio Sciences \n  We are a rapidly growing, award winning, SaaS (Software as a Service) informatics company servicing the Life Sciences market. We are passionate about helping our customers digitally transform their laboratory operations through data consolidation, automation and artificial intelligence. Our LIMS (Laboratory Information Management System), ELN (Electronic Laboratory Notebook) and SDMS (Scientific Data Management System) are built on a single unified platform with the goal of empowering scientists and streamlining the discovery process. \n  Founded more than 15 years ago, the Company has spearheaded a quiet revolution in the digitization of labs. We count some of the world\u2019s largest Life Science companies as our customers including Novartis, Bristol-Myers Squibb, Roche, GSK, Regeneron, PTC Therapeutics and specialist health systems such as Baylor College of Medicine Human Genome Sequencing Center, Memorial Solan Kettering Cancer Center and Earlham Inst. Following strong commercial traction in the US market and significant investment from GHO Capital we are excited to establish our dedicated European operations, head quartered in London, UK. \n   \n Our values: \n \n  Remember why we\u2019re here \n  Lead with empathy \n  Tell the truth \n  Embrace the future \n  Have a bias for action \n  Prioritize product quality \n  Be True partners \n \n  Summary \n  Sapio Science is seeking a highly motivated and self-starting Business Analyst (BA). You will join a rapidly growing team, that builds on our recent customer wins.  \n An experienced BA you will be confident working in Life Sciences to consultatively define, implement and help support our solution to the delight of customers. You will have deep knowledge of LIMS, ELN and SDMS products and their application within the Laboratory setting. \n  Key Responsibilities \n \n  Understands customer\u2019s business case, current state and implementation roadmaps  \n Interpret customer requests and translate them into functional requirements (recognize the distinction between user requests from the underlying true needs) \n  Helps clarify details, questions and formalizes use cases between customer and Solution Architects.  \n Effectively document, communicate (oral and written) and explain technical material and document of business processes and workflow (\u2018as is\u2019 and \u2018to be\u2019), \n  Provides input to Gantt chart on implementation tasks and communicates scope creep to PM \n  Identify customers key success criteria and ensure they are tracked and measured within the solution \n  Maintain a thorough understanding of the Sapio Sciences product portfolio and how it is configured \n  Be able to perform configurations of the workflow/experiment in the Sapio Platform, and/or in some cases creates wireframe mockups of the workflow/experiment \n  Perform root cause analysis, uncover critical business requirements and facilitate solution delivery \n  Expected to function independently and produce results that meet standards of quality, timeliness and acceptability with minimal supervision \n \n \n  Liaise with Product to ensure new customer driven feature requirements are captured \n  Become our customer\u2019s trusted technical partner, regularly review their deployment to ensure successful leverage our full product and service portfolio \n  Work closely with the Sapio Sciences delivery team to establish scalable processes and working methods \n  Meticulously manage all BA related deployment services documentation \n  As needed, assist in testing sprint deliverables to ensure they meet requirements \n \n  Personal Qualities  \n \n You are a self-starter who is happy to learn from first principals \n  You are highly respected and trusted by the customers you work with \n  You are a problem solver and propose improvements and solutions \n  You are focused on activities that add value and avoiding those that do not \n  You are determined and persistent \n  You maintain the highest standards of integrity and respect for co-workers, customers and prospects \n \n  Essential Experience \n \n  Several years of Life Sciences experience working within a Lab (preferably introducing new IT solutions)  \n Previous experience with LIMS or ELN software \n  Experience in Agile methodology  \n Substantial experience of effectively managing external stakeholders including strong communication skills and ability to build working relationships at all levels \n  Experience leading external client workshops, documenting business needs and outcomes \n  Proven organizational skills, with the ability to deliver agreed workload within deadlines whilst maintaining a high degree of accuracy and attention to detail \n  Strong analytical, problem solving, interviewing, data analysis and fact-finding skills \n \n  Additional Requirements \n \n  A BS/BA degree or equivalent in Life Science (e.g. molecular biology, chemistry) \n  Excellent listening and presentation skills \n  Ability to travel, < 20% of the time \n  Demonstrated success working responsibly and effectively from a home office \n  Effective time management, and prioritization skills \n  Excellent computer skills (Excel, PowerPoint, Word, drawing tools, CRM Programs) \n  Fluent in English language \n \n  The above job description is not intended to be an all-inclusive list of duties and standards of the position. Incumbents will follow any other instructions, and perform any other related duties, as assigned by their supervisor. \n  COMMITMENT TO EQUAL OPPORTUNITY   Sapio Sciences provides equal employment opportunities to all employees and applicants for employment without regard to race, colour, ancestry, national origin, gender, sexual orientation, marital status, religion, age, disability, gender identity, results of genetic testing, service in the military, or any other group protected by federal, state, or local law. \n   \n crAYU8UnVk", "cleaned_desc": "  Experience leading external client workshops, documenting business needs and outcomes \n  Proven organizational skills, with the ability to deliver agreed workload within deadlines whilst maintaining a high degree of accuracy and attention to detail \n  Strong analytical, problem solving, interviewing, data analysis and fact-finding skills \n \n  Additional Requirements \n \n  A BS/BA degree or equivalent in Life Science (e.g. molecular biology, chemistry) \n  Excellent listening and presentation skills \n  Ability to travel, < 20% of the time \n  Demonstrated success working responsibly and effectively from a home office \n  Effective time management, and prioritization skills \n  Excellent computer skills (Excel, PowerPoint, Word, drawing tools, CRM Programs) \n  Fluent in English language ", "techs": ["excel", "powerpoint", "word", "drawing tools", "crm programs"]}, "9a9f3eaba5f75459": {"terms": ["data analyst"], "salary_min": 72000.0, "salary_max": 109000.0, "title": "Business Systems Analyst - Virtual", "company": "Protective", "desc": "The work we do has an impact on millions of lives, and you can be a part of it.\n  \n \n   We help protect our customers against life\u2019s uncertainties. Regardless of where you work within the company, you\u2019ll be helping provide protection and peace of mind when our customers need it most.\n   \n \n \n \n \n  The Senior Analyst Business Systems works under the general direction of Management. It relies on extensive experience and sound judgment to plan and accomplish goals and perform a variety of complicated assignments. This position acts independently, as a senior project team member, or as a project team leader and is responsible for providing the highest level of technical guidance on the business implications of proposed technology solutions. This position guides and advises less experienced Analysts.\n  \n \n \n  The objectives of the position are to provide the most complex business consulting support to the Information Technology (IT) staff and customers, to analyze proposed changes, enhancements, and other needs relative to the assigned system(s), to certify changes made to these system(s), to develop and maintain certain system values and settings, and to influence others in IT and customer areas through teaching/sharing of specialized knowledge and/or leadership roles.\n  \n Responsibilities: \n \n  Project Management:  \n Acts as a consultant on projects that are very large, the most complex and/or deals with multiple divisions. \n  Key contributor to developing project plans, schedules and dates. \n  Fully utilizes appropriate project tracking methodologies and database(s) to manage multiple projects through the project life cycle.  \n Documents the most complex project phases, provides regular status updates and creates effective release/implementation plans. \n  Enterprise Analysis:  \n Takes a lead role on the highest level of all phases of the most complex system analysis and design. \n  Identifies business opportunities or solutions to the most complex business problems and determines the optimum project investment path for the enterprise including implementation of new business and technical system solutions.  \n Acts as a resource to define the scope and objectives of the most complex system-related projects using various analysis, research and fact-finding techniques, and evaluates the proposed new systems or enhancements for estimates and feasibility.  \n Takes the lead on challenging the status quo and recommends innovative solutions that provide the most valuable outcome the fastest and poses the least risk to the enterprise.  \n Understands the pros and cons and the corresponding impact of proposed solutions/changes for the most complex projects to the business and other systems.  \n IT Influence/Leadership:  \n Provides leadership by modeling excellence in high productivity, strong work ethic, and positive interaction with others.  \n Represents the team in interdepartmental projects and corporate initiatives; consults with the business area on the most complex system-related issues and makes recommendations for better utilizing the available technology. \n  Recognized by peers and business partners as a subject matter expert in their assigned area(s) and shares business and systems knowledge with others. \n  Quality Assurance/Testing:  \n Ensures the most complex products, goods and services satisfy customer requirements using a standard process methodology.  \n Takes the lead role in review of code, specifications, requirements, etc. to develop, document and implement repeatable test plans.  \n Proactively diagnoses problems through research, analysis, and understanding of structured languages.  \n Absorbs the project documentation to gain an in-depth understanding of the requirements and how Quality Assurance will be used for each individual project \n  Requirement Process:   \n Using a structured approach, takes a lead role in developing a comprehensive set of requirements or business models necessary to create the desired sustainable \u201cto be\u201d business environment.  \n Translates end-user requirements into workflow and procedural changes and specifications for the most complex system modifications as appropriate.  \n Ensures that requirement statements are complete, consistent, concise, traceable, unambiguous and verifiable and that they conform to defined standards. \n  Uses a variety of analysis tools to resolve the most complex business problems. \n  Application Configuration:  \n Develops, maintains, and coordinates updates to the most critical system data elements and configurable applications, including rules, values, tables, and codes. Updates might include tasks of a technical nature where programming assistance is not provided \n  Understands the most complex data structure and database schemas.  \n Creates and runs most complex queries to generate, validate, and analyze data from various applications. \n  Training:  \n Prepares end-user procedures and manuals and provides end-user training for the most complex, new or changed system functionality. \n  Motivates fellow team members through knowledge, sharing and mentoring. \n  Production Support:  \n Diagnoses and manages the most complex software defects and system anomalies to a successful resolution.  \n Works with software developers to resolve the most complex problems and provides detailed diagnostic information.  \n Consults with end-users concerning corrective actions as needed in a calm and professional manner. \n \n \n  Work Experience, Education, Certification / Training Required:  \n \n Typically, 15+ years of applicable work and/or technical experience \n  Typically, post-secondary degree, preferably in Management Information Services, Computer Science or Math related field will substitute for 4 years of work and/or technical experience \n  Certifications in industry-related or discipline-related organizations are desirable and may substitute for some work and/or technical experience. (Organization could include LOMA, SILA, American College, PMI, IIBA, IIST) \n  Recognized expertise in their assigned areas of responsibility \n \n  Knowledge, Skills and Abilities Required:  \n \n Demonstrates the most complex knowledge of applicable industry requirements and business systems and related products, requirements and processes; \n  Demonstrates the most complex understanding of the system's underlying values, tables, and codes and their impact on system processing; \n  Demonstrates most complex understanding of and the ability to apply industry standard business systems analysis tools, techniques and best practices. Able to deliver industry standard business systems analysis artifacts. \n  Demonstrates most complex understanding of and the ability to apply industry standard quality assurance tools, techniques and best practices. Able to deliver industry standard quality assurance artifacts. \n  Most complex analytical skills and problem solving skills; competent to work in all system analysis phases and to consider most complex business implications of applying technology to the current and future business environment; \n  Understands and uses appropriate requirements elicitation techniques and tools \n  Demonstrates a most complex working knowledge of systems and software development concepts, practices and procedures; \n  Ability to work effectively under very tight deadline pressure; \n  Proficiency in pertinent tools and software necessary for the position; \n  Ability to plan, schedule and execute multiple concurrent activities; \n  Clear use of the English language in written and oral communications and the ability to interpret and communicate technical information to non-technical personnel and vice versa; \n  Most complex presentation and negotiation skills and the ability to lead meetings professionally and effectively; \n  Ability to act with diplomacy in resolving conflict, consulting and in acting as an agent in charge \n  Must be self- motivated, able to work independently and willing to self-teach and take responsibility for ongoing professional development; \n \n \n   Protective\u2019s targeted salary range for this position is $72,000 to $109,000. Actual salaries may vary depending on factors, including but not limited to, job location, skills, and experience. The range listed is just one component of Protective\u2019s total compensation package for employees.\n  \n \n \n  This position also offers additional incentive opportunities through an annual incentive based on individual and Company performance.\n  \n \n  Employee Benefits: \n \n \n   We aim to protect our employees' wellbeing through a broad benefits offering. For example, we protect physical wellbeing through health, dental and vision insurance. We protect mental wellbeing through mental health benefits and an employee assistance program. We protect time away from work with a variety of paid time away benefits (\n   e.g. , paid time off, paid parental leave, short-term disability, and a cultural observance day). We protect financial wellbeing through contributions to healthcare accounts, a pension plan, and a 401(k) plan with Company matching. All employees are encouraged to protect their overall wellbeing by engaging in ProHealth Rewards, Protective\u2019s platform to improve wellbeing while earning cash rewards. \n  \n \n \n \n  Eligibility for certain benefits may vary by position in accordance with the terms of the Company\u2019s benefit plans.\n  \n \n \n  Diversity and Inclusion \n :  \n \n \n  At Protective, we are committed to providing an inclusive culture where all employees are able to fully contribute and thrive. Our goal is to grow and develop our people, attract diverse talent and support strong, diverse communities.\n  \n \n \n  We support diversity, equity, and inclusion by working to develop a culture of inclusion and belonging led by leaders who develop potential and embrace unique skills and abilities. Our aim is to create an equitable and accountable environment for all leaders and employees that will drive performance and impact business strategy. In this way, we can increase overall diversity for leadership roles and pipelines of talent by maturing our hiring practices, robust development opportunities and focus on retention of key talent.\n  \n \n \n  We are proud to be an equal opportunity employer committed to being inclusive and attracting, retaining, and growing the talents of a diverse and inclusive workforce.", "cleaned_desc": "  Understands the most complex data structure and database schemas.  \n Creates and runs most complex queries to generate, validate, and analyze data from various applications. \n  Training:  \n Prepares end-user procedures and manuals and provides end-user training for the most complex, new or changed system functionality. \n  Motivates fellow team members through knowledge, sharing and mentoring. \n  Production Support:  \n Diagnoses and manages the most complex software defects and system anomalies to a successful resolution.  \n Works with software developers to resolve the most complex problems and provides detailed diagnostic information.  \n Consults with end-users concerning corrective actions as needed in a calm and professional manner. \n \n \n  Work Experience, Education, Certification / Training Required:  \n \n Typically, 15+ years of applicable work and/or technical experience \n  Typically, post-secondary degree, preferably in Management Information Services, Computer Science or Math related field will substitute for 4 years of work and/or technical experience \n  Certifications in industry-related or discipline-related organizations are desirable and may substitute for some work and/or technical experience. (Organization could include LOMA, SILA, American College, PMI, IIBA, IIST) \n  Recognized expertise in their assigned areas of responsibility \n \n  Knowledge, Skills and Abilities Required:  \n \n Demonstrates the most complex knowledge of applicable industry requirements and business systems and related products, requirements and processes; \n  Demonstrates the most complex understanding of the system's underlying values, tables, and codes and their impact on system processing; ", "techs": ["database schemas", "queries", "end-user procedures", "end-user training", "software defects", "system anomalies", "diagnostic information", "work experience", "education", "certifications", "industry-related organizations", "expertise", "industry requirements", "business systems", "tables", "codes", "system processing"]}, "56677b876992bbd4": {"terms": ["data analyst"], "salary_min": 57858.6, "salary_max": 73261.88, "title": "(Remote) Business Analyst - Information Technology Services", "company": "Careers | West Virginia University", "desc": "The department of Information Technology Services at West Virginia University is currently accepting applications for a Business Analyst. \n  This position allows for flexibility in work location, including remote work/work from home options. Travel to the WVU Campus may need to occur based on organizational needs throughout the year. \n  About the Opportunity \n  The Business Analyst (BA) reports to the Director of Systems of Engagement (SOE) and will serve as a liaison between the stakeholders and ITS. The BA's responsibilities encompass furnishing business process solutions and technical assistance to fulfill departmental and institutional requisites. Collaborating with domain experts, the BA will extract and define business requirements, tackle usability issues, and record solutions. Additionally, they will aid team members across the project and software development lifecycle. Their contribution will extend to documenting both business requirements and holistic solutions for multiple projects. \n  We strongly believe in work-life balance and keeping time for things we love outside our work. WVU offers generous benefits, including: \n \n 13 paid holidays (staff holiday calendar) \n 24 annual leave (vacation) days per year \n 18 sick days per year, and the flexibility to use that time to care for immediate family members \n WVU offers a range of health insurance and other benefits \n 401(a) retirement savings with 6% employee contribution match, eligibility to continue health insurance, and other retiree perks. Looking for more retirement benefits information? Check out retirement health insurance benefits, retirement income, and FAQ\u2019s. \n Wellness programs \n \n What You'll Do \n  Application/Technical Assignments \n \n Create epics, user stories, and process flow documentation to facilitate the design and development of application solutions. \n Apply industry best practices for business analysis (BABOK) to enhance, support, and refine BA processes and artifacts within the team. \n Offer documentation assistance for diverse tasks including requirements gathering, process analysis, gap analysis, new feature design, and product testing, contributing to the success of ITS projects. \n Adhere to established methodologies, ensuring timely completion of project deliverables within set deadlines. \n Provide support for CRM \u2013 Salesforce, WVU Portal and third-party tools. \n Apply analytical skills to identify and resolve issues, constantly researching and adopting evolving methodologies to stay aligned with technological advancements. \n Collaborate efficiently with both technical and functional teams, fostering the exchange of knowledge and ideas to achieve project objectives. \n Produce technical documentation, encompassing the interpretation of business requirements, process flows, and code, following established standards. \n Communicate information clearly and promptly to team members and stakeholders, exerting the necessary efforts for project accomplishment. \n Dedicate time and effort to ongoing learning, staying current with the evolving technology and application landscape. \n \n General Business \n \n Build positive and professional relationships with functional users and staff, establishing effective communication channels with project and team members and addressing concerns appropriately. \n Demonstrate comprehension of and adherence to established project management protocols, encompassing tasks such as adhering to project work plans, delivering status reports, and maintaining documentation. \n Collaborate effectively with peers to achieve project deliverables, contributing through knowledge sharing and idea exchange. Convey information clearly, accurately, and promptly to project team members, dedicating the necessary efforts for task completion. \n Uphold WVU's confidentiality and security protocols, ensuring compliance with application requirements. \n Recognize and actively work toward achieving WVU's strategic and operational objectives. Take the initiative to devise solutions for perceived gaps or deficiencies in organizational processes. \n Communicate in a concise and clear manner, both in written and oral communication. Cultivate an environment of professionalism and teamwork. \n Perform additional duties as directed by the supervisor. \n \n \n \n \n \n  Qualifications \n \n \n \n Bachelor\u2019s degree in Computer Science, Computer Engineering, Information Systems or equivalent combination of relevant education, certifications, and work experience. \n A minimum of four (4) years of experience involving: \n     \n Experience with defining problems, collecting data, gathering requirements, and establishing structure and analysis for complex business issues. \n Experience in writing technical design/solution documents, and business process and improvement. \n \n Excellent organizational skills \n Proficiency in conveying ideas to both technical and non-technical individuals. \n Skill in leading discussions among multiple subject matter experts to extract their insights and achieve consensus. \n Proficiency in communication (both oral and written), problem-solving, and process analysis. Sound understanding of IT processes and system workflows. Comfortable with presenting and communicating to audiences with technical and business backgrounds. \n Aptitude for analytical thinking and process assessment. \n Proven capability to collaborate effectively within teams of varying skill sets, adeptly managing numerous tasks and priorities as required. \n Comprehensive knowledge of the Software Development Life Cycle and agile/scrum development methodology \n Familiarity with PL/SQL and SQL. \n Extensive familiarity with business application development methodologies. \n Comprehension of logical and physical database design and modeling. \n Adaptable and quick to acclimate evolving technical landscapes. \n Proficient use of Excel, PowerPoint, Word, and Visio software. \n Understanding of industry best practices within the higher education industry. \n Ability to devise holistic solutions that consider consequences and provide feasible resolutions. \n \n \n \n \n  About WVU \n \n  At West Virginia University, we leverage our talents and resources to create a better future for our state and the world. As West Virginia's land-grant university, WVU has three campuses that touch each corner of the state. The WVU System includes 518 buildings on 15,880 acres, Extension Service offices in all 55 counties, ten experimental farms and four forests.    From the groundbreaking R1 research of our flagship campus in Morgantown to the career-oriented programs of WVU Potomac State in Keyser to the technology-intensive programs at WVU Tech in Beckley \u2014 the contributions of WVU employees directly impact the 1.8 million people of West Virginia every day, no matter their role or position. \n  Service, curiosity, respect, accountability, and appreciation are the core values that unite Mountaineers, inspiring one another to work tirelessly and support others as they seek to reach new heights. After all, when you're a Mountaineer, impossible is just another mountain to climb.    Creating an inclusive, engaged, and dynamic learning environment is core to WVU\u2019s academic mission. We welcome candidates who can contribute a range of ideas, approaches and experiences. \n  To learn more about West Virginia University, visit go.wvu.edu. View current career opportunities at careers.wvu.edu. \n  West Virginia University is proud to be an Equal Opportunity employer and is the recipient of an NSF ADVANCE award for gender equity. The University values diversity among its faculty, staff, and students, and invites applications from all qualified applicants regardless of race, ethnicity, color, religion, gender identity, sexual orientation, age, nationality, genetics, disability, or Veteran status. \n \n \n \n \n \n  Job Posting \n :  Sep 8, 2023\n  \n \n  Posting Classification \n :  Non-Classified\n  \n \n  Exemption Status \n :  Exempt\n  \n \n  Benefits Eligible \n :  Yes\n  \n \n  Schedule \n :  Full-time", "cleaned_desc": " Adhere to established methodologies, ensuring timely completion of project deliverables within set deadlines. \n Provide support for CRM \u2013 Salesforce, WVU Portal and third-party tools. \n Apply analytical skills to identify and resolve issues, constantly researching and adopting evolving methodologies to stay aligned with technological advancements. \n Collaborate efficiently with both technical and functional teams, fostering the exchange of knowledge and ideas to achieve project objectives. \n Produce technical documentation, encompassing the interpretation of business requirements, process flows, and code, following established standards. \n Communicate information clearly and promptly to team members and stakeholders, exerting the necessary efforts for project accomplishment. \n Dedicate time and effort to ongoing learning, staying current with the evolving technology and application landscape. \n \n General Business \n \n Build positive and professional relationships with functional users and staff, establishing effective communication channels with project and team members and addressing concerns appropriately. \n Demonstrate comprehension of and adherence to established project management protocols, encompassing tasks such as adhering to project work plans, delivering status reports, and maintaining documentation. \n Collaborate effectively with peers to achieve project deliverables, contributing through knowledge sharing and idea exchange. Convey information clearly, accurately, and promptly to project team members, dedicating the necessary efforts for task completion. \n Uphold WVU's confidentiality and security protocols, ensuring compliance with application requirements. \n Recognize and actively work toward achieving WVU's strategic and operational objectives. Take the initiative to devise solutions for perceived gaps or deficiencies in organizational processes. \n Communicate in a concise and clear manner, both in written and oral communication. Cultivate an environment of professionalism and teamwork. \n Perform additional duties as directed by the supervisor. \n \n   \n \n \n  Qualifications \n \n \n \n Bachelor\u2019s degree in Computer Science, Computer Engineering, Information Systems or equivalent combination of relevant education, certifications, and work experience. \n A minimum of four (4) years of experience involving: \n     \n Experience with defining problems, collecting data, gathering requirements, and establishing structure and analysis for complex business issues. \n Experience in writing technical design/solution documents, and business process and improvement. \n \n Excellent organizational skills \n Proficiency in conveying ideas to both technical and non-technical individuals. \n Skill in leading discussions among multiple subject matter experts to extract their insights and achieve consensus. \n Proficiency in communication (both oral and written), problem-solving, and process analysis. Sound understanding of IT processes and system workflows. Comfortable with presenting and communicating to audiences with technical and business backgrounds. \n Aptitude for analytical thinking and process assessment. \n Proven capability to collaborate effectively within teams of varying skill sets, adeptly managing numerous tasks and priorities as required. ", "techs": ["salesforce", "wvu portal", "technical documentation", "project management protocols", "confidentiality and security protocols", "computer science", "computer engineering", "information systems", "organizational skills", "conveying ideas", "leading discussions", "communication (oral and written)", "problem-solving", "process analysis", "it processes and system workflows", "analytical thinking", "collaboration within teams."]}, "9d90650008a0dc39": {"terms": ["data analyst"], "salary_min": null, "salary_max": null, "title": "Digital Marketing Paid Social Media Analyst (Remote)", "company": "Lands' End", "desc": "Lands' End is looking for the best and the brightest to join our rapidly growing Digital Marketing team. We're proud of being in the US Internet Retailers Top 50, but we're even prouder of the people behind this accomplishment. That's where you come in! \n  The ideal candidate will have a 3-7+ year background in performance marketing social media including program development, planning, buying, technical implementation, optimizations, and campaign stewardship. They have deep paid advertising experience across all major social platforms, including; Facebook, Instagram, Pinterest, Snapchat, YouTube, and LinkedIn. They are curious and stay up to date on platform updates, beta tests, and upcoming new platform releases. \n  Responsibilities \n \n Effectively develop ad campaigns, audiences, and coordinate creative across all Digital Marketing managers within advertising platforms and advertising management resources. \n Coordinate administrative deliverables and support for the Paid Social & Media team. \n Optimize campaigns, including ensuring budgets are pacing effectively, and tactics are being optimized towards relevant targets and KPIs. \n Recommend bid, budget, targeting, and optimization adjustments to reach assigned KPIs and metrics. \n Manage time efficiently across multiple workstreams and collaborate with internal teams (Creative, Content, Merchants, and IT) and external partners. \n Provide cohesive and actionable campaign performance analysis. \n Exceptional experience with all social platforms and deep experience with Facebook and Instagram. \n Possess and attains expertise in Meta, YouTube, Pinterest, and emerging social channels with a focus on direct response optimization for advertising. \n Proficiently handle technical aspects of paid social advertising, programmatic advertising, optimization, and reporting for various platforms. \n Set up proper account structures and tracking implementation. \n Understand IAB creative specs and advise on best practices. \n Collaborate across internal teams, including digital merchandising, analytics, and creative, to create holistic advertiser results that understand the customer journey. In addition, collaborate with platform partners to continuously scope and research new tests in social. \n Manage partner communication and deliverables including IOs, creative, billing, and performance. \n Ability to communicate effectively, to recognize, understand, and manage one\u2019s own emotions as well as others, and foster positive working relationships across all levels of the organization. \n Holding oneself responsible and being self-driven in accomplishing business goals, adhering to policies, and being responsible for one\u2019s own actions, performance, and decisions. \n \n Requirements \n \n Bachelor\u2019s Degree in Business, Marketing, Economics, Mathematics, or eCommerce. \n 3-5 years of experience in Digital Marketing, including Paid Social, Programmatic Display, Affiliate Marketing, or related areas. \n Experience in product feed-based advertising management and implementation \n Experience of managing paid social campaigns with conversion-focused objectives. \n Excellent data analysis and reporting skills with a grasp of numbers. \n Experience with pulling reporting metrics and providing topline observations and campaign implications to internal teams and senior stakeholders. \n Excellent organization and ability to collaborate with multiple teams. \n Excellent verbal and written communication skills are required for communicating with internal and external parties in a manner that is both articulate and professional. \n Proficiency in Microsoft Office Suite, especially PowerPoint and Excel. \n Innate curiosity and passion for paid social and programmatic advertising. \n Understanding of paid social and auction-based ad technology platforms such as Smartly, Kenshoo, Marin, and Sprinklr. \n \n \n A plus: \n \n Experience working within an agency setting managing multiple clients. \n Experience utilizing ad management software like Smartly.io preferred. \n Experience utilizing Tableau or other BI software to analyze data preferred. \n \n \n \n   \n \n Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities \n  The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor\u2019s legal duty to furnish information. 41 CFR 60-1.35(c)", "cleaned_desc": " Experience with pulling reporting metrics and providing topline observations and campaign implications to internal teams and senior stakeholders. \n Excellent organization and ability to collaborate with multiple teams. \n Excellent verbal and written communication skills are required for communicating with internal and external parties in a manner that is both articulate and professional. \n Proficiency in Microsoft Office Suite, especially PowerPoint and Excel. \n Innate curiosity and passion for paid social and programmatic advertising. \n Understanding of paid social and auction-based ad technology platforms such as Smartly, Kenshoo, Marin, and Sprinklr. \n \n \n A plus:   \n Experience working within an agency setting managing multiple clients. \n Experience utilizing ad management software like Smartly.io preferred. \n Experience utilizing Tableau or other BI software to analyze data preferred. \n \n \n \n   \n ", "techs": ["smartly", "kenshoo", "marin", "sprinklr", "smartly.io", "tableau"]}, "92b9962c9bd7aad9": {"terms": ["data analyst"], "salary_min": 74547.2, "salary_max": -1.0, "title": "Sr. Analyst - Business Analytics", "company": "Ascension", "desc": "Details  \n \n Department:  IT Operations & Governance - Value Management \n  Schedule:  Full time, 8 hour day shift, Monday - Friday \n  Location:  Remote \n \n  Benefits  \n \n  Paid time off (PTO)\n    Various health insurance options & wellness plans\n    Retirement benefits including employer match plans\n    Long-term & short-term disability\n    Employee assistance programs (EAP)\n    Parental leave & adoption assistance\n    Tuition reimbursement\n    Ways to give back to your community\n  \n \n \n  As a military friendly organization, Ascension promotes career flexibility and offers many benefits to help support the well-being of our military families, spouses, veterans and reservists. Our associates are empowered to apply their military experience and unique perspective to their civilian career with Ascension.\n  \n \n \n   Please note, benefits and benefits eligibility can vary by position, exclusions may apply for some roles (for example: PRN, Short-Term Option, etc.). Connect with your Talent Advisor today for additional specifics. \n \n \n  Responsibilities  \n \n Position Description : As a Senior Analyst in Business Analytics, this individual will support Technology Business Management program initiatives and in-system reporting development including Apptio, ServiceNow, and Tableau.\n  \n \n \n  Responsibilities :\n  \n \n  Support all aspects of the Technology Business Management (TBM) software solution (Apptio) to include monthly updates and validation of financial and operational data, ongoing system maintenance, and periodic platform upgrades. \n  Support the buildout and management of the TBM taxonomy in Apptio, ingestion of vendor utilization metrics and price tables to provide actionable insights to manage 3rd party costs, support cost reduction initiatives, and create cost transparency. \n  Provide the organization's information systems leaders with clearly defined reporting and metrics for use in decision making and managing the IT business. \n  Perform exploratory data analysis to assist in identifying opportunities that drive more effective and efficient operations throughout the organization's information systems. \n  Identify and resolve data gaps to ensure that TBM information can be relied upon by decision makers. \n  Build partnerships with AT Leadership / Extended Leadership to become the \"go-to\" group for development needs \n  Analyze chain of events and apply technical knowledge following established procedures. \n  Successfully troubleshoot most applications problems independently. \n  Participate in design, contributing technical insights and ideas. Help formulate project scope and objectives. \n  Work with customers to understand and explain business and technical issues, respond to user problems, explain new technologies, and present deliverables. \n  Write basic documentation of a new or proposed system. Share knowledge effectively within the work team. Contributes to project plans, RFP's and RFI's. \n \n \n \n  Work Experience Requirements :\n  \n \n  Experience with the Technology Business Management taxonomy & Apptio is preferred \n  3+ years experience preferred \n  Excellent communication skills (both verbal and written) \n  Demonstrated presentation skills and audience engagement \n  Ability to influence and communicate effectively with stakeholders \n  Drives continuous improvement \n  Efficient with Apptio, ServiceNow, Tableau, and Google Sheets \n  Must be comfortable with changing and competing priorities and be able to deliver across multiple projects and demands \n  Highly disciplined, organized, self-motivated, and delivery-focused \n \n  Requirements  \n \n  Education:\n  \n \n  High School diploma equivalency with 2 years of cumulative experience OR Associate's degree/Bachelor's degree OR 4 years of applicable cumulative job specific experience required. \n \n  Additional Preferences  \n \n  No additional preferences.\n  \n \n \n  #ascensiontechnologies\n  \n \n   #LI-Remote\n  \n  Why Join Our Team  \n \n  When you join Ascension, you join a team of over 150,000 individuals across the country committed to a Mission of serving others and providing compassionate, personalized care to all. Our inclusive culture, continuing education programs, career coaches and benefit offerings are just a few of the resources and tools that team members can use to create a rewarding career path. In fact, Ascension spent nearly $46 million in tuition assistance alone to support associate growth and development. If you are looking for a career where you can grow and make a difference in your community, we invite you to join our team today.\n  \n  Equal Employment Opportunity Employer  \n \n  Ascension will provide equal employment opportunities (EEO) to all associates and applicants for employment regardless of race, color, religion, national origin, citizenship, gender, sexual orientation, gender identification or expression, age, disability, marital status, amnesty, genetic information, carrier status or any other legally protected status or status as a covered veteran in accordance with applicable federal, state and local laws.\n  \n \n \n  For further information, view the EEO Know Your Rights (English) poster or EEO Know Your Rights (Spanish) poster.\n  \n \n \n  Pay Non-Discrimination Notice\n  \n \n \n  Please note that Ascension will make an offer of employment only to individuals who have applied for a position using our official application. Be on alert for possible fraudulent offers of employment. Ascension will not solicit money or banking information from applicants.\n  \n  E-Verify Statement  \n \n  This employer participates in the Electronic Employment Verification Program. Please click the E-Verify link below for more information.\n   \n  E-Verify", "cleaned_desc": "  Build partnerships with AT Leadership / Extended Leadership to become the \"go-to\" group for development needs \n  Analyze chain of events and apply technical knowledge following established procedures. \n  Successfully troubleshoot most applications problems independently. \n  Participate in design, contributing technical insights and ideas. Help formulate project scope and objectives. \n  Work with customers to understand and explain business and technical issues, respond to user problems, explain new technologies, and present deliverables. \n  Write basic documentation of a new or proposed system. Share knowledge effectively within the work team. Contributes to project plans, RFP's and RFI's. \n \n \n \n  Work Experience Requirements :\n  \n \n  Experience with the Technology Business Management taxonomy & Apptio is preferred \n  3+ years experience preferred \n  Excellent communication skills (both verbal and written) \n  Demonstrated presentation skills and audience engagement \n  Ability to influence and communicate effectively with stakeholders \n  Drives continuous improvement \n  Efficient with Apptio, ServiceNow, Tableau, and Google Sheets \n  Must be comfortable with changing and competing priorities and be able to deliver across multiple projects and demands ", "techs": ["at leadership", "extended leadership", "apptio", "technology business management taxonomy", "chain of events analysis", "troubleshooting", "design participation", "project scope formulation", "customer interaction", "documentation writing", "knowledge sharing", "project plans", "rfp's", "rfi's", "technology business management taxonomy", "apptio", "communication skills", "presentation skills", "audience engagement", "stakeholder communication", "continuous improvement", "apptio", "servicenow", "tableau", "google sheets."]}, "4409d87fbc12ff56": {"terms": ["data analyst"], "salary_min": 90.0, "salary_max": 95.0, "title": "Oracle ERP Business Analyst ( Techno functional Business analyst)", "company": "Infipact", "desc": "Techno-functional Business Analyst \n Location:  Bay Area (Condition is to work on-site at an office of the client in the Bay area) \n Contract Length:  +6 months (extendable) \n Skills: \n \n Oracle ERP - Order Management experience \n Service contracts background \n Oracle EBS experience \n \n Job Types: Full-time, Contract \n Pay: $90.00 - $95.00 per hour \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Experience: \n \n Oracle ERP: 5 years (Required) \n Oracle EBS: 4 years (Required) \n Service contracts background: 5 years (Required) \n Business analyst: 10 years (Required) \n \n Work Location: In person", "cleaned_desc": "", "techs": ""}, "50d181c4cc9bde7c": {"terms": ["data analyst"], "salary_min": 64613.074, "salary_max": 81814.55, "title": "SaaS Business Analyst", "company": "Land Intelligence Inc", "desc": "Who We Are \n  Land Intelligence is a software technology company serving the commercial real estate industry. We focus on Land Development. We have been recognized as an industry technology leader in providing solutions on a national scale. Our team are visionaries that see a better, faster, and more valuable way to research, finance, and trade land.\n  \n \n Our Culture \n  We are entrepreneurs first. Which means we manage the people, processes, and products. We create new ways of doing things to drive value. We are builders and growth minded. Our leadership team has been recognized as a Best Places to work in the industry nationally. Our team drives for personal and professional development, as personal growth is instrumental to our success. Your learning will be supported by specialized in-house training programs and mentoring by the industry\u2019s leading experts, many of whom are our investors and strategic partners.\n  \n  We are a startup, but this isn't our first time doing this. As a result, you can get the thrill of working at a startup, with the resources of a publicly traded company. We offer a best-in-class benefits package, as we are a Professional Employment Organization (PEO) with our partner Insperity that includes medical, vision, dental and life insurance. Our 401 (k) program offers an employer match, along with a 401(k)-profit sharing and performance-based bonuses and generous paid time off.\n  \n \n Your Impact at Land Intelligence \n  We are seeking a highly motivated, organized, and innovative Business Analyst to join our growing product development team. We rely on this position to bridge the gap between business needs and technology solutions relevant to our flagship LandSUITE\u00ae platform. This role \n  Responsibilities: \n \n  Collaborate with the software development and IT teams to understand the needs for system support in the development process. \n  Develop and implement strategies for continuous deployment to enhance the production environment. \n  Automate and streamline operations and processes to enhance speed and efficiency. \n  Design and implement tools for automated deployment and monitoring of multiple environments. \n  Troubleshoot and resolve issues in development, test, and production environments. \n  Manage the tools necessary to ensure automated testing and continuous integration. \n  Document processes and monitor system performance metrics. \n  Work with the software development and IT teams to identify and implement new technologies and update existing infrastructure. \n \n  Qualifications: \n \n  Bachelor's degree in Business Administration, Computer Science, or related field. A Master's degree is a plus. \n  Proven experience as a Business Analyst, preferably in the SaaS industry. \n  Strong analytical skills with the ability to translate complex concepts into clear and concise requirements. \n  Excellent written and verbal communication skills to effectively collaborate with cross-functional teams and stakeholders at all levels. \n  Proficient in data analysis and reporting tools, such as Excel, Power BI, or Tableau. \n  Familiarity with Agile methodologies and experience working in an Agile development environment. \n  Detail-oriented with exceptional organizational and time management skills. \n  Ability to thrive in a fast-paced, dynamic environment and manage multiple projects simultaneously. \n  Strong problem-solving skills and the ability to think creatively to overcome challenges. \n  Knowledge of software development life cycle (SDLC) and familiarity with SaaS architecture principles. \n \n  Land Intelligence is an EOE/Affirmative Action Employer M/F/D/V. If you are interested in applying for employment and need special assistance to apply for a posted position, please send an e-mail to careers@landintelligence.net. Land Intelligence also participates in the federal E-Verify Program.", "cleaned_desc": "  Work with the software development and IT teams to identify and implement new technologies and update existing infrastructure. \n \n  Qualifications: \n \n  Bachelor's degree in Business Administration, Computer Science, or related field. A Master's degree is a plus. \n  Proven experience as a Business Analyst, preferably in the SaaS industry. \n  Strong analytical skills with the ability to translate complex concepts into clear and concise requirements.    Excellent written and verbal communication skills to effectively collaborate with cross-functional teams and stakeholders at all levels. \n  Proficient in data analysis and reporting tools, such as Excel, Power BI, or Tableau. \n  Familiarity with Agile methodologies and experience working in an Agile development environment. \n  Detail-oriented with exceptional organizational and time management skills. \n  Ability to thrive in a fast-paced, dynamic environment and manage multiple projects simultaneously. \n  Strong problem-solving skills and the ability to think creatively to overcome challenges. \n  Knowledge of software development life cycle (SDLC) and familiarity with SaaS architecture principles. ", "techs": ["excel", "power bi", "tableau", "agile methodologies", "saas architecture principles"]}, "cc599309fa105702": {"terms": ["data analyst"], "salary_min": 91523.195, "salary_max": 115888.76, "title": "Tableau Analyst", "company": "Keeper Security, Inc.", "desc": "We are currently seeking a skilled Tableau Analyst to join our team at Keeper Security. As our Tableau Analyst, you will be responsible for creating and delivering data insights that will be used by our team to drive data-informed decisions. You will be working directly with our Director of Engineering and Data Engineering team to help develop and maintain dashboards, data models, and reporting tools. \n  About Keeper Security \n  Keeper Security is transforming cybersecurity for people and organizations around the world. Keeper\u2019s affordable and easy-to-use solutions are built on a foundation of zero-trust and zero-knowledge security to protect every user on every device. Our next-generation privileged access management solution deploys in minutes and seamlessly integrates with any tech stack to prevent breaches, reduce help desk costs and ensure compliance. Trusted by millions of individuals and thousands of organizations, Keeper is the leader for best-in-class password management, secrets management, privileged access, secure remote access and encrypted messaging. Learn more at KeeperSecurity.com. \n  Responsibilities \n \n Develop high-quality business dashboards and reports using Tableau Desktop. \n Collaborate with stakeholders across the organization to understand what data insights are needed to help drive better business decisions. \n Design, develop, and maintain data models and solutions in Tableau. \n Analyze and interpret data, producing clear and actionable insights. \n Review existing codebase, optimize queries, and make recommendations for improving performance. \n Identify trends and patterns within data and provide recommendations for potential business opportunities to internal teams. \n Work with colleagues to ensure efficient management of data sources and reporting processes. \n \n Requirements \n \n 5+ years experience using Tableau Desktop for business intelligence or related field. \n Experience in data analysis, data manipulation, and data visualization. \n Proficient in SQL, Excel and other data-driven tools. \n Knowledge of data mining techniques and tools. \n Ability to write clean, maintainable, and efficient code. \n Strong analytical and problem-solving skills. \n Proven ability to work cooperatively with cross-functional teams. \n Bachelor's Degree in Computer Science, Mathematics or related field. \n \n Benefits \n \n Medical, Dental & Vision (Inclusive of domestic partnerships) \n Employer Paid Life Insurance & Employee/Spouse/Child Supplemental life \n Voluntary Short/Long Term Disability Insurance \n 401k (Roth/Traditional) \n A generous PTO plan that celebrates your commitment and seniority (including paid Bereavement/Jury Duty, etc) \n Above market annual bonuses \n \n Keeper Security, Inc. is an equal opportunity employer and participant in the U.S. Federal E-Verify program. We celebrate diversity and are committed to creating an inclusive environment for all employees.  Classification: Exempt", "cleaned_desc": " Collaborate with stakeholders across the organization to understand what data insights are needed to help drive better business decisions. \n Design, develop, and maintain data models and solutions in Tableau. \n Analyze and interpret data, producing clear and actionable insights. \n Review existing codebase, optimize queries, and make recommendations for improving performance. \n Identify trends and patterns within data and provide recommendations for potential business opportunities to internal teams. \n Work with colleagues to ensure efficient management of data sources and reporting processes.   \n Requirements \n \n 5+ years experience using Tableau Desktop for business intelligence or related field. \n Experience in data analysis, data manipulation, and data visualization. \n Proficient in SQL, Excel and other data-driven tools.   Knowledge of data mining techniques and tools. \n Ability to write clean, maintainable, and efficient code. \n Strong analytical and problem-solving skills. \n Proven ability to work cooperatively with cross-functional teams. \n Bachelor's Degree in Computer Science, Mathematics or related field. \n ", "techs": ["tableau", "sql", "excel"]}, "63cf2395f62b1b1d": {"terms": ["data analyst"], "salary_min": null, "salary_max": null, "title": "Financial Analyst - Actuarial", "company": "Nationwide", "desc": "As a team member in the Finance and Internal Audit department at Nationwide, the opportunities are endless! You can grow and learn in diverse areas across many disciplines such as Advanced Analytics, Investments, Actuarial, Accounting, Risk Management, Critical Business Advisor and so much more. Let Nationwide help create your career journey!\n  \n \n   This role is within the In-Force Management team within Actuarial. Strong technical skills be used to assist implementation of new products and maintenance of the in-force block of business to partner with IT, other actuarial teams, and the business.\n  \n \n \n   This role will be supporting the life insurance products. Experience with Excel, VBA, and SQL are highly preferred.\n  \n \n \n   This role will be staffed at the E band.\n  \n \n \n   #LI-SM1\n  \n \n   #LI-remote\n  \n \n \n   Job Description Summary\n   Our Nationwide Financial Actuarial team members have opportunities to use state-of-the-art tools, are encouraged to innovate and learn to master actuarial methodologies. If this sounds like a place you could thrive, then we want to know more about you!\n  \n  As an Analyst you will be expected to learn about the insurance and finance industry, products Nationwide provides and a variety of actuarial software. You will be trained in company and business operations while performing mathematical and quantitative actuarial analyses that are essential to the success of Nationwide and the protection we provide our customers.\n  \n \n   Job Description\n  \n \n   Key Responsibilities:\n  \n \n \n \n     Compiles, categorizes and analyzes data, with direction and oversight.\n    \n \n \n     Assists in building testing tools, analyzing product calculations and product illustrations.\n    \n \n \n     Aids in building, maintaining and running actuarial, projection and valuation models.\n    \n \n \n     Completes financial reporting and analysis with direct oversite.\n    \n \n \n     Responsible for pricing and implementation of products with input and oversite.\n    \n \n \n     Assists in valuation/reserving under multiple accounting regimes (statutory, GAAP and tax) for products.\n    \n \n \n     Assists with analysis of reinsurance opportunities and risks for Nationwide Financial business products.\n    \n \n \n     Assists in activities related to a specialized phase of data and analytics which may include data governance, data strategy development, data modeling, data requirements, data manipulation, and/or requirements elicitation when needed.\n    \n \n \n     Assists in the analysis and validation of moderately sophisticated business processes and in the development and implementation of new programs and/or modifications of existing applications when required. May assist in the design, prototyping, coding and other methodologies for new and/or existing applications including development of test plans.\n    \n \n \n   May perform other responsibilities as assigned.\n  \n \n \n   Reporting Relationships: Reports to Financial Director, Actuarial, or other financial/actuarial manager.\n  \n \n \n   Typical Skills and Experiences:\n  \n \n  Education: Undergraduate degree in actuarial science, mathematics, statistics, CIS or similar background requiring modeling, programming or quantitative analysis.\n  \n \n \n   License/Certification/Designation: May have passed an actuarial exam, LOMA or other industry exams.\n  \n \n \n   Experience: No finance or actuarial experience needed. Typical candidate may have internship, associate teaching or tutoring experience.\n  \n \n \n   Knowledge, Abilities and Skills: Basic understanding of Microsoft Office products (including VBA within Access and Excel) programming languages, and likely actuarial models. Basic level of perspective, customer focus, dealing with ambiguity, drive for results, business perspective and analysis. Proficient level of problem solving, learning on the fly, and making the best use of system capabilities.\n  \n \n \n   Other criteria, including leadership skills, competencies and experiences may take precedence.\n  \n \n  Staffing exceptions to the above must be approved by Chief Actuary and Human Resources Consultant/Manager.\n  \n \n \n   Values: Regularly and consistently demonstrates the Nationwide Values.\n  \n \n \n   Job Conditions:\n  \n \n  Overtime Eligibility: Exempt (Not eligible)\n  \n \n \n   Working Conditions: Normal office environment.\n  \n \n \n   ADA: The above statements cover what are generally believed to be principal and essential functions of this job. Specific circumstances may allow or require some people assigned to the job to perform a somewhat different combination of duties.\n  \n \n \n   Benefits\n  \n \n   We have an array of benefits to fit your needs, including: medical/dental/vision, life insurance, short and long term disability coverage, paid time off with newly hired associates receiving a minimum of 18 days paid time off each full calendar year pro-rated quarterly based on hire date, nine paid holidays, 8 hours of Lifetime paid time off, 8 hours of Unity Day paid time off, 401(k) with company match, company-paid pension plan, business casual attire, and more. To learn more about the benefits we offer, \n   \n   click here\n   .\n  \n \n \n   Nationwide is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive culture where everyone feels challenged, appreciated, respected and engaged. Nationwide prohibits discrimination and harassment and affords equal employment opportunities to employees and applicants without regard to any characteristic (or classification) protected by applicable law.\n  \n  #actuarial\n   Smoke-Free Iowa Statement: Nationwide Mutual Insurance Company, its affiliates and subsidiaries comply with the Iowa Smokefree Air Act. Smoking is prohibited in all enclosed areas on or around company premises as well as company issued vehicles. The company offers designated smoking areas in which smoking is permitted at each individual location. The Act prohibits retaliation for reporting complaints or violations. For more information on the Iowa Smokefree Air Act, individuals may contact the Smokefree Air Act Helpline at 888-944-2247.\n  \n  For NY residents please review the following state law information: Notice of Employee Rights, Protections, and Obligations LS740 (ny.gov)\n   Nationwide pays on a geographic-specific salary structure and placement within the actual starting salary range for this position will be determined by a number of factors including the skills, education, training, credentials and experience of the candidate; the scope, complexity and location of the role as well as the cost of labor in the market; and other conditions of employment. If a Sales job, Sales Incentives, based on performance goals are possible in addition to this range.\n   The national salary range for Analyst, Financial - Actuarial : 52,000.00-109,000.00\n   The expected starting salary range for Analyst, Financial - Actuarial : 58,000.00 - 88,000.00", "cleaned_desc": "     Assists in valuation/reserving under multiple accounting regimes (statutory, GAAP and tax) for products.\n    \n \n \n     Assists with analysis of reinsurance opportunities and risks for Nationwide Financial business products.\n    \n \n \n     Assists in activities related to a specialized phase of data and analytics which may include data governance, data strategy development, data modeling, data requirements, data manipulation, and/or requirements elicitation when needed.\n    \n \n \n     Assists in the analysis and validation of moderately sophisticated business processes and in the development and implementation of new programs and/or modifications of existing applications when required. May assist in the design, prototyping, coding and other methodologies for new and/or existing applications including development of test plans.\n    \n \n \n   May perform other responsibilities as assigned.\n  \n \n \n   Reporting Relationships: Reports to Financial Director, Actuarial, or other financial/actuarial manager.\n  \n \n \n   Typical Skills and Experiences:\n  \n \n  Education: Undergraduate degree in actuarial science, mathematics, statistics, CIS or similar background requiring modeling, programming or quantitative analysis.", "techs": ["valuation/reserving tools", "reinsurance analysis tools", "data governance tools", "data strategy development tools", "data modeling tools", "data manipulation tools", "requirements elicitation tools", "business process analysis tools", "application development tools", "test planning tools"]}, "fc90e9b394a8b61f": {"terms": ["data analyst"], "salary_min": 120000.0, "salary_max": 120000.0, "title": "Senior Business Analyst", "company": "Oran Inc", "desc": "We are seeking an experienced Senior Business Analyst with a strong background in SharePoint Collabspace, LMS365, and PowerApps environments to join our team. The ideal candidate will play a pivotal role in bridging the gap between business needs and technology solutions, ensuring the effective utilization of these platforms in alignment with our organizational goals. If you are a skilled analyst with expertise in these Microsoft 365 technologies, we encourage you to apply for this senior-level position. \n Responsibilities \n Collaborate with stakeholders to understand business needs and objectives related to SharePoint Collabspace, LMS365, and PowerApps environments. \n Elicit, document, and analyze business requirements, ensuring they are clear, concise, and aligned with best practices. \n Evaluate the current M365 environment for usability, security and compliance including SharePoint, Collabspace/Collabmail, Teams, PowerApps, and LMS365. \n Work closely with technical teams to translate business requirements into technical specifications. \n Document detailed system requirements, data models, workflows, and use cases. \n Create and maintain comprehensive documentation for SharePoint Collabspace, LMS365, and PowerApps projects \n Identify opportunities for process improvement within the SharePoint Collabspace, LMS365, and PowerApps environments. \n Develop and execute test plans and test cases to validate the functionality of SharePoint Collabspace, LMS365, and PowerApps solutions. \n Provide training and support to end-users, helping them maximize the value of SharePoint Collabspace, LMS365, and PowerApps applications. \n Assist in resolving user issues and inquiries. \n Monitor the performance and usage of SharePoint Collabspace, LMS365, and PowerApps environments. \n Ongoing collaboration and training of IT staff and eLearning Training Specialist on the redevelopment and implementation of newly developed and redeveloped PowerApps and features \n Procure and manage licensing for Collabspace, LMS365 and other add-ons as recommended and accepted by FMCS. \n \n \n Qualifications: \n  Bachelor's degree in Business Administration, Information Technology, or a related field. \n 5+ years of experience working directly with small to medium governmental agencies \n Completed background investigation at the Tier 2 moderate risk level (formerly MBI) with a favorable adjudication. \n 5+ years of experience as a Business Analyst, with a focus on SharePoint Collabspace, LMS365, and PowerApps environments. \n Strong knowledge of Microsoft 365 technologies and platforms. \n Proficiency in requirements gathering, analysis, and documentation. \n Excellent communication and interpersonal skills to facilitate effective collaboration. \n Ability to translate business needs into technical specifications. \n Experience with process improvement and workflow optimization. \n Strong problem-solving and analytical skills. \n Ability to work in a dynamic, fast-paced environment. \n Relevant certifications in business analysis or Microsoft 365 technologies (e.g., Power Platform certifications) preferred. \n \n \n Desired Qualifications: \n  Experience with Agile development \n Experience with cloud-based SharePoint solutions", "cleaned_desc": " Excellent communication and interpersonal skills to facilitate effective collaboration. \n Ability to translate business needs into technical specifications. \n Experience with process improvement and workflow optimization. \n Strong problem-solving and analytical skills. \n Ability to work in a dynamic, fast-paced environment. \n Relevant certifications in business analysis or Microsoft 365 technologies (e.g., Power Platform certifications) preferred. ", "techs": ["excellent communication", "interpersonal skills,\nability to translate business needs into technical specifications,\nexperience with process improvement", "workflow optimization,\nstrong problem-solving", "analytical skills,\nability to work in a dynamic", "fast-paced environment,\nrelevant certifications in business analysis", "microsoft 365 technologies (e.g.", "power platform certifications)"]}, "cbe127ba696b313e": {"terms": ["data analyst"], "salary_min": null, "salary_max": null, "title": "Business Analyst/Trainer - Laboratory Software", "company": "Precise Software Solutions, Inc.", "desc": "Work Location \n  Successful candidate will remotely support a group of laboratories distributed across the continental U.S. and Puerto Rico.  \n All work will be performed remotely from within the U.S. M-F. Location in the Eastern or Central time zone is preferred. \n  We are looking for a versatile candidate with experience in the areas of  Business Analysis ,  Training, and Testing  to provide support during a software transition and data migration. Confidence in leading virtual meetings is required and some exposure to work in a laboratory environment or supporting laboratory analysts in the use of a Laboratory Information Management System (LIMS) is preferred. A successful candidate will be able to work remotely with end users with varied skill levels and be sensitive to their unique needs and concerns. Strong communication and customer service skills are required. \n  The  Laboratory Software Business Analyst/Trainer  is responsible for conducting small team meetings to collect requirements for a new inventory management software application, completing ad hoc testing of application updates, and providing virtual training to end users of that application. Limited manual migration of data may be required with training provided. \n  Roles and Responsibilities include: \n \n  Lead virtual small group elicitations to identify requirements for the new application. \n  Conduct User Acceptance Testing (UAT) sessions and perform ad hoc testing of the software application as development proceeds. \n  Using vendor-provided guides, provide virtual end user training in the use of new inventory tracking software.  \n Collaborate effectively with team members, end users, and the software vendor to customize software to meet user needs. \n  Adhere to project documentation requirements and work with required urgency to remain on schedule for delivery. \n  Assist with manual data compilation and migration as needed for limited unique small data sets that may require special considerations. \n  Maintain a first-class level of customer service ensuring that all customers are treated efficiently and in an appropriate manner. \n  Maintain excellent verbal communication skills with the ability to communicate effectively with technical and non-technical colleagues at all levels in the organization. \n  Be a highly motivated team player with the skills and ability to manage changing priorities. \n  Undertake other duties not specifically stated which from time to time are necessary. \n \n  Requirements \n \n  Minimum 2 years\u2019 experience as a Business Analyst is required. \n  Must be able to pass a background check and secure Public Trust clearance. \n  Business Analysis experience conducting requirement elicitations is required. \n  Experience in training end users in the use of a software application or other group training experience is strongly preferred.  \n Must be able provide patient and thorough training to groups with varied proficiency levels. \n  Experience in laboratory testing, use of a Laboratory Information Management System (LIMS), testing of software, and data migration experience is helpful but is trainable. \n  Familiarity with STARLIMS and/or Chemical Safety software is a plus. \n  Proficiency with a variety of software applications and confidence in learning new applications is preferred. \n  Experience with direct user contact and demonstrated gold-standard customer service. \n  Passion for solving problems and helping people resolve issues.  \n Demonstrated exceptional oral and written communication skills. \n  Ability to work as part of a remote team or with geographically dispersed team. \n  Customer service attitude: Patience, people skills and a sense of humor. \n \n  Minimum Education \n   BA/BS and/or B.A. certification and 2 years of Business Analysis experience. \n \n  About Us \n  Precise Software Solutions, Inc., an SBA 8(a) program participant, is an innovative small business with a proven record of success delivering quality services and solutions to government organizations. A CMMI Level 3 company, Precise serves as a trusted advisor to senior technology executives and helps government agencies enhance and expand their information technology capabilities. Precise helps their customers capitalize on the efficiencies offered by technological advancements and ensures the integrity of their IT systems and programs so they can perform their public mission more effectively. The company is known for delivering agile and innovative solutions and specializes in strategic consulting, system modernization and integration, digital transformation and experience, infrastructure and cloud implementation, and data management and analytics. \n  Benefits and Perks \n \n  Health Benefits (Medical, Dental and Vision) \n  Flexible Spending Accounts (FSA) & Health Savings Account (HSA) \n  Retirement Plan with 4% match \n  Paid Time Off \n  Parental Leave \n  Life Insurance \n  Training and Development \n  Two Innovation Days \n  Employee Referral Program \n  Annual Charity Donation Match \n  Awards and Recognition \n \n  Our Equal Employment Opportunity Policy \n  Precise is an equal opportunity employer. The company shall not discriminate against any employee or applicant because of race, color, religion, creed, sex, sexual orientation, gender or gender identity (except where gender is a bona fide occupational qualification), national origin, age, disability, military/veteran status, marital status, genetic information or any other factor protected by law. We are committed to equal employment opportunity in all decisions related to employment, promotion, wages, benefits and all other privileges, terms and conditions of employment. The company is dedicated to seeking all qualified applicants. \n \n   \n   \n V749Ru0oni", "cleaned_desc": "  Business Analysis experience conducting requirement elicitations is required. \n  Experience in training end users in the use of a software application or other group training experience is strongly preferred.  \n Must be able provide patient and thorough training to groups with varied proficiency levels. \n  Experience in laboratory testing, use of a Laboratory Information Management System (LIMS), testing of software, and data migration experience is helpful but is trainable. \n  Familiarity with STARLIMS and/or Chemical Safety software is a plus. \n  Proficiency with a variety of software applications and confidence in learning new applications is preferred. \n  Experience with direct user contact and demonstrated gold-standard customer service. \n  Passion for solving problems and helping people resolve issues.  \n Demonstrated exceptional oral and written communication skills. \n  Ability to work as part of a remote team or with geographically dispersed team. \n  Customer service attitude: Patience, people skills and a sense of humor. ", "techs": ["business analysis experience", "software application", "group training", "laboratory information management system (lims)", "data migration experience", "starlims", "chemical safety software", "software applications", "direct user contact", "oral and written communication skills", "remote team", "geographically dispersed team", "customer service"]}, "8c6ec57b44183ebb": {"terms": ["data analyst"], "salary_min": 91528.12, "salary_max": 115894.99, "title": "Business Analyst, Partner Relationship Management", "company": "Rockwell Automation", "desc": "Rockwell Automation is a global technology leader focused on helping the world\u2019s manufacturers be more productive, sustainable, and agile. With more than 28,000 employees who make the world better every day, we know we have something special. Behind our customers - amazing companies that help feed the world, provide life-saving medicine on a global scale, and focus on clean water and green mobility - our people are energized problem solvers that take pride in how the work we do changes the world for the better.\n  \n \n \n   We welcome all makers, forward thinkers, and problem solvers who are looking for a place to do their best work. And if that\u2019s you we would love to have you join us!\n  \n \n \n   Job Description\n  \n \n   Think of an app you simply cannot live without. For our company, the Commercial Operations organization is like that app. It is a strategic and indispensable component of an organized and knowledgeable sales organization.\n  \n \n   Rockwell Automation's partnerships are governed by multiple programs and include main elements such as partnership agreements, incentives, pricing, policies, and communications. These programs require a modern, the best PRM platform to maximize our digital experience and engagement with our partners \u2013 including Distributors, System Integrators, OEM Partners and Technology Partners.\n  \n \n   Commercial Operations ties together strategy and roles, supported by digital applications and analytics. The PRM team is responsible for an important part of this effort. You will work with a diverse, dynamic PRM team. This position is remote-friendly. You will report to PRM Platform Owner and support PRM Platform Owner by engaging with the Market Access regional teams, Commercial Program Managers, Marketing, Partner Enablement and Engagement team, and other BU team members to gather IT requirements and manage projects related to PRM evolution and adoption. The Business Analyst will be a PRM subject-matter expert able to demonstrate current functionality and provide support and analysis to identify or meet our needs.\n  \n \n \n   Your Responsibilities:\n  \n \n \n  Guide Business Analysis activities and create Agile artifacts required to deliver relevant User Stories for IT. \n  Support IT Development efforts by analyzing and communicating with partners and IT, finding answers, and coordinating efforts to resolve issues and meet business commitments. \n  Support Product Owner in Agile ceremonies to help prioritize or find input from Market Access Operational team and other team members to ensure IT development or prioritization. \n  Facilitate meetings to gather customer approval, perform User Acceptance Testing and follow-up items after Agile Spring Demos. \n  Create or enhance PRM training documentation (videos, slides, and contents). \n  Communicate with partners to share training documentation and information on PRM current functionality, changes, and processes for Partners. \n  Track IT commitments and escalate if needed to meet delivery dates. \n  Help troubleshooting of ad-hoc issues with IT and Global Market Access team members. \n  Additional responsibilities to support peers in Global Market Access team when needed. \n \n \n \n   The Essentials - You Will Have:\n  \n \n  Bachelor's Degree or equivalent experience \n  You can work within the United States without sponsorship. We will not sponsor for this role now or in the future. \n \n \n \n   The Preferred - You Might Also Have:\n  \n \n  Bachelor's technical degree or equivalent experience \n  2+ years in requirements gathering, process mapping, wireframing, creating analysis artifacts, Agile and Software Development Life Cycle. \n  1+ year background in analytics or sales operations . \n  Understanding of CRM Systems, PowerApps, PowerBI, Websites, Jira, and Scrum methodology. \n  Understanding of APIs, Data Analysis, and data integrity. \n  Fluent in English \n \n \n   What We Offer:\n  \n \n  Health Insurance including Medical, Dental and Vision \n  401k \n  Paid Time off \n  Parental and Caregiver Leave \n  Flexible Work Schedule where you will work with your manager to enjoy a work schedule that works with your personal life. \n  To learn more about our benefits package, please visit at www.\n    \n    raquickfind.com\n    . \n \n \n \n  We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace. \n \n \n \n  At Rockwell Automation we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right person for this or other roles. \n \n \n \n   #LI-Remote\n  \n \n   #LI-SH2\n  \n \n \n   Rockwell Automation is an Equal Opportunity Employer including disability and veterans.\n  \n \n   If you are someone with a disability and you need assistance or reasonable accommodation during the application process, please contact our services team at +1 (844) 404-7427.\n  \n \n \n   We are an Equal Opportunity Employer including disability and veterans.\n  \n \n \n   If you are an individual with a disability and you need assistance or a reasonable accommodation during the application process, please contact our services team at +1 (844) 404-7247.", "cleaned_desc": "", "techs": ""}, "82b87753c9fc85b3": {"terms": ["data analyst"], "salary_min": 91528.12, "salary_max": 115894.99, "title": "Business Analyst, Partner Relationship Management", "company": "Rockwell Automation", "desc": "Rockwell Automation is a global technology leader focused on helping the world\u2019s manufacturers be more productive, sustainable, and agile. With more than 28,000 employees who make the world better every day, we know we have something special. Behind our customers - amazing companies that help feed the world, provide life-saving medicine on a global scale, and focus on clean water and green mobility - our people are energized problem solvers that take pride in how the work we do changes the world for the better.\n  \n \n \n   We welcome all makers, forward thinkers, and problem solvers who are looking for a place to do their best work. And if that\u2019s you we would love to have you join us!\n  \n \n \n   Job Description\n  \n \n   Think of an app you simply cannot live without. For our company, the Commercial Operations organization is like that app. It is a strategic and indispensable component of an organized and knowledgeable sales organization.\n  \n \n   Rockwell Automation's partnerships are governed by multiple programs and include main elements such as partnership agreements, incentives, pricing, policies, and communications. These programs require a modern, the best PRM platform to maximize our digital experience and engagement with our partners \u2013 including Distributors, System Integrators, OEM Partners and Technology Partners.\n  \n \n   Commercial Operations ties together strategy and roles, supported by digital applications and analytics. The PRM team is responsible for an important part of this effort. You will work with a diverse, dynamic PRM team. This position is remote-friendly. You will report to PRM Platform Owner and support PRM Platform Owner by engaging with the Market Access regional teams, Commercial Program Managers, Marketing, Partner Enablement and Engagement team, and other BU team members to gather IT requirements and manage projects related to PRM evolution and adoption. The Business Analyst will be a PRM subject-matter expert able to demonstrate current functionality and provide support and analysis to identify or meet our needs.\n  \n \n \n   Your Responsibilities:\n  \n \n \n  Guide Business Analysis activities and create Agile artifacts required to deliver relevant User Stories for IT. \n  Support IT Development efforts by analyzing and communicating with partners and IT, finding answers, and coordinating efforts to resolve issues and meet business commitments. \n  Support Product Owner in Agile ceremonies to help prioritize or find input from Market Access Operational team and other team members to ensure IT development or prioritization. \n  Facilitate meetings to gather customer approval, perform User Acceptance Testing and follow-up items after Agile Spring Demos. \n  Create or enhance PRM training documentation (videos, slides, and contents). \n  Communicate with partners to share training documentation and information on PRM current functionality, changes, and processes for Partners. \n  Track IT commitments and escalate if needed to meet delivery dates. \n  Help troubleshooting of ad-hoc issues with IT and Global Market Access team members. \n  Additional responsibilities to support peers in Global Market Access team when needed. \n \n \n \n   The Essentials - You Will Have:\n  \n \n  Bachelor's Degree or equivalent experience \n  You can work within the United States without sponsorship. We will not sponsor for this role now or in the future. \n \n \n \n   The Preferred - You Might Also Have:\n  \n \n  Bachelor's technical degree or equivalent experience \n  2+ years in requirements gathering, process mapping, wireframing, creating analysis artifacts, Agile and Software Development Life Cycle. \n  1+ year background in analytics or sales operations . \n  Understanding of CRM Systems, PowerApps, PowerBI, Websites, Jira, and Scrum methodology. \n  Understanding of APIs, Data Analysis, and data integrity. \n  Fluent in English \n \n \n   What We Offer:\n  \n \n  Health Insurance including Medical, Dental and Vision \n  401k \n  Paid Time off \n  Parental and Caregiver Leave \n  Flexible Work Schedule where you will work with your manager to enjoy a work schedule that works with your personal life. \n  To learn more about our benefits package, please visit at www.\n    \n    raquickfind.com\n    . \n \n \n \n  We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace. \n \n \n \n  At Rockwell Automation we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right person for this or other roles. \n \n \n \n   #LI-Remote\n  \n \n   #LI-SH2\n  \n \n \n   Rockwell Automation is an Equal Opportunity Employer including disability and veterans.\n  \n \n   If you are someone with a disability and you need assistance or reasonable accommodation during the application process, please contact our services team at +1 (844) 404-7427.\n  \n \n \n   We are an Equal Opportunity Employer including disability and veterans.\n  \n \n \n   If you are an individual with a disability and you need assistance or a reasonable accommodation during the application process, please contact our services team at +1 (844) 404-7247.", "cleaned_desc": "", "techs": ""}, "c0f2ae9906809c1d": {"terms": ["data analyst"], "salary_min": 55.0, "salary_max": 60.0, "title": "Information Security Business Analyst", "company": "Dwellfox", "desc": "Job Title: Information Security Business Analyst \n Job Location: Remote \n Job Type: Contract (W2) \n Job Description: \n Skills:  Information Security, Cyber Security, Business Analysis, business cases, use cases, modeling (UML, BPMN, etc.), risk analysis, HPALMe, SDLC, CCBA or CBAP certification \n Key Responsibilities \n Leads the development of detailed business and functional requirements for approved changes and projects including: \n \n Facilitating requirements elicitation sessions with the user community \n Developing and documenting processes, use cases, etc. to demonstrate requirements \n Partner with Quality SQA and Test Analyst to ensure test plans are developed and executed in alignment with internal standards and external compliance requirements. \n Ensures User Acceptance and Functional Testing is complete and confirms test results meet documented business needs \n \n Qualifications \n \n Bachelor\u2019s degree in cybersecurity, information technology, computer science, or a related field \n Experience working with Access Control, Vulnerability Management, Risk Management teams. \n Experience in process modeling \n Ability to Think analytically and solve complex problems, including understanding of how a change to a component affects the system as a whole \n Effectively collaborate with both business and technical Associates to develop both business and functional requirements. \n Collaborate with Quality and teams to identify, design, and develop test plans and test cases \n Provide leadership, oversight, and training related to test case development to multi-functional teams \n Quickly build trusting relationships \n Facilitate meetings/group interactions and negotiate outcomes, including effectively resolving conflict \n Adjust style based on the needs of the situation \n Effectively work virtually across multiple time zones and cultures \n Strong oral and written communication skills \n Proficiency in English \n \n Preferred \n \n Experience working with: Business Analysis Methods, Tools, and Techniques, particularly those contained in the IIBA Business Analysis Book of Knowledge (BABOK) \u2013 e.g. business cases, use cases, modeling (UML, BPMN, etc.), risk analysis, etc. \n Testing methodologies and supporting systems, e.g. HPALMe \n SDLC processes and methodologies \n CCBA or CBAP certification \n Experience supporting business and IT governance processes \n \n Job Type: Contract \n Pay: $55.00 - $60.00 per hour \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Experience level: \n \n 5 years \n \n Schedule: \n \n 8 hour shift \n \n Experience: \n \n Information Security: 3 years (Preferred) \n Cyber Security: 2 years (Preferred) \n Business Analyst: 5 years (Preferred) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "e7178849b65bf01f": {"terms": ["data analyst"], "salary_min": 75182.18, "salary_max": 95197.4, "title": "Technical Business Analyst: Mid-Level", "company": "Jack Henry and Associates, Inc.", "desc": "At Jack Henry, we deliver technology solutions that are digitally transforming and empowering community banks and credit unions to provide enhanced and streamlined user experiences to their customers and members. Our best-in-class products are just the start as we lay the groundwork for the future of digital banking and payments. We hope you\u2019ll join us. We can\u2019t do it without you.\n  \n \n \n  Symitar Project Implementations is looking for a customer focused, detailed oriented individual to join the team. You must be a self-starter and can manage multiple projects at once. You will be responsible for working with credit union staff to implement Integrated Solutions and core module products and processes. This position can be worked remotely within the United States. \n \n \n  What you\u2019ll be responsible for: \n \n \n \n \n  Analyzes the customer\u2019s existing products/processes and consults with customer to map existing system to the JH product. \n  Prepares the customer for the installation, takes the customer through the installation process, and provides training or support to the customer post-installation. \n  Oversees/performs system set-up for customer (i.e., parameter set-up, creates job files for processing, etc). Ensures related business processes will run on software. \n  Provides software support/guidance by answering questions on functions, features and usage of software products. Support will be remote. \n  Communicates the customer\u2019s needs/expectations with programmers, other team members, and team leader. \n  Maintains effective communication with customer throughout entire project/case. \n  Identifies/maintains customer issues and ensures proper resolution. Maintains customer issue list by application and ensures all are forwarded to the appropriate personnel for resolution. \n  Prepares training materials and documentation for customers and internal users. \n  Provides training to less experienced peers. \n  May perform other job duties as assigned. \n \n \n \n  What \n  you\u2019ll need to have: \n \n \n \n \n  Minimum of 5 years of work experience in a financial institution or FinTech. \n \n \n \n  What would be nice for you to have:  \n \n \n \n \n  Symitar Core System Experience to include strong ACH knowledge \n  Previous work in application analysis/product management \n  Strong knowledge of Jack Henry products, equivalent competitor products, and understanding of application functions. \n  Strong computer skills and knowledge of information systems. \n  Strong business operations including concepts, structures, etc. \n  Strong organizational skills. \n  Strong ability to analyze/convert customer information and processes for setup in Jack Henry system. Analyze business information and processes. \n  Ability to communicate complex information in user-friendly terms. \n \n \n \n   If you got this far, we hope you're feeling excited about this opportunity. Even if you don't feel you meet every single requirement on this posting, we still encourage you to apply. We're eager to meet motivated people who align with Jack Henry\u2019s mission and can contribute to our company in a variety of ways.\n   \n \n \n \n \n Why Jack Henry? \n \n \n \n  At Jack Henry, we pride ourselves through our motto of, \"Do the right thing, do whatever it takes, and have fun.\" We recognize the value of our associates and believe much of our company\u2019s strength and success depends on their well-being.\n   \n \n \n \n \n  We demonstrate our commitment by offering outstanding benefit programs to ensure the physical, mental & financial wellbeing of our people is always met.\n  \n \n \n \n \n  Culture of Commitment \n \n \n \n \n \n   Ask our associates why they love Jack Henry, and many will tell you it is because our culture is exceptional. We do great things together. Rising to meet challenges and seeking opportunities is part of who we are as an organization. Our culture has helped us stay strong through challenging times and we credit our dedicated associates for our success. Visit our Corporate Responsibility site to learn more about our culture and commitment to our people, customers, community, environment, and shareholders.\n   \n \n \n \n \n Equal Employment Opportunity \n \n \n \n \n \n   At Jack Henry, we know we are better together. We value, respect, and protect the uniqueness each of us brings. Innovation flourishes by including all voices and makes our business\u2014and our society\u2014stronger. Jack Henry is an equal opportunity employer and we are committed to providing equal opportunity in all of our employment practices, including selection, hiring, performance management, promotion, transfer, compensation, benefits, education, training, social, and recreational activities to all persons regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, genetic information, pregnancy, marital status, sex, gender, gender identity, gender expression, age, sexual orientation, and military and veteran status, or any other protected status protected by local, state or federal law.\n  \n \n \n \n \n   No one will be subject to, and Jack Henry prohibits, any form of discipline, reprisal, intimidation, or retaliation for good faith reports or complaints of incidents of discrimination of any kind, pursuing any discrimination claim, or cooperating in related investigations.\n  \n \n \n \n \n  Requests for full corporate job description may be requested through the interview process at any time.", "cleaned_desc": "", "techs": ""}, "71b77e6f51066545": {"terms": ["data analyst"], "salary_min": 100000.0, "salary_max": 110000.0, "title": "ERP Business Analyst preferred with Momentum experience", "company": "Fathom Management LLC", "desc": "ERP Business Analyst preferred with Momentum experience \n  Location- Remote \n  End Customer- Department of Veterans Affairs \n  The salary range of $100,000 - $110,000 will be based on experience and technical interview. \n  Position Summary \n  Seeking a Business Analyst to formulate and define systems scope and objective based on both users' needs and comprehension of applicable business systems and industry requirements. The business analyst works with intuitive judgment, supplying requested information and making independent decisions regarding planning, organizing, and scheduling work within established guidelines. Arrange special events, meetings, conferences, and working groups in accordance with the procedures and schedules set by the requestor, including notifying participants, providing agendas and directions, and offer arrangements for equipment. Must have experience on some ERP financial systems. Experience with the Momentum Financial Management system is preferred. \n  Responsibilities/Skill Sets \n \n  Organized, detail-oriented personality with the ability to multi-task in an entrepreneurial environment \n  Knowledge of various software types, including spreadsheets, word processing, and graphics, databases, and the functions and features of programs in each category \n  Strong oral and written communication skills to summarize and share analytic insight \n  Identify and document requirements for integration, development, and enhancement of solutions \n  Participate in definition, design, testing, training, implementation, and support of system requirements \n  Assist in planning, identification, and tracking of risks and issues \n  Devise or modify procedures to solve complex problems \n  Train and mentor other BA team members in procedures and technical knowledge \n  Comfort performing analyses to distill and draw out insights from complex data sources \n  Ability to work well and build rapport with individuals at all levels of the organization \n  Ability to interface with senior leadership and brief government customers \n  Experience with VA financial management systems a plus \n \n  Basic Qualifications \n \n  BA or BS degree or higher \n  Minimum of 5-10 years Business Analyst experience \n  Must have experience with Financial Systems \n  Preferred candidates with CGI Momentum experience \n  Ability to obtain a security clearance. \n  Experience with the VA or Federal programs preferred. \n  Must be able to obtain a Public trust clearance. \n  Team player with high initiative to learn. \n  Veterans highly encouraged to apply \n \n  Benefits Overview:  Full-time employees are offered comprehensive and competitive benefits package including paid vacation, sick leave, holidays, health insurance, life insurance, military leave, training, tuition reimbursement, a wellness program, short- and long-term disability, 401(k) retirement plan with company matches/immediate vesting, commuter benefits, and more. \n  EEO Policy:  It is our policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits, and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability, or any other characteristic protected by applicable federal, state or local law.", "cleaned_desc": "  Responsibilities/Skill Sets \n \n  Organized, detail-oriented personality with the ability to multi-task in an entrepreneurial environment \n  Knowledge of various software types, including spreadsheets, word processing, and graphics, databases, and the functions and features of programs in each category \n  Strong oral and written communication skills to summarize and share analytic insight \n  Identify and document requirements for integration, development, and enhancement of solutions    Minimum of 5-10 years Business Analyst experience \n  Must have experience with Financial Systems \n  Preferred candidates with CGI Momentum experience \n  Ability to obtain a security clearance. \n  Experience with the VA or Federal programs preferred. \n  Must be able to obtain a Public trust clearance. ", "techs": ["cgi momentum", "spreadsheets", "word processing", "graphics", "databases"]}, "8e22cb34a8d435ee": {"terms": ["data analyst"], "salary_min": 55000.0, "salary_max": 65000.0, "title": "COMMERCIAL ANALYST", "company": "Kampgrounds Of America", "desc": "KOA, INC. DEI STATEMENT \n At KOA, we believe the outdoors is fun and for everyone. We are committed to having a diverse, equitable and inclusive environment where all are treated with dignity and respect. We strive to: \n \n \n intentionally create a sense of community and belonging for our guests, employees and franchise partners \n continually educate ourselves and advance our understanding about DEI \n sustain a culture that promotes diversity of thought and experiences \n ensure everyone has the ability to experience the outdoors and that our facilities are accessible to all \n drive change in our company and industry through action and implementation \n  ABOUT KAMPGROUNDS OF AMERICA, INC. \n Kampgrounds of America, Inc. (KOA) is the world\u2019s largest network of privately owned campgrounds and the leader in outdoor hospitality. KOA has 500+ locations across the United States and Canada including a mix of franchised and company-owned parks. KOA has approximately 90 employees at its corporate headquarters in Billings, Montana, and 1200+ across its location. Founded in 1962, the mission of KOA is \u201cconnecting people to the outdoors and each other,\u201d and those who represent the brand share the values of being family-oriented, passionate, entrepreneurial, customer-focused and progressive. \n REPORTS TO \n Commercial Strategy, Revenue Technology Manager \n \n  GENERAL DUTIES \n The Commercial Analyst is responsible for examining campground revenue, operations, expenses, and competition to find ways to improve the properties\u2019 financial standing. At Kampgrounds of America, Inc. we strive to ensure each campground is profitable and a leader within its market. The Commercial Analyst will supply critical analysis of rates, occupancy, pricing, margins, yield strategies, and expenses to find more ways to make each campground profitable. In addition to analysis, this role will support with building and maintaining analysis tools to ensure resources are optimized to support campground and department needs. \n SPECIFIC DUTIES \n \n \n Gather and analyze data from proprietary campground management system \n Interpret gathered data to identify patterns and trends and uncover revenue opportunities \n Produce property-level and company-wide business reporting of all relevant performance KPIs on a daily, weekly, monthly, quarterly, and yearly basis \n Submit reports to relevant departments including but not limited to operations, marketing, revenue management, and business development \n Leverage available data to assist in setting revenue goals and strategy \n Monitor demand generators and competitor strategy, reviewing pricing and availability to help predict demand in markets \n Work with revenue and technology teams to establish new data gathering and analysis techniques within the organization \n Perform any special projects needed to provide business decision makers visibility to key data \n Identify, propose, and implement potential new opportunities to improve processes and tools to drive revenue \n Ensure relevant revenue and pricing issues are investigated and analyzed \n Assist in the maintenance of any software programs which contribute to the management of revenue \n  EXPECTED RESULTS \n \n \n In collaboration with operations, marketing, revenue management, and business development, strive to meet and exceed registration revenue, camper nights and additional KPI goals for each property in your influence \n Align to and execute all commercial strategy processes \n Delivery of projects within defined budget in a timely manner as defined by manager \n Development of a collaborative and team focused style while working closely with Marketing, Revenue, and Business Development Departments \n JOB QUALIFICAITONS \n A bachelor\u2019s degree, preferably in hospitality, hotel mgmt., or a highly analytical field. MBA a plus. \n 1-3 years\u2019 experience as an analyst, data scientist, or revenue related role \n Proficiency in mathematics with the skill to translate complex mathematical information into understandable reports \n Works well independently as well as in a team environment. \n Deep and broad quantitative and analytical skills \n Highly computer literate with a strong command of Microsoft Excel \n Business Intelligence (BI) tool experience a plus, especially with Microsoft Power BI \n Professional in Business Analysis, Agile Analysis, and Business Analysis Professionals Certifications a plus \n Strong oral and written communication skills \n Report writing and presentation skills \n Has a valid Driver\u2019s License \n  PHYSICAL REQUIREMENTS \n \n \n Ability to sit for extended periods \n Able to lift 25 pounds \n Able to travel up to 30% of the time \n \n Benefits Offered: \n \n \n Medical Insurance \n Dental Insurance \n Vision Insurance \n Health Savings Account \n Flexible Spending Account \n Dependent Care Account \n Paid Life Insurance \n Paid Long Term Disability \n Voluntary Life Insurance \n Voluntary Short-Term Disability \n Voluntary Critical Illness Insurance \n Voluntary Accident Insurance \n Paid Time Off \n Paid Parental Leave \n Employee Assistance Program \n 401K Retirement Plan \n 401K Company Contributions", "cleaned_desc": "  EXPECTED RESULTS \n \n \n In collaboration with operations, marketing, revenue management, and business development, strive to meet and exceed registration revenue, camper nights and additional KPI goals for each property in your influence \n Align to and execute all commercial strategy processes \n Delivery of projects within defined budget in a timely manner as defined by manager \n Development of a collaborative and team focused style while working closely with Marketing, Revenue, and Business Development Departments \n JOB QUALIFICAITONS \n A bachelor\u2019s degree, preferably in hospitality, hotel mgmt., or a highly analytical field. MBA a plus. \n 1-3 years\u2019 experience as an analyst, data scientist, or revenue related role \n Proficiency in mathematics with the skill to translate complex mathematical information into understandable reports \n Works well independently as well as in a team environment. \n Deep and broad quantitative and analytical skills \n Highly computer literate with a strong command of Microsoft Excel \n Business Intelligence (BI) tool experience a plus, especially with Microsoft Power BI ", "techs": ["microsoft excel", "microsoft power bi"]}, "e4aae49518fc9811": {"terms": ["data analyst"], "salary_min": 50.0, "salary_max": 67.0, "title": "Form Developer/Data Analyst (CSV/CMC)\u2013 W2 Only - 100% Remote \u2013 BB0908", "company": "TechData Service Company, LLC", "desc": "Form Developer/Data Analyst (CSV/CMC)\u2013 W2 Only - 100% Remote \u2013 BB0908 \n Location \u2013 100% Remote/Philadelphia \n 6+ Months (up to 18 Months) \n \n Extensive experience with Microsoft Excel \n Working knowledge of Adobe Acrobat (Required) \n Form creation \n Implementing JavaScript in Acrobat forms (Required) \n Working knowledge of computer programming languages including, but not limited to: \n VBA or VB6 (highly desirable) \n Python \n C/C# \n Javascript \n Some knowledge of Adobe Illustrator (optional) \n Desire to learn and use new software/application \n \n Additional Details: \n \u00b7 Possess the ability to hit the ground running \n \u00b7 Prior Life Science/Pharma Industry Preferred \n \u00b7 CMC experience is Highly Preferred \n \u00b7 The ability to hit the ground running with limited training \n \u00b7 Looking for a 1 stop shop for Computer System Validations \n TechData is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. \n Job Types: Contract, Temporary \n Pay: $50.00 - $67.00 per hour \n Expected hours: 40 per week \n Benefits: \n \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Day shift \n Monday to Friday \n \n Application Question(s): \n \n What hourly rate are you seeking on a W2? \n \n Experience: \n \n CMC: 1 year (Preferred) \n Computer System Validation: 1 year (Required) \n Python: 1 year (Preferred) \n C/C#: 1 year (Preferred) \n VBA: 1 year (Preferred) \n VB6: 1 year (Preferred) \n MS Excel: 1 year (Preferred) \n Adobe Acrobat: 1 year (Required) \n Form Creating in Adobe Acrobat: 1 year (Required) \n Implementing JavaScript in Acrobat forms: 1 year (Required) \n Data Analysis: 1 year (Required) \n Pharma/Bio-Tech Industry: 1 year (Preferred) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "7495072245c0e807": {"terms": ["data analyst"], "salary_min": 77651.52, "salary_max": 98324.13, "title": "Business Analyst-Construction (NON-IT)", "company": "OpTech LLC", "desc": "OpTech is a woman-owned company that values your ideas, encourages your growth, and always has your back. When you work at OpTech, not only do you get health and dental benefits on the first day of employment, but you also have training opportunities, flexible work options, growth opportunities, 401K and competitive pay. Apply today! To view our complete list of openings, please visit our website at www.optechus.com. \n We have a top client in the banking industry that is looking to hire a  Construction Business Analyst!  \n This is a 100% remote opportunity! Must be able to work EST hours! \n (*_ Please note-This is NOT an IT/Technical Business Analyst position!!!)*_ \n Job Responsibilities: \n \n Document current state in the new system \n Work with colleagues to obtain information \n Supporting commercial/retail building line of credit. \n Ensure the payments are processed through the line of credit and properly document. \n Obtain collateral, fund the draw, review the construction progress, prepare demands for loans, clear borrowing based exceptions and balance the budget. \n Setup people in the construction loan based systems \n Provide excellent customer service \n Follow all required compliance guidelines \n \n Job Requirements: \n \n Minimum of 5 years' experience with business analysis \n Strong understanding construction lending \n Minimum of 5 years' experience with Microsoft Excel/Word \n \n Education/Certifications: \n \n Bachelor Degree is preferred \n \n OpTech LLC is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law. \n Job Type: Full-time \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n Paid time off \n Vision insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Education: \n \n Bachelor's (Preferred) \n \n Experience: \n \n Business Analysis: 5 years (Required) \n Construction Lending: 3 years (Required) \n Microsoft Excel: 5 years (Required) \n Microsoft Word: 5 years (Required) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "6bb1d7ebea23792c": {"terms": ["data analyst"], "salary_min": 55.0, "salary_max": 60.0, "title": "Information Security Business Analyst", "company": "Synergistic Systems Inc", "desc": "Our client, a global materials science company is seeking a Information Security Business Analyst for a 12 month minimum contract position. This position is completely REMOTE. No subcontracting.\n   Team is located on the east coast so would prefer someone within EST or at least has ability to work a flex schedule when needed.\n  \n \n Key Responsibilities \n \n \n  Leads the development of detailed business and functional requirements for approved changes and projects including:\n  \n \n \n Facilitating requirements elicitation sessions with the user community \n Developing and documenting processes, use cases, etc. to demonstrate requirements \n Partner with Quality SQA and Test Analyst to ensure test plans are developed and executed in alignment with internal standards and external compliance requirements. \n Ensures User Acceptance and Functional Testing is complete and confirms test results meet documented business needs \n \n \n Qualifications \n \n \n \n Bachelor\u2019s degree in cybersecurity, information technology, computer science, or a related field \n Experience working with Access Control, Vulnerability Management, Risk Management teams. \n Experience in process modeling \n A true business analyst \n Ability to Think analytically and solve complex problems, including understanding of how a change to a component affects the system as a whole \n Effectively collaborate with both business and technical Associates to develop both business and functional requirements. \n Collaborate with Quality and teams to identify, design, and develop test plans and test cases \n Facilitate meetings/group interactions and negotiate outcomes, including effectively resolving conflict \n Effectively work virtually across multiple time zones and cultures \n Strong oral and written communication skills \n Proficiency in English \n \n \n Preferred \n \n \n \n Experience working with: Business Analysis Methods, Tools, and Techniques, particularly those contained in the IIBA Business Analysis Book of Knowledge (BABOK) \u2013 e.g. business cases, use cases, modeling (UML, BPMN, etc.), risk analysis, etc. \n Testing methodologies and supporting systems, e.g. HP ALM \n SDLC processes and methodologies \n CCBA or CBAP certification is nice to have \n \n \n This is a remote position.", "cleaned_desc": "", "techs": ""}, "80a222927f0b1f95": {"terms": ["data analyst"], "salary_min": 60.0, "salary_max": 65.0, "title": "Public Health Informatics Business Analyst", "company": "BNL Consulting", "desc": "BNL Consulting is actively seeking a contractor with experience as a Public Health Informatics Business Analyst. This position will be remote and will be a 1099 contract. \n Role Description : \n You will assist in meeting business needs requiring IT based solutions for public health including: identifying and analyzing business needs, creating functional and technical specifications/requirements documentation, defining scope and objectives, making recommendations for solutions or improvements to business processes that can be accomplished through new technology or alternative uses of existing technology, researching business requirements and documenting the relationships between the components of the application system (i.e., end users, business processes, data, applications, and devices), and translating business requirements into application requirements to parallel overall business strategies. Experience with public health organizations and specifically with public health informatics is strongly preferred. \n Required Experience: \n - Four plus years of experience with business process analysis \n - Four plus years of experience with requirements analysis and development \n - Four plus years of working with stakeholders to understand the current business processes and the outcomes and impact they are looking to see from business process changes \n - Four plus years of working with public health data, especially as part of a public health informatics team \n The must-haves: \n -Minimum of Bachelor of Science or Bachelor of Arts degree - preferably in Healthcare, Business, or related discipline \n - Experience with Public Health, especially with Public Health Informatics \n - Business Analyst Certification from PMI (PMI Professional in Business Analysis (PMI-PBA)) \n -Experience working with laboratory public health data \n -4 years of experience acting as a business analyst \n -Experience analyzing and developing requirements for new systems and enhancements for existing systems \n -Experience evaluating business processes, anticipate requirements, uncover areas for improvement, and use analytical skills to gather, document, and optimize the requirements \n -Experience conducting discovery, workflow analysis, or landscape analysis to understand current and future state, needs and capabilities \n -Experience meeting with clients, internal business Subject Matter Experts (SMEs), operations analysts, and/or members of the development team to gather, define and document system requirements \n The nice-to-haves: \n - Strong problem-solving skills \n - Good self-starter/self-manager \n - Good communication skills (verbal and written) \n About Us: \n Founded in 2007, BNL Consulting began by designing and implementing small to enterprise-level business intelligence (BI) systems. At our core, we\u2019re still dedicated to providing SAS-centric services, but over the past decade, we\u2019ve expanded our offerings to include custom analytics platforms, highly interactive user experiences, seamless enterprise integration solutions, and secure cloud architecture. While the relationships we cultivate with our clients give us a wide spectrum of experience, we have an especially successful track record of providing solutions to the Public Sector, Sports and Entertainment industries, and the Health and Life Sciences fields. For more information on us, please visit our website at www.bnlconsulting.com. \n Job Type: Contract \n Pay: $60.00 - $65.00 per hour \n Experience level: \n \n 4 years \n \n Schedule: \n \n 8 hour shift \n \n Education: \n \n Bachelor's (Required) \n \n Experience: \n \n Business process analysis: 4 years (Required) \n Requirements analysis: 4 years (Required) \n Public Health Data: 2 years (Required) \n \n License/Certification: \n \n PMI Professional in Business Analysis (PMI-PBA) (Required) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "317a2b5271fea112": {"terms": ["data analyst"], "salary_min": 91528.12, "salary_max": 115894.99, "title": "Business Analyst, Partner Relationship Management", "company": "Rockwell Automation", "desc": "Rockwell Automation is a global technology leader focused on helping the world\u2019s manufacturers be more productive, sustainable, and agile. With more than 28,000 employees who make the world better every day, we know we have something special. Behind our customers - amazing companies that help feed the world, provide life-saving medicine on a global scale, and focus on clean water and green mobility - our people are energized problem solvers that take pride in how the work we do changes the world for the better. \n \n  We welcome all makers, forward thinkers, and problem solvers who are looking for a place to do their best work. And if that\u2019s you we would love to have you join us! \n \n  Job Description \n  Think of an app you simply cannot live without. For our company, the Commercial Operations organization is like that app. It is a strategic and indispensable component of an organized and knowledgeable sales organization. \n  Rockwell Automation's partnerships are governed by multiple programs and include main elements such as partnership agreements, incentives, pricing, policies, and communications. These programs require a modern, the best PRM platform to maximize our digital experience and engagement with our partners \u2013 including Distributors, System Integrators, OEM Partners and Technology Partners. \n  Commercial Operations ties together strategy and roles, supported by digital applications and analytics. The PRM team is responsible for an important part of this effort. You will work with a diverse, dynamic PRM team. This position is remote-friendly. You will report to PRM Platform Owner and support PRM Platform Owner by engaging with the Market Access regional teams, Commercial Program Managers, Marketing, Partner Enablement and Engagement team, and other BU team members to gather IT requirements and manage projects related to PRM evolution and adoption. The Business Analyst will be a PRM subject-matter expert able to demonstrate current functionality and provide support and analysis to identify or meet our needs. \n \n  Your Responsibilities: \n \n \n  Guide Business Analysis activities and create Agile artifacts required to deliver relevant User Stories for IT. \n  Support IT Development efforts by analyzing and communicating with partners and IT, finding answers, and coordinating efforts to resolve issues and meet business commitments. \n  Support Product Owner in Agile ceremonies to help prioritize or find input from Market Access Operational team and other team members to ensure IT development or prioritization. \n  Facilitate meetings to gather customer approval, perform User Acceptance Testing and follow-up items after Agile Spring Demos. \n  Create or enhance PRM training documentation (videos, slides, and contents). \n  Communicate with partners to share training documentation and information on PRM current functionality, changes, and processes for Partners. \n  Track IT commitments and escalate if needed to meet delivery dates. \n  Help troubleshooting of ad-hoc issues with IT and Global Market Access team members. \n  Additional responsibilities to support peers in Global Market Access team when needed. \n \n \n  The Essentials - You Will Have: \n \n  Bachelor's Degree or equivalent experience \n  You can work within the United States without sponsorship. We will not sponsor for this role now or in the future. \n \n \n  The Preferred - You Might Also Have: \n \n  Bachelor's technical degree or equivalent experience \n  2+ years in requirements gathering, process mapping, wireframing, creating analysis artifacts, Agile and Software Development Life Cycle. \n  1+ year background in analytics or sales operations . \n  Understanding of CRM Systems, PowerApps, PowerBI, Websites, Jira, and Scrum methodology. \n  Understanding of APIs, Data Analysis, and data integrity. \n  Fluent in English \n \n  What We Offer: \n \n  Health Insurance including Medical, Dental and Vision \n  401k \n  Paid Time off \n  Parental and Caregiver Leave \n  Flexible Work Schedule where you will work with your manager to enjoy a work schedule that works with your personal life. \n  To learn more about our benefits package, please visit at www.raquickfind.com. \n \n \n  We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace. \n \n  At Rockwell Automation we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right person for this or other roles. \n \n  #LI-Remote \n  #LI-SH2 \n \n  Rockwell Automation is an Equal Opportunity Employer including disability and veterans. \n  If you are someone with a disability and you need assistance or reasonable accommodation during the application process, please contact our services team at +1 (844) 404-7427. \n \n  We are an Equal Opportunity Employer including disability and veterans. \n \n  If you are an individual with a disability and you need assistance or a reasonable accommodation during the application process, please contact our services team at +1 (844) 404-7247.", "cleaned_desc": "", "techs": ""}, "90c77ab393bc9466": {"terms": ["data analyst"], "salary_min": 91528.12, "salary_max": 115894.99, "title": "Business Analyst, Partner Relationship Management", "company": "Rockwell Automation", "desc": "Rockwell Automation is a global technology leader focused on helping the world\u2019s manufacturers be more productive, sustainable, and agile. With more than 28,000 employees who make the world better every day, we know we have something special. Behind our customers - amazing companies that help feed the world, provide life-saving medicine on a global scale, and focus on clean water and green mobility - our people are energized problem solvers that take pride in how the work we do changes the world for the better. \n \n  We welcome all makers, forward thinkers, and problem solvers who are looking for a place to do their best work. And if that\u2019s you we would love to have you join us! \n \n  Job Description \n  Think of an app you simply cannot live without. For our company, the Commercial Operations organization is like that app. It is a strategic and indispensable component of an organized and knowledgeable sales organization. \n  Rockwell Automation's partnerships are governed by multiple programs and include main elements such as partnership agreements, incentives, pricing, policies, and communications. These programs require a modern, the best PRM platform to maximize our digital experience and engagement with our partners \u2013 including Distributors, System Integrators, OEM Partners and Technology Partners. \n  Commercial Operations ties together strategy and roles, supported by digital applications and analytics. The PRM team is responsible for an important part of this effort. You will work with a diverse, dynamic PRM team. This position is remote-friendly. You will report to PRM Platform Owner and support PRM Platform Owner by engaging with the Market Access regional teams, Commercial Program Managers, Marketing, Partner Enablement and Engagement team, and other BU team members to gather IT requirements and manage projects related to PRM evolution and adoption. The Business Analyst will be a PRM subject-matter expert able to demonstrate current functionality and provide support and analysis to identify or meet our needs. \n \n  Your Responsibilities: \n \n \n  Guide Business Analysis activities and create Agile artifacts required to deliver relevant User Stories for IT. \n  Support IT Development efforts by analyzing and communicating with partners and IT, finding answers, and coordinating efforts to resolve issues and meet business commitments. \n  Support Product Owner in Agile ceremonies to help prioritize or find input from Market Access Operational team and other team members to ensure IT development or prioritization. \n  Facilitate meetings to gather customer approval, perform User Acceptance Testing and follow-up items after Agile Spring Demos. \n  Create or enhance PRM training documentation (videos, slides, and contents). \n  Communicate with partners to share training documentation and information on PRM current functionality, changes, and processes for Partners. \n  Track IT commitments and escalate if needed to meet delivery dates. \n  Help troubleshooting of ad-hoc issues with IT and Global Market Access team members. \n  Additional responsibilities to support peers in Global Market Access team when needed. \n \n \n  The Essentials - You Will Have: \n \n  Bachelor's Degree or equivalent experience \n  You can work within the United States without sponsorship. We will not sponsor for this role now or in the future. \n \n \n  The Preferred - You Might Also Have: \n \n  Bachelor's technical degree or equivalent experience \n  2+ years in requirements gathering, process mapping, wireframing, creating analysis artifacts, Agile and Software Development Life Cycle. \n  1+ year background in analytics or sales operations . \n  Understanding of CRM Systems, PowerApps, PowerBI, Websites, Jira, and Scrum methodology. \n  Understanding of APIs, Data Analysis, and data integrity. \n  Fluent in English \n \n  What We Offer: \n \n  Health Insurance including Medical, Dental and Vision \n  401k \n  Paid Time off \n  Parental and Caregiver Leave \n  Flexible Work Schedule where you will work with your manager to enjoy a work schedule that works with your personal life. \n  To learn more about our benefits package, please visit at www.raquickfind.com. \n \n \n  We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace. \n \n  At Rockwell Automation we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right person for this or other roles. \n \n  #LI-Remote \n  #LI-SH2 \n \n  Rockwell Automation is an Equal Opportunity Employer including disability and veterans. \n  If you are someone with a disability and you need assistance or reasonable accommodation during the application process, please contact our services team at +1 (844) 404-7427. \n \n  We are an Equal Opportunity Employer including disability and veterans. \n \n  If you are an individual with a disability and you need assistance or a reasonable accommodation during the application process, please contact our services team at +1 (844) 404-7247.", "cleaned_desc": "", "techs": ""}, "5927a127c6853845": {"terms": ["data analyst"], "salary_min": 60.0, "salary_max": 65.0, "title": "Senior Business Analyst (Health Insurance industry)", "company": "CT Solutions", "desc": "Role: Senior Business Analyst (Health Insurance industry) - only W2 \n Location: Remote from EST or CST \n Duration: 1-year contract \n Looking for a BSA with Agile experience that will be working on a scaled agility team building user stories and assisting the Product Owner in prioritization of user stories/epics for 2-week sprint cycles. Assisting in facilitating Agile ceremonies and responsible for scheduling and facilitating key meetings. \n Healthcare experience a plus and moves the candidate to the top of the list. Experience in Claims Admin and Billing Systems a plus. Experience in Azure DevOps a plus. \n Qualifications/Experience: \n 5+ years of experience working in health insurance \n Experience creating BRDs and FRDs from scratch \n Experience gathering requirements from executive stakeholders \n Facilitating meetings \n Excellent communication skills \n Job Type: Contract \n Pay: $60.00 - $65.00 per hour \n Experience level: \n \n 5 years \n \n Schedule: \n \n 8 hour shift \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "0ec28c543a023d32": {"terms": ["data analyst"], "salary_min": 84956.914, "salary_max": 107574.38, "title": "Manufacturing Business Analyst", "company": "Lake Shore Associates", "desc": "Manufacturing Business Analyst \n   \n Job Description: \n  Do you want to work within a fast-paced, challenging and very rewarding environment driving exciting new product development and innovation as well as continuous improvements? Do you have experience with industry-standard Business Analysis principles? Your responsibilities will include working with small cross-functional teams across the supply chain to develop high-level requirements for an operation to insource manufacturing processes. Working at least 3 days in the office. \n   \n The Skills You Bring : \n \n  Review and analyze business requirements for manufacturing processes.  \n Partner with manufacturing business teams, leadership, and stakeholders to facilitate and translate business strategies and requirements into current and future state process maps, specifications, and user stories. \n Responsible for ensuring unique business needs are properly translated to functional specifications and supported by product components. \n Obtain necessary approvals and provide the appropriate level of planning, documentation, and reporting to stakeholders throughout the development lifecycle. \n Conduct validation and user testing with stakeholders to assess effectiveness and ensure that delivered solutions meet business needs and improve the user experience. \n Excellent written, oral communication and presentation skills. \n \n  The Expertise You Have: \n \n  5+ years of Business Analysis experience with a strong background in manufacturing systems, and exceptional problem-solving skills. \n Develop and maintain detailed documentation, including system requirements, process maps, user manuals, and standard operating procedures (SOPs). \n Business exposure across core Integrated Supply Chain (ISC) functions with deep experience in a complex manufacturing environment. \n \n  The Value You Deliver: \n   \n \n Results-Driven - understands and tenaciously works at meeting or exceeding goals. \n Ability to work in a fast-paced environment where a premium is placed on speed and accuracy. \n Work independent of supervision and quickly adapt to changing priorities. \n A team player that possesses the willingness and commitment to accomplish team goals. \n \n \n   \n \n This is a remote position.", "cleaned_desc": "", "techs": ""}, "9ede4284e442e64b": {"terms": ["data analyst"], "salary_min": 30.0, "salary_max": 35.0, "title": "Business Analyst (CRM) W2 Only", "company": "Infosoft", "desc": "Here are the job position details for your review: \n Job Title:  Business Analyst (CRM) Pay Rate: $30 to 35/Hr Duration:  9 Months Location:  Remote \n Shift Timings :  1st Shift (Pacific time zone)  -  8:00 AM to 5:00 PM Pacific time \n Responsibilities: \n \n Support a diverse community of 450+ business users (sales, marketing, education) and external hair professionals using SAP Cloud for Customer (C4C) \n Create user stories for end-to-end processes using SAP Cloud for Customer (C4C) in close collaboration with business experts, IT experts, and stakeholders from other teams \n Validate the solution against user story requirements, document discrepancies, and partner with IT colleagues to deliver high-quality, high-impact solutions \n Review and assess quarterly SAP release updates, document and communicate impact within the team and user community \n Create and continually improve training materials, lead periodic training sessions to promote application adoption, user understanding \n Respond to application and process questions, share knowledge, and provide a high level of customer service to application users \n \n Qualifications: \n \n 4-year college degree (Bachelors) in Information Systems preferred or other technical certification in a related field \n Minimum 2 years of experience in business consulting/analysis \n Professional beauty salon industry experience is not required but very attractive \n Prior experience with sales process concepts and SAP Cloud for Customer (C4C) preferred \n Knowledge of change management techniques a plus \n Highly proficient in Microsoft Office - Word, Excel, PowerPoint, Outlook, Teams \n Excellent communication skills, both oral and written, with an ability to express complex topics in business terms \n High level of passion, initiative, and assertiveness \n Helps others succeed \u2013 a true team player \n Demonstrated collaborative approach to problem-solving \n Self-starter able to effectively manage multiple, simultaneous tasks remotely with limited supervision \n \n Job Types: Contract, Full-time \n Salary: $30.00 - $35.00 per hour \n Schedule: \n \n Monday to Friday \n \n Application Question(s): \n \n \"Please share alternate email\" \n \n Experience: \n \n SAP Cloud for Customer (C4C): 2 years (Preferred) \n User stories, requirements, and document discrepancies: 2 years (Required) \n \n Work Location: Remote", "cleaned_desc": " Professional beauty salon industry experience is not required but very attractive \n Prior experience with sales process concepts and SAP Cloud for Customer (C4C) preferred \n Knowledge of change management techniques a plus \n Highly proficient in Microsoft Office - Word, Excel, PowerPoint, Outlook, Teams \n Excellent communication skills, both oral and written, with an ability to express complex topics in business terms \n High level of passion, initiative, and assertiveness \n Helps others succeed \u2013 a true team player \n Demonstrated collaborative approach to problem-solving ", "techs": ["sap cloud for customer (c4c)", "microsoft office (word", "excel", "powerpoint", "outlook", "teams)"]}, "5b4b5a10f11813cb": {"terms": ["data analyst"], "salary_min": null, "salary_max": null, "title": "Senior Operations Analyst", "company": "Work from Home", "desc": "Introduction \n  Do you have the career opportunities as a(an) Senior Operations Analyst you want with your current employer? We have an exciting opportunity for you to join Work from Home which is part of the nation's leading provider of healthcare services, HCA Healthcare. \n  Benefits \n  Work from Home, offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include: \n \n  Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation. \n  Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more. \n  Free counseling services and resources for emotional, physical and financial wellbeing   \n  401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)   \n  Employee Stock Purchase Plan with 10% off HCA Healthcare stock   \n  Family support through fertility and family building benefits with Progyny and adoption assistance.   \n  Referral services for child, elder and pet care, home and auto repair, event planning and more   \n  Consumer discounts through Abenity and Consumer Discounts   \n  Retirement readiness, rollover assistance services and preferred banking partnerships   \n  Education assistance (tuition, student loan, certification support, dependent scholarships)   \n  Colleague recognition program   \n  Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)   \n  Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income. \n \n  Learn more about Employee Benefits \n  Note: Eligibility for benefits may vary by location.   \n Our teams are a committed, caring group of colleagues. Do you want to work as a(an) Senior Operations Analyst where your passion for creating positive patient interactions are valued? If you are dedicated to caring for the well-being of others, this could be your next opportunity. We want your knowledge and expertise! \n  Job Summary and Qualifications \n \n  The Senior Operations Analyst has a strong understanding of revenue cycle both at the macro and micro level. They answer complex operational questions through both detail account level review and trend analysis. They are an integral part of any process improvement team and are able to analyze data from multiple internal systems, but also payer systems. They provide key support when implementing new policies or systems and are able to independently monitor to ensure expected results are achieved. The Senior Operations analyst collaborates well with others but is also able to conceive and execute process improvement independently. The Senior Financial Analyst will work closely with Parallon Executive Management and be able to clearly and accurately key concepts and learnings to a broad audience. \n  What you will do in this role: \n \n \n  Builds a strong understanding of Parallon revenue cycle operations, including business processes, tools, and technologies  \n Analyzes data from internal tools at both the macro and micro levels to understand trends or assist with process improvement. \n  Gathers needed information from various systems independently. For example, queries eligibility systems, calls payers, explores all avenues when researching accounts.  \n Interprets data from the payer and other sources, understanding how it relates to our processes. \n  Synthesizes results from account analysis into actionable data.  \n Determines relevant patterns and trends that will drive efficiencies. \n  Works with IT&S and other departments to assist with requirements for automation projects. \n  Collaborates with varying stakeholders and conveys key learnings accurately and clearly in a way that is readily understood boy others. \n  Alerts and escalates anomalies quickly and appropriately. \n  Constantly encounters new technology and systems and learns how they fit it into the overall revenue cycle \n  Uses understanding of revenue cycle to research and explain month end variances to Executive leadership  \n Analyzes accounts receivable and is instrumental in ensuring it is properly stated at month end. Effectively communicates items that need to be addressed by the Shared Service Center. \n  Understands SSC and Revenue Cycle operations and technologies as well as company-wide technologies currently in place. Understands HCA and/or other healthcare company operations and accounting. \n  Plays a role in the development of month-end reports for internal and external use. \n  Supports department in retrieving and organizing the necessary data for reporting.  \n Communicate results of analysis to various levels of the organization including Executive Leadership. \n \n \n  What qualifications you will need: \n \n \n  Bachelor\u2019s Degree required \n  Minimum 5+ years relevant experience \n \n   \n \n Parallon  provides full-service revenue cycle management, or total patient account resolution, for HCA Healthcare. Our services include scheduling, registration, insurance verification, hospital billing, revenue integrity, collections, payment compliance, credentialing, health information management, customer service, payroll and physician billing. We also provide full-service revenue cycle management as well as targeted solutions, such as Medicaid Eligibility, for external clients across the country. Parallon has over 17,000 colleagues, and serves close to 1,000 hospitals and 3,000 physician practices, all making an impact on patients, providers and their communities. \n  HCA Healthcare has been recognized as one of the World\u2019s Most Ethical Companies\u00ae by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses. \n \n \n \n \"Bricks and mortar do not make a hospital. People do.\"- Dr. Thomas Frist, Sr.  HCA Healthcare Co-Founder \n If you are looking for an opportunity that provides satisfaction and personal growth, we encourage you to apply for our Senior Operations Analyst opening. We promptly review all applications. Highly qualified candidates will be contacted for interviews.  Unlock the possibilities and apply today! \n  We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.", "cleaned_desc": "", "techs": ""}, "6776a52393332bd5": {"terms": ["data analyst"], "salary_min": 85577.87, "salary_max": 108360.65, "title": "Manufacturing Business Analyst", "company": "Lake Shore Associates", "desc": "Manufacturing Business Analyst \n   \n Job Description: \n  Do you want to work within a fast-paced, challenging and very rewarding environment driving exciting new product development and innovation as well as continuous improvements? Do you have experience with industry-standard Business Analysis principles? Your responsibilities will include working with small cross-functional teams across the supply chain to develop high-level requirements for an operation to insource manufacturing processes. Working at least 3 days in the office. \n   \n The Skills You Bring : \n \n  Review and analyze business requirements for manufacturing processes.  \n Partner with manufacturing business teams, leadership, and stakeholders to facilitate and translate business strategies and requirements into current and future state process maps, specifications, and user stories. \n Responsible for ensuring unique business needs are properly translated to functional specifications and supported by product components. \n Obtain necessary approvals and provide the appropriate level of planning, documentation, and reporting to stakeholders throughout the development lifecycle. \n Conduct validation and user testing with stakeholders to assess effectiveness and ensure that delivered solutions meet business needs and improve the user experience. \n Excellent written, oral communication and presentation skills. \n \n  The Expertise You Have: \n \n  5+ years of Business Analysis experience with a strong background in manufacturing systems, and exceptional problem-solving skills. \n Develop and maintain detailed documentation, including system requirements, process maps, user manuals, and standard operating procedures (SOPs). \n Business exposure across core Integrated Supply Chain (ISC) functions with deep experience in a complex manufacturing environment. \n \n  The Value You Deliver: \n   \n \n Results-Driven - understands and tenaciously works at meeting or exceeding goals. \n Ability to work in a fast-paced environment where a premium is placed on speed and accuracy. \n Work independent of supervision and quickly adapt to changing priorities. \n A team player that possesses the willingness and commitment to accomplish team goals. \n \n \n   \n \n This is a remote position.", "cleaned_desc": "", "techs": ""}, "5d838317c26e1556": {"terms": ["data analyst"], "salary_min": null, "salary_max": null, "title": "Data & Analytics Sales Lead", "company": "AllCloud", "desc": "As a Data & Analytics Sales Lead at AllCloud, you will be the face of our business owning the full sales life cycle and coaching Account Executives. This sales role requires enterprise sales experience with high-level decision-makers, but with a strong focus on prospecting and developing new opportunities to build a new book of business. Candidates will be experienced in managing customer engagements in both virtual and face-to-face environments. The candidate will have B2B sales experience engaging with multiple personas from engineers to C-suite executives, and they will understand how to identify, develop, negotiate, and close complex AllCloud Data & Analytics opportunities. Candidate should be a self-starter with a proven track record of exceeding monthly and quarterly goals. \n \n  Summary of Key Responsibilities \n \n  The Data & Analytics Sales Lead position is a quota-carrying lead position \n  Possess a comprehensive understanding of AllCloud Data & Analytics solutions and connect that knowledge directly to customer ROI \n  Proactively identify opportunity signals and generate expansion opportunities (up-sell, add-on, cross-sell, etc.) from existing clients in AllCloud\u2019s Data and Analytics practice. \n  Selling Professional Services for AllCloud\u2019s Data & Analytics Practice in North America. \n  Identify and engage with potential clients, understanding their data and analytics needs, and presenting tailored solutions. \n  Build and maintain strong relationships with key stakeholders, including C-level executives, to understand their business objectives and position our services as a solution. \n  Collaborate with our technical team to design and propose data analytics solutions that align with client requirements. \n  Stay up-to-date with the latest trends and advancements in data analytics, Snowflake, and Matillion technologies to effectively articulate the value proposition to clients. \n \n  Requirements: \n  \n \n Sales experience selling Professional Services to Enterprise prospects and clients in a quota carrying role. \n  Technical background in Hyperscaler Cloud, Data, Analytics, IT or High-Tech Professional Services \n  Demonstrated track record of leading a team, meeting sales goals, executing campaigns, and negotiating deals in an enterprise environment. \n  Experience with a two-step sales process developing relationships with AWS, Snowflake etc. as a lead source. \n  Experience with a defined sales methodology used to produce quarter-over-quarter target attainment \n  Proven track record closing million dollar+ deals. Ability to connect relationally with both technical and business executives \n  Ability to work independently, or as a member of a team, depending on the current situation. \n  Experience negotiating large contracts, working through conflicts, and working with legal teams and procurement processes \n \n \n  Advantages \n \n  Enterprise transformation experience, including a track record of selling targeted engagements that will underpin Data & Analytics and AllCloud strategy \n  Experience working in a matrixed organization \n \n \n  Why Work For Us? \n  Our team inspires progress in each other and in our customers through our relentless pursuit of excellence; you will work with leaders who promote learning and personal development. We offer competitive salaries, bonus incentives, benefits, flexible hours, and mentoring. Apply now to become part of the team. \n  AllCloud is also committed to diversity, equity, and inclusion, and we want teammates who are similarly passionate about pushing for change in our industry. \n  AllCloud is for everyone, and we\u2019re committed to anti-racist work. We welcome and employ people regardless of race, color, gender identity, religion, genetic information, parental or pregnancy status, national origin, sexual orientation, age, citizenship, marital status, disability, or Veteran status. We are proud to be an equal opportunity employer.", "cleaned_desc": "", "techs": ""}, "05475c6b4097ef20": {"terms": ["data analyst"], "salary_min": 77578.914, "salary_max": 98232.195, "title": "Business Process Transformation / Automation ROI Analyst - Department of Veterans Affairs (VBA Central Office)", "company": "Sprezzatura Management Consulting", "desc": "Position Description: \n \n \n  Sprezzatura is seeking a skilled and experienced Business Process Transformation / Automation ROI Analyst to help improve efficiency, effectiveness, and customer service within the VBA Central Office. In this role you must demonstrate the ability to design recommendations for advancing automation initiatives, preferably in a Fed benefits environment. The candidate will demonstrate an understanding of the automation transformation decision making process by understanding, analyzing, and diagramming the nuts and bolts of the existing processes and apply this learning to create proposed redesign solutions. Then candidate must then be able to effectively articulate (written and verbal) these proposed redesign solutions, to both internal and external senior level stakeholders. It\u2019s key that the candidate have experience documenting, mapping, and redesigning business processes and procedures utilizing BPMN and be able to articulate recommendations expressed in terms of benefit to the Veteran. Candidate will contribute to the VA's mission of providing exceptional services to our nation's veterans by consistently demonstrating knowledge of process improvement methodologies and change management strategies throughout the period of performance. \n \n \n  Responsibilities: \n \n Process Analysis and Assessment: Conduct comprehensive analysis of existing business processes within the VBA Central Office. Identify areas for improvement, inefficiencies, and opportunities to enhance effectiveness and customer satisfaction. \n Process Redesign and Optimization: Collaborate with stakeholders to redesign processes, incorporating industry best practices and innovative solutions. Streamline workflows, eliminate bottlenecks, and implement process improvements to drive efficiency and productivity. \n Change Management: Develop and implement change management strategies to support process reengineering initiatives. Engage stakeholders, communicate changes effectively, and address concerns to ensure smooth implementation and adoption of new processes. \n Data Collection and Analysis: Collect and analyze data related to process performance, productivity, and customer feedback. Identify key performance indicators (KPIs) to measure process effectiveness and track progress towards improvement goals. \n Process Documentation and Standardization: Document redesigned processes, including workflows, standard operating procedures (SOPs), and process maps. Ensure clear and comprehensive documentation to guide process implementation and facilitate knowledge transfer. \n Stakeholder Engagement: Collaborate with cross-functional teams, including subject matter experts, end-users, and leadership, to gather requirements, validate process designs, and obtain buy-in for proposed changes. Foster a collaborative and inclusive approach to process reengineering. \n Training and Education: Develop training materials and conduct training sessions to educate stakeholders on new processes and associated tools. Provide ongoing support and guidance to ensure successful process implementation and adherence. \n Continuous Improvement: Establish mechanisms for ongoing monitoring, evaluation, and refinement of reengineered processes. Collect feedback, measure performance against KPIs, and identify opportunities for further optimization and innovation. \n \n \n \n  Qualifications: \n \n Must have a minimum of 2 years\u2019 proven experience in business process reengineering, process improvement, or related roles. \n Must have experience supporting Veterans Affairs, Veterans Benefits Administration (VBA). Preferably to have background in Medical Disability, Claims, Compensation, Pension, VA Digital Services, VBMS or VBA Backend systems. . \n Understanding of Process Improvement Methodologies; Lean Six Sigma, Business Process Reengineering, Business Process Notation, Root Cause Analysis \n Bachelor\u2019s or Master\u2019s degree in one of the following disciplines is desired; Business Administration, Industrial Engineering, Operations Management, Applied Mathematics, Statistics, Supply Chain Management, Information Systems or Information Technology  \n Experience in analyzing and redesigning complex business processes in large organizations. \n Familiarity with process mapping techniques and process modeling tools. \n Solid understanding of change management principles and experience in driving organizational change. \n Strong analytical and problem-solving skills, with the ability to identify root causes and develop practical solutions. \n Excellent communication and interpersonal skills to effectively engage stakeholders at all levels of the organization. \n Understanding of government operations and experience in the healthcare industry is a plus. \n \n Transitioning military and/or Veterans with relevant experience are invited to apply. Sprezzatura is an equal opportunity employer. Sprezzatura offers benefits including healthcare, 401K, vacation, and paid sick leave. \n \n  Company Description \n  Sprezzatura Management Consulting, LLC (www.sprezzmc.com) is a Washington, DC-area Service-Disabled Veteran-Owned Small Business (SDVOSB) that enables government transformation by supplying insight and leadership at the intersection of people, processes, and technology. We apply knowledge, project, and life-cycle management best practices to catalyze", "cleaned_desc": "", "techs": ""}, "e3bcd9cf81c1cb1b": {"terms": ["data analyst"], "salary_min": 65473.316, "salary_max": 82903.805, "title": "Business Implementation Analyst (Selerix Experience Required) (Remote)", "company": "BenefitHub", "desc": "Would you like to be part of one of the fastest-growing benefits technology companies in the U.S.? Join our mission to help millions of people improve the quality of their daily lives through accessing the world\u2019s largest selection of personally relevant benefits. Help us achieve our goal of becoming the world\u2019s most broadly accessed gateway for group and individual benefits. Already used by thousands of employers and their millions of employees, our platform is disrupting the $1 trillion benefits market. Clients include 5 of the top 10 largest employers in the U.S. and dozens of Fortune 500 companies. Now that we are one of the industry\u2019s fastest-growing companies, we have no plans of slowing down. \n    BenefitHub does not currently offer H-1B Sponsorship \n \n  Key Duties & Responsibilities: \n \n Point of contact for the testing process to ensure a smooth file integration and implementation onto the various BenefitHub platforms, including BenefitHub portal, SmartPay, and SmartEnroll products.  \n Translates business/user requirements to technical specifications and non-functional requirements for development using various approaches (user stories, system flows, context diagrams, source-to-target mappings, etc.) \n Serve as liaison for business, developer, and QA to ensure solution delivery aligns to system requirements \n Responsible for file testing activities with clients/vendors. Configure test environment, test accounts, create and maintain testing plans. \n SFTP testing \n Responsible for production support of files and sftp.  \n Process Improvements \u2013 Recommend and implement changes via automation, simplification and efficiency measures. \n \n \n \n  Qualifications: \n \n Bachelor's degree in a Computer Science or related technical discipline; or the equivalent combination of education; technical certifications or training; or work experience \n 3+ years combined experience in one or more of: Technical or EDI Analyst, Data Analyst, application development \n Experience utilizing the Selerix application to configure electronic files \n Experience creating mapping documents between Third party systems and internal data structures \n Experience with Health Insurance/Employee Benefits industry \n Competency in understanding and documenting: use cases, user stories, system/process flows, system/business context diagrams, and business/systems requirements documentation \n Experience in application support and troubleshooting in a multi-application environment \n Experience with creating test scenarios based on the specific plan design and carrying out electronic file testing \n Strong analytical, interpersonal, problem solving, organizational, and time management skills \n Can work independently; is self-motivated and deadline driven \n Able to work on multiple concurrent tasks, both administrative and project-based \n Strong written skills for developing and maintaining system documentation \n Able to translate business concepts to technical audience and technical concepts to business \n Flexible team player, able to work collaboratively with and through others, supportive of a cooperative work environment \n Ability to communicate effectively with team members, customers, partners and IT and Business leadership. \n An understanding of how system components enable business processes and ability to communicate clearly to IT and business partners and to at all times be able to interact with others in an effective manner. As one example, the incumbent must translate business requirements into technical requirements in a format that is understandable to both the business user and the technical implementer \n Familiarity with production acceptance and change management processes \n Solid working knowledge of Microsoft product suite \n \n \n \n  BenefitHub is proud to offer highly competitive salaries along with a very generous perks and benefits package for full-time Team Members: \n \n \n Medical and Dental (Employer paid 75% of monthly contributions) \n Vision Insurance (Employer paid 100% of monthly contributions) \n Domestic Partner Coverage for all insurance plans \n 100% employer-paid Life Insurance \n 401(K) retirement plan options (including employer contribution) \n Generous Paid Time Off (PTO) \n Generous Volunteer Paid Time Off (VTO) Program \n Stock options from the company \n Generous Paid Paternity and Maternity Leave Policies \n 11 Holidays (Including office closed day after Thanksgiving) \n Flexible work schedules \n Paid wellness rewards program \n Flexible Spending Account program \n Voluntary Benefit Programs (STD, LTD, Life Insurance, Accident, Cancer) \n \n \n \n  Please click here for the Voluntary Self-Identification of Disability Form  which can be emailed to  humanresources@benefithub.com", "cleaned_desc": "", "techs": ""}, "a2894935e16a37d7": {"terms": ["data analyst"], "salary_min": 107153.18, "salary_max": 135679.8, "title": "Senior Business Process Transformation / Automation ROI Analyst- Department of Veterans Affairs (VBA Central Office)", "company": "Sprezzatura Management Consulting", "desc": "Position Description:  \n Sprezzatura is seeking a skilled and experienced Senior Business Process Transformation / Automation ROI Analyst to help improve efficiency, effectiveness, and customer service within the VBA Central Office. As a Senior Business Process Transformation / Automation ROI Analyst, you must demonstrate the ability to build ROI business cases for supporting and advancing automation initiatives, preferably in Fed benefits environment. The candidate will demonstrate an understanding of the automation transformation decision making process by understanding, analyzing, and diagramming the nuts and bolts of the existing processes and apply this learning to create proposed redesign solutions. Then candidate must then be able to effectively articulate (written and verbal) these proposed redesign solutions, to both internal and external senior level stakeholders. It\u2019s key that the candidate have experience documenting, mapping, and redesigning business processes and procedures utilizing BPMN and be able to articulate recommendations expressed in terms of benefit to the Veteran and Return on Investment. Candidate will contribute to the VA's mission of providing exceptional services to our nation's veterans by consistently demonstrating knowledge of process improvement methodologies and change management strategies throughout the period of performance. \n \n \n  Responsibilities: \n \n Process Analysis and Assessment: Conduct comprehensive analysis of existing business processes within the VBA Central Office. Identify areas for improvement, inefficiencies, and opportunities to enhance effectiveness and customer satisfaction. \n Process Redesign and Optimization: Collaborate with stakeholders to redesign processes, incorporating industry best practices and innovative solutions. Streamline workflows, eliminate bottlenecks, and implement process improvements to drive efficiency and productivity. \n Change Management: Develop and implement change management strategies to support process reengineering initiatives. Engage stakeholders, communicate changes effectively, and address concerns to ensure smooth implementation and adoption of new processes. \n Data Collection and Analysis: Collect and analyze data related to process performance, productivity, and customer feedback. Identify key performance indicators (KPIs) to measure process effectiveness and track progress towards improvement goals. \n Process Documentation and Standardization: Document redesigned processes, including workflows, standard operating procedures (SOPs), and process maps. Ensure clear and comprehensive documentation to guide process implementation and facilitate knowledge transfer. \n Stakeholder Engagement: Collaborate with cross-functional teams, including subject matter experts, end-users, and leadership, to gather requirements, validate process designs, and obtain buy-in for proposed changes. Foster a collaborative and inclusive approach to process reengineering. \n Training and Education: Develop training materials and conduct training sessions to educate stakeholders on new processes and associated tools. Provide ongoing support and guidance to ensure successful process implementation and adherence. \n Continuous Improvement: Establish mechanisms for ongoing monitoring, evaluation, and refinement of reengineered processes. Collect feedback, measure performance against KPIs, and identify opportunities for further optimization and innovation. \n \n \n \n  Qualifications: \n \n Must have a minimum of 5 years\u2019 proven experience in business process reengineering, process improvement, or related roles. \n Must have experience supporting Veterans Affairs, Veterans Benefits Administration (VBA). Preferably to have background in Claims, Compensation, Pension, VBMS or VBA Backend systems.  \n Understanding of Process Improvement Methodologies; Lean Six Sigma, Business Process Reengineering, Business Process Notation, Root Cause Analysis \n Bachelor\u2019s or Master\u2019s degree in one of the following disciplines is desired; Business Administration, Industrial Engineering, Operations Management, Applied Mathematics, Statistics, Supply Chain Management, Information Systems or Information Technology  \n Experience in analyzing and redesigning complex business processes in large organizations. \n Familiarity with process mapping techniques and process modeling tools. \n Solid understanding of change management principles and experience in driving organizational change. \n Strong analytical and problem-solving skills, with the ability to identify root causes and develop practical solutions. \n Excellent communication and interpersonal skills to effectively engage stakeholders at all levels of the organization. \n Understanding of government operations and experience in the healthcare industry is a plus. \n \n Transitioning military and/or Veterans with relevant experience are invited to apply. Sprezzatura is an equal opportunity employer. Sprezzatura offers benefits including healthcare, 401K, vacation, and paid sick leave. \n \n  Company Description \n  Sprezzatura Management Consulting, LLC (www.sprezzmc.com) is a Washington, DC-area Service-Disabled Veteran-Owned Small Business (SDVOSB) that enables government transformation by supplying insight and leadership at the intersection of people, processes, and technology. We apply knowledge, project, and life-cycle management best practices to catalyze change.", "cleaned_desc": " Familiarity with process mapping techniques and process modeling tools. \n Solid understanding of change management principles and experience in driving organizational change. \n Strong analytical and problem-solving skills, with the ability to identify root causes and develop practical solutions. \n Excellent communication and interpersonal skills to effectively engage stakeholders at all levels of the organization. \n Understanding of government operations and experience in the healthcare industry is a plus. \n ", "techs": ["process mapping techniques", "process modeling tools", "change management principles", "analytical skills", "problem-solving skills", "communication skills", "interpersonal skills", "government operations", "healthcare industry"]}, "647ea171d3d71bb4": {"terms": ["data analyst"], "salary_min": 103272.23, "salary_max": 130765.65, "title": "Sr. Functional/Business Analyst (Remote)", "company": "IT Concepts", "desc": "Founded in 2003, IT Concepts\u2019 core values \u2013 customer-centricity, teamwork, driven to deliver, innovation, and integrity \u2013 ensure we work together to be the best, realize objectives, and make a positive impact in our communities. We intentionally created and sustain our ITC culture that embraces change, experimentation, continuous learning, and improvement. We bring our design thinking problem solving approach that challenges assumptions, prioritizes curiosity, and invites complexity to deliver innovative, efficient, and effective solutions. As we continue to grow in the support of our government customers, we are looking for driven and innovative individuals to join our team. \n  IT Concepts is seeking to hire a qualified Sr. Functional/Business Analyst to support our DOC customer. The Salesforce platform and application software development will support ITA\u2019s Office of the Chief Information Officer (OCIO) in the delivery and maintenance of the mission critical Salesforce platform and applications. As a Sr. Functional/Business Analyst you will be part of a team helping DOC, ITA achieve mission-driven success. \n  Responsibilities: \n \n  Collaborate with stakeholders and the development team, analyzing business and user needs to capture and refine system requirements and develop user stories  \n \n \n Collaborate with stakeholders and team members to provide system requirement refinement, user story confirmation,  \n Translate business requirements into solutions in the Salesforce environment.  \n Understand the impacts of business requirements on the configuration, processes, and workflows within Salesforce.  \n Collaborate on UAT test scenarios, developing test scripts, UI/UX testing (as needed) and conducting testing and verification exercises.  \n Lead the development and monitoring of user acceptance criteria.  \n \n \n Ensuring high-quality deliveries of code through the development and maintenance of test cases, for initial validation of new features as well as for existing features updated due to requirements/environmental changes.  \n Performing functional testing, regression testing and keep track of all the new developments.  \n Producing the evaluated test/defects reports and take part in software walk through.  \n Working alongside a team of Business Analysts, Business stakeholders and developers to deliver Agile solutions.  \n Participating in the product increment planning and document test plan and test scenarios in collaboration with the product owner, product analysts and engineers.  \n \n \n Conducting peer reviews of test cases and test documents.  \n \n Requirements \n \n Bachelor\u2019s degree in Computer Science, Mathematics, Information Technology or related field  \n Experience supporting Salesforce platform and application development efforts for federal government.  \n 4+ years of experience with requirements analysis, solutioning, functional/technical design documentation, and training \n    \n 4+ years of Salesforce Business Analyst experience, providing functional support for the development of Salesforce applications.  \n Experience using common enterprise agile planning tools, such as Jira.  \n Proven experience working in Agile/Scrum development methodologies.  \n Strong ability to guide and work with cross-functional teams.  \n Experience supporting federal IT projects.  \n \n \n Excellent at identifying and solving problems.  \n Must be a U.S Citizen  \n Required Trailhead Certifications:  \n \n \n \n Salesforce Business Analyst; or  \n Salesforce Certified Administrator  \n \n \n \n Preferred :  \n \n Experience being part of Salesforce implementations at the Department of Commerce  \n \n Benefits \n  The Company   \n We believe in generating success collaboratively, enabling long-term mission success, and building trust for the next challenge. With you as our partner, let\u2019s solve challenges, think innovatively, and maximize impact. As a valued member of our ITC community, you have the unique opportunity to work in a diverse range of technology and business career paths, all while supporting our nation and delivering innovative technology solutions. We are a close community of experts that pride ourselves on creating an environment defined by teamwork, dedication, and excellence.  \n We hold three ISO certifications (27001:2013, 20000-1:2011, 9001:2015) and two CMMI ML 3 ratings (DEV and SVC).  \n Industry Recognition   \n Growth | Inc 5000\u2019s Fastest Growing Private Companies, DC Metro List Fastest Growing; Washington Business Journal: Fastest Growing Companies, Top Performing Small Technology Companies in Greater D.C.  \n Culture | Northern Virginia Technology Council Tech 100 Honoree; Virginia Best Place to Work; Washington Business Journal: Best Places to Work, Corporate Diversity Index Winner \u2013 Mid-Size Companies, Companies Owned by People of Color; Department of Labor\u2019s HireVets for our work helping veterans transition; SECAF Award of Excellence finalist; Victory Military Friendly Brand; Virginia Values Veterans (V3); Cystic Fibrosis Foundation Corporate Breath Award  \n Benefits   \n We offer great benefits \u2013 Competitive Paid Time Off, Medical, Dental and Vision Insurance, Identity Theft Protection, Legal Resources Coverage, 401(k) with company matching with NO vesting period. ITC Health benefits have $0 premium for certain plans as an employee.  \n We invest in our employees \u2013 Every employee is eligible for education reimbursement for certifications, degrees, or professional development. Reimbursement amount may fluctuate due to IRS limitations. We want you to grow as an expert and a leader and offer flexibility for you to take a course, complete a certification, or other professional growth and networking. We are committed to supporting your curiosity and sustaining a culture that prioritizes commitment to continuous professional development.  \n We work hard, we play hard. ITC is committed to incorporating fun into every day. We dedicate funds for activities \u2013 virtual and in-person \u2013 e.g., we have free tickets to Nationals games available upon employee request; we host happy hours, holiday events, fitness events, and annual celebrations. In alignment with our commitment to our communities, we host and attend charity galas/events. We believe in appreciating your commitment and building a positive workspace for you to be creative, innovative, and happy.  \n AAEO & VEVRAA   \n IT Concepts is an Affirmative Action/Equal Opportunity employer and a VEVRAA (Vietnam Era Veterans' Readjustment Assistance Act) Federal Contractor. As such, any personnel decisions (hire, promotion, job status, etc.) on applicants and/or employees are based on merit, qualifications, competence and business needs, not on race, color, citizenship status, national origin, ancestry, sexual orientation, gender identity, age, religion, creed, physical or mental disability, pregnancy, childbirth or related medical condition, genetic information of the employee or family member of the employee, marital status, veteran status, political affiliation, or any other factor protected by federal, state or local law.  \n IT Concepts maintains a strong commitment to compliance with VEVRAA and other applicable federal, state, and local laws governing equal employment opportunity. We have developed comprehensive policies and procedures to ensure that our hiring practices align with these requirements.  \n As a part of our VEVRAA compliance efforts, [Company Name] has established an affirmative action plan that outlines our commitment to the recruitment, hiring, and advancement of protected veterans. This plan is regularly reviewed and updated to ensure its effectiveness.  \n We encourage protected veterans to self-identify during the application process. This information is strictly confidential and will only be used for reporting and compliance purposes as required by law. Providing this information is voluntary, and it will not impact your eligibility for employment.  \n Our commitment to equal employment opportunity extends beyond legal compliance. We are dedicated to fostering an inclusive workplace where all employees, including protected veterans, are treated with dignity, respect, and fairness.  \n How to Apply   \n To apply to IT Concept Position- Please click on the: \u201cApply for this Job\u201d button at the bottom of this Job Description or the button at the top: \u201cApplication.\u201d You can upload your resume and complete all the application steps. You must submit the application for IT Concepts to receive. If you need alternative application methods, please email careers@useitc.com and request assistance.  \n Accommodations   \n To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable Accommodations may be made to enable qualified individuals with disabilities to perform the essential functions.", "cleaned_desc": " Ensuring high-quality deliveries of code through the development and maintenance of test cases, for initial validation of new features as well as for existing features updated due to requirements/environmental changes.  \n Performing functional testing, regression testing and keep track of all the new developments.  \n Producing the evaluated test/defects reports and take part in software walk through.  \n Working alongside a team of Business Analysts, Business stakeholders and developers to deliver Agile solutions.  \n Participating in the product increment planning and document test plan and test scenarios in collaboration with the product owner, product analysts and engineers.  \n \n \n Conducting peer reviews of test cases and test documents.  \n \n Requirements \n \n Bachelor\u2019s degree in Computer Science, Mathematics, Information Technology or related field  \n Experience supporting Salesforce platform and application development efforts for federal government.  \n 4+ years of experience with requirements analysis, solutioning, functional/technical design documentation, and training      \n 4+ years of Salesforce Business Analyst experience, providing functional support for the development of Salesforce applications.  \n Experience using common enterprise agile planning tools, such as Jira.  \n Proven experience working in Agile/Scrum development methodologies.  \n Strong ability to guide and work with cross-functional teams.  \n Experience supporting federal IT projects.  \n \n \n Excellent at identifying and solving problems.  \n Must be a U.S Citizen  \n Required Trailhead Certifications:  \n \n \n ", "techs": ["jira", "salesforce", "trailhead"]}, "ea1e2d471790adc6": {"terms": ["data analyst"], "salary_min": 79523.94, "salary_max": 100695.03, "title": "Senior Business Analyst - Mine Planning", "company": "Freeport McMoRan", "desc": "Freeport-McMoRan  is a leading international mining company with headquarters in Phoenix, Arizona. We operate large, long-lived, geographically diverse assets with significant proven and probable reserves of copper, gold, and molybdenum. The company has a dynamic portfolio of operating, expansion and growth projects in the copper industry. Freeport-McMoRan is one of the world\u2019s largest publicly traded copper producers, the world\u2019s largest producer of molybdenum and a significant gold producer. We have a long and successful history of conducting our business in a safe, highly efficient and socially-responsible manner. \n  We have the assets, the talent, the drive and the financial strength to provide attractive and rewarding careers of our employees. We encourage you to take the time to explore the opportunity to advance your career at Freeport-McMoRan. \n  Please note:  This position has the possibility to work remotely up to 100% of the time. The position will require occasional travel to the Phoenix corporate offices and/or site locations . This position may be performed anywhere in the U.S. except California, Connecticut, New Hampshire, Massachusetts, Michigan, Illinois, Kentucky and New York. Additional states may be excluded from remote work based on business factors. Should the positions shift to in-office work in the future, the company will offer relocation benefits at that time should the position meet the established eligibility for these benefits. \n \n \n \n  Description \n \n \n \n      Provide support for all work related to the evaluation and reporting of mineral reserves and resources company wide. Use advanced data management methods to provide customers with reports and information for mineral reserves and resources. Perform complex economic analysis of projects.\n     \n \n  Manage and verify data received from sites regarding mineral reserves and resources from all sites. Maintain and consolidate into one database. Prepare consolidation reports. Prepare and review mineral reserves and resources disclosures for the annual 10-K report. Make recommendations for areas of study and improvements. \n  Identify and lead opportunities for improved data analyses for use in evaluating mineral reserves and resources, including assisting with the development of costs, data collection, and collaboration with corporate groups and mine site personnel. \n  Manage the supporting information for the reserve process SOX controls, including coordination with external auditors regarding testing of SOX controls \n  Review Life of Mine (LOM) plans with respect to technical assumptions and financial estimates and analysis. \n  Prepare consolidated metals production plans and forecast summaries. \n  Prepare and review economic analysis of projects. \n  Make project recommendations and lead project implementations. \n  Act as Sharepoint Owner for Central Mine Planning - Reserves Group file and data management. \n  Perform other duties as requested. \n \n \n \n \n \n  Qualifications \n \n \n \n  Minimum Qualifications \n \n \n \n  Bachelor's degree in Accounting, Finance, Science or Mining Engineering  and  five (5) years of related experience \n  Proficiency with Microsoft Office applications \n  Strong analytical skills and technical accounting/finance/engineering skills, with close attention to detail \n  Strong leadership skills, and presentation of a positive and professional image \n  Strong interpersonal skills, and strong written and verbal communication skills \n  Ability to prioritize work, manage multiple processes, and maintain composure under pressure to meet deadlines in a fast-paced Corporate environment \n  Skill in following safety practices and recognizing hazards \n \n \n \n  Preferred \n \n \n \n  None \n \n \n \n  Criteria/Conditions \n \n \n \n  Ability to understand and apply verbal and written work and safety-related instructions and procedures given in English \n  Ability to communicate in English with respect to job assignments, job procedures, and applicable safety standards \n  Must be able to work in a potentially stressful environment \n  Position is in busy, non-smoking office located in downtown Phoenix, AZ \n  Location requires mobility in an office environment; each floor is accessible by elevator and internal staircase \n  Occasionally work may be performed in a mine, outdoor or manufacturing plant setting \n  Must be able to frequently sit, stand and walk \n  Must be able to frequently lift and carry up to ten (10) pounds \n  Personal protective equipment is required when performing work in a mine, outdoor, manufacturing or plant environment, including hard hat, hearing protection, safety glasses, safety footwear, and as needed, respirator, rubber steel-toe boots, protective clothing, gloves and any other protective equipment as required \n  Freeport-McMoRan promotes a drug/alcohol free work environment through the use of mandatory pre-employment drug testing and on-going random drug testing as per applicable State Laws \n \n \n \n \n  At Freeport, we are committed to providing an employment package that recognizes excellence, rewards value and impact, and encourages safe production. Benefits and compensation are foundational elements of this package, along with career development opportunities, job progression and a culture supported by our core values, among others. Learn more at: FCX Jobs - Working Here \n \n  Benefits: \n  We provide an industry-leading benefits package with some of the lowest cost to employees \u2013 offering health, wellness, life insurance, paid time off, retirement savings and more. These benefits are available to you and your dependents starting day one. Our comprehensive benefits program is important to how we support the health and wellness of employees and their families. For further benefits information please click here:  Benefits Details \n \n  Compensation: \n  The estimated annual pay range for this role is currently  $79,000 - $109,000.  This range reflects base salary only and does not include bonus payments, benefits or retirement contributions. Actual base pay is determined by experience, qualifications, skills and other job-related factors. This role is eligible for additional discretionary and incentive payment considerations based on company and individual performance. More details will be shared during the hiring process. To view an example of a Total Rewards Estimate for this role click here:  Total Rewards Estimate \n \n  Safety / Work Conditions: \n  Candidates will be required to participate in a post-offer, pre-employment medical examination for the following positions which may have essential job duties that can impact both their own safety and the safety of others:  \n \n Site-based positions, or positions which require unescorted access to site-based operational areas, which are held by employees who are required to receive MSHA, OSHA, DOT, HAZWOPER and/or Hazard Recognition Training; or \n  Positions which are held by employees who operate equipment, machinery or motor vehicles in furtherance of performing the essential functions of their job duties, including operating motor vehicles while on Company business or travel (for this purpose \u201cmotor vehicles\u201d includes Company owned or leased motor vehicles and personal motor vehicles used by employees in furtherance of Company business or while on Company travel). \n \n \n  Equal Opportunity Employer", "cleaned_desc": " \n  Bachelor's degree in Accounting, Finance, Science or Mining Engineering  and  five (5) years of related experience \n  Proficiency with Microsoft Office applications \n  Strong analytical skills and technical accounting/finance/engineering skills, with close attention to detail \n  Strong leadership skills, and presentation of a positive and professional image \n  Strong interpersonal skills, and strong written and verbal communication skills \n  Ability to prioritize work, manage multiple processes, and maintain composure under pressure to meet deadlines in a fast-paced Corporate environment \n  Skill in following safety practices and recognizing hazards \n \n \n \n  Preferred \n \n \n \n  None \n ", "techs": ["microsoft office applications"]}, "999d059b733e57dc": {"terms": ["data analyst"], "salary_min": 55000.0, "salary_max": 65000.0, "title": "Excel Reporting & Data Analyst", "company": "Robert Half", "desc": "Robert Half is looking for a certified Data Analyst to join our client! This is a remote position but the candidate must live in one of the following states where they operate out of: Virginia, North Carolina Georgia Tennessee Florida Ohio Texas. Please only apply if you currently reside in one of those states. \n \n  Excel Reporting & Data Analyst will work as part of a team processing, auditing, and researching data and data related issues in support of the inventory accounting services we provide to our healthcare customers. \n \n \n Responsibilities: \n \n \n Perform thorough examinations of all working data and finished reports \n Conduct and report internal audits and evaluations \n Implement and improve quality control operating procedures \n Validate data received from customers \n \n Qualifications: \n \n \n Required \n  o Strong attention to detail \n o Strong analytical and critical thinking skills \n o Excellent written and verbal communication skills \n o Proficiency with MS Excel \n \n \n Preferred \n  o Experience with MS Access \n o Experience with advanced text editors (e.g. Notepad++) \n o Healthcare supply chain experience \n \n  Technology Doesn't Change the World, People Do. \u00ae \n \n  Robert Half is the world\u2019s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles. \n \n  Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity \u2013 even on the go. Download the Robert Half app and get 1-tap apply, instant notifications for AI-matched jobs, and more. \n \n  All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit roberthalf.gobenefits.net for more information. \n \n  \u00a9 2023 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking \u201cApply Now,\u201d you\u2019re agreeing to Robert Half\u2019s Terms of Use .", "cleaned_desc": " \n \n Required \n  o Strong attention to detail \n o Strong analytical and critical thinking skills \n o Excellent written and verbal communication skills \n o Proficiency with MS Excel   \n \n Preferred \n  o Experience with MS Access \n o Experience with advanced text editors (e.g. Notepad++) \n o Healthcare supply chain experience \n ", "techs": ["ms excel", "ms access", "advanced text editors (e.g. notepad++)", "healthcare supply chain experience"]}, "25b430bb7775df6d": {"terms": ["data analyst"], "salary_min": 38.0, "salary_max": 41.0, "title": "Data Reporting Analyst", "company": "The Intersect Group", "desc": "SAP Success Factors Data Analyst \n Hybrid- Nashville, TN \n 3 month contract (possibility of extension or full time) \n $38-41/hr \n Work Auth:  US CITIZEN OR GC HOLDER ONLY (NO C2C OR SPONSORSHIP) \n Responsibilities: \n \n Meet with business stakeholders to document reporting requirements for HR in SuccessFactors \n Create report design/wireframe and identify gaps if any from requirements / current reporting solution \n Work with Reporting Team to build reports and test \n Assist functional team with troubleshooting functional/reporting gaps at the process and/or field level \n Test resolutions for defects and document findings in the tracking system \n Work with business users for UAT, troubleshoot issues with reports, and determine data discrepancies \n Support Report rollout to the user community, through communication and participation in OCM activities \n \n Requirements: \n \n Has SAP SuccessFactors Employee Central modules knowledge \n Experience reporting in SAP Success Factors \n Experience with developing reports in SAP Analytic Cloud \n Experience with reporting, including table, dashboard, canvas, and story reporting \n Develop reporting requirements for business or HR requests \n Build ORD (Report Center) reports from SF to meet the reporting requests \n Hands-on with Static and runtime filters \n \n Preferred: \n \n Experience with Databases / Data Lake (ex: Snowflake) \n Experience writing test scripts \n Experience writing system and end-user documentation** This will start as a 3 month contract with extensions and possibility of permanent \n \n Job Types: Full-time, Contract \n Pay: $38.00 - $41.00 per hour \n Benefits: \n \n Dental insurance \n Health insurance \n Vision insurance \n \n Experience level: \n \n 2 years \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Experience: \n \n SAP SuccessFactors: 2 years (Required) \n Data analytics: 2 years (Required) \n SAP Analytic Cloud: 2 years (Preferred) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "8139f3d9413e4573": {"terms": ["data analyst"], "salary_min": 100000.0, "salary_max": 114000.0, "title": "Data Analyst (Remote)", "company": "Hi-Tek Professionals", "desc": "This opportunity will be supporting the Program Management portfolio within the Rotary & Mission Systems (RMS) Chief Data & Analytics Office (CDAO). The selected candidate will work closely the Program Management team, team leads, team members across the organization, and key partners to drive effective business operations, increase business value, and enable strategic growth utilizing analytical and project management skills in a highly collaborative environment. The goal of the RMS CDAO is to drive affordability and innovation through data-driven decision making throughout the company and our team is committed to promote the adoption and use of analytical approaches across the organization. \n This position is 100% remote and the candidate will be expected to work Monday - Thursday, 10 hours per day (with no work on Fridays). \n Duties and responsibilities may include but are not limited to: \n \n Support the facilitation and management of the RMS CDAO\u2019s Scaled Agile Framework (SAFe) implementation and general agile team support \n Develop data-driven analytics products and other deliverables in support of the RMS CDAO \n Enable cross-organization collaboration with RMS CDAO team members as well as other stakeholders \n Support the implementation as well as iterative improvement of team processes in an effort to reduce the burden and roadblocks for the team \n Engage with the team in support of various talent and workforce strategy initiatives focused on team culture, growth and development, training, and innovation. \n \n Job Types: Full-time, Contract \n Pay: $100,000.00 - $114,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Experience level: \n \n 5 years \n \n Experience: \n \n Agile Scrum: 5 years (Required) \n Scaled Agile Framework (SAFe): 5 years (Required) \n Meeting facilitation: 5 years (Required) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "117e46ce41a7899b": {"terms": ["data analyst"], "salary_min": 42.0, "salary_max": 42.0, "title": "Business Analyst - HEDIS Analyst", "company": "TotalMed", "desc": "HEDIS Analyst - Business Analyst \n Pay:  $42.00/hour Hours : Monday - Friday 8:00am - 5:00pm Location : Remote - Must be local to Chicago \n HEDIS Analyst - Business Analyst - Details : \n \n Drafting, preparation, and maintenance of SBC\u2019s (summary of benefit coverage). \n Working in a team environment this position will interpret health benefits and prepare documentation that will be distributed to health plan members. \n This position will work with teams that file documentation with Federal and State Insurance Regulators and will be required to maintain the highest level of quality. \n This position will be required to regularly speak with legal counsel, product teams, and other internal stakeholders to ensure that all language on SBC\u2019s are interpreted truthfully and accurately. \n The ideal candidate will possess a curiosity about insurance regulations/standards, process improvement, and the overall operation of contract administration. \n \n HEDIS Analyst - Business Analyst - Education and Skills : \n \n 2+ years of Data Analyst experience \n Healthcare experience \n Experience working with Benefits \n JIRA usage and experience \n Bachelor's Degree required \n \n #INDPM \n Job Type: Full-time \n Pay: $42.00 per hour \n Benefits: \n \n Dental insurance \n Health insurance \n Vision insurance \n \n Experience level: \n \n 2 years \n \n Schedule: \n \n 8 hour shift \n Day shift \n Monday to Friday \n No nights \n No weekends \n \n Education: \n \n Bachelor's (Required) \n \n Experience: \n \n Data Analysis: 2 years (Required) \n Business Analysis: 2 years (Preferred) \n Healthcare: 1 year (Required) \n working with benefits: 1 year (Preferred) \n Project Management: 1 year (Preferred) \n JIRA: 1 year (Preferred) \n Lotus Notes: 1 year (Preferred) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "492cf86393b3fe2a": {"terms": ["data analyst"], "salary_min": 45.0, "salary_max": 50.0, "title": "Business Analyst/Communications Support Analyst", "company": "The Judge Group", "desc": "The Communications Support Analyst / Business Analyst/Technical Writer for the Client Security Communications & Outreach team will focus on communications and training activities, which educate and raise awareness of security topics for colleagues within the organization and the customers we serve. \n Description: \n \n This role requires the aptitude to learn and participate in the development and coordination of communications and training initiatives related to the efforts of the Client Security Communications & Outreach team. \n To be successful, the candidate must be driven, well-organized and able to thrive in a collaborative, fast-paced environment, while completing tasks as scheduled. \n \n Must Have:  \n \n Communications \n Content Development \n \n Responsibilities: \n \n Draft, review and edit communication drafts according to business requirements and ensure compliance with Client?s standards and brand guidelines. \n Participate in the implementation of communication and training strategies. \n Draft user guides and other training materials according to business needs and requirements \n Adhere to standards and best practices related to the creation of communication and training deliverables. \n \n Required Qualifications: \n \n Excellent verbal and written communications, including the ability to collect, analyze, and disseminate information with a high level of accuracy and attention to detail. \n Knowledge of communication techniques and methods, including alternative ways to inform via written, oral, and visual media. \n High school diploma and 3-5 years of corporate communications or similar experience \n Exceptional organizational and interpersonal skills including teamwork and time management. \n Highly efficient in Microsoft Office suite \n \n Preferred Qualifications: \n \n Knowledge of cyber security. \n Previous experience effectively establishing cross functional partnership. \n Able to convey security capabilities and threats in business terms. \n Experience working and delivering in an agile environment. \n \n Competencies:  \n \n Conceptual and analytical thinking \n Accountability and attention to detail \n Commitment to quality \n Continuous improvement \n Teamwork and collaboration \n Customer service orientation \n Written and verbal communication skills \n Initiative and focused execution \n Flexibility \n Time management and organizational Skills.", "cleaned_desc": " Adhere to standards and best practices related to the creation of communication and training deliverables. \n \n Required Qualifications: \n \n Excellent verbal and written communications, including the ability to collect, analyze, and disseminate information with a high level of accuracy and attention to detail. \n Knowledge of communication techniques and methods, including alternative ways to inform via written, oral, and visual media. \n High school diploma and 3-5 years of corporate communications or similar experience \n Exceptional organizational and interpersonal skills including teamwork and time management. ", "techs": ["none"]}, "2f159f5e53b374f8": {"terms": ["data analyst"], "salary_min": 102234.73, "salary_max": 129451.945, "title": "Business Systems Analyst - Hybrid in Rolling Meadows", "company": "Gallagher", "desc": "About Us: \n  \n  Gallagher is a global leader in insurance, risk management and consulting services. We help businesses grow, communities thrive and people prosper. We live a culture defined by The Gallagher Way, our set of shared values and guiding tenets. A culture driven by our people, over 45,000 strong, serving our clients with customized solutions that will protect them and fuel their futures.\n  \n \n \n \n \n  As a member of our benefits and HR consulting team, you\u2019ll help our clients - employers of all sizes, across all industries - build workplaces that work better. \n  Overview: \n  \n  Gallagher Benefit Services is looking for a Business Systems Analyst to join its growing team!\n  \n \n   Focused on supporting processes and implementing process improvements, this role is tasked with formulating and defining systems scope and objectives through research and fact-finding combined with an understanding of applicable business systems and industry requirements under general supervision.\n  \n \n \n  Please see position details below:\n  \n \n  This is Hybrid position (in office and work from home). Office is located at 2850 West Gold Road, Rolling Meadows IL. \n  The hours are Monday through Friday with some opportunities for extended hours during the week and weekends. \n  Temp to Perm \n  Responsibilities: \n  \n Analyzes business processes, identifies process deficiencies, researches solutions and plans and coordinates the remediation of these deficiencies. \n  Analyze data, write up source to target mapping, read data models and data mining and data profiling. \n  Assists with the design and implementation of operational and system enhancements to ensure production capabilities are both effective and efficient. \n  As appropriate, write and validate business requirements and conduct system tests. \n  Develops and maintains relationships with key stakeholders and technology staff, including off-shore personnel, to ensure that communication is open and ongoing. \n  Provides administrative and project support to the team. \n  Supporting operational and project time reporting. \n  Establishing, administering, and maintaining project and knowledge management processes and tools. \n  Ensuring appropriate controls are in place and at time work with internal and external auditors on documentation. \n  Qualifications: \n  \n Required: \n \n \n  Bachelor's degree in Business, Information Technology and Engineering. \n  Strong 10 years related experience required including project support and data analysis. \n  Understanding of financial system, including but not limited to AP, AR, RevRec, Budgeting, Bank Management, Project management, Time and Billing and Estimation. \n  Excellent verbal and written communication skills. \n  Advance level proficiency with Microsoft Office include Word, Excel, Power Point, Visio, SQL, MS Dynamics 365 F&O, Power BI, Azure DevOps and Planview. \n \n \n  Preferred: \n \n \n  PMI-PBA certification \n \n \n  Work Traits: \n \n \n  Demonstrated ability to prioritize and manage multiple tasks simultaneously \n  Good analytical and problem solving skills \n  Strong attention to details. \n \n \n \n  #contingent\n  \n \n   #LI-KB3\n   Additional Information: \n  \n  Click Here to review our U.S. Eligibility Requirements\n  \n \n \n  We offer competitive salaries and benefits, including: medical/dental/vision plans, life and accident insurance, 401(K), employee stock purchase plan, educational expense reimbursement, employee assistance program, flexible work hours (availability varies by office and job function), training programs, matching gift program, and more.", "cleaned_desc": "  Assists with the design and implementation of operational and system enhancements to ensure production capabilities are both effective and efficient. \n  As appropriate, write and validate business requirements and conduct system tests. \n  Develops and maintains relationships with key stakeholders and technology staff, including off-shore personnel, to ensure that communication is open and ongoing. \n  Provides administrative and project support to the team. \n  Supporting operational and project time reporting. \n  Establishing, administering, and maintaining project and knowledge management processes and tools. \n  Ensuring appropriate controls are in place and at time work with internal and external auditors on documentation. \n  Qualifications: \n  \n Required: \n \n \n  Bachelor's degree in Business, Information Technology and Engineering. \n  Strong 10 years related experience required including project support and data analysis.    Understanding of financial system, including but not limited to AP, AR, RevRec, Budgeting, Bank Management, Project management, Time and Billing and Estimation. \n  Excellent verbal and written communication skills. \n  Advance level proficiency with Microsoft Office include Word, Excel, Power Point, Visio, SQL, MS Dynamics 365 F&O, Power BI, Azure DevOps and Planview. \n \n \n  Preferred: \n \n \n  PMI-PBA certification \n \n \n  Work Traits: \n \n ", "techs": ["microsoft office", "word", "excel", "powerpoint", "visio", "sql", "ms dynamics 365 f&o", "power bi", "azure devops", "planview", "pmi-pba certification"]}, "438fc7e93cb715d4": {"terms": ["data engineer"], "salary_min": 92281.75, "salary_max": 116849.266, "title": "Data Engineer", "company": "BRIGHTWAY INSURANCE LLC", "desc": "About Brightway:  \n Brightway is an insurance agency that has achieved remarkable success over the past 15 years and is now set for an even more exciting phase of growth as we begin a digital transformation to become a modern, high-tech digital agency. \n As we move towards a tech-driven future, we recognize the importance of investing in a strong technology team. The strategic decision to modernize our tech team and infrastructure will play a key role in accelerating our company's growth beyond what is possible in the industry today. You are joining us in our earliest days as we build and shape our team, culture, platform, and other decisions. This has the advantage of giving you a great opportunity to help shape and determine our direction, balanced by the fact that where we are today isn\u2019t where we want to be. We are looking for an individual who is excited by that challenge and is interested in helping us solve many of the business, team, tech, and process we have today. \n By choosing to join us now, you'll become an integral part of shaping a high-tech digital insurance agency from its early days. Your contributions will leave a lasting impact, shaping the very foundation of our future success. \n About the role:  \n As a Data Engineer at Brightway, you will join our small team of skilled data engineers and analysts to design and construct a modern data platform. You will work closely with your team to build and maintain a secure and performant data architecture from top to bottom including data pipelines, integrations, modeling, transformations, and optimizations. Additionally, you and your team members will collaborate directly with stakeholders across the business to deliver reports and analytics that enable informed decision making. This team has the exciting challenge and opportunity to create the platform that will be vital to Brightway\u2019s continued success and ability to make data-driven business decisions. \n Education and Experience \n This position requires a Bachelor\u2019s degree and 2-6 years\u2019 experience in data analytics or a related field. Applicants should be extremely competent in RDBMS, Azure Data Factory, SQL, and Python. \n Job Responsibilities & Skills \n \n 2-5 years of relevant hands-on working knowledge on the following: \n Azure Data Factory, SQL Server / Azure Synapse  \n SQL Queries / Stored Procs / Performance tuning/ Query optimization \n Experience with Python, Data Bricks, Data Lakes, and Power BI is a plus. \n Experience in Agile methodologies is a plus. \n Uses independent judgment and decision making to complete some tasks. \n Proactively identifies workflow or system improvements and articulate benefits clearly. \n Ability to learn, embrace and put into practice new concepts and skills. \n Able to gather and analyze facts and data to solve very complex problems, draw inferences, weigh alternatives and present logical solutions. \n Expert at managing own time, activities, and resources; acts as a role model to others inside, and outside, the department. \n A desire and ability to deliver information accurately and on time. Strong problem-solving skills and attention to detail. \n Excellent written and verbal communicator; must be able to convey complex ideas in a clear and concise manner. \n Knowledge of insurance industry is a plus. \n \n Employees at Brightway enjoy: \n \n Competitive salaries (based on research) \n Paid Time Off \n Medical/Dental/Life/Disability Insurance \n 401(k) plan with up to a 4% company-match \n Tuition reimbursement (regardless of course of study)", "cleaned_desc": " Education and Experience \n This position requires a Bachelor\u2019s degree and 2-6 years\u2019 experience in data analytics or a related field. Applicants should be extremely competent in RDBMS, Azure Data Factory, SQL, and Python. \n Job Responsibilities & Skills \n \n 2-5 years of relevant hands-on working knowledge on the following: \n Azure Data Factory, SQL Server / Azure Synapse    SQL Queries / Stored Procs / Performance tuning/ Query optimization \n Experience with Python, Data Bricks, Data Lakes, and Power BI is a plus. \n Experience in Agile methodologies is a plus. \n Uses independent judgment and decision making to complete some tasks. \n Proactively identifies workflow or system improvements and articulate benefits clearly. \n Ability to learn, embrace and put into practice new concepts and skills. ", "techs": ["rdbms", "azure data factory", "sql", "python", "sql server", "azure synapse", "sql queries", "stored procs", "performance tuning", "query optimization", "data bricks", "data lakes", "power bi", "agile methodologies"]}, "8c52480c38772d40": {"terms": ["data engineer"], "salary_min": 97508.79, "salary_max": 123467.85, "title": "Data Engineer", "company": "Tandem Theory", "desc": "Tandem Theory is made up of people just like you \u2013 smart, funny folks who know that building an agency is and should be a fun endeavor. Working here is very much like working for a startup \u2013 things move quickly, and you get to wear many hats. You\u2019ll get the opportunity to work on high-level projects, and the opportunity to engage directly with clients. Tandem Theory offers a flat, collaborative working environment, right from the comfort of your home. But please don\u2019t wear your pajamas to work. \n \n  We are seeking a  Data Engineer  to join our growing   team. \n  Here is a summary of the role: \n  This experienced data engineer must be able to develop and manage data pipelines   across the disciplinary teams and support the requirements of the business: planning, design, documentation, integration, quality assurance, automation including marketing and business intelligence integrations. \n \n  The Data Engineer must demonstrate strong domain expertise in  relational and non-relational data models, emerging technologies , command of key programming languages and be the influential and objective technologist voice in the room. We are looking for a  modern-day ETL/ELT/IPaaS developer , one who can work in today\u2019s analytical environment and understands the marketing landscape, where disparate data is everywhere. We do whatever it takes to solve problems, while embracing change each day. \n \n  What you will do: \n \n  Manage data transformation development life cycle for marketing and analytical platforms \n  Responsible for collecting requirements, performing data exploration and developing gap analysis \n  Recommend and design database solutions \n  Manage critical steps to success and timelines \n  Develop data pipelines and integrations with complex source data and API services \n  Responsible for moving data across multiple platforms using established ETL/ELT patterns \n  Leverage 3rd party data and tools for data enrichment and hygiene \n  Coordinate development of best practices needed to support new and evolving sources of data used in marketing communications, transaction processing and analytics \n  Conduct data platform performance tuning, troubleshooting, support and capacity estimation \n  Develop, adhere to, and be a proponent for data warehouse development standards \n  Develop and maintain documentation for all developed data movement/transformation processes, including process flow and data mapping \n  Create and manage audit reporting and quality improvement processes to ensure accurate and consistent delivery of data \n  Work closely with internal and external teams to rapidly troubleshoot issues \n \n \n  What you should have: \n \n \n \n  Bachelor's degree or equivalent combination of education and experience preferred, in computer sciences or Information systems \n  An understanding of data warehouse design principles \n  3-5 years' experience with an RDBMS such as PostgreSQL, SQL Server, SnowFlake & Google Big Query \n  3-5 years ETL/ELT development in SQL, Python, AWS and Apache Airflow \n  Advanced SQL query techniques and automating import/export tasks \n  Proficient in creating stored procedures, index and database IO performance tuning \n  A big plus if candidate has any of the following skills and experiences: \n  API development \n  DBA duties and responsibilities \n  Python, Spark \n  Stitch, Fivetran \n  Proactively identify, troubleshoot and resolve complex data integrity issues \n  Reverse engineer existing SQL code to determine business logic \n  Agency and marketing experience preferred \n  Strong analytical skills, with ability to communicate technical details \n  Excellent verbal and written communication skills \n  Work with minimal supervision in an agile environment \n  Ability to pivot quickly based on changes in analytical needs \n \n \n  We are looking for a candidate to start on a contract basis with us with full-time hours, and then transition to a full-time employee. \n \n  Here is a little bit about us: \n \n  Tandem Theory is a culture-first agency built to create better client experiences. We deliver on this promise by focusing on three core behaviors -  teamwork, transparency, and trust  \u2013 the three things that we found were most often missing from the traditional agency experience. They\u2019re not just words on a poster. We apply them every day to guide our internal and client relationships, and our clients tell us they feel the difference. \n \n  Headquartered in Dallas, Texas, Tandem partners with several leading brands. These brands value working with an agency led by a proven team that provides simple but impactful solutions, transparent pricing, and performance-based compensation. \n \n  Other than waking up and being excited to get to work each day, here are some additional benefits: \n  The salary for this exempt-level position will be based on experience and qualifications within an established pay range. Benefits include 85% employer paid medical, eligibility to participate in vision and dental insurance as well company paid STD, LTD coverage plus a flat amount company paid toward Basic Life and AD&D, as well as a generous 401(k) retirement contribution plan eligible after 90-days of employment. \n \n  Working Conditions: \n  We operate as a work, not live, from anywhere model and utilize the office space as a co-working space as needed. That said, whether in the office or at a remote location, we try to operate in the traditional office norm model. You will be working with a computer, telephone, copier, or other office equipment as necessary for long stretches of time. There may be some occasional long, irregular hours. You are expected to communicate effectively (verbal and written), and maintain professionalism under stress. Occasionally you will need to bend, pull, push and/or reach to access job-related materials. You will also be expected to work with frequent interruptions such as Slack messages, emails, phone calls, drive-by visits to your desk, whether that is from a co-worker at the office or your dog barking while working remotely. All our calls are held on video, as it is important to see facial expressions and reactions to work in a collaborative environment. \n \n  Eligibility \n \n  All new employees must provide documented proof of their identity and employment authorization and pass a background screening process. No work Visa sponsorships available at this time. We are currently not accepting applicants that reside in the State of California. \n \n  Here is where we are located in case you are curious: \n  Tandem Theory, LLC \n  15400 Knoll Trail #503 \n  Dallas, TX 75248 \n  #LI-JOB", "cleaned_desc": "Tandem Theory is made up of people just like you \u2013 smart, funny folks who know that building an agency is and should be a fun endeavor. Working here is very much like working for a startup \u2013 things move quickly, and you get to wear many hats. You\u2019ll get the opportunity to work on high-level projects, and the opportunity to engage directly with clients. Tandem Theory offers a flat, collaborative working environment, right from the comfort of your home. But please don\u2019t wear your pajamas to work. \n \n  We are seeking a  Data Engineer  to join our growing   team. \n  Here is a summary of the role: \n  This experienced data engineer must be able to develop and manage data pipelines   across the disciplinary teams and support the requirements of the business: planning, design, documentation, integration, quality assurance, automation including marketing and business intelligence integrations. \n \n  The Data Engineer must demonstrate strong domain expertise in  relational and non-relational data models, emerging technologies , command of key programming languages and be the influential and objective technologist voice in the room. We are looking for a  modern-day ETL/ELT/IPaaS developer , one who can work in today\u2019s analytical environment and understands the marketing landscape, where disparate data is everywhere. We do whatever it takes to solve problems, while embracing change each day. \n \n  What you will do: \n \n  Manage data transformation development life cycle for marketing and analytical platforms \n  Responsible for collecting requirements, performing data exploration and developing gap analysis \n  Recommend and design database solutions \n  Manage critical steps to success and timelines    Develop data pipelines and integrations with complex source data and API services \n  Responsible for moving data across multiple platforms using established ETL/ELT patterns \n  Leverage 3rd party data and tools for data enrichment and hygiene \n  Coordinate development of best practices needed to support new and evolving sources of data used in marketing communications, transaction processing and analytics \n  Conduct data platform performance tuning, troubleshooting, support and capacity estimation \n  Develop, adhere to, and be a proponent for data warehouse development standards \n  Develop and maintain documentation for all developed data movement/transformation processes, including process flow and data mapping \n  Create and manage audit reporting and quality improvement processes to ensure accurate and consistent delivery of data \n  Work closely with internal and external teams to rapidly troubleshoot issues \n \n \n  What you should have: \n \n   \n  Bachelor's degree or equivalent combination of education and experience preferred, in computer sciences or Information systems \n  An understanding of data warehouse design principles \n  3-5 years' experience with an RDBMS such as PostgreSQL, SQL Server, SnowFlake & Google Big Query \n  3-5 years ETL/ELT development in SQL, Python, AWS and Apache Airflow \n  Advanced SQL query techniques and automating import/export tasks \n  Proficient in creating stored procedures, index and database IO performance tuning \n  A big plus if candidate has any of the following skills and experiences: \n  API development \n  DBA duties and responsibilities \n  Python, Spark \n  Stitch, Fivetran \n  Proactively identify, troubleshoot and resolve complex data integrity issues \n  Reverse engineer existing SQL code to determine business logic ", "techs": ["tandem theory", "data engineer", "data pipelines", "disciplinary teams", "business intelligence", "relational data models", "non-relational data models", "emerging technologies", "programming languages", "etl/elt/ipaas developer", "marketing landscape", "data transformation", "database solutions", "etl/elt patterns", "3rd party data", "data enrichment", "data hygiene", "data platform performance tuning", "troubleshooting", "data warehouse development standards", "audit reporting", "quality improvement processes", "rdbms", "postgresql", "sql server", "snowflake", "google big query", "sql", "python", "aws", "apache airflow", "advanced sql query techniques", "stored procedures", "index", "database io performance tuning", "api development", "dba duties and responsibilities", "spark", "stitch", "fivetran", "complex data integrity issues", "reverse engineer sql code"]}, "6b314a71f6217428": {"terms": ["data engineer"], "salary_min": 125002.73, "salary_max": 150540.92, "title": "Cloud Data Engineer", "company": "Titan Cloud Software, LLC", "desc": "ESSENTIAL FUNCTIONS: \n \n Direct ownership of the data processing architecture, design, development, and operational management. \n Execute and deliver full data lifecycle including ingestion, storage, querying, and processing using big data and cloud processing toolsets. \n Develop and deploy AWS CloudFormation templates and work with DevOps engineers for automated deployments and infrastructure management. \n Work closely with Product and Engineering team to continuously evaluate the lowest cost of ownership of data modeling, throughput processing, retention, and lifecycle management. \n Identifies, researches, and resolves technical problems. \n Perform other duties as assigned \n \n EDUCATION/EXPERIENCE/SKILL REQUIREMENTS: \n \n Bachelor\u2019s degree in information technology or related field or additional work experience in lieu of degree is required. \n 5+ years of experience in data processing and cloud engineering roles is required. \n 3+ years of experience with AWS development, SNS, SQS, Kinesis, S3 and Data Lake, AWS Glue, Lambda Functions via C#/.Net and Python, Serverless development, Rest API, Powershell, and SQL required. \n Experience with Microsoft PowerBI or other Analytical BI platforms preferred \n Exceptional customer service skills with experience in troubleshooting, diagnosing, and solving complex engineering and throughput challenges. \n \n LICENSES/DESIGNATIONS/CERTIFICATIONS: \n \n AWS Certification(s) Preferred \n \n PHYSICAL REQUIREMENTS: \n \n Sitting for prolonged periods of time in a temperature-controlled office environment for approximately 8 hours per day. \n Frequent and repetitive use of fingers, hands, and arms for tasks such as typing, writing, moving a mouse, etc. \n Occasionally lifts office equipment, supplies, or other items of approximately 50lbs. \n Ability to travel by air, vehicle, or other means approximately 5% of the time. \n To accommodate travel needs, must be able to transport suitcase and laptop estimated at 40lbs. \n To accommodate travel needs, must be able to operate rental or personal motor vehicle. \n \n Job Type: Full-time \n Pay: $125,002.73 - $150,540.92 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n Paid time off \n Vision insurance \n \n Experience level: \n \n 5 years \n \n Schedule: \n \n 8 hour shift \n Day shift \n Monday to Friday \n \n Application Question(s): \n \n Will you now or in the future require visa sponsorship from your employer to continue working in the United States? \n \n Experience: \n \n AWS Development: 3 years (Required) \n \n Work Location: Remote", "cleaned_desc": "ESSENTIAL FUNCTIONS: \n \n Direct ownership of the data processing architecture, design, development, and operational management. \n Execute and deliver full data lifecycle including ingestion, storage, querying, and processing using big data and cloud processing toolsets. \n Develop and deploy AWS CloudFormation templates and work with DevOps engineers for automated deployments and infrastructure management. \n Work closely with Product and Engineering team to continuously evaluate the lowest cost of ownership of data modeling, throughput processing, retention, and lifecycle management. \n Identifies, researches, and resolves technical problems. \n Perform other duties as assigned \n \n EDUCATION/EXPERIENCE/SKILL REQUIREMENTS: \n   Bachelor\u2019s degree in information technology or related field or additional work experience in lieu of degree is required. \n 5+ years of experience in data processing and cloud engineering roles is required. \n 3+ years of experience with AWS development, SNS, SQS, Kinesis, S3 and Data Lake, AWS Glue, Lambda Functions via C#/.Net and Python, Serverless development, Rest API, Powershell, and SQL required. \n Experience with Microsoft PowerBI or other Analytical BI platforms preferred \n Exceptional customer service skills with experience in troubleshooting, diagnosing, and solving complex engineering and throughput challenges. \n \n LICENSES/DESIGNATIONS/CERTIFICATIONS: \n \n AWS Certification(s) Preferred \n \n PHYSICAL REQUIREMENTS: ", "techs": ["aws cloudformation", "devops", "aws glue", "lambda functions", "c#", ".net", "python", "rest api", "powershell", "sql", "microsoft powerbi"]}, "6df07d9838597d13": {"terms": ["data engineer"], "salary_min": 120000.0, "salary_max": 130000.0, "title": "GCP Data Engineer", "company": "Connvertex Technologies Inc.", "desc": "GCP \n Big Query \n Data Flow \n SQL \n Kafka \n Data Ingestion and Transformation \n Java / Spring Boot \n Job Type: Full-time \n Pay: $120,000.00 - $130,000.00 per year \n Schedule: \n \n 8 hour shift \n Day shift \n Monday to Friday \n \n Experience: \n \n Data engineering: 9 years (Required) \n GCP: 9 years (Required) \n Java: 6 years (Preferred) \n \n Work Location: Remote", "cleaned_desc": "", "techs": ""}, "47153f82ee6a5842": {"terms": ["data engineer"], "salary_min": 118998.49, "salary_max": 150678.61, "title": "Data Engineer II", "company": "dv01", "desc": "dv01 is lifting the curtain on the largest financial market in the world: structured finance. The $16+ trillion market is the backbone of everyday activities that empower financial freedom, from consolidating credit card debt and refinancing student loans, to buying a home and starting a small business. \n  dv01's data analytics platform brings unparalleled transparency into investment performance and risk for lenders and Wall Street investors in structured products. As a data-first company, we wrangle critical loan data and build modern analytical tools that enable strategic decision-making for responsible lending. In a nutshell, we're helping prevent a repeat of the 2008 global financial crisis by offering the data and tools required to make smarter data-driven decisions resulting in a safer world for all of us. \n  More than 400 of the largest financial institutions use dv01 for our coverage of over 75 million loans spanning mortgages, personal loans, auto, buy-now-pay-later programs, small business, and student loans. dv01 continues to expand coverage of new markets, adding loans monthly, and developing new technologies for the structured products universe. \n  To get a better idea of what a year at dv01 looks like, check out our 2022 Year in Review page here: https://dv01.co/year-in-review/2022/ If that looks like fun to you, get in touch because we'd love to hear from you. \n \n \n  YOU WILL: \n \n Own our data platform.  You will own the data platform that powers all of dv01's offerings. You will work with systems that ingest data to the cloud, perform transformations at scale, and deliver data to internal and external clients. Our platform code is written in   Scala to get the maximum performance from Apache Spark and Python for new projects to develop features quickly. Our tech stack is built on the Google Cloud Platform - leveraging Cloud Composer (Apache Airflow), Dataproc (Apache Spark), BigQuery, CloudSQL, GKE (kubernetes); in-house tooling - scheduling/monitoring tools, API crawlers, PDF parsers/scrapers, SFTP servers; and ad-hoc analysis services such as Databricks. \n Work extensively with open source technology.  You'll work heavily within the Spark ecosystem, as well as explore new open source technologies to solve customer needs. The skills you develop here will serve you well beyond dv01. \n Interact with a diverse team.  You will collaborate closely with other teams at dv01 to expand and improve the capabilities of our data platform. This includes working closely with several analyst teams to understand business domains, the Integrations team to resolve scaling and functionality pain points in data processing, the Internal Applications Team to build solutions for our internal workflows, and several Product Teams to scale data ingestion, storage, and database access to ship new products. \n Gain knowledge of the financial industry.  You'll have an exclusive view into the system that enables Americans to afford houses, cars, and college. You will learn about the participants, terminology, and mathematics behind this sector of the financial markets. \n \n \n \n  YOU ARE: \n \n Experienced with all aspects of working with big data.  You have 5+ years of professional data engineering experience working with large data sets and are intimately familiar with the construction of scalable ETL pipelines, intricacies of accurate data processing, and infrastructure involved with ensuring the reliability of hundreds of daily processes. \n A Google Cloud Platform enthusiast.  3+ years of experience architecting large-scale distributed computing systems with GCP technologies. \n A well-rounded engineer.  You have 5+ years of professional programming experience with  Python  or  Scala  and a deep appreciation for engineering fundamentals. You understand the importance of writing tests, designing systems for long term maintainability, and evaluating both sides of common engineering considerations. \n A systems engineer.  You are able to collaborate with a wide range of people from diverse backgrounds to investigate and solve problems, often integrating multiple in-house and cloud-based systems. You are able to balance your time between gathering information, teaching others, and contributing well-documented code. \n Proficient with data governance practices.  Familiar with governance concepts and have implemented systems in the past (e.g. collibra, atlan, etc.). You are familiar with both the technical and business aspects of data governance and understand how it all ties together with lineage processes, tagging data elements to assist with risk assessment, business requirement gathering, and impact analysis. \n \n NICE TO HAVES: \n \n Familiarity with GitHub Actions and Harness for CI/CD \n Comfortable with SQL \n Experience building data management systems (lineage, metadata, quality) \n Familiarity working in an agile environment \n Familiar working with workflow management tools like Jira and Confluence \n A background in Structured Finance \n \n In good faith our salary range for this role is $150,000 - $160,000 but are not tied to it. Final offer amount will be at the company's sole discretion and determined by multiple factors, including years and depth of experience, expertise, and other business considerations. \n  Our community is fueled by diverse people who welcome differing points of view and the opportunity to learn from each other. Our team is passionate about building a product people love and a culture where everyone can innovate and thrive. \n  BENEFITS & PERKS : \n \n Unlimited PTO . Unplug and rejuvenate, however you want\u2014whether that's vacationing on the beach or at home on a mental-health day. \n In-House Personal & Performance Development Coach.  Recharge with our in-house personal & performance development coach, who is here to listen and help guide in your self-development and overall wellness. \n $1,000 Learning & Development Fund . No matter where you are in your career, always invest in your future. We encourage you to attend conferences, take classes, and lead workshops. We also host hackathons, brunch & learns, and other employee-led learning opportunities. \n Remote-First Environment.  People thrive in a flexible and supportive environment that best invigorates them. You can work from your home, cafe, or hotel. You decide. \n Health Care and Financial Planning.  We offer a comprehensive medical, dental, and vision insurance package for you and your family. We also offer a 401(k) for you to contribute. \n Free Equinox Membership or $1,650 Annual Fitness Fund.  Regular exercise offers a plethora of mental and physical health benefits. You can either enroll in an all-access Equinox membership or at your preferred gym. Or take advantage of our fitness fund, which can be used toward at-home workout equipment (yes, including a Peloton). \n New Family Bonding.  Primary caregivers can take 12 weeks off 100% paid leave, while secondary caregivers can take 3 weeks. Returning to work after bringing home a new child isn't easy, which is why we're flexible and empathetic to the needs of new parents. \n \n dv01 is an equal opportunity employer and all qualified applicants and employees will receive consideration for employment opportunities without regard to race, color, religion, creed, sex, sexual orientation, gender identity or expression, age, national origin or ancestry, citizenship, veteran status, membership in the uniformed services, disability, genetic information or any other basis protected by applicable law.", "cleaned_desc": " Own our data platform.  You will own the data platform that powers all of dv01's offerings. You will work with systems that ingest data to the cloud, perform transformations at scale, and deliver data to internal and external clients. Our platform code is written in   Scala to get the maximum performance from Apache Spark and Python for new projects to develop features quickly. Our tech stack is built on the Google Cloud Platform - leveraging Cloud Composer (Apache Airflow), Dataproc (Apache Spark), BigQuery, CloudSQL, GKE (kubernetes); in-house tooling - scheduling/monitoring tools, API crawlers, PDF parsers/scrapers, SFTP servers; and ad-hoc analysis services such as Databricks. \n Work extensively with open source technology.  You'll work heavily within the Spark ecosystem, as well as explore new open source technologies to solve customer needs. The skills you develop here will serve you well beyond dv01. \n Interact with a diverse team.  You will collaborate closely with other teams at dv01 to expand and improve the capabilities of our data platform. This includes working closely with several analyst teams to understand business domains, the Integrations team to resolve scaling and functionality pain points in data processing, the Internal Applications Team to build solutions for our internal workflows, and several Product Teams to scale data ingestion, storage, and database access to ship new products. \n Gain knowledge of the financial industry.  You'll have an exclusive view into the system that enables Americans to afford houses, cars, and college. You will learn about the participants, terminology, and mathematics behind this sector of the financial markets. \n \n \n \n  YOU ARE:   \n Experienced with all aspects of working with big data.  You have 5+ years of professional data engineering experience working with large data sets and are intimately familiar with the construction of scalable ETL pipelines, intricacies of accurate data processing, and infrastructure involved with ensuring the reliability of hundreds of daily processes. \n A Google Cloud Platform enthusiast.  3+ years of experience architecting large-scale distributed computing systems with GCP technologies. \n A well-rounded engineer.  You have 5+ years of professional programming experience with  Python  or  Scala  and a deep appreciation for engineering fundamentals. You understand the importance of writing tests, designing systems for long term maintainability, and evaluating both sides of common engineering considerations. \n A systems engineer.  You are able to collaborate with a wide range of people from diverse backgrounds to investigate and solve problems, often integrating multiple in-house and cloud-based systems. You are able to balance your time between gathering information, teaching others, and contributing well-documented code. \n Proficient with data governance practices.  Familiar with governance concepts and have implemented systems in the past (e.g. collibra, atlan, etc.). You are familiar with both the technical and business aspects of data governance and understand how it all ties together with lineage processes, tagging data elements to assist with risk assessment, business requirement gathering, and impact analysis. \n \n NICE TO HAVES:   \n Familiarity with GitHub Actions and Harness for CI/CD \n Comfortable with SQL \n Experience building data management systems (lineage, metadata, quality) \n Familiarity working in an agile environment \n Familiar working with workflow management tools like Jira and Confluence \n A background in Structured Finance \n ", "techs": ["scala", "apache spark", "python", "google cloud platform", "cloud composer (apache airflow)", "dataproc (apache spark)", "bigquery", "cloudsql", "gke (kubernetes)", "in-house tooling", "scheduling/monitoring tools", "api crawlers", "pdf parsers/scrapers", "sftp servers", "databricks", "github actions", "harness", "sql", "data management systems", "agile environment", "jira", "confluence", "structured finance."]}, "7b38bcaa62643f5e": {"terms": ["data engineer"], "salary_min": 101366.016, "salary_max": 128351.97, "title": "Data Engineer III [Sr.]", "company": "AmSurg", "desc": "AMSURG is a leading national healthcare provider, focused on acquiring, developing and operating ambulatory surgery centers (ASCs) in partnership with physicians throughout the United States. We operate and hold ownership in more than 250 ASCs in 34 states and the District of Columbia with medical specialties ranging from gastroenterology to ophthalmology and orthopedics. In 1992, AMSURG launched as a pioneer in the ambulatory surgery center healthcare sector, originating a physician partnership model that endures today. In the three decades since, our company has become an innovator, advocate and source for patients to find high quality care in their communities. \n  In this role, the Senior Data Engineer\u2019s primary job responsibilities involve preparing data for analytical or operational uses. The specific tasks handled by the Senior Data Engineers include, but not limited to, guiding/executing the work of all Data Engineer resources, both individually and collective, building data pipelines to pull together information from different source systems; integrating, consolidating, and cleansing data; and structuring it for use in individual analytics applications. The Senior Data Engineer often works as part of an analytics team, in a team lead and individual contributor role, providing data aggregations to executives, business analysts and other end users for more basic types of analysis to aid in ongoing operations. \n \n \n  RESPONSIBILITIES \n \n Design, construct, install, test and maintain highly scalable data management systems. \n Ensure systems meet business requirements and industry practices. \n Build high-performance algorithms, prototypes, predictive models, and proof of concepts. \n Research opportunities for data acquisition and new uses for existing data \n Lead and Develop data set processes for data modeling, mining, and production. \n Direct and integrate new data management technologies and software engineering tools into existing structures. \n Employ a variety of techniques and tools to marry systems together. \n Recommend and execute ways to improve data reliability, efficiency, and quality. \n Manage projects, resources, customer expectations, and business priorities to achieve customer satisfaction. \n Collaborate with data architects, modelers, and IT team members on project goals. \n \n \n \n  QUALIFICATIONS \n \n Experience in Azure Data Factory and SSIS \n Extensive experience with Microsoft SQL Server \n Advanced level user of Microsoft Office products. Advanced/power user of Excel \n Advanced knowledge of relational database principles including SQL and MS-Office products \n Excellent quantitative and analytical skills as well as the ability to translate findings into meaningful information appropriate to the audience/stakeholder. \n High level of comfort with many types of data including financial, quality, clinic, and security. \n Relational database training and data modeling skills. Must demonstrate a history of project management, technology investigation, technology implementation and technology oversight in various capacities. \n Ability to be a self-starter that can provide leadership. \n Comprehensive understanding of the Agile Development process. \n Presentation and PowerPoint skills, with a demonstrated ability to tell a data story to executive leadership. \n Strong ability to understand and analyze user requirements as they relate to organizational goals and objectives. \n Strong attention to detail with the ability to work under deadlines and switch quickly and comfortably between projects as business needs dictate. \n Superior written and oral communication skills. \n Strong interpersonal skills with the ability to effectively collaborate across teams. \n Strong work ethic and ability to work autonomously in a high production environment. \n Ability to work independently and prioritize work appropriately. \n Ability to work under tight deadlines leading multiple projects and switch quickly and comfortably between projects as business needs dictate. \n Manage and mentor team members. \n \n Education/Experience \n \n Bachelor\u2019s degree from an accredited college or university with a minimum of six (6) years previous data management/ETL experience required. \n Highly proficient in database management and query tools, including SQL and/or other query languages, required \n \n Must pass a background check and drug screen. \n  We do not discriminate in practices or employment opportunities on the basis of an individual's race, color, national or ethnic origin, religion, age, sex, gender, sexual orientation, marital status, veteran status, disability, or any other prohibited category set forth in federal or state regulations. \n  We are an equal-opportunity employer. \n  #LI-DK1 \n  #LI-remote", "cleaned_desc": "AMSURG is a leading national healthcare provider, focused on acquiring, developing and operating ambulatory surgery centers (ASCs) in partnership with physicians throughout the United States. We operate and hold ownership in more than 250 ASCs in 34 states and the District of Columbia with medical specialties ranging from gastroenterology to ophthalmology and orthopedics. In 1992, AMSURG launched as a pioneer in the ambulatory surgery center healthcare sector, originating a physician partnership model that endures today. In the three decades since, our company has become an innovator, advocate and source for patients to find high quality care in their communities. \n  In this role, the Senior Data Engineer\u2019s primary job responsibilities involve preparing data for analytical or operational uses. The specific tasks handled by the Senior Data Engineers include, but not limited to, guiding/executing the work of all Data Engineer resources, both individually and collective, building data pipelines to pull together information from different source systems; integrating, consolidating, and cleansing data; and structuring it for use in individual analytics applications. The Senior Data Engineer often works as part of an analytics team, in a team lead and individual contributor role, providing data aggregations to executives, business analysts and other end users for more basic types of analysis to aid in ongoing operations. \n \n \n  RESPONSIBILITIES \n \n Design, construct, install, test and maintain highly scalable data management systems. \n Ensure systems meet business requirements and industry practices. \n Build high-performance algorithms, prototypes, predictive models, and proof of concepts.   Research opportunities for data acquisition and new uses for existing data \n Lead and Develop data set processes for data modeling, mining, and production. \n Direct and integrate new data management technologies and software engineering tools into existing structures. \n Employ a variety of techniques and tools to marry systems together. \n Recommend and execute ways to improve data reliability, efficiency, and quality. \n Manage projects, resources, customer expectations, and business priorities to achieve customer satisfaction. \n Collaborate with data architects, modelers, and IT team members on project goals. \n \n   \n  QUALIFICATIONS \n \n Experience in Azure Data Factory and SSIS \n Extensive experience with Microsoft SQL Server \n Advanced level user of Microsoft Office products. Advanced/power user of Excel \n Advanced knowledge of relational database principles including SQL and MS-Office products \n Excellent quantitative and analytical skills as well as the ability to translate findings into meaningful information appropriate to the audience/stakeholder. \n High level of comfort with many types of data including financial, quality, clinic, and security.   Relational database training and data modeling skills. Must demonstrate a history of project management, technology investigation, technology implementation and technology oversight in various capacities. \n Ability to be a self-starter that can provide leadership. \n Comprehensive understanding of the Agile Development process. \n Presentation and PowerPoint skills, with a demonstrated ability to tell a data story to executive leadership. \n Strong ability to understand and analyze user requirements as they relate to organizational goals and objectives. \n Strong attention to detail with the ability to work under deadlines and switch quickly and comfortably between projects as business needs dictate. \n Superior written and oral communication skills. \n Strong interpersonal skills with the ability to effectively collaborate across teams. \n Strong work ethic and ability to work autonomously in a high production environment.   Ability to work independently and prioritize work appropriately. \n Ability to work under tight deadlines leading multiple projects and switch quickly and comfortably between projects as business needs dictate. \n Manage and mentor team members. \n \n Education/Experience \n \n Bachelor\u2019s degree from an accredited college or university with a minimum of six (6) years previous data management/ETL experience required. \n Highly proficient in database management and query tools, including SQL and/or other query languages, required \n ", "techs": ["amsurg", "azure data factory", "ssis", "microsoft sql server", "microsoft office products", "excel", "relational database principles", "sql", "ms-office products", "agile development process."]}, "694143e4e807858b": {"terms": ["data engineer"], "salary_min": 100000.0, "salary_max": 140000.0, "title": "Data Engineer (Military Invited)", "company": "Career Mentors, LLC", "desc": "4242 - Data Engineer \n  ***Knowledge of Air Force supply and logistics systems is Huge Plus! \n  Remote Full-Time IT Mid-Level As a Data Engineer, you will work hands-on with challenging data engineering, data management, and analytics projects. You will collaborate with data scientists, analysts, business users, and IT teams to design, implement, and deploy data services and analytics. \n \n  Must-Haves : \n \n  Minimum of 6 years of experience as a data engineer \n  Strong SQL skills in multiple data base platforms \n  Experience with Snowflake, Databricks, Spark SQL, PySpark, and Python \n  Cloud experience: Azure, AWS, or GCP \n  Develop and maintain ETL pipelines \n  Database design and principles \n  Data modeling, schema development, and data-centric documentation \n  Experience integrating data from a variety of data source types \n  Recommend and advise on optimal data models for data ingestion, integration,  and visualization \n  Experience improving code performance and query optimization \n  Use Continuous Integration/Continuous Delivery (CI/CD) concepts to engineer a  standardized data environment \n  Outstanding problem-solving skills \n  Excellent verbal and written communication skills and the ability to interact  professionally with a diverse group, including executives, managers, and subject  matter experts \n  Salary : $100,000-$140,000 \n \n  Location : Varies by client: \n \n  Columbus, OH, US \n  Cincinnati, OH, US \n  Dayton, OH, US \n  Remote Considered \n \n  Benefits: \n  Generous PTO \n  Medical Insurance \n  Dental & Vision Insurance \n  401K Match \n  Short/Long term Disability Insurance", "cleaned_desc": "4242 - Data Engineer \n  ***Knowledge of Air Force supply and logistics systems is Huge Plus! \n  Remote Full-Time IT Mid-Level As a Data Engineer, you will work hands-on with challenging data engineering, data management, and analytics projects. You will collaborate with data scientists, analysts, business users, and IT teams to design, implement, and deploy data services and analytics. \n \n  Must-Haves : \n    Minimum of 6 years of experience as a data engineer \n  Strong SQL skills in multiple data base platforms \n  Experience with Snowflake, Databricks, Spark SQL, PySpark, and Python \n  Cloud experience: Azure, AWS, or GCP \n  Develop and maintain ETL pipelines \n  Database design and principles    Data modeling, schema development, and data-centric documentation \n  Experience integrating data from a variety of data source types \n  Recommend and advise on optimal data models for data ingestion, integration,  and visualization \n  Experience improving code performance and query optimization \n  Use Continuous Integration/Continuous Delivery (CI/CD) concepts to engineer a  standardized data environment \n  Outstanding problem-solving skills ", "techs": ["snowflake", "databricks", "spark sql", "pyspark", "python", "azure", "aws", "gcp", "etl pipelines", "database design", "data modeling", "schema development", "data-centric documentation", "continuous integration/continuous delivery (ci/cd)"]}, "3f8f54441730364c": {"terms": ["data engineer"], "salary_min": 114085.03, "salary_max": 144457.08, "title": "Senior Data Engineer", "company": "Cooperative Computing", "desc": "Who we are \n Cooperative Computing (C|C) is a digital enablement organization enabling organizations to effectively operate in the automated economy. The future of business is in maximizing relationships through the effective use of technology. With our clients, we discover, strategically engineer a digital strategy and enable these strategies through the implementation of best-in-class applications to achieve clients 10x growth. \n Our performance culture is built through our team members, working together to help our clients succeed. We inspire growth with our team members in delivering fanatical and passionate client experiences, knowing effective technology is built with and for people. \n Our Values: \n \n Be Fanatical and Passionate Delivering Superior Client Experiences  - It\u2019s who we are! Our customers are the center of every idea, process, and decision we create in building sustainable relationships. We over communicate, over deliver & outperform ourselves every time \n Growth is Contagious  - I grow, You grow, We all grow! \n Be Innovative  - Looking at tomorrow today. We live outside our comfort zone; we ask difficult questions of ourselves; we take risks, and we are fearless to experiment and lead the way forward \n Show Empathy & Be Honest  - Every single word spoken, or action performed for our Customers, Team Members, Partners & Stakeholders will be filled with kindness, candor and honesty \n High Performance  - It\u2019s not for everyone - Our culture is our team members. We make the lives of our fellow team members better by first recognizing \u201cI\u201d am a team member first. We measure our progress constantly to be a better version of ourselves with every new day \n \n Life at CC: \n Life at CC is a fusion of ambition, recognition, and lifestyle, where your career takes flight. We champion a high-performance culture with top-tier compensation and flexible working models. With us, enjoy robust benefits, milestone celebrations, and unparalleled learning opportunities. We foster a vibrant community through dynamic team activities. Join CC - embark on a journey where every day is rewarding and growth is a guaranteed promise. \n About the Role: \n The Data Engineer position at our company entails joining a dedicated team of analytics professionals. The role is primarily focused on broadening and enhancing our data and data pipeline architecture while streamlining data flow and collection for multiple functional teams. The successful candidate will play a vital role in creating an environment that supports optimal data extraction, transformation, and loading from diverse data sources. \n Mission: \n The Data Engineer's mission is to ensure an optimal data pipeline architecture and gather complex data sets that meet the business's functional and non-functional requirements. Their goal is to identify, design, and implement internal process improvements, automate manual processes, enhance data delivery, and re-architect infrastructure for greater scalability. \n Capabilities (Key Behaviors): \n The Data Engineer will exhibit the following capabilities: \n \n 4 years of experience in a Data Engineer role. \n Bachelor's degree in Software Engineering/Computer Science or equivalent experience in a related field. \n Capability to create and maintain optimal data pipeline architecture, and compile large, complex data sets that meet business requirements. \n Profound knowledge of infrastructure for optimal data extraction, transformation, and loading from diverse data sources using SQL and AWS 'big data' technologies. \n Ability to identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. \n Skill to build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. \n Competency to work with various teams to assist with data-related technical issues and support their data infrastructure needs. \n Proficiency in performing root cause analysis on internal and external data and processes to address specific business questions and identify improvement opportunities. \n Expertise in interpreting trends and patterns, conducting complex data analysis, and reporting on results. \n \n Results: \n \n Successful maintenance and improvement of the data pipeline architecture, leading to better management and use of complex data sets. \n Effective implementation of process improvements, resulting in increased efficiency and scalability of the data infrastructure. \n Constructive collaboration with various teams, leading to the resolution of data-related technical issues and better support for data infrastructure needs. \n Accurate root cause analysis leading to the identification of areas for improvement and solutions to business queries. \n Insightful interpretation of trends and patterns, allowing for comprehensive data analysis and valuable reporting on results. \n \n Job Type: Full-time \n Application Question(s): \n \n What ETL Tools have you used? \n \n Experience: \n \n Data Engineer: 3 years (Preferred) \n Python: 4 years (Preferred) \n \n Work Location: Remote", "cleaned_desc": " The Data Engineer will exhibit the following capabilities: \n \n 4 years of experience in a Data Engineer role. \n Bachelor's degree in Software Engineering/Computer Science or equivalent experience in a related field. \n Capability to create and maintain optimal data pipeline architecture, and compile large, complex data sets that meet business requirements. \n Profound knowledge of infrastructure for optimal data extraction, transformation, and loading from diverse data sources using SQL and AWS 'big data' technologies. \n Ability to identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. \n Skill to build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. \n Competency to work with various teams to assist with data-related technical issues and support their data infrastructure needs. ", "techs": ["sql", "aws"]}, "eb66d500d7e1b5c1": {"terms": ["data engineer"], "salary_min": 80000.0, "salary_max": 85000.0, "title": "Data Engineer", "company": "Two Point Conversions, Inc", "desc": "Data Engineer \n Salary $80,000 - $85,000 annually \n Two Point Conversions, Inc. is a leading global healthcare data company with over 30 years of experience helping migrate data for thousands of pharmacies and hospitals. Join a company that is just as employee-focused as it is on its customers. Be the reason we help thousands of stores convert their health care and pharmacy data seamlessly. \n A data engineer is responsible for building systems that collect, manage, and convert raw data into usable information to fulfill business needs. \n Responsibilities \n \n Design, develop, and maintain optimal data pipelines  leveraging open source tools and data processing frameworks that align with business requirements. \n Identify, design, and implement internal process improvements . \n Build the infrastructure required for optimal extraction, transformation, and loading (ETL) of data  from a wide variety of data sources using (R)DBMS and Cloud technologies. \n Maintain various codebases for tools and services  used by other stakeholders. \n Create analytics tools and visualizations  such as dashboards and reports that provide actionable insights to stakeholders. \n Work with internal and external stakeholders  to assist with data-related technical issues and support their data infrastructure needs. \n Ensure compliance with data governance and security policies . \n Proactively improve encountered modules, services, systems and codebases . \n Improve documentation in wikis and internal knowledge bases that is insufficient or incorrect and proactively raise spotted issues . This includes issue status tracking and documenting the resolution for future use. \n Apply software development practices  related to testing, style, readability, compilation, continuous integration, and continuous deployment. \n Participate in reverse engineering of various data formats , often at the binary level. \n \n Qualifications \n \n Proficient in  multiple object-oriented and procedural programming languages . \n Experience with  Relational and non-relational databases . Desired experience includes advanced working SQL knowledge and experience working with relational databases, query authoring (SQL), data modeling, and common database interfaces (e.g. ODBC). \n Strong analytic skills  related to working with  structured and unstructured datasets . \n Experience with  ETL (extract, transform, and load) systems . \n Experience  building and optimizing data pipelines, architectures and data sets . \n Experience with  data stored in various formats and locations . \n Experience with  automation and scripting . \n Experience  leveraging relevant open source and internal development and data-related tools . \n Experience with  cloud computing and storage . \n Strong understanding of methods of  securely managing and storing data to protect it . \n Experience  supporting and working with cross-functional teams in a dynamic environment . \n Experience performing  root cause analysis  on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \n Experience with  version control systems and code reviews.  Desired experience also CI/CD pipelines. \n \n This is an entry-level position. We're looking for someone with 1-3 years of experience in data engineering or software development with a general purpose programming language, has a BS degree in computer science or a similar area. \n You\u2019ll be joining the pharmacy data engineering team where you develop programs for migrating customers between pharmacy management systems. \n Familiarity with the following software/tools is a nice to have: \n (list of languages, tools, cloud providers here) \n Languages may include Perl, Python, Bash, Batch, Powershell, VFP, and Golang. \n Cloud technology platforms may include GCP, Azure, and AWS. \n For consideration, please apply directly to : hr@twopoint.com \n Job Type: Full-time \n Pay: $80,000.00 - $85,000.00 per year \n Benefits: \n \n 401(k) \n 401(k) matching \n Dental insurance \n Employee assistance program \n Flexible schedule \n Flexible spending account \n Health insurance \n Life insurance \n Paid time off \n Parental leave \n Professional development assistance \n Tuition reimbursement \n Vision insurance \n \n Experience level: \n \n 3 years \n \n Schedule: \n \n 8 hour shift \n \n Experience: \n \n Informatica: 1 year (Preferred) \n SQL: 1 year (Preferred) \n Data warehouse: 1 year (Preferred) \n \n Work Location: Remote", "cleaned_desc": "Data Engineer \n Salary $80,000 - $85,000 annually \n Two Point Conversions, Inc. is a leading global healthcare data company with over 30 years of experience helping migrate data for thousands of pharmacies and hospitals. Join a company that is just as employee-focused as it is on its customers. Be the reason we help thousands of stores convert their health care and pharmacy data seamlessly. \n A data engineer is responsible for building systems that collect, manage, and convert raw data into usable information to fulfill business needs. \n Responsibilities \n \n Design, develop, and maintain optimal data pipelines  leveraging open source tools and data processing frameworks that align with business requirements. \n Identify, design, and implement internal process improvements . \n Build the infrastructure required for optimal extraction, transformation, and loading (ETL) of data  from a wide variety of data sources using (R)DBMS and Cloud technologies. \n Maintain various codebases for tools and services  used by other stakeholders. \n Create analytics tools and visualizations  such as dashboards and reports that provide actionable insights to stakeholders. \n Work with internal and external stakeholders  to assist with data-related technical issues and support their data infrastructure needs. \n Ensure compliance with data governance and security policies . \n Proactively improve encountered modules, services, systems and codebases .   Improve documentation in wikis and internal knowledge bases that is insufficient or incorrect and proactively raise spotted issues . This includes issue status tracking and documenting the resolution for future use. \n Apply software development practices  related to testing, style, readability, compilation, continuous integration, and continuous deployment. \n Participate in reverse engineering of various data formats , often at the binary level. \n \n Qualifications \n \n Proficient in  multiple object-oriented and procedural programming languages . \n Experience with  Relational and non-relational databases . Desired experience includes advanced working SQL knowledge and experience working with relational databases, query authoring (SQL), data modeling, and common database interfaces (e.g. ODBC). \n Strong analytic skills  related to working with  structured and unstructured datasets . \n Experience with  ETL (extract, transform, and load) systems . \n Experience  building and optimizing data pipelines, architectures and data sets . \n Experience with  data stored in various formats and locations . \n Experience with  automation and scripting . \n Experience  leveraging relevant open source and internal development and data-related tools .   Experience with  cloud computing and storage . \n Strong understanding of methods of  securely managing and storing data to protect it . \n Experience  supporting and working with cross-functional teams in a dynamic environment . \n Experience performing  root cause analysis  on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \n Experience with  version control systems and code reviews.  Desired experience also CI/CD pipelines. \n \n This is an entry-level position. We're looking for someone with 1-3 years of experience in data engineering or software development with a general purpose programming language, has a BS degree in computer science or a similar area. \n You\u2019ll be joining the pharmacy data engineering team where you develop programs for migrating customers between pharmacy management systems. \n Familiarity with the following software/tools is a nice to have: \n (list of languages, tools, cloud providers here) \n Languages may include Perl, Python, Bash, Batch, Powershell, VFP, and Golang. \n Cloud technology platforms may include GCP, Azure, and AWS. \n For consideration, please apply directly to : hr@twopoint.com \n Job Type: Full-time ", "techs": ["data engineer", "open source tools", "data processing frameworks", "(r)dbms", "cloud technologies", "analytics tools", "visualizations", "dashboards", "reports", "data governance", "data security", "software development", "testing", "continuous integration", "continuous deployment", "reverse engineering", "object-oriented programming languages", "procedural programming languages", "relational databases", "non-relational databases", "sql", "data modeling", "etl systems", "data pipelines", "data architectures", "automation", "scripting", "cloud computing", "storage", "securely managing and storing data", "cross-functional teams", "root cause analysis", "version control systems", "code reviews", "ci/cd pipelines", "pharmacy management systems", "perl", "python", "bash", "batch", "powershell", "vfp", "golang", "gcp", "azure", "aws."]}, "daeda5307e92df58": {"terms": ["data engineer"], "salary_min": 161700.0, "salary_max": 200000.0, "title": "Principal Data Innovation Engineer", "company": "McGraw Hill LLC.", "desc": "Overview: \n  \n  Impact the moment: \n \n \n   When was the last time you experienced the impact of your work? Our Data & AI team thrives on building meaningful relationships with educators and learners. With that comes the unique opportunity to touch lives across the world and experience first-hand the difference your hard work makes.\n  \n \n \n  How can you make impact? \n \n \n   The Principal Innovation Data Engineer will design the end-to-end (e2e) data & AI platform architecture and lead the build of both our modernized D&A Lakehouse platform, which includes our Lakehouse, AI/ML ops platform, and our modernized self-service BI (SSBI) platform at MH. This role will report into the Vice President of Data & AI Platform Innovation, as part of Enterprise Data & Analytics (D&A). This role will be a technical thought leader at MH and help all BUs AND central platform teams. This role must continuously identify ways to leverage emerging technologies to improve our data and SSBI platform capabilities. This position will also be expected to create executive-level presentations around proposed architecture and options to consider for the Head of Enterprise D&A and the D&A senior leadership team (D&A SLT).\n  \n \n \n  What you will be doing: \n \n \n  Will own the end-to-end (e2e) data, SSBI, and ML/AI architecture across McGraw Hill. \n  Will be the senior-most technical expert across all of data engineering, which includes reviewing code developed by data engineering teammates for our most complex deliverables. \n  Must be an expert in creating analytics-ready data, which includes creating data models that are customer-friendly and defining when to create data in Lakehouse vs. SSBI tool semantic layer vs. both. \n  Must consistently monitor emerging technologies to determine how best to continue to innovate and improve both our data, SSBI, and ML/AI platforms. \n  Will monitor actual vs. targets for platform performance and optimization and will raise areas of opportunity for improvement WITH recommendations on how to improve. \n  Must rapidly prototype on new data and SSBI technologies to help evaluation process for D&A SLT to consider. \n  Will own the technical design and evaluate the technical build against that design of our core Enterprise BI/Reporting Products. \n \n \n  What can you bring to the role? \n \n \n  10+ years leading data model design and builds of analytics-ready data. \n  10+ years leading the design and build of some combination of Self-Service BI platforms (PowerBI, Qlik, LookerBI, SSRS, etc.) \n  10+ years leading code optimization efforts of data/BI engineering. \n  7+ years leading the design and build of some combination of modernized data lakes and/or data warehouses with some combination of the following cloud providers: AWS, Azure, Databricks, etc. \n  7+ years with some combination of SQL, Python, Azure Data Factory, AWS Glue, SSIS, etc. \n \n \n  Preferred experience: \n \n \n  Ideally 2+ years experience in a multi-cloud or cloud + on-prem environment. \n  Ideally 2+ years experience with cloud migrations from on-prem to cloud environments. \n \n \n \n  At McGraw Hill, you will be empowered to make a real impact on a global scale. Every day your individual efforts contribute to the lives of millions.\n  \n \n \n  The pay range for this position is between $161,700 -$200,000 annually, however, base pay offered may vary depending on job-related knowledge, skills, experience, and location. An annual bonus plan may be provided as part of the compensation package, in addition to a full range of medical and/or other benefits, depending on the position offered. Click here to learn more about our benefit offerings.", "cleaned_desc": " \n  10+ years leading data model design and builds of analytics-ready data. \n  10+ years leading the design and build of some combination of Self-Service BI platforms (PowerBI, Qlik, LookerBI, SSRS, etc.) \n  10+ years leading code optimization efforts of data/BI engineering. \n  7+ years leading the design and build of some combination of modernized data lakes and/or data warehouses with some combination of the following cloud providers: AWS, Azure, Databricks, etc. \n  7+ years with some combination of SQL, Python, Azure Data Factory, AWS Glue, SSIS, etc. \n \n \n  Preferred experience: \n ", "techs": ["powerbi", "qlik", "lookerbi", "ssrs", "aws", "azure", "databricks", "sql", "python", "azure data factory", "aws glue", "ssis"]}, "3a3ce5560de5158d": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "codebuild", "codedeploy", "python", "postgresql"]}, "ca89998846dc439c": {"terms": ["data engineer"], "salary_min": 110046.18, "salary_max": 139342.98, "title": "Data Engineer", "company": "ProducePay", "desc": "At ProducePay, we believe strongly in driving business strategies and decisions with evidence-based data, rather than intuition, opinion, or assumption. As a senior member of our top-notch and fast-paced engineering team, we are looking for a Senior Data Engineer who will help oversee ProducePay's vast and diverse data set, data integration, and data migration, including developing and maintaining a data warehouse and analytics environment, and writing scripts for data integration, scrubbing, and ingestion. This role will work closely and collaboratively with other engineers, as well as other functional teams to define requirements, mine and analyze data, integrate data from a variety of sources, including data scrubbing, and deploy high-quality data pipelines in support of the analytics needs of ProducePay. As a senior member of the engineering team, you will be expected to lead and mentor junior engineers on various assignments, and help ensure projects are progressing in a timely manner. \n \n  Responsibilities \n \n Developing solutions to improve company\u2019s data management practices \n Working with data architects to design and improve on data warehouses  \n Reviewing and analyzing data sets to identify patterns or trends that may have business implications \n Developing and maintaining data mining and ingestion solutions \n Assisting and providing feedback for new and existing data models, databases  \n Providing consultation on data management and integrity issues to other members of the company \n Recommending changes to existing databases to improve performance or resolve issues \n \n Requirements \n \n At least a Bachelor\u2019s Degree in Computer Science \n Passionate about data and the power of data \n Willing to step outside of comfort and immediate responsibility zones to drive results and desired outcome \n Committed to making data a key focus for overall company strategy, growth, and product development \n Experience working with Snowflake and Postgres \n Excellent communication and presentation skills \n Highly organized with good time management skills \n Strong attention to detail \n Familiar with AWS cloud infrastructure and setup \n Familiar with cloud IaC concepts, a plus if experience with Pulumi or Terraform \n Exceptional problem-solving ability \n Ability to succeed in a collaborative work environment and work cross-functionally \n \n Benefits \n \n Health Insurance: Medical, Dental, and Vision benefits \n Competitive comp package of base, bonus, and equity \n 401K - company automatically contributes 3% \n Work-from-Home Stipend \n Quarterly Volunteer Days \n Pet Insurance \n Unlimited Paid-Time-Off \n Brilliant, motivated, and fun team members", "cleaned_desc": "At ProducePay, we believe strongly in driving business strategies and decisions with evidence-based data, rather than intuition, opinion, or assumption. As a senior member of our top-notch and fast-paced engineering team, we are looking for a Senior Data Engineer who will help oversee ProducePay's vast and diverse data set, data integration, and data migration, including developing and maintaining a data warehouse and analytics environment, and writing scripts for data integration, scrubbing, and ingestion. This role will work closely and collaboratively with other engineers, as well as other functional teams to define requirements, mine and analyze data, integrate data from a variety of sources, including data scrubbing, and deploy high-quality data pipelines in support of the analytics needs of ProducePay. As a senior member of the engineering team, you will be expected to lead and mentor junior engineers on various assignments, and help ensure projects are progressing in a timely manner. \n \n  Responsibilities \n \n Developing solutions to improve company\u2019s data management practices \n Working with data architects to design and improve on data warehouses  \n Reviewing and analyzing data sets to identify patterns or trends that may have business implications   At least a Bachelor\u2019s Degree in Computer Science \n Passionate about data and the power of data \n Willing to step outside of comfort and immediate responsibility zones to drive results and desired outcome \n Committed to making data a key focus for overall company strategy, growth, and product development \n Experience working with Snowflake and Postgres \n Excellent communication and presentation skills \n Highly organized with good time management skills   Strong attention to detail \n Familiar with AWS cloud infrastructure and setup \n Familiar with cloud IaC concepts, a plus if experience with Pulumi or Terraform \n Exceptional problem-solving ability \n Ability to succeed in a collaborative work environment and work cross-functionally \n \n Benefits ", "techs": ["producepay", "senior data engineer", "data integration", "data migration", "data warehouse", "analytics environment", "data integration scripts", "data scrubbing", "data ingestion", "data pipelines", "data management practices", "data architects", "data warehouses", "analyzing data sets", "patterns", "trends", "bachelor's degree in computer science", "snowflake", "postgres", "communication skills", "presentation skills", "organization skills", "time management skills", "attention to detail", "aws cloud infrastructure", "pulumi", "terraform", "problem-solving ability", "collaborative work environment", "cross-functional work."]}, "80274a8e26805da8": {"terms": ["data engineer"], "salary_min": 109805.71, "salary_max": 139038.5, "title": "GCP Data Engineer", "company": "emids", "desc": "This is us, your new colleagues!  Our organization is based on people and great teamwork. \n  We work at the intersection of design, engineering and domain expertise, our passionate team of problem solvers work closely with customers to blaze new trails that will positively impact the future of health. We are a truly global company, we rely on diversity and together we create a workplace that brings the best out of everyone. Using technology and insights, we move nimbly and provide trusted advice, seeking ways to amplify results. \n \n \n  We're committed to bettering healthcare and empowering wellness. \n    Do you want to be a part of this journey? \n    The role \n  As a team we are together responsible for providing a modern, efficient and secured environment supporting our employees as we believe that our employees are our greatest strength. We are looking for someone with strong understanding of the management, soft and people's skills, excellent communication, leadership and planning skills. \n \n  Emids is a leading provider of digital transformation solutions to the healthcare industry, serving payers, providers, life sciences, and technology firms. Headquartered in Nashville, Emids helps bridge critical gaps in providing accessible, affordable, and high-quality healthcare by providing digital transformation services, custom application development, data engineering, business intelligence solutions, and specialized consulting services to all parts of the healthcare ecosystem. With nearly 3,500 professionals globally, Emids leverages strong domain expertise in healthcare-specific platforms, regulations, and standards to provide tailored, cutting-edge solutions and services to its clients. \n  Responsibilities : \n \n Design and implement ETL workflows to extract, transform, and load data from various sources into GCP platforms. \n Build and maintain data pipelines to ensure data quality, accuracy, and consistency. \n Develop and maintain data models that enable efficient and effective data analysis. \n Work with other team members to identify and troubleshoot data quality issues. \n Collaborate with data analysts, data scientists, and other stakeholders to understand their data requirements and provide data support. \n Monitor and optimize data workflows to ensure efficient and reliable performance. \n Document data processes, including data lineage and data definitions, for future reference. \n \n 'Required Qualification: \n \n Possess excellent knowledge of SQL along with its variation for popular database like BigQuery etc. \n Experience writing Python & Pyspark scripts. \n Strong experience in data analytics and business intelligence. \n Development experience building ETL pipelines using cloud tools like dataflow, lambda. \n Experience in tuning SQL queries to maximize performance. \n Working knowledge on implementing Data quality checks. \n Experience with Airflow or Tidal to orchestrate the data pipelines. \n Excellent critical reasoning, problem-solving skills and teamwork skills. \n Solid written and verbal communication skills and able to articulate complex solutions to technical and non-technical personnel. \n Experience with VCS such as git and build tools such as Jenkins or Maven. \n Experience working for clients in healthcare space. \n \n 'Must to Have' Experience: \n \n Experienced in healthcare domain \n Hands-on with Google Cloud services for data engineering- Data Proc, Big Query, Cloud storage, \n Manage end to end data pipeline from extraction from source, landing on google platform and transformation \n Implemented data management best practices \u2013 Data quality, Capture of metadata \n Experience with DevOps and GKE patterns \n \n \n \n  Emids is an Equal Opportunity Employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, alienage or national origin, ancestry, citizenship status, age, disability or handicap, sex, marital status, veteran status, sexual orientation, or any other characteristic protected by applicable federal, state or local laws. Our management team is dedicated to this policy with respect to recruitment, hiring, placement, promotion, transfer, training, compensation, benefits, employee activities and general treatment during employment. \n \n  What can we offer you? \n  You will be part of a team that offers you a fulfilling career, great results through amazing team, strong relationships and a high performance culture. We are using the latest technologies, high security mediums, and service platforms top market providers. We strongly promote agile mind-set and ways of working, followed by agile methods used in practice. We offer proper guidance and take care of our people and offer top notch services including flexible work timings and training required. \n \n \n  We also offer: \n \n Benefits and leave management \n A great learning platform \n A challenging environment where 2 days never look the same \n A high performing team and a positive atmosphere where mistakes are welcome as part of the learning \n \n About Us \n  Emids is healthcare's digital transformation leader, delivering business and tech solutions that help payers, providers and tech-enablers maximize technology to deliver care better since 1999 \n  As a global partner headquartered in Nashville, TN, emids helps bridge the critical gaps in accessible, affordable, high-quality healthcare by providing advisory consulting services, custom application development, and data solutions. Services include EHR application deployment and management, analytics, data integration and governance, software development and testing, and business intelligence. \n \n \n  Emids specialises in Healthcare Expertise, Mobile Health Solutions, Healthcare Reform & Benefits Exchange, Regulatory Compliance, HIE & HIS Implementation, Clinical Systems Engineering, Care Analytics, Testing & QA, Implementation & Integration, and Cloud Enablement \n \n  Website: http://www.emids.com", "cleaned_desc": " Design and implement ETL workflows to extract, transform, and load data from various sources into GCP platforms. \n Build and maintain data pipelines to ensure data quality, accuracy, and consistency. \n Develop and maintain data models that enable efficient and effective data analysis. \n Work with other team members to identify and troubleshoot data quality issues. \n Collaborate with data analysts, data scientists, and other stakeholders to understand their data requirements and provide data support. \n Monitor and optimize data workflows to ensure efficient and reliable performance. \n Document data processes, including data lineage and data definitions, for future reference. \n \n 'Required Qualification: \n \n Possess excellent knowledge of SQL along with its variation for popular database like BigQuery etc. \n Experience writing Python & Pyspark scripts.   Strong experience in data analytics and business intelligence. \n Development experience building ETL pipelines using cloud tools like dataflow, lambda. \n Experience in tuning SQL queries to maximize performance. \n Working knowledge on implementing Data quality checks. \n Experience with Airflow or Tidal to orchestrate the data pipelines. \n Excellent critical reasoning, problem-solving skills and teamwork skills. \n Solid written and verbal communication skills and able to articulate complex solutions to technical and non-technical personnel. \n Experience with VCS such as git and build tools such as Jenkins or Maven. \n Experience working for clients in healthcare space. \n \n 'Must to Have' Experience: \n ", "techs": ["etl workflows", "gcp platforms", "data pipelines", "data quality", "data models", "data analysis", "sql", "bigquery", "python", "pyspark", "data analytics", "business intelligence", "etl pipelines", "cloud tools", "dataflow", "lambda", "sql query tuning", "data quality checks", "airflow", "tidal", "critical reasoning", "problem-solving skills", "teamwork skills", "written and verbal communication skills", "vcs", "git", "jenkins", "maven"]}, "6bb03a57d44f7e90": {"terms": ["data engineer"], "salary_min": null, "salary_max": null, "title": "Extract Data Engineer Heath Care Informatics", "company": "Providence", "desc": "Description \n This position is remote and can sit in the footprint of Providence in the states of WA, OR, TX, or MT. \n The Data Engineer designs and builds extract code, data pipelines and transformations, data enrichment processes, provisioning layers, and secure transmission methods to support clinical and operational processes across all parts of the healthcare system. This person will place a priority on collaboration with meticulous source control and documentation, an emphasis on simple solutions to complex problems, the ability to extract very specific data in defined formats for secure transmission. Knowledge of healthcare data and experience with data extraction from an EMR essential. \n Position requires a highly collaborative, analytical, and forward-thinking self-starter, who takes initiative and thrives in a fast-paced environments. \n This is an exciting opportunity to be part of a team to explore data and create meaningful analysis for rest of providence and vendors involved. Providence teams are known for their dynamic, diverse, and inclusive culture, grounded in trust, hospitality, collaboration, and innovation. These are the core values that inspire our work, and what we are looking for in Data Engineer. \n Providence caregivers are not simply valued \u2013 they\u2019re invaluable. Join our team at Enterprise Information Services and thrive in our culture of patient-focused, whole-person care built on understanding, commitment, and mutual respect. Your voice matters here, because we know that to inspire and retain the best people, we must empower them. \n Required Qualifications: \n \n Bachelor's Degree in Computer Engineering, Computer Science, Mathematics, Engineering or equivalent education/experience \n 1 year related experience; 1-3 years preferred \n Healthcare data knowledge and experience \n \n Preferred Qualifications: \n \n EPIC certification(s) \n \n The salary range listed for this position MIN:$43.39 to MAX:$70.03 per hour is based upon the primary work location Renton, WA posted. This position is remote. Salary range and offers are determined by internal pay equity and geographic cost of living differences. Salary range will vary from State and region. Salary max is limited to 75% range in order to continue to offer internal pay growth. We welcome open and transparent discussions on salary at Providence \n Our best-in-class benefits are uniquely designed to support you and your family in staying well, growing professionally and achieving financial security. We take care of you, so you can focus on delivering our Mission of caring for everyone, especially the most vulnerable in our communities. \n About Providence \n At Providence, our strength lies in Our Promise of \u201cKnow me, care for me, ease my way.\u201d Working at our family of organizations means that regardless of your role, we\u2019ll walk alongside you in your career, supporting you so you can support others. We provide best-in-class benefits and we foster an inclusive workplace where diversity is valued, and everyone is essential, heard and respected. Together, our 120,000 caregivers (all employees) serve in over 50 hospitals, over 1,000 clinics and a full range of health and social services across Alaska, California, Montana, New Mexico, Oregon, Texas and Washington. As a comprehensive health care organization, we are serving more people, advancing best practices and continuing our more than 100-year tradition of serving the poor and vulnerable. \n The amounts listed are the base pay range; additional compensation may be available for this role, such as shift differentials, standby/on-call, overtime, premiums, extra shift incentives, or bonus opportunities. \n Check out our benefits page for more information about our Benefits and Rewards. \n Requsition ID:  218202  \n Company:  Providence Jobs  \n Job Category:  Development/Engineering  \n Job Function:  Information Technology  \n Job Schedule:  Full time  \n Job Shift:   \n Career Track:   \n Department:  4011 SS IS HI DP 3  \n Address:  WA Renton 2001 Lind Ave SW  \n Work Location:  Providence Southgate 2-Renton  \n Pay Range:  $43.39 - $70.03  \n The amounts listed are the base pay range; additional compensation may be available for this role, such as shift differentials, standby/on-call, overtime, premiums, extra shift incentives, or bonus opportunities.  \n Check out our benefits page for more information about our Benefits and Rewards. \n Providence is proud to be an Equal Opportunity Employer. Providence does not discriminate on the basis of race, color, gender, disability, veteran, military status, religion, age, creed, national origin, sexual identity or expression, sexual orientation, marital status, genetic information, or any other basis prohibited by local, state, or federal law.", "cleaned_desc": "", "techs": ""}, "764581b0a40304b5": {"terms": ["data engineer"], "salary_min": 122768.48, "salary_max": 155452.25, "title": "Senior Data Engineer - Consulting (Remote Possible)", "company": "Allata LLC", "desc": "Allata is a fast-growing technology strategy and data development consulting firm delivering scalable solutions to our enterprise clients. Our mission is to inspire our clients to achieve their most strategic goals through uncompromising delivery, engaged listening, and personal accountability. We are a group who thrive in fast-paced environments, solving complex problems, continually learning, and working alongside colleagues to be better together. \n WHAT YOU'LL BE DOING \n \n Work with customers to build cloud-based data platforms, including integration, data storage and analytics \n Develop innovative architectures to solve complex business problems utilizing the latest cloud technologies \n \n WHAT YOU'LL NEED \n \n Data Architecture Best Practices.  You\u2019ve successfully built data solutions that use industry best practices and fit with an organization\u2019s needs. You are excited by solving problems and voraciously consume technology to do so. You have a broad and deep technical background  \n Communication.  You have a natural charisma and use it to build consensus. You can have a conversation with developers, business analysts, managers of all levels, and individuals in a business function. You are comfortable presenting in front of groups and explaining architectures in a variety of levels of detail. \n Make teams better.  You're excited to be part of a team that delivers with quality and works hard on new opportunities. You work well in fast-moving environments and have no problem working with others to resolve difficult problems. You support teams as much as others supports you. \n \n DESIRED SKILLS & EXPERIENCE \n \n 8-10 years of experience \n Expertise in building data platforms in Azure  \n Experienced with ETL tools such as Azure Data Factory, AWS Glue, WhereScape RED, Streamsets, Informatica and SAP Convergent Mediation  \n Experience in one or more Cloud Data Warehouse (Azure SQL Data Warehouse / Synapse Analytics, Snowflake, Amazon Redshift, Google BigQuery) \n Experience in one or more Data Visualization tool (Tableau, PowerBI)  \n Expertise modeling architectures and integrations for data environments including data pipelines, data lakes, data warehouses, and data marts. \n Experience with data backup and recovery strategies, optimization of clusters, structured/semi structured data, and changing database storage and utilization requirements \n Experience with scripting languages such as Python / R for Business \n Experience with Event Driven Architecture (Kafka) \n Experience with large-scale distributed storage and database systems (SQL or NoSQL, e.g. MySQL, Cassandra) \n \n At Allata, we value differences. \n Allata is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. \n Allata makes employment determinations without regard to race, color, creed, religion, age, ancestry, national origin, veteran status, sex, sexual orientation, gender, gender identity, gender expression, marital status, disability or any other legally protected category. \n This policy applies to all terms and conditions of employment, including but not limited to, recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.", "cleaned_desc": " Expertise in building data platforms in Azure  \n Experienced with ETL tools such as Azure Data Factory, AWS Glue, WhereScape RED, Streamsets, Informatica and SAP Convergent Mediation  \n Experience in one or more Cloud Data Warehouse (Azure SQL Data Warehouse / Synapse Analytics, Snowflake, Amazon Redshift, Google BigQuery) \n Experience in one or more Data Visualization tool (Tableau, PowerBI)  \n Expertise modeling architectures and integrations for data environments including data pipelines, data lakes, data warehouses, and data marts.   Experience with data backup and recovery strategies, optimization of clusters, structured/semi structured data, and changing database storage and utilization requirements \n Experience with scripting languages such as Python / R for Business \n Experience with Event Driven Architecture (Kafka) \n Experience with large-scale distributed storage and database systems (SQL or NoSQL, e.g. MySQL, Cassandra) \n ", "techs": ["azure data factory", "aws glue", "wherescape red", "streamsets", "informatica", "sap convergent mediation", "azure sql data warehouse", "synapse analytics", "snowflake", "amazon redshift", "google bigquery", "tableau", "powerbi", "python", "r", "kafka", "mysql", "cassandra"]}, "dd14416920d35f20": {"terms": ["data engineer", "mlops"], "salary_min": 80.0, "salary_max": 90.0, "title": "Lead Azure Data Engineer", "company": "KPI Partners", "desc": "KPI Partners, A global consulting firm focused on strategy, technology, and digital transformation. We help companies tackle their most ambitious projects and build new capabilities. We provide solutions in Cloud, Data, Application Development & BI spaces. \n We enable your growth \n At KPI, you can become who you want to be and learn skills that will take you further in your career \n Continuously upgrade yourself \n Develop as a future leader \n Drive cloud enablement around the world \n Engineering Excellence \n Enhance your engineering expertise with our unique approach \n This program gives engineers the opportunity to excel in product and software engineering by learning our industry-leading practices, tools, and technologies to build excellence by enhancing their competencies and skills \n Visit to Know more : https://www.kpipartners.com/ \n Title: Lead Azure Data Engineer \u2013 6 Months Contract \n Location: 100% Remote \n Key Skills: ADF, ADLS, Synapse (Azure Sql Datawarehouse), T-Sql \n Description \n 1. Enterprise Data modelling / Design \n 2. Azure SQL Data Warehouse (Synapse) \n 3. T-SQL ( Hands-on experience, writing queries, building stored procs, performance optimization, etc..) \n 4. ADF / Any Enterprise ETL Tool \n 5. ADLS / any cloud storage \n 6. Should be able to understand the technical specifications and able to work independently with minimal or no supervision \n Must have Skills : \n Extensive experience providing practical direction within azure native services , implementing data migration and data processing using Azure services: ADLS, Azure Data Factory, Synapse/DW /Azure SQL DB, Fabric. \n Proven experience with SQL, namely schema design and dimensional data modelling \n Solid knowledge of data warehouse best practices, development standards and methodologies \n Strong experience with Azure Cloud on data integration with Databricks \n Be an independent self-learner with the \u201clet\u2019s get this done\u201d approach and ability to work in Fast paced and Dynamic environment \n Nice-to-Have Skills: \n Basic understanding on ML Studio, AI/ML, MLOps etc. \n Good to have Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, Cosmo Db knowledge. \n Good to have SAP Hana knowledge \n Intermediate knowledge on Power BI \n Good to have knowledge in DevOps and CI/CD deployments, Cloud migration methodologies and processes. \n Job Type: Contract \n Salary: $80.00 - $90.00 per hour \n Experience level: \n \n 10 years \n \n Schedule: \n \n 8 hour shift \n \n Application Question(s): \n \n We are only willing to accept consultants who are fine to work on W2 for this role. Are you fine to work on W2? \n \n Experience: \n \n Azure Synapse: 2 years (Required) \n Azure Databricks: 2 years (Required) \n Azure Data Lake: 2 years (Required) \n \n Work Location: Remote", "cleaned_desc": " Title: Lead Azure Data Engineer \u2013 6 Months Contract \n Location: 100% Remote \n Key Skills: ADF, ADLS, Synapse (Azure Sql Datawarehouse), T-Sql \n Description \n 1. Enterprise Data modelling / Design \n 2. Azure SQL Data Warehouse (Synapse) \n 3. T-SQL ( Hands-on experience, writing queries, building stored procs, performance optimization, etc..) \n 4. ADF / Any Enterprise ETL Tool \n 5. ADLS / any cloud storage \n 6. Should be able to understand the technical specifications and able to work independently with minimal or no supervision   Must have Skills : \n Extensive experience providing practical direction within azure native services , implementing data migration and data processing using Azure services: ADLS, Azure Data Factory, Synapse/DW /Azure SQL DB, Fabric. \n Proven experience with SQL, namely schema design and dimensional data modelling \n Solid knowledge of data warehouse best practices, development standards and methodologies \n Strong experience with Azure Cloud on data integration with Databricks \n Be an independent self-learner with the \u201clet\u2019s get this done\u201d approach and ability to work in Fast paced and Dynamic environment \n Nice-to-Have Skills: \n Basic understanding on ML Studio, AI/ML, MLOps etc. \n Good to have Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, Cosmo Db knowledge. \n Good to have SAP Hana knowledge ", "techs": ["adf", "adls", "synapse", "t-sql", "azure sql data warehouse", "t-sql", "adf", "adls", "azure data factory", "synapse/dw", "azure sql db", "fabric", "sql", "databricks", "ml studio", "ai/ml", "mlops", "event hub", "iot hub", "azure stream analytics", "azure analysis service", "cosmo db", "sap hana."]}, "4467818a7b613d42": {"terms": ["data engineer"], "salary_min": 113972.85, "salary_max": 144315.03, "title": "Cloud Data Engineer", "company": "Baer Group", "desc": "**Federal Project - Applicant must be a United States Citizen, with the ability to obtain a Public Trust. ** \n \n \n Baer is looking for Cloud Data Engineer for a 12 month Federal Remote Project. \n \n   \n \n Title:  Cloud Data Engineer\n  \n Location:  Remote (Must be based in US)\n  \n Duration:  12 months\n  \n Rate:  All-Inclusive\n  \n Alignment:  W2 or C2C (Vendors Not Permitted)\n  \n \n Description: \n \n \n Provides the ETL support to the data science and software engineering team members. Build, modify, support infrastructure for optimal extraction, transformation, and loading of data from variety of structure, unstructured data sources and multi-terabyte distributed file system.  \n Provide current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments  \n Manage/maintain structured, semi-structured, and unstructured data, structuring and wrangling data as appropriate for statistical analysis  \n Implement data warehouse concepts and relational databases, big data management techniques and tools (e.g. Hadoop, MAPReduce)  \n Communicate with technical and non-technical users and managers, and server administration, to include hardware and software support to existing servers.  \n Provide software engineering support to operate, maintain and enhance systems that are integrated with and/or relied upon by the data engineering lifecycle  \n Integrate, analyze, and visualize data and information in near real-time (within 24 hours) from multiple disparate data sources.  \n Optimize data storage and access  \n Proficiency with Python and Java, Oracle enterprise manager, SQL, AWS  \n \n \n Requirements: \n \n \n 5 years experience conducting ETL tasks, performance engineering, run-time optimization, large data volume transfers  \n 3 years experience with Regular Expressions, SQL (PostgreSQL), No-SQL (MongoDB)  \n 1 year experience with Version control systems (Git)  \n Preference to developer with experience working with healthcare data and Health IT  \n Apache Hadoop (Cloudera), AWS Data Platforms (Redshift, S3, EMR/Hive), SQL, Java, Kafka, Scala, Kotlin, Neo4j, NiFi, Flink, Sqoop, PostgreSQL, EMR, Apache Spark, Python, PHP, Oracle, Splunk, BDD, testing framework: Cucumber  \n Knowledge of and experience using various NLP approaches, particularly:  \n Pattern recognition/feature extraction  \n Supervised, Unsupervised, and Semi-Supervised learning techniques  \n Understanding of various language models (N-Gram, Skipgram, NLM, etc.)  \n Chunking/Tokenization  \n Semantic parsing  \n Healthcare IT experience  \n Statistical model building (particularly classification)  \n Masters degree in related field + 5 years experience; or PhD +1 year experience; or Bachelor\u2019s degree in related field + 7 years experience  \n \n \n Public Trust Security Clearance  is the lowest level of additional background screening that the federal government requires for applicants of certain jobs, which includes completing a Standard Form 85 (SF85) form.\n  \n \n \n Company Overview: \n \n  Baer is an Enterprise Performance Partner providing job opportunities with several 1st Tier Global Systems Integrators and a wide array of Fortune 1000 clients. Baer consultants and employees enjoy access to the highest profile job opportunities across leading Enterprise Technology Solutions ranging from Digital Transformation programs utilizing the latest technologies from SAP and Oracle to a wide range of emerging Cloud based infrastructure, application and AI related solutions.\n  \n  At Baer we aim to provide a best-in-class engagement experience for our consultants. Our job requirements are carefully vetted and are typically associated with pivotal programs offering tremendous opportunities to expand your skills leveraging the latest solutions.\n  \n  Baer is an equal opportunity employer including disability/veteran.", "cleaned_desc": "  \n Rate:  All-Inclusive\n  \n Alignment:  W2 or C2C (Vendors Not Permitted)\n  \n \n Description: \n \n \n Provides the ETL support to the data science and software engineering team members. Build, modify, support infrastructure for optimal extraction, transformation, and loading of data from variety of structure, unstructured data sources and multi-terabyte distributed file system.  \n Provide current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments  \n Manage/maintain structured, semi-structured, and unstructured data, structuring and wrangling data as appropriate for statistical analysis    Implement data warehouse concepts and relational databases, big data management techniques and tools (e.g. Hadoop, MAPReduce)  \n Communicate with technical and non-technical users and managers, and server administration, to include hardware and software support to existing servers.  \n Provide software engineering support to operate, maintain and enhance systems that are integrated with and/or relied upon by the data engineering lifecycle  \n Integrate, analyze, and visualize data and information in near real-time (within 24 hours) from multiple disparate data sources.  \n Optimize data storage and access  \n Proficiency with Python and Java, Oracle enterprise manager, SQL, AWS  \n \n \n Requirements: \n \n \n 5 years experience conducting ETL tasks, performance engineering, run-time optimization, large data volume transfers    3 years experience with Regular Expressions, SQL (PostgreSQL), No-SQL (MongoDB)  \n 1 year experience with Version control systems (Git)  \n Preference to developer with experience working with healthcare data and Health IT  \n Apache Hadoop (Cloudera), AWS Data Platforms (Redshift, S3, EMR/Hive), SQL, Java, Kafka, Scala, Kotlin, Neo4j, NiFi, Flink, Sqoop, PostgreSQL, EMR, Apache Spark, Python, PHP, Oracle, Splunk, BDD, testing framework: Cucumber  \n Knowledge of and experience using various NLP approaches, particularly:  \n Pattern recognition/feature extraction  \n Supervised, Unsupervised, and Semi-Supervised learning techniques  \n Understanding of various language models (N-Gram, Skipgram, NLM, etc.)  \n Chunking/Tokenization  \n Semantic parsing  \n Healthcare IT experience  \n Statistical model building (particularly classification)  ", "techs": ["etl", "hadoop", "mapreduce", "python", "java", "oracle enterprise manager", "sql", "aws", "postgresql", "mongodb", "git", "apache hadoop (cloudera)", "aws data platforms (redshift", "s3", "emr/hive)", "kafka", "scala", "kotlin", "neo4j", "nifi", "flink", "sqoop", "emr", "apache spark", "php", "oracle", "splunk", "cucumber", "nlp"]}, "9a1c535ce3d676dc": {"terms": ["data engineer"], "salary_min": 60.0, "salary_max": 60.0, "title": "Data Engineer - 230123 (No C2C or third parties)", "company": "Bridgewater Consulting Group", "desc": "Position: 230123 \u2013 Data Engineer \n Location: Remote \n Responsibilities \n \u00b7 Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation. \n \u00b7 Collaborate with product and technology teams to design and validate the capabilities of the data platform \n \u00b7 Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability \n \u00b7 Provide technical support and usage guidance to the users of our platform\u2019s services. \n \u00b7 Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services. \n Requirements: \n \u00b7 13-15 years\u2019 experience building and optimizing data pipelines in a distributed environment \n \u00b7 Experience supporting and working with cross-functional teams \n \u00b7 Proficiency working in Linux environment \n \u00b7 4+ years of advanced working knowledge of SQL, Python, Spark, and PySpark \n \u00b7 2+ years of experience with using a broad range of AWS technologies \n \u00b7 Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline \n \u00b7 Experience with platform monitoring and alerts tools \n Job Type: Contract \n Pay: $60.00 per hour \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n Vision insurance \n \n Schedule: \n \n Monday to Friday \n \n Experience: \n \n platform monitoring and alerts tools: 10 years (Required) \n develop and enhance data-processing: 10 years (Required) \n building & optimizing data pipelines in a distributed envir.: 10 years (Required) \n working in Linux environment: 10 years (Required) \n working knowledge of SQL, Python, Spark, and PySpark: 4 years (Required) \n using a broad range of AWS technologies: 2 years (Required) \n Git/Bitbucket, Jenkins/CodeBuild, CodePipeline: 10 years (Required) \n \n Work Location: Remote", "cleaned_desc": "Position: 230123 \u2013 Data Engineer \n Location: Remote \n Responsibilities \n \u00b7 Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation. \n \u00b7 Collaborate with product and technology teams to design and validate the capabilities of the data platform \n \u00b7 Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability \n \u00b7 Provide technical support and usage guidance to the users of our platform\u2019s services.   \u00b7 Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services. \n Requirements: \n \u00b7 13-15 years\u2019 experience building and optimizing data pipelines in a distributed environment \n \u00b7 Experience supporting and working with cross-functional teams \n \u00b7 Proficiency working in Linux environment \n \u00b7 4+ years of advanced working knowledge of SQL, Python, Spark, and PySpark \n \u00b7 2+ years of experience with using a broad range of AWS technologies ", "techs": ["aws", "gitlab", "sql", "python", "spark", "pyspark"]}, "c2f1c0999a61fd4c": {"terms": ["data engineer"], "salary_min": 92795.12, "salary_max": 117499.29, "title": "Data Engineer II", "company": "Careoperative Llc", "desc": "The Data Engineer will support our efforts to integrate, validate, enrich, and aggregate healthcare data for use by our analysts and data scientists. You\u2019ll help find new and useful sources of information and lead the charge to go get it and make sure we can keep this data up to date over time. You will also help ensure this data is translated to the rest of the analytics teams in a useable format. You will be expected to master the intricacies of efficiently acquiring large amounts of data from various sources, designing pipelines to move that data to and from our hosted and cloud environments, and push the evolution of these processes to accommodate new types of data. \n \n  Essential Duties and Responsibilities: \n \n \n  Support continuous improvements to data processing across the company \n  Designing and implementing performant data ingestion pipelines from multiple sources using Databricks on Azure \n  Build and maintain data pipelines and automate existing manual processes. \n  Work with Data Integration to build and maintain efficient processes for loading acquired data into our relational, dimensional, and no-SQL databases \n  Implement data lake architecture best practices to ensure a standardized and scalable way that we store and process our data \n  Work with other teams to identify new sources of data, and estimate feasibility of acquiring specific data sources \n  Take an active role in agile processes \n \n  This is a mid-level role, and the ideal candidate will have several of the following. \n \n \n  2+ years of work experience in a highly technical and hands-on data engineering, or data science role \n  Experience in building ETL processes. \n  Experience working with Data Lakehouse and / or migrating RDBMS systems to a Lakehouse. \n  Experience working with the most common types of healthcare data (medical claims, eligibility, provider network rosters, Rx claims, etc.) from a variety of sources. \n  Strong organizational skills and time management capacity to balance multiple projects with limited supervision. \n  Ability to build and re-evaluate a process from the ground up. \n  Strong investigative skills with ability to search beyond the initial results. \n  Have a fanatical attention to detail, with overwhelming desire to test and double-check your own results. \n  Comfortable working with messy data and ambiguous results \n  Education (Bachelor\u2019s degree or greater, or equivalent practical experience) in a quantitative field such as statistics, mathematics, engineering, or computer science \n \n \n  Technical Proficiencies: \n \n  Our infrastructure consists of a hybrid solution (traditional hosting + cloud hosting) that uses the technologies below. Ideal candidates will have experience with several of these. While we use Azure for our cloud environment, experience with equivalent cloud services from AWS and/or Google is acceptable. \n \n \n  Delta Data Lakehouse \n  Databricks / Spark \n  SQL Server \n  .NET Framework and C# \n  Azure Blob Storage \n  Azure Data Factory \n  Azure Synapse Analytics \n  Python and Jupyter Notebooks \n  Version control systems (Git, Azure DevOps, etc) \n  Batch or streaming data ingestion \n  Experience interacting with APIs \n  Experience building workflows \n \n \n  Healthcare Bluebook Core Values: \n \n  As a member of the Bluebook team, it is expected that you will live and breathe our Core Values each day in everything you do striving to help us achieve our purpose of Protecting Patients by exposing the truth and empowering choice. \n \n \n  Substance  Create Value over splash. \n  Humility  Serve with Gratitude. \n  Accountability  Own it no matter what. \n  Resourcefulness  Always agile with change. \n  Integrity  Honor the truth.", "cleaned_desc": "The Data Engineer will support our efforts to integrate, validate, enrich, and aggregate healthcare data for use by our analysts and data scientists. You\u2019ll help find new and useful sources of information and lead the charge to go get it and make sure we can keep this data up to date over time. You will also help ensure this data is translated to the rest of the analytics teams in a useable format. You will be expected to master the intricacies of efficiently acquiring large amounts of data from various sources, designing pipelines to move that data to and from our hosted and cloud environments, and push the evolution of these processes to accommodate new types of data. \n \n  Essential Duties and Responsibilities: \n \n \n  Support continuous improvements to data processing across the company \n  Designing and implementing performant data ingestion pipelines from multiple sources using Databricks on Azure \n  Build and maintain data pipelines and automate existing manual processes. \n  Work with Data Integration to build and maintain efficient processes for loading acquired data into our relational, dimensional, and no-SQL databases \n  Implement data lake architecture best practices to ensure a standardized and scalable way that we store and process our data \n  Work with other teams to identify new sources of data, and estimate feasibility of acquiring specific data sources    Take an active role in agile processes \n \n  This is a mid-level role, and the ideal candidate will have several of the following. \n \n \n  2+ years of work experience in a highly technical and hands-on data engineering, or data science role \n  Experience in building ETL processes. \n  Experience working with Data Lakehouse and / or migrating RDBMS systems to a Lakehouse. \n  Experience working with the most common types of healthcare data (medical claims, eligibility, provider network rosters, Rx claims, etc.) from a variety of sources. \n  Strong organizational skills and time management capacity to balance multiple projects with limited supervision. \n  Ability to build and re-evaluate a process from the ground up.    Strong investigative skills with ability to search beyond the initial results. \n  Have a fanatical attention to detail, with overwhelming desire to test and double-check your own results. \n  Comfortable working with messy data and ambiguous results \n  Education (Bachelor\u2019s degree or greater, or equivalent practical experience) in a quantitative field such as statistics, mathematics, engineering, or computer science \n \n \n  Technical Proficiencies: \n \n  Our infrastructure consists of a hybrid solution (traditional hosting + cloud hosting) that uses the technologies below. Ideal candidates will have experience with several of these. While we use Azure for our cloud environment, experience with equivalent cloud services from AWS and/or Google is acceptable. \n \n    Delta Data Lakehouse \n  Databricks / Spark \n  SQL Server \n  .NET Framework and C# \n  Azure Blob Storage \n  Azure Data Factory \n  Azure Synapse Analytics \n  Python and Jupyter Notebooks \n  Version control systems (Git, Azure DevOps, etc) \n  Batch or streaming data ingestion \n  Experience interacting with APIs ", "techs": ["databricks", "azure", "data lakehouse", "sql server", ".net framework", "c#", "azure blob storage", "azure data factory", "azure synapse analytics", "python", "jupyter notebooks", "git", "azure devops", "batch", "streaming", "apis"]}, "24e27419023d9d98": {"terms": ["data engineer"], "salary_min": 65.8, "salary_max": 70.87, "title": "Compliance +Actimize Data Engineer (Must have Banking)", "company": "proit-inc", "desc": "Role: DATA Engineer + (Cloud or GCP) +Banking Domain exp ( Any compliance technology + Actimize \n Compliance Technology \u2013 (AML Transaction Monitoring, CDD, Sanctions Screening etc.) \n Client: Manufacturers Bank \n Remote \n \n Experience : 9+ years \n \n \n Domain - Actimize on Cloud \n \n - Experience in any Compliance Technology ( AML Transaction Monitoring, CDD, Sanctions Screening etc.) \n - Experience with Actimize SAM on Cloud is a BIG PLUS or Actimize on Prem \n - Experience with any of the core banking platforms - FIS, Mission Lane, Fircosoft \n - Experience with GCP/Azure, ETL, Operations Data Store (ODS) \n Must have skills: \n \n Experience in any Compliance Technology ( AML Transaction Monitoring, CDD, Sanctions Screening etc.) \n \n \n Experience with Actimize SAM on Cloud is a BIG PLUS or Actimize on Prem \n \n Nice to have skills: \n \n Experience with GCP/Azure, ETL, Operations Data Store (ODS) \n \n \n Experience with any of the core banking platforms - FIS, Mission Lane, Fircosoft \n \n Job Types: Temporary, Contract, Permanent \n Salary: $65.80 - $70.87 per hour \n Experience level: \n \n 10 years \n 11+ years \n 9 years \n \n Application Question(s): \n \n Please Mention Work Authorization like USC OR GC etc \n \n Experience: \n \n Actimize: 5 years (Required) \n Compliance Technology: 5 years (Required) \n AML Transaction Monitoring, CDD, Sanctions Screening: 5 years (Required) \n data Engineer: 10 years (Preferred) \n Actimize SAM: 2 years (Required) \n \n Work Location: Remote", "cleaned_desc": " \n - Experience in any Compliance Technology ( AML Transaction Monitoring, CDD, Sanctions Screening etc.) \n - Experience with Actimize SAM on Cloud is a BIG PLUS or Actimize on Prem \n - Experience with any of the core banking platforms - FIS, Mission Lane, Fircosoft \n - Experience with GCP/Azure, ETL, Operations Data Store (ODS) \n Must have skills: \n \n Experience in any Compliance Technology ( AML Transaction Monitoring, CDD, Sanctions Screening etc.) \n   \n Experience with Actimize SAM on Cloud is a BIG PLUS or Actimize on Prem \n \n Nice to have skills: \n \n Experience with GCP/Azure, ETL, Operations Data Store (ODS) \n \n \n Experience with any of the core banking platforms - FIS, Mission Lane, Fircosoft ", "techs": ["aml transaction monitoring", "cdd", "sanctions screening", "actimize sam on cloud", "actimize on prem", "fis", "mission lane", "fircosoft", "gcp/azure", "etl", "operations data store (ods)"]}, "c9442150bc47cd96": {"terms": ["data engineer"], "salary_min": 80000.0, "salary_max": 85000.0, "title": "Data Engineer", "company": "Two Point Conversions, Inc", "desc": "Data Engineer \n Salary $80,000 - $85,000 annually \n Two Point Conversions, Inc. is a leading global healthcare data company with over 30 years of experience helping migrate data for thousands of pharmacies and hospitals. Join a company that is just as employee-focused as it is on its customers. Be the reason we help thousands of stores convert their health care and pharmacy data seamlessly. \n A data engineer is responsible for building systems that collect, manage, and convert raw data into usable information to fulfill business needs. \n Responsibilities \n \n Design, develop, and maintain optimal data pipelines  leveraging open source tools and data processing frameworks that align with business requirements. \n Identify, design, and implement internal process improvements . \n Build the infrastructure required for optimal extraction, transformation, and loading (ETL) of data  from a wide variety of data sources using (R)DBMS and Cloud technologies. \n Maintain various codebases for tools and services  used by other stakeholders. \n Create analytics tools and visualizations  such as dashboards and reports that provide actionable insights to stakeholders. \n Work with internal and external stakeholders  to assist with data-related technical issues and support their data infrastructure needs. \n Ensure compliance with data governance and security policies . \n Proactively improve encountered modules, services, systems and codebases . \n Improve documentation in wikis and internal knowledge bases that is insufficient or incorrect and proactively raise spotted issues . This includes issue status tracking and documenting the resolution for future use. \n Apply software development practices  related to testing, style, readability, compilation, continuous integration, and continuous deployment. \n Participate in reverse engineering of various data formats , often at the binary level. \n \n Qualifications \n \n Proficient in  multiple object-oriented and procedural programming languages . \n Experience with  Relational and non-relational databases . Desired experience includes advanced working SQL knowledge and experience working with relational databases, query authoring (SQL), data modeling, and common database interfaces (e.g. ODBC). \n Strong analytic skills  related to working with  structured and unstructured datasets . \n Experience with  ETL (extract, transform, and load) systems . \n Experience  building and optimizing data pipelines, architectures and data sets . \n Experience with  data stored in various formats and locations . \n Experience with  automation and scripting . \n Experience  leveraging relevant open source and internal development and data-related tools . \n Experience with  cloud computing and storage . \n Strong understanding of methods of  securely managing and storing data to protect it . \n Experience  supporting and working with cross-functional teams in a dynamic environment . \n Experience performing  root cause analysis  on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \n Experience with  version control systems and code reviews.  Desired experience also CI/CD pipelines. \n \n This is an entry-level position. We're looking for someone with 1-3 years of experience in data engineering or software development with a general purpose programming language, has a BS degree in computer science or a similar area. \n You\u2019ll be joining the pharmacy data engineering team where you develop programs for migrating customers between pharmacy management systems. \n Familiarity with the following software/tools is a nice to have: \n (list of languages, tools, cloud providers here) \n Languages may include Perl, Python, Bash, Batch, Powershell, VFP, and Golang. \n Cloud technology platforms may include GCP, Azure, and AWS. \n For consideration, please apply directly to : hr@twopoint.com \n We are unable to offer sponsorship. \n Job Type: Full-time \n Pay: $80,000.00 - $85,000.00 per year \n Benefits: \n \n 401(k) \n 401(k) matching \n Dental insurance \n Employee assistance program \n Flexible spending account \n Health insurance \n Life insurance \n Paid time off \n Professional development assistance \n Vision insurance \n \n Compensation package: \n \n Yearly pay \n \n Experience level: \n \n 2 years \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Education: \n \n Bachelor's (Preferred) \n \n Work Location: Remote", "cleaned_desc": "Data Engineer \n Salary $80,000 - $85,000 annually \n Two Point Conversions, Inc. is a leading global healthcare data company with over 30 years of experience helping migrate data for thousands of pharmacies and hospitals. Join a company that is just as employee-focused as it is on its customers. Be the reason we help thousands of stores convert their health care and pharmacy data seamlessly. \n A data engineer is responsible for building systems that collect, manage, and convert raw data into usable information to fulfill business needs. \n Responsibilities \n \n Design, develop, and maintain optimal data pipelines  leveraging open source tools and data processing frameworks that align with business requirements. \n Identify, design, and implement internal process improvements . \n Build the infrastructure required for optimal extraction, transformation, and loading (ETL) of data  from a wide variety of data sources using (R)DBMS and Cloud technologies. \n Maintain various codebases for tools and services  used by other stakeholders. \n Create analytics tools and visualizations  such as dashboards and reports that provide actionable insights to stakeholders. \n Work with internal and external stakeholders  to assist with data-related technical issues and support their data infrastructure needs. \n Ensure compliance with data governance and security policies . \n Proactively improve encountered modules, services, systems and codebases .   Improve documentation in wikis and internal knowledge bases that is insufficient or incorrect and proactively raise spotted issues . This includes issue status tracking and documenting the resolution for future use. \n Apply software development practices  related to testing, style, readability, compilation, continuous integration, and continuous deployment. \n Participate in reverse engineering of various data formats , often at the binary level. \n \n Qualifications \n \n Proficient in  multiple object-oriented and procedural programming languages . \n Experience with  Relational and non-relational databases . Desired experience includes advanced working SQL knowledge and experience working with relational databases, query authoring (SQL), data modeling, and common database interfaces (e.g. ODBC). \n Strong analytic skills  related to working with  structured and unstructured datasets . \n Experience with  ETL (extract, transform, and load) systems . \n Experience  building and optimizing data pipelines, architectures and data sets . \n Experience with  data stored in various formats and locations . \n Experience with  automation and scripting . \n Experience  leveraging relevant open source and internal development and data-related tools .   Experience with  cloud computing and storage . \n Strong understanding of methods of  securely managing and storing data to protect it . \n Experience  supporting and working with cross-functional teams in a dynamic environment . \n Experience performing  root cause analysis  on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \n Experience with  version control systems and code reviews.  Desired experience also CI/CD pipelines. \n \n This is an entry-level position. We're looking for someone with 1-3 years of experience in data engineering or software development with a general purpose programming language, has a BS degree in computer science or a similar area. \n You\u2019ll be joining the pharmacy data engineering team where you develop programs for migrating customers between pharmacy management systems. \n Familiarity with the following software/tools is a nice to have: \n (list of languages, tools, cloud providers here) \n Languages may include Perl, Python, Bash, Batch, Powershell, VFP, and Golang. \n Cloud technology platforms may include GCP, Azure, and AWS. \n For consideration, please apply directly to : hr@twopoint.com \n We are unable to offer sponsorship. ", "techs": ["data engineer", "sql", "relational databases", "non-relational databases", "etl systems", "data pipelines", "cloud computing", "automation", "scripting", "cross-functional teams", "root cause analysis", "version control systems", "code reviews", "ci/cd pipelines", "perl", "python", "bash", "batch", "powershell", "vfp", "golang", "gcp", "azure", "aws."]}, "215f0965525d4331": {"terms": ["data engineer"], "salary_min": 94181.56, "salary_max": 119254.84, "title": "Product Support Engineer - Data Security", "company": "Imperva", "desc": "Data Security is essential for any business, large or small, and they need to have a security partner they can trust and grow with. Imperva is that partner. \n  Imperva is committed to data security and is a recognized leader in both Application and Database security, appearing the Gartner\u2019s magic Quadrant as a leader in Application Security, WAF, since a WAF quadrant was introduced. Imperva is also recognized as an established leader in Database Security. \n  The Opportunity    Being a leader in Data Security, Imperva is positioned so that it can protect in any environment; on-sight, public cloud and hybrid. Imperva currently supports the 3 major cloud providers Azure, AWS, and Google Cloud. Imperva\u2019s hybrid solution allows customers to manage an on-sight and cloud deployment from a central management platform. \n  Imperva recognizes that customers are aggressively moving to the cloud, and therefore Imperva is delivering cloud based solutions to ensure they retain and enhance the security profile they had with their on-site deployments. \n  Imperva is looking for individuals who want to be a part of securing these dynamic environments and grow with Imperva as we expand our coverage and provide the latest in security methodologies. \n  A Product Support Engineer (PSE) is a member of a global team of support engineers that is dedicated to providing superior support and enabling customers to customize their defenses to fit their environment. A PSE also engages with all Customer Success teams within Imperva, and often is looked too, to drive solutions with the customer in mind \n  Responsibilities: \n \n  Handling level 2-3 support cases, working directly with Imperva customers, partners and Sales Engineers.\n    \n  Assess the severity and criticality of issues as they present themselves \n  Perform Case Management, customer communication and proactive follow ups through our CRM system. \n  Prioritize investigation efforts to attack the highest priority problems first. \n \n  Escalating customer\u2019s technical product issues to the Imperva Engineering department and working with them to achieve a resolution \n  Maintain a solid understanding of existing Imperva product offerings, while tracking new feature releases. \n  Contribute to the Imperva knowledge base, writing guides, technical how-to articles, etc. \n  Participate in rotating weekend shift schedule \n \n  Required Skills: \n \n  BSCS Degree or equivalent experience. \n  Ability to clearly communicate in English both verbally and in writing \n  Minimum 3 years of high-level Technical Support experience\n    \n  Additional experience working with Security or Network related products is preferred. \n \n  Support experience with a record of delivering customer satisfaction \n  Solid support experience working with technologies such as TCP/IP, HTTP, Load balancers, Proxies and Firewalls \n  Experience with troubleshooting tools such as tracert, tcpdump, and Wireshark \n  Troubleshooting experience with Relational Databases (MsSQL, Oracle, MySQL, DB2) is desirable. \n  Prior hands on experience with Linux and other Unix operating systems. \n  Highly motivated with the ability to work independently and in a team environment. \n  Well-organized with the ability to multi-task and prioritize with minimal supervision. \n  Excellent problem solving skills with a strong sense of customer commitment. \n  Excellent communication (written and verbal) and interpersonal skills. \n  Ability to understand and communicate concepts quickly, succinctly and accurately. \n  Demonstrated aptitude for learning new technologies while leveraging past experience \n \n  Desired Skills: \n \n  Experience with troubleshooting and deploying cloud solutions like those offered by AWS, Azure, and Google Cloud \n  Knowledge of a scripting language such as Perl, Python, Shell, Regex \n  Experience with SQL \n  Experience with Container deployments and virtual storage solutions \n \n  Our Company:  Imperva is an analyst-recognized, cybersecurity leader\u2014championing the fight to secure data and applications wherever they reside. Once deployed, our solutions proactively identify, evaluate, and eliminate current and emerging threats, so you never have to choose between innovating for your customers and protecting what matters most. Imperva\u2014Protect the pulse of your business. Learn more: www.imperva.com, our blog, on Twitter. \n  Rewards:  Imperva offers a competitive compensation package that includes base salary, medical, flexible time off and more. It\u2019s an exciting time to work in the security space. Check out our products and services at www.imperva.com and career opportunities at www.imperva.com/careers     Legal Notice:  Imperva is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, ancestry, pregnancy, age, sexual orientation, gender identity, marital status, protected veteran status, medical condition or disability, or any other characteristic protected by law.    #LI-Remote  #LI-VL", "cleaned_desc": " \n  Required Skills: \n \n  BSCS Degree or equivalent experience. \n  Ability to clearly communicate in English both verbally and in writing \n  Minimum 3 years of high-level Technical Support experience\n    \n  Additional experience working with Security or Network related products is preferred. \n    Support experience with a record of delivering customer satisfaction \n  Solid support experience working with technologies such as TCP/IP, HTTP, Load balancers, Proxies and Firewalls \n  Experience with troubleshooting tools such as tracert, tcpdump, and Wireshark \n  Troubleshooting experience with Relational Databases (MsSQL, Oracle, MySQL, DB2) is desirable. \n  Prior hands on experience with Linux and other Unix operating systems. \n  Highly motivated with the ability to work independently and in a team environment. \n  Well-organized with the ability to multi-task and prioritize with minimal supervision. \n  Excellent problem solving skills with a strong sense of customer commitment. \n  Excellent communication (written and verbal) and interpersonal skills.    Ability to understand and communicate concepts quickly, succinctly and accurately. \n  Demonstrated aptitude for learning new technologies while leveraging past experience \n \n  Desired Skills: \n \n  Experience with troubleshooting and deploying cloud solutions like those offered by AWS, Azure, and Google Cloud \n  Knowledge of a scripting language such as Perl, Python, Shell, Regex \n  Experience with SQL \n  Experience with Container deployments and virtual storage solutions ", "techs": ["tracert", "tcpdump", "wireshark", "mssql", "oracle", "mysql", "db2", "linux", "unix", "aws", "azure", "google cloud", "perl", "python", "shell", "regex", "sql", "container deployments", "virtual storage solutions"]}, "b9eaa87e9c364099": {"terms": ["data engineer", "machine learning engineer"], "salary_min": 110000.0, "salary_max": 130000.0, "title": "Senior Data Engineer - Cloud Infrastructure and Video", "company": "Sports Info Solutions", "desc": "Title:  Senior Data Engineer - Cloud Infrastructure and Video \n  Department:  BIDS \n  Reports to:  Chief Data Scientist \n \n  About Sports Info Solutions \n  Pioneers in the Sports Data Industry \n  SIS was founded on the belief that decision making in sports could be improved and that we could help teams win more games through the use of better data, analytics, and technology. \n  That belief has been validated repeatedly since our founding in 2002 as we continue to revolutionize the way the game is played, both on and off the field. \n  Company overview \n  Our mission is to enrich and optimize the decision-making process for sports teams, sportsbooks, and sports fans. \n  We are proud to be a leader in collecting, analyzing and distributing the deepest data sets and insights to professional sports teams across the MLB, NBA and NFL. \n  We are now doubling down on what\u2019s made us successful by further advancing our data, technology, insights and partners as we drive forward the next innovations in Sports Data and Analytics. \n  Position overview \n  Sports Info Solutions (SIS) is looking for a new team member to fill a full time position in our BIDS department as Senior Data Engineer. \n  We seek a skilled and experienced Data Engineer to join our dynamic and growing team at Sports Info Solutions. As a Senior Data Engineer, you will be crucial in building and maintaining our cloud-based data infrastructure, designing and optimizing data lakes for structured and semi-structured data, and ensuring seamless data integration and preparation for machine learning training and inference. \n \n  This position is considered remote. \n \n  What you\u2019ll do as  Senior Data Engineer  on the team at SIS: \n \n  Includes (but is not limited to): \n \n \n \n  Design, develop, and maintain pipelines optimized for video ingestion, processing, and management, utilizing cloud-based services for scalability and reliability. \n  Collaborate closely with data scientists and computer vision experts to understand the requirements for building deep learning models and implement video preprocessing and transformation steps. \n  Leverage your expertise in cloud infrastructure, e.g., AWS (preferred), GCP, or Azure, to create scalable and efficient data solutions for CV applications. \n  Lead the development of pipelines to preprocess and augment video data, ensuring compatibility with deep learning frameworks. \n  Build and maintain data pipelines that enable real-time or near-real-time video data processing for inference and analytics. \n  Collaborate with cross-functional teams to deliver CV insights and predictions to clients successfully. \n \n \n  Why work with SIS? \n  We believe in making sports better through data, analysis and insights. For that reason, we have an incredible team of technologists, scouts, analysts, and operators helping our partners win more games. \n  It is our ultimate vision to create an unparalleled platform of sporting data and insights, through best-in-class technology, products and partnerships. \n  We believe in a flexible, energetic, enjoyable working environment where we band together as teammates to do great things. We are committed to creating a diverse environment, working in a collaborative, team-centric environment. \n \n  Qualifications \n  If you possess the following, you are well on your way to making an impact at SIS: \n \n  Bachelor's degree in Computer Science, Engineering, a related field, or equivalent work experience; advanced degree is a plus. \n  Proven experience designing and implementing data pipelines for video ingest and management in a cloud environment. \n  Strong understanding of cloud-based services and infrastructure and their video processing, content delivery, and storage integration. \n  Strong programming skills in languages such as Python (preferred), Rust, C++, or Java. \n  Desire and ability to write maintainable, tested code. \n  Knowledge of video codecs, formats, and processing techniques for efficient video data handling. \n  Familiarity with machine learning concepts and experience preparing data for training and inference purposes. \n  Familiarity with computer vision concepts and practices, including data preprocessing, augmentation, and annotation. \n  Strong problem-solving skills and ability to work collaboratively in a fast-paced environment. \n  Excellent communication skills to convey complex concepts to technical and non-technical stakeholders. \n \n \n  EEO commitment \n  SIS provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, gender, national origin, age, veteran status, military status, disability, gender identity, sexual orientation, genetic information, or any other characteristic protected by law. In addition to federal law requirements, SIS complies with applicable state and local laws governing nondiscrimination in employment in every location where the company operates. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. \n  Abilities required \n  These physical demands are representative of the physical requirements necessary for an employee to successfully perform the essential functions of the job. Reasonable accommodation can be made to enable people with disabilities to perform the described essential functions of the job. \n  While performing the responsibilities of the job, the employee is required to talk and hear. The employee is often required to sit and use their hands and fingers, to handle or feel. The employee is occasionally required to stand, walk, reach with arms and hands, climb or balance, and to stoop, kneel, crouch or crawl. Vision abilities required by this job include close vision, including intensive computer usage. \n  Additional info \n  Sponsorship is not available for this position. Applicants must be currently authorized to work in the United States on a full-time basis. \n \n  Sports Info Solutions uses E-Verify and is an Equal Opportunity Employer. \n   \n   \n p7VbyrjRGL", "cleaned_desc": "  Design, develop, and maintain pipelines optimized for video ingestion, processing, and management, utilizing cloud-based services for scalability and reliability. \n  Collaborate closely with data scientists and computer vision experts to understand the requirements for building deep learning models and implement video preprocessing and transformation steps. \n  Leverage your expertise in cloud infrastructure, e.g., AWS (preferred), GCP, or Azure, to create scalable and efficient data solutions for CV applications. \n  Lead the development of pipelines to preprocess and augment video data, ensuring compatibility with deep learning frameworks. \n  Build and maintain data pipelines that enable real-time or near-real-time video data processing for inference and analytics. \n  Collaborate with cross-functional teams to deliver CV insights and predictions to clients successfully. \n \n \n  Why work with SIS? \n  We believe in making sports better through data, analysis and insights. For that reason, we have an incredible team of technologists, scouts, analysts, and operators helping our partners win more games. \n  It is our ultimate vision to create an unparalleled platform of sporting data and insights, through best-in-class technology, products and partnerships. \n  We believe in a flexible, energetic, enjoyable working environment where we band together as teammates to do great things. We are committed to creating a diverse environment, working in a collaborative, team-centric environment.   \n  Qualifications \n  If you possess the following, you are well on your way to making an impact at SIS: \n \n  Bachelor's degree in Computer Science, Engineering, a related field, or equivalent work experience; advanced degree is a plus. \n  Proven experience designing and implementing data pipelines for video ingest and management in a cloud environment. \n  Strong understanding of cloud-based services and infrastructure and their video processing, content delivery, and storage integration. \n  Strong programming skills in languages such as Python (preferred), Rust, C++, or Java. \n  Desire and ability to write maintainable, tested code. \n  Knowledge of video codecs, formats, and processing techniques for efficient video data handling. \n  Familiarity with machine learning concepts and experience preparing data for training and inference purposes. \n  Familiarity with computer vision concepts and practices, including data preprocessing, augmentation, and annotation. ", "techs": ["video ingestion", "processing", "and management", "cloud-based services", "scalability", "reliability", "deep learning models", "video preprocessing", "transformation steps", "cloud infrastructure", "aws", "gcp", "azure", "data pipelines", "deep learning frameworks", "real-time", "near-real-time", "inference", "analytics", "cv insights", "sis", "technologists", "scouts", "analysts", "operators", "sporting data", "best-in-class technology", "products", "partnerships", "flexible", "energetic", "enjoyable working environment", "diverse environment", "bachelor's degree in computer science", "engineering", "cloud environment", "programming skills", "python", "rust", "c++", "java", "maintainable code", "video codecs", "formats", "processing techniques", "machine learning concepts", "data preparation", "computer vision concepts", "data preprocessing", "augmentation", "annotation"]}, "ee45336043614eb9": {"terms": ["data engineer", "machine learning engineer"], "salary_min": 116371.12, "salary_max": 147351.77, "title": "Data Engineer III - Weather Technology", "company": "AccuWeather Careers", "desc": "SUMMARY \n  To support AccuWeather's continued focus on innovation and generating superior weather products, we are seeking a skilled and experienced Data Engineer III to join our talented data engineering team. If you are passionate about transforming raw data into actionable insights and enjoy working with the latest tools in the data engineering space, we'd love to hear from you.    As a Data Engineer III, you will play a critical role in designing, building, and maintaining our data infrastructure, enabling efficient data processing, analysis, and reporting. You will work closely with cross-functional teams, including data scientists, analysts, and software engineers, to ensure the availability, reliability, and performance of our data pipelines and platforms. Your expertise in Databricks will be essential in optimizing data workflows, ensuring data quality, and contributing to the continuous improvement of our data engineering practices. \n  DUTIES AND RESPONSIBILITIES: \n \n Design, develop, and maintain scalable and robust data pipelines using Databricks, Spark, and related technologies. \n Collaborate with data scientists and analysts to understand data requirements, and implement solutions that support their analytical needs. \n Optimize and fine-tune data workflows for performance, reliability, and efficiency. \n Implement best practices for data modeling, ETL processes, and data integration. \n Monitor, troubleshoot, and resolve issues related to data pipelines and platform performance. \n Work with other Data Engineers to provide guidance, support, and mentorship. \n Collaborate with Data Ops team members to ensure smooth deployment and operation of data engineering solutions. \n Stay current with industry trends and advancements in data engineering, Databricks, and related technologies. \n Contribute to the development and enhancement of data engineering standards, practices, and documentation. \n \n QUALIFICATIONS: \n \n Bachelor's degree in Computer Science, Engineering, or a related field. \n Proven experience as a Data Engineer, with a strong focus on Databricks, Spark, and cloud-based data platforms (e.g., AWS, Azure, Google Cloud). \n Expertise in designing, developing, and optimizing complex ETL processes and data pipelines. \n Proficiency in programming languages like Python, Scala, or Java. \n Solid understanding of data modeling, data warehousing, and data integration concepts. \n Familiarity with DevOps practices and tools for CI/CD and infrastructure automation. \n Strong problem-solving skills and ability to troubleshoot and resolve complex technical issues. \n Excellent communication and teamwork skills, with the ability to collaborate effectively with cross-functional teams. \n Experience with big data technologies, real-time data processing, and machine learning pipelines is a plus. \n Relevant certifications in Databricks, Spark, or cloud platforms are a strong advantage. \n \n WHY JOIN US: \n \n Opportunity to work in a dynamic and innovative environment at the forefront of data-driven decision-making. \n Chance to make a significant impact by contributing to the development of advanced data engineering solutions. \n Collaborative and inclusive company culture that values continuous learning and professional growth. \n Competitive compensation and benefits package. \n Access to cutting-edge technologies and opportunities to stay current with industry trends. \n \n PHYSICAL DEMANDS \n  The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. While performing the duties of this job, the employee is regularly required to sit; use hands to finger, handle, or feel; reach with hands and arms; and talk or hear. The employee frequently is required to walk. The employee is occasionally required to stand and stoop, kneel, crouch, or crawl. The employee must frequently lift and/or move up to 25 pounds. Specific vision abilities required by this job include close vision, and ability to adjust focus. Minimal travel (10%) is required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. \n \n \n  WORK ENVIRONMENT \n  The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. \n  While performing the duties of this job, the employee is occasionally exposed to outside weather conditions. The noise level in the work is usually quiet.", "cleaned_desc": "SUMMARY \n  To support AccuWeather's continued focus on innovation and generating superior weather products, we are seeking a skilled and experienced Data Engineer III to join our talented data engineering team. If you are passionate about transforming raw data into actionable insights and enjoy working with the latest tools in the data engineering space, we'd love to hear from you.    As a Data Engineer III, you will play a critical role in designing, building, and maintaining our data infrastructure, enabling efficient data processing, analysis, and reporting. You will work closely with cross-functional teams, including data scientists, analysts, and software engineers, to ensure the availability, reliability, and performance of our data pipelines and platforms. Your expertise in Databricks will be essential in optimizing data workflows, ensuring data quality, and contributing to the continuous improvement of our data engineering practices. \n  DUTIES AND RESPONSIBILITIES: \n \n Design, develop, and maintain scalable and robust data pipelines using Databricks, Spark, and related technologies. \n Collaborate with data scientists and analysts to understand data requirements, and implement solutions that support their analytical needs. \n Optimize and fine-tune data workflows for performance, reliability, and efficiency. \n Implement best practices for data modeling, ETL processes, and data integration.   Monitor, troubleshoot, and resolve issues related to data pipelines and platform performance. \n Work with other Data Engineers to provide guidance, support, and mentorship. \n Collaborate with Data Ops team members to ensure smooth deployment and operation of data engineering solutions. \n Stay current with industry trends and advancements in data engineering, Databricks, and related technologies. \n Contribute to the development and enhancement of data engineering standards, practices, and documentation. \n \n QUALIFICATIONS: \n   Bachelor's degree in Computer Science, Engineering, or a related field. \n Proven experience as a Data Engineer, with a strong focus on Databricks, Spark, and cloud-based data platforms (e.g., AWS, Azure, Google Cloud). \n Expertise in designing, developing, and optimizing complex ETL processes and data pipelines. \n Proficiency in programming languages like Python, Scala, or Java. \n Solid understanding of data modeling, data warehousing, and data integration concepts. \n Familiarity with DevOps practices and tools for CI/CD and infrastructure automation. \n Strong problem-solving skills and ability to troubleshoot and resolve complex technical issues. \n Excellent communication and teamwork skills, with the ability to collaborate effectively with cross-functional teams.   Experience with big data technologies, real-time data processing, and machine learning pipelines is a plus. \n Relevant certifications in Databricks, Spark, or cloud platforms are a strong advantage. \n \n WHY JOIN US: \n \n Opportunity to work in a dynamic and innovative environment at the forefront of data-driven decision-making. \n Chance to make a significant impact by contributing to the development of advanced data engineering solutions. \n Collaborative and inclusive company culture that values continuous learning and professional growth. ", "techs": ["databricks", "spark", "aws", "azure", "google cloud", "python", "scala", "java", "devops"]}, "c3e49dc759a882fb": {"terms": ["data engineer", "machine learning engineer"], "salary_min": 115000.0, "salary_max": 115000.0, "title": "AgBio Organism Engineer 3: Data Science & Analytics", "company": "Ginkgo Bioworks", "desc": "Our mission is to make biology easier to engineer. Ginkgo is constructing, editing, and redesigning the living world in order to answer the globe's growing challenges in health, energy, food, materials, and more. Our bioengineers make use of an in-house automated foundry for designing and building new organisms. \n \n \n  Job Description \n  On the Data Science and Analytics team, our mission is to drive scientifically sound decision-making by providing innovative solutions using FAIR data principles to maximize the power of data. As an Ag Biologicals Engineer 3, you'll join a distributed and highly collaborative team composed of data scientists, statisticians, bioinformaticians and data engineers with a wide range of backgrounds and experience. You will be part of the decision science subteam where you will leverage your foundational statistical and data science knowledge to collaborate with your colleagues and stakeholders to address scientific project needs and enable data-driven decision-making. You are inherently curious, thrive in a fast paced collaborative environment, enjoy mentoring others and excel at storytelling with data. \n  Target: AgBio Organism Engineer 3 Ginkgo level descriptions \n  [Statistics] [programming (python/R)] [Machine learning] [Biology] \n \n \n  Responsibilities \n \n Statistical analysis and decision support for the advancement of projects using frequentist and Bayesian approaches. \n Collaborate with bench scientists and stakeholders to assist and lead efficient design of experiments at diverse scales and across disciplines. \n \n \n Experimental design and optimization of experimental assays through the modeling (and discovery) of relevant covariates to minimize their impact while maximizing power. \n Statistical modeling and machine learning to support assay analyses with high data content. \n Fluency and practical experience with data visualization tools to aid researchers in extracting relevant information and enable data driven decision making . \n Contributing to the overall increase of data acumen including statistical knowledge of the team and Ginkgo as a whole. \n Capable of critical thinking and comfortable objectively critiquing existing approaches and ways of working. \n Willingness to grow and work outside your comfort zone, e.g. proactively exploring new tools, identifying new ways to solve problems, and applying pragmatism to drive solutions. \n \n \n \n  Minimum Requirements \n \n Bachelor's degree and 4 years relevant industry experience or Masters or PhD degree in a quantitative field. \n Training or experience in statistical inference using parametric and non-parametric frequentist and Bayesian approaches, including the use of corrections for multiple testing, techniques for addressing confounders/covariates. \n Strong communication skills with the ability to explain complex technical and scientific information in a clear, precise and actionable manner to stakeholders with differing technical expertise is essential for success in this role. \n Proficiency in R or Python with demonstrated knowledge of best coding practices (documentation and version control with git) and performing reproducible analyses to generate standardized summary reports. \n Highly collaborative and effective at building partnerships with colleagues with diverse technical expertise, communication styles, and cultures. \n Strong project and time management skills with the ability to adapt to changing priorities in a fast paced environment. \n \n \n \n  Preferred Capabilities and Experience \n \n Training or experience in machine learning using various approaches, including regression, neural networks, cross-validation, and hyperparameter tuning, using various data types. \n Training or experience in frequentist approaches to power and multiple testing. \n Experience with biological datasets and the covariates and confounders typically found in biological systems, especially those in the environment. \n Experience with experimental design and validation, especially of highly variable biological systems in the environment. \n Experience developing and deploying statistically focussed analysis pipelines in cloud architecture (AWS/Google Cloud) is a plus. \n Prior experience working in and communicating with highly distributed teams is a plus. \n \n \n Total compensation for this role is market driven, with a starting salary of $ 115,000 , as well as company stock awards. Base pay is ultimately determined based on a candidate's skills, expertise, and experience. We also offer a comprehensive benefits package including medical, dental & vision coverage, health spending accounts, voluntary benefits, leave of absence policies, Employee Assistance Program, 401(k) program with employer contribution, 8 paid holidays in addition to a full-week winter shutdown and unlimited Paid Time Off policy. \n \n \n \n    To learn more about Ginkgo, visit www.ginkgobioworks.com/press/ or check out some curated press below:\n   \n \n What is it really like to take your company public via a SPAC? One Boston biotech shares its journey (Fortune) \n Ginkgo Bioworks resizes the definition of going big in biotech, raising $2.5B in a record SPAC deal that weighs in with a whopping $15B-plus valuation (Endpoints News) \n Ginkgo Bioworks CEO on scaling up Covid-19 testing: 'If we try, we can win' (CNBC) \n Ginkgo raises $70 million to ramp up COVID-19 testing for employers, universities (Boston Globe) \n Ginkgo Bioworks Redirects Its Biotech Platform to Coronavirus (Wall Street Journal) \n Ginkgo Bioworks Provides Support on Process Optimization to Moderna for COVID-19 Response (PRNewswire) \n The Life Factory: Synthetic Organisms From This $1.4 Billion Startup Will Revolutionize Manufacturing (Forbes) \n Synthetic Bio Pioneer Ginkgo Raises $290 Million in New Funding (Bloomberg) \n Ginkgo Bioworks raises $350 million fund for biotech spinouts (Reuters) \n Can This Company Convince You to Love GMOs? (The Atlantic) \n \n \n \n   We also feel that it's important to point out the obvious here \u2013 there's a serious lack of diversity in our industry, and that needs to change. Our goal is to help drive that change. Ginkgo is deeply committed to diversity, equity, and inclusion in all of its practices, especially when it comes to growing our team. Our culture promotes inclusion and embraces how rewarding it is to work with people from all walks of life.\n   \n \n \n   We're developing a powerful biological engineering platform, so we must remain mindful of the many ways our technology can \u2013 and will \u2013 impact people around the world. We care about how our platform is used, and having a diverse team to build it gives us the best chance that it's something we'll be proud of as it continues to grow. Therefore, it's critical that we incorporate the diverse voices and visions of all those who play a role in the future of biology.\n   \n \n \n   It is the policy of Ginkgo Bioworks to provide equal employment opportunities to all employees, employment applicants, and EOE disability/vet.\n   \n \n \n Privacy Notice \n \n \n   I understand that I am applying for employment with Ginkgo Bioworks and am being asked to provide information in connection with my application. I further understand that Ginkgo gathers this information through a third-party service provider and that Ginkgo may also use other service providers to assist in the application process. Ginkgo may share my information with such third-party service providers in connection with my application and for the start of employment. Ginkgo will treat my information in accordance with Ginkgo's Privacy Policy. By submitting this job application, I am acknowledging that I have reviewed and agree to Ginkgo's Privacy Policy as well as the privacy policies of the third-party service providers used by Ginkgo's associated with the application process.", "cleaned_desc": "Our mission is to make biology easier to engineer. Ginkgo is constructing, editing, and redesigning the living world in order to answer the globe's growing challenges in health, energy, food, materials, and more. Our bioengineers make use of an in-house automated foundry for designing and building new organisms. \n \n \n  Job Description \n  On the Data Science and Analytics team, our mission is to drive scientifically sound decision-making by providing innovative solutions using FAIR data principles to maximize the power of data. As an Ag Biologicals Engineer 3, you'll join a distributed and highly collaborative team composed of data scientists, statisticians, bioinformaticians and data engineers with a wide range of backgrounds and experience. You will be part of the decision science subteam where you will leverage your foundational statistical and data science knowledge to collaborate with your colleagues and stakeholders to address scientific project needs and enable data-driven decision-making. You are inherently curious, thrive in a fast paced collaborative environment, enjoy mentoring others and excel at storytelling with data. \n  Target: AgBio Organism Engineer 3 Ginkgo level descriptions \n  [Statistics] [programming (python/R)] [Machine learning] [Biology] \n \n \n  Responsibilities \n \n Statistical analysis and decision support for the advancement of projects using frequentist and Bayesian approaches. \n Collaborate with bench scientists and stakeholders to assist and lead efficient design of experiments at diverse scales and across disciplines. \n \n \n Experimental design and optimization of experimental assays through the modeling (and discovery) of relevant covariates to minimize their impact while maximizing power.   Statistical modeling and machine learning to support assay analyses with high data content. \n Fluency and practical experience with data visualization tools to aid researchers in extracting relevant information and enable data driven decision making . \n Contributing to the overall increase of data acumen including statistical knowledge of the team and Ginkgo as a whole. \n Capable of critical thinking and comfortable objectively critiquing existing approaches and ways of working. \n Willingness to grow and work outside your comfort zone, e.g. proactively exploring new tools, identifying new ways to solve problems, and applying pragmatism to drive solutions. \n \n \n \n  Minimum Requirements \n \n Bachelor's degree and 4 years relevant industry experience or Masters or PhD degree in a quantitative field. \n Training or experience in statistical inference using parametric and non-parametric frequentist and Bayesian approaches, including the use of corrections for multiple testing, techniques for addressing confounders/covariates. \n Strong communication skills with the ability to explain complex technical and scientific information in a clear, precise and actionable manner to stakeholders with differing technical expertise is essential for success in this role. \n Proficiency in R or Python with demonstrated knowledge of best coding practices (documentation and version control with git) and performing reproducible analyses to generate standardized summary reports. \n Highly collaborative and effective at building partnerships with colleagues with diverse technical expertise, communication styles, and cultures. \n Strong project and time management skills with the ability to adapt to changing priorities in a fast paced environment. ", "techs": ["automated foundry", "data visualization tools", "statistical modeling", "machine learning", "python", "r", "statistical inference", "bayesian approaches", "experimental design", "data acumen", "data-driven decision making", "critical thinking", "parametric and non-parametric approaches", "corrections for multiple testing", "documentation", "version control with git", "reproducible analyses", "project management"]}, "ee83af5b5186b4a4": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "etl", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "ac1af3de232af228": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "66a787146e05e1a3": {"terms": ["data engineer"], "salary_min": 97836.19, "salary_max": 123882.41, "title": "Data Engineer", "company": "Land Intelligence Inc", "desc": "Who We Are  Land Intelligence is a software technology company serving the commercial real estate industry. We focus on Land Development. We have been recognized as an industry technology leader in providing solutions on a national scale. Our team are visionaries that see a better, faster, and more valuable way to research, finance, and trade land.     Our Culture  We are entrepreneurs first. Which means we manage the people, processes and product. We create new ways of doing things to drive value. We are builders and growth minded. Our leadership team has been recognized as a Best Places to work in the industry nationally. Our team drives for personal and professional development, as personal growth is instrumental to our success. Your learning will be supported by specialized in-house training programs and mentoring by the industry\u2019s leading experts, many of whom are our investors and strategic partners. \n  Job Overview  Land Intelligence is seeking a savvy Data Engineer to join our growing team and help us continue to enhance our SaaS platform, LandSUITE\u00ae. The hire will be responsible for expanding and optimizing our data and data pipeline architecture to support product development and internal tools. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of re-designing our company's data architecture to support our next generation of products and data initiatives.\n   Responsibilities \n \n  Create and maintain optimal data pipeline architecture \n  Assemble large, complex datasets that meet business requirements \n  Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \n  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies \n  Build analytics tools that use the data pipeline to provide actionable insights into user behavior and market trends \n  Work with stakeholders including the executive and product development teams to assist with data-related technical issues and support their data infrastructure needs \n  Keep our data separated and secure across AWS regions \n  Create data tools for analytics and team members that assist them in building and optimizing LandSUITE\u00ae into an innovative industry leader \n \n  Qualifications \n \n  We are looking for a candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field \n  Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases \n  Experience building and optimizing \u2018big data\u2019 data pipelines, architectures and data sets \n  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement \n  Strong analytic skills related to working with unstructured datasets \n  Build processes supporting data transformation, data structures, metadata, dependency and workload management \n  A successful history of manipulating, processing and extracting value from large, disconnected datasets \n  Working knowledge of message queuing, stream processing, and highly scalable \u2018big data\u2019 data stores \n  Strong project management and organizational skills \n  Experience supporting and working with cross-functional teams in a dynamic environment \n  Experience with the following software/tools:\n    \n  Experience with relational SQL and NoSQL databases, including Postgres, MySQL, and Cassandra \n  Experience with data pipeline and workflow management tools \n  Experience with stream-processing systems \n  Experience with AWS cloud services: EC2, EMR, RDS, Redshift \n  Experience with object-oriented/object function scripting languages: Python, Java \n \n  We are a startup, but this isn't our first time doing this. As a result, you can get the thrill of working at a startup, with the resources of a publicly traded company. We offer a best-in-class benefits package, as we are a Professional Employment Organization (PEO) with our partner Insperity that includes medical, vision, dental and life insurance. Our 401 (k) program offers an employer match, along with a 401(k)-profit sharing and performance-based bonuses. \u2022 Generous paid time off\n  \n  Land Intelligence is an EOE/Affirmative Action Employer M/F/D/V. If you are interested in applying for employment and need special assistance to apply for a posted position, please send an e-mail to careers@landintelligence.net.", "cleaned_desc": "  Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies \n  Build analytics tools that use the data pipeline to provide actionable insights into user behavior and market trends \n  Work with stakeholders including the executive and product development teams to assist with data-related technical issues and support their data infrastructure needs \n  Keep our data separated and secure across AWS regions \n  Create data tools for analytics and team members that assist them in building and optimizing LandSUITE\u00ae into an innovative industry leader \n \n  Qualifications   \n  We are looking for a candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field \n  Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases \n  Experience building and optimizing \u2018big data\u2019 data pipelines, architectures and data sets \n  Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement \n  Strong analytic skills related to working with unstructured datasets \n  Build processes supporting data transformation, data structures, metadata, dependency and workload management    A successful history of manipulating, processing and extracting value from large, disconnected datasets \n  Working knowledge of message queuing, stream processing, and highly scalable \u2018big data\u2019 data stores \n  Strong project management and organizational skills \n  Experience supporting and working with cross-functional teams in a dynamic environment \n  Experience with the following software/tools:\n    \n  Experience with relational SQL and NoSQL databases, including Postgres, MySQL, and Cassandra ", "techs": ["sql", "aws 'big data' technologies", "postgres", "mysql", "cassandra"]}, "298f52c78c5693a7": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "fbb1d75808499a72": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql."]}, "2c38e9a073c0d510": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "2702a0542671e041": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["etl", "aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "b07eedcc8b54d00d": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "a41430c86de49f37": {"terms": ["data engineer", "machine learning engineer"], "salary_min": 110000.0, "salary_max": 130000.0, "title": "Senior Data Engineer - Cloud Infrastructure and Video", "company": "Sports Info Solutions", "desc": "Title:  Senior Data Engineer - Cloud Infrastructure and Video \n  Department:  BIDS \n  Reports to:  Chief Data Scientist \n \n  About Sports Info Solutions \n  Pioneers in the Sports Data Industry \n  SIS was founded on the belief that decision making in sports could be improved and that we could help teams win more games through the use of better data, analytics, and technology. \n  That belief has been validated repeatedly since our founding in 2002 as we continue to revolutionize the way the game is played, both on and off the field. \n  Company overview \n  Our mission is to enrich and optimize the decision-making process for sports teams, sportsbooks, and sports fans. \n  We are proud to be a leader in collecting, analyzing and distributing the deepest data sets and insights to professional sports teams across the MLB, NBA and NFL. \n  We are now doubling down on what\u2019s made us successful by further advancing our data, technology, insights and partners as we drive forward the next innovations in Sports Data and Analytics. \n  Position overview \n  Sports Info Solutions (SIS) is looking for a new team member to fill a full time position in our BIDS department as Senior Data Engineer - Cloud Infrastructure and Video. \n  We seek a skilled and experienced Data Engineer to join our dynamic and growing team at Sports Info Solutions. As a Senior Data Engineer, you will be crucial in building and maintaining our cloud-based data infrastructure, designing and optimizing data lakes for structured and semi-structured data, and ensuring seamless data integration and preparation for machine learning training and inference. \n \n  This position is considered remote. \n \n  What you\u2019ll do as  Senior Data Engineer - Cloud Infrastructure and Video  on the team at SIS: \n \n  Includes (but is not limited to): \n \n \n \n  Design, develop, and maintain pipelines optimized for video ingestion, processing, and management, utilizing cloud-based services for scalability and reliability. \n  Collaborate closely with data scientists and computer vision experts to understand the requirements for building deep learning models and implement video preprocessing and transformation steps. \n  Leverage your expertise in cloud infrastructure, e.g., AWS (preferred), GCP, or Azure, to create scalable and efficient data solutions for CV applications. \n  Lead the development of pipelines to preprocess and augment video data, ensuring compatibility with deep learning frameworks. \n  Build and maintain data pipelines that enable real-time or near-real-time video data processing for inference and analytics. \n  Collaborate with cross-functional teams to deliver CV insights and predictions to clients successfully. \n \n \n  Why work with SIS? \n  We believe in making sports better through data, analysis and insights. For that reason, we have an incredible team of technologists, scouts, analysts, and operators helping our partners win more games. \n  It is our ultimate vision to create an unparalleled platform of sporting data and insights, through best-in-class technology, products and partnerships. \n  We believe in a flexible, energetic, enjoyable working environment where we band together as teammates to do great things. We are committed to creating a diverse environment, working in a collaborative, team-centric environment. \n \n  Qualifications \n  If you possess the following, you are well on your way to making an impact at SIS: \n \n  Bachelor's degree in Computer Science, Engineering, a related field, or equivalent work experience; advanced degree is a plus. \n  Proven experience designing and implementing data pipelines for video ingest and management in a cloud environment. \n  Strong understanding of cloud-based services and infrastructure and their video processing, content delivery, and storage integration. \n  Strong programming skills in languages such as Python (preferred), Rust, C++, or Java. \n  Desire and ability to write maintainable, tested code. \n  Knowledge of video codecs, formats, and processing techniques for efficient video data handling. \n  Familiarity with machine learning concepts and experience preparing data for training and inference purposes. \n  Familiarity with computer vision concepts and practices, including data preprocessing, augmentation, and annotation. \n  Strong problem-solving skills and ability to work collaboratively in a fast-paced environment. \n  Excellent communication skills to convey complex concepts to technical and non-technical stakeholders. \n \n \n  EEO commitment \n  SIS provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, gender, national origin, age, veteran status, military status, disability, gender identity, sexual orientation, genetic information, or any other characteristic protected by law. In addition to federal law requirements, SIS complies with applicable state and local laws governing nondiscrimination in employment in every location where the company operates. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. \n  Abilities required \n  These physical demands are representative of the physical requirements necessary for an employee to successfully perform the essential functions of the job. Reasonable accommodation can be made to enable people with disabilities to perform the described essential functions of the job. \n  While performing the responsibilities of the job, the employee is required to talk and hear. The employee is often required to sit and use their hands and fingers, to handle or feel. The employee is occasionally required to stand, walk, reach with arms and hands, climb or balance, and to stoop, kneel, crouch or crawl. Vision abilities required by this job include close vision, including intensive computer usage. \n  Additional info \n  Sponsorship is not available for this position. Applicants must be currently authorized to work in the United States on a full-time basis. \n \n  Sports Info Solutions uses E-Verify and is an Equal Opportunity Employer. \n   \n   \n 34bPse0KU0", "cleaned_desc": "  Design, develop, and maintain pipelines optimized for video ingestion, processing, and management, utilizing cloud-based services for scalability and reliability. \n  Collaborate closely with data scientists and computer vision experts to understand the requirements for building deep learning models and implement video preprocessing and transformation steps. \n  Leverage your expertise in cloud infrastructure, e.g., AWS (preferred), GCP, or Azure, to create scalable and efficient data solutions for CV applications. \n  Lead the development of pipelines to preprocess and augment video data, ensuring compatibility with deep learning frameworks. \n  Build and maintain data pipelines that enable real-time or near-real-time video data processing for inference and analytics. \n  Collaborate with cross-functional teams to deliver CV insights and predictions to clients successfully. \n \n \n  Why work with SIS? \n  We believe in making sports better through data, analysis and insights. For that reason, we have an incredible team of technologists, scouts, analysts, and operators helping our partners win more games. \n  It is our ultimate vision to create an unparalleled platform of sporting data and insights, through best-in-class technology, products and partnerships. \n  We believe in a flexible, energetic, enjoyable working environment where we band together as teammates to do great things. We are committed to creating a diverse environment, working in a collaborative, team-centric environment.   \n  Qualifications \n  If you possess the following, you are well on your way to making an impact at SIS: \n \n  Bachelor's degree in Computer Science, Engineering, a related field, or equivalent work experience; advanced degree is a plus. \n  Proven experience designing and implementing data pipelines for video ingest and management in a cloud environment. \n  Strong understanding of cloud-based services and infrastructure and their video processing, content delivery, and storage integration. \n  Strong programming skills in languages such as Python (preferred), Rust, C++, or Java. \n  Desire and ability to write maintainable, tested code. \n  Knowledge of video codecs, formats, and processing techniques for efficient video data handling. \n  Familiarity with machine learning concepts and experience preparing data for training and inference purposes. \n  Familiarity with computer vision concepts and practices, including data preprocessing, augmentation, and annotation. ", "techs": ["video ingestion", "processing", "management", "cloud-based services", "scalability", "reliability", "deep learning models", "video preprocessing", "transformation steps", "cloud infrastructure", "aws", "gcp", "azure", "data solutions", "pipelines", "deep learning frameworks", "video data", "real-time", "near-real-time", "inference", "analytics", "cv insights", "predictions", "sis", "technologists", "scouts", "analysts", "operators", "sporting data", "insights", "technology", "products", "partnerships", "flexible working environment", "teamwork", "diversity", "impact", "bachelor's degree", "computer science", "engineering", "related field", "advanced degree", "data pipelines", "video ingest", "management", "cloud environment", "cloud-based services", "infrastructure", "video processing", "content delivery", "storage integration", "programming skills", "python", "rust", "c++", "java", "maintainable code", "video codecs", "formats", "processing techniques", "efficient video data handling", "machine learning concepts", "data preparation", "computer vision concepts", "data preprocessing", "augmentation", "annotation"]}, "977b877d0c501d8c": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql", "sql"]}, "117fd33c3f37e3e6": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "bed4fd38a914cae6": {"terms": ["data engineer"], "salary_min": null, "salary_max": null, "title": "Senior Data Engineer (Remote Available)", "company": "Vanderbilt University Medical Center", "desc": "Discover Vanderbilt University Medical Center: Located in Nashville, Tennessee, and operating at a global crossroads of teaching, discovery, and patient care, VUMC is a community of diverse individuals who come to work each day with the simple aim of changing the world. It is a place where your expertise will be valued, your knowledge expanded, and your abilities challenged. Vanderbilt Health recognizes that diversity is essential for excellence and innovation. We are committed to an inclusive environment where everyone has the chance to thrive and where your diversity of culture, thinking, learning, and leading is sought and celebrated. It is a place where employees know they are part of something that is bigger than themselves, take exceptional pride in their work and never settle for what was good enough yesterday. Vanderbilt\u2019s mission is to advance health and wellness through preeminent programs in patient care, education, and research.\n  \n \n \n   Organization:\n   HealthIT Data Platform Svcs\n  \n \n   Job Summary:\n   JOB SUMMARY\n   The Data Platform Services team is seeking an experienced Data Engineer with an inquisitive and analytical mindset to join our team. In your role, you will be responsible for developing, maintaining, and optimizing our cloud-based data infrastructure, as well as designing and implementing efficient data extraction and ingestion processes. You will also be responsible for working with stakeholders to identify data needs, investigating opportunities to improve platform scalability, building generic solutions for patterned problems, and ensuring data accuracy and integrity.\n  \n \n   .\n  \n \n \n   KEY RESPONSIBILITIES\n  \n \n   Independently and full proficient to:\n  \n \n  Design and develop performant data pipelines that support a variety of source system types (flat files, APIs, databases, etc.) \n  Troubleshoot complex issues with production pipelines and work with internal stakeholders to deploy fixes \n  Develop tools and services used by other teams to create, test, and deliver data-related assets \n  Design and develop cloud infrastructure-as-code to support the data platform \n  Participate in code review sessions for merge requests and assist in mentoring developers in best development practices \n  May assist with onboarding and training new employees as needed \n \n \n   TECHNICAL CAPABILITIES\n  \n \n \n   Minimum Qualifications\n  \n \n  Bachelor's Degree \n  4+ years C# and/or Python in an OOP paradigm experience \n  2+ years with SQL experience \n  2+ years implementing cloud solutions with Azure, AWS, or GCP \n  2+ years with containerization with Docker or Podman \n \n \n \n   Preferred Qualifications\n  \n \n  2+ years with CI/CD automation in GitLab, GitHub, or Azure DevOps \n  Experience with REST data services, APIs, and microservices \n  Experience with Infrastructure as Code solutions using Terraform, Bicep, or ARM \n  Experience with Git \n \n \n \n   Bonus Qualifications\n  \n \n  Experience with Databricks or Apache Spark \n  Experience with PowerShell and/or Bash \n  GitHub profile link to some personal code examples \n \n \n \n   Our professional administrative functions include critical supporting roles in information technology and informatics, finance, administration, legal and community affairs, human resources, communications and marketing, development, facilities, and many more.\n  \n \n \n   At our growing health system, we support each other and encourage excellence among all who are part of our workforce. High-achieving employees stay at Vanderbilt Health for professional growth, appreciation of benefits, and a sense of community and purpose.\n  \n \n \n   Core Accountabilities:\n  \n  Organizational Impact: Independently delivers on objectives with understanding of how they impact the results of own area/team and other related teams. Problem Solving/ Complexity of work: Utilizes multiple sources of data to analyze and resolve complex problems; may take a new perspective on existing solution. Breadth of Knowledge: Has advanced knowledge within a professional area and basic knowledge across related areas. Team Interaction: Acts as a \"go-to\" resource for colleagues with less experience; may lead small project teams.\n  \n \n   Core Capabilities :\n  \n  Supporting Colleagues: - Develops Self and Others: Invests time, energy, and enthusiasm in developing self/others to help improve performance e and gain knowledge in new areas. - Builds and Maintains Relationships: Maintains regular contact with key colleagues and stakeholders using formal and informal opportunities to expand and strengthen relationships. - Communicates Effectively: Recognizes group interactions and modifies one's own communication style to suit different situations and audiences. Delivering Excellent Services: - Serves Others with Compassion: Seeks to understand current and future needs of relevant stakeholders and customizes services to better address them. - Solves Complex Problems: Approaches problems from different angles; Identifies new possibilities to interpret opportunities and develop concrete solutions. - Offers Meaningful Advice and Support: Provides ongoing support and coaching in a constructive manner to increase employees' effectiveness. Ensuring High Quality: - Performs Excellent Work: Engages regularly in formal and informal dialogue about quality; directly addresses quality issues promptly. - Ensures Continuous Improvement: Applies various learning experiences by looking beyond symptoms to uncover underlying causes of problems and identifies ways to resolve them. - Fulfills Safety and Regulatory Requirements: Understands all aspects of providing a safe environment and performs routine safety checks to prevent safety hazards from occurring. Managing Resources Effectively: - Demonstrates Accountability: Demonstrates a sense of ownership, focusing on and driving critical issues to closure. - Stewards Organizational Resources: Applies understanding of the departmental work to effectively manage resources for a department/area. - Makes Data Driven Decisions: Demonstrates strong understanding of the information or data to identify and elevate opportunities. Fostering Innovation: - Generates New Ideas: Proactively identifies new ideas/opportunities from multiple sources or methods to improve processes beyond conventional approaches. - Applies Technology: Demonstrates an enthusiasm for learning new technologies, tools, and procedures to address short-term challenges. - Adapts to Change: Views difficult situations and/or problems as opportunities for improvement; actively embraces change instead of emphasizing negative elements.\n  \n \n   Position Qualifications:\n  \n \n \n   Responsibilities:\n  \n \n \n   Certifications:\n  \n \n \n   Work Experience:\n   Relevant Work Experience\n  \n \n  Experience Level:\n   5 years\n  \n \n   Education:\n   Bachelor's\n  \n \n   Vanderbilt Health recognizes that diversity is essential for excellence and innovation. We are committed to an inclusive environment where everyone has the chance to thrive and to the principles of equal opportunity and affirmative action. EOE/AA/Women/Minority/Vets/Disabled", "cleaned_desc": "  \n \n  Design and develop performant data pipelines that support a variety of source system types (flat files, APIs, databases, etc.) \n  Troubleshoot complex issues with production pipelines and work with internal stakeholders to deploy fixes \n  Develop tools and services used by other teams to create, test, and deliver data-related assets \n  Design and develop cloud infrastructure-as-code to support the data platform \n  Participate in code review sessions for merge requests and assist in mentoring developers in best development practices \n  May assist with onboarding and training new employees as needed \n \n \n   TECHNICAL CAPABILITIES\n  \n \n \n   Minimum Qualifications\n  \n \n  Bachelor's Degree \n  4+ years C# and/or Python in an OOP paradigm experience \n  2+ years with SQL experience \n  2+ years implementing cloud solutions with Azure, AWS, or GCP    2+ years with containerization with Docker or Podman \n \n \n \n   Preferred Qualifications\n  \n \n  2+ years with CI/CD automation in GitLab, GitHub, or Azure DevOps \n  Experience with REST data services, APIs, and microservices \n  Experience with Infrastructure as Code solutions using Terraform, Bicep, or ARM \n  Experience with Git \n \n \n \n   Bonus Qualifications\n  \n \n  Experience with Databricks or Apache Spark \n  Experience with PowerShell and/or Bash \n  GitHub profile link to some personal code examples \n ", "techs": ["c#", "python", "sql", "azure", "aws", "gcp", "docker", "podman", "gitlab", "github", "azure devops", "terraform", "bicep", "arm", "git", "databricks", "apache spark", "powershell", "bash", "github"]}, "36dcc802d51952ab": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "cc70b12ccaf1c83c": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "ec8033370c9049ca": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "3eb08db9c32b3f5d": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "codebuild", "codedeploy", "python", "postgresql"]}, "e0b99affb9d327d3": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "python", "postgresql", "sql", "unit test suites"]}, "9eff80b76c55c5ec": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "python", "postgresql", "sql", "unit test suites"]}, "04f7236f05c4fb87": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "python", "postgresql", "sql", "codepipeline", "codebuild", "codedeploy"]}, "d654153252e80d1a": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "c015da39f0399244": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql."]}, "82be7872f20d9c51": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "21dbe5e2f9befb3d": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "633f8dfa7a8c338f": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "c806f32f26debac6": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "70186c5e4fda3c52": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "etl", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "codebuild", "codedeploy", "python", "postgresql."]}, "2b4299c42e65c6fa": {"terms": ["data engineer", "machine learning engineer"], "salary_min": 107398.66, "salary_max": 135990.62, "title": "Sr. Data Engineer", "company": "Genesis Financial Solutions, Inc.", "desc": "Overview: \n  \n  *This position may be remote* \n \n \n \n  At Genesis, we put our customers and employees first. We are the national leader in non-Prime credit cards serving millions of customers who are underserved by others in the industry. We are fair and transparent in all interactions.\n  \n \n \n  We\u2019re entrepreneurial and we look for people who want to make an impact. We hire people, not just positions.\n  \n \n \n  We\u2019re an established company with over 20 years of experience, but now we\u2019re taking things to the next level. We are looking for someone that wants to make an impact\u2014someone who wants to influence the business and play a pivotal role in leading the charge for change.\n  \n \n \n  Join the nation\u2019s leader in second-look finance servicing as our Sr. Data Engineer! \n \n \n \n  What\u2019s In It For You: \n \n \n  Making an impact \n  Being an integral part of a team that is critical to our business success \n  An entrepreneurial culture in a growth business \n \n \n   Check out our audio job descriptions: https://open.spotify.com/episode/0IgCQDoeJj3oiwSQqsvyfv\n   Responsibilities: \n  \n  As our Sr. Data Engineer you will: \n \n \n  Work with a high-performing Agile team to build out next gen data ecosystem in Azure \n  Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes \n  Building required infrastructure for optimal extraction, transformation and loading of data from various data sources like flat files or other format of files using Databricks, Azure Data Factory, Notebooks and SQL technologies \n  Ability to tokenize/detokenize specific fields from files at scale in batch and real time. \n  Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition, Risk metrics. \n  Work with stakeholders including operations, data, design, and product teams, assisting them with data infrastructure, and data-related technical issues \n  Responsible for finding trends in data sets and developing algorithms to help make raw data more useful to the enterprise. \n  Develop, construct, test and maintain systems. \n  Identify ways to improve data reliability, efficiency and quality \n  Deploy sophisticated analytics programs, machine learning and statistical methods \n  Understand data for predictive and prescriptive modeling \n  Ability to Find patterns using data \n  Use data to discover tasks that can be automated \n  Qualifications: \n  \n  Requirements: \n \n \n  BS/MS in CIS with Data engineering focus. \n  5+ years of related work experience in an Azure/Databricks and SQL Server environment. \n  Demonstratable working knowledge of database design and data movement in a cloud-based environment. \n  Hands on experience with SQL Server, Azure, Databricks, ADF and Power BI strongly preferred. \n  Working knowledge of modern programming languages such as C#, Python and Scala. \n  Ability to think both strategically and analytically while working across departments. \n  Process driven with the ability to define, document and implement team-based policies and procedures. \n  Excellent oral and written communication skills combined with effective relationship management skills. \n  Ability to communicate throughout the organization including Executive leadership. \n  Experience with MDM and Data Governance (ie: Alation, Profisee) \n  Strong communication skills to work across departments to understand what business stakeholders want to gain from the company\u2019s datasets. \n  Process driven and document process to be repeatable/reusable (Engineering mindset not developer/coder mindset) \n  Attention to detail \n  Ability to adapt and solution focused approach \n \n  Genesis understands experience comes in many forms, many skills are transferable, if your experience is this close to what we\u2019re looking for, consider applying. We know that diversity of thought has made us the entrepreneurial and innovative company that we are today. \n \n  What\u2019s In It For You: \n \n \n  Health insurance including individual and family Medical, Dental and Vision insurance \n  Relax and recharge with Paid Time Off (PTO) Program; plus 6 paid holidays and 3 paid floating holidays \n  Financial health with 401k plus employer match up to 4% \n  Take care of your well-being through Wellness perks \n  We invest in your future through Tuition Reimbursement and ongoing Learning & Development \n  Save on taxes with Flexible Spending Accounts \n  Peace of mind with Life and AD&D Insurance \n  Protect yourself with company paid Long-Term Disability \n \n \n \n  Genesis Financial Solutions provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.", "cleaned_desc": " \n \n  Work with a high-performing Agile team to build out next gen data ecosystem in Azure \n  Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes \n  Building required infrastructure for optimal extraction, transformation and loading of data from various data sources like flat files or other format of files using Databricks, Azure Data Factory, Notebooks and SQL technologies \n  Ability to tokenize/detokenize specific fields from files at scale in batch and real time. \n  Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition, Risk metrics. \n  Work with stakeholders including operations, data, design, and product teams, assisting them with data infrastructure, and data-related technical issues \n  Responsible for finding trends in data sets and developing algorithms to help make raw data more useful to the enterprise. \n  Develop, construct, test and maintain systems. \n  Identify ways to improve data reliability, efficiency and quality \n  Deploy sophisticated analytics programs, machine learning and statistical methods \n  Understand data for predictive and prescriptive modeling \n  Ability to Find patterns using data \n  Use data to discover tasks that can be automated \n  Qualifications: \n     Requirements: \n \n \n  BS/MS in CIS with Data engineering focus. \n  5+ years of related work experience in an Azure/Databricks and SQL Server environment. \n  Demonstratable working knowledge of database design and data movement in a cloud-based environment. \n  Hands on experience with SQL Server, Azure, Databricks, ADF and Power BI strongly preferred. \n  Working knowledge of modern programming languages such as C#, Python and Scala. \n  Ability to think both strategically and analytically while working across departments. \n  Process driven with the ability to define, document and implement team-based policies and procedures. \n  Excellent oral and written communication skills combined with effective relationship management skills. \n  Ability to communicate throughout the organization including Executive leadership. \n  Experience with MDM and Data Governance (ie: Alation, Profisee) \n  Strong communication skills to work across departments to understand what business stakeholders want to gain from the company\u2019s datasets. \n  Process driven and document process to be repeatable/reusable (Engineering mindset not developer/coder mindset) \n  Attention to detail \n  Ability to adapt and solution focused approach ", "techs": ["azure", "databricks", "azure data factory", "notebooks", "sql", "power bi", "c#", "python", "scala", "alation", "profisee"]}, "34a5080b37973c74": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "81c1335fad0d0ecf": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "codebuild", "codedeploy", "python", "postgresql"]}, "1535c9a8c51fc6a6": {"terms": ["data engineer"], "salary_min": 131082.34, "salary_max": 165979.47, "title": "Senior Data Engineer", "company": "Keeper Security, Inc.", "desc": "Keeper Security, Inc. is seeking a Senior Data Engineer to join our team of experts in transforming the way businesses and individuals protect their passwords and sensitive digital assets. As a Senior Data Engineer, on our Data Engineering team, you will play a vital role in designing and building data pipelines and analytical solutions that power our business intelligence and data-driven decision making.  \n About Keeper \n  Keeper Security is transforming cybersecurity for people and organizations around the world. Keeper\u2019s affordable and easy-to-use solutions are built on a foundation of zero-trust and zero-knowledge security to protect every user on every device. Our next-generation privileged access management solution deploys in minutes and seamlessly integrates with any tech stack to prevent breaches, reduce help desk costs and ensure compliance. Trusted by millions of individuals and thousands of organizations, Keeper is the leader for best-in-class password management, secrets management, privileged access, secure remote access and encrypted messaging. Learn more at KeeperSecurity.com. \n  Responsibilities \n \n Design and develop data pipelines to ingest and process large volumes of data from various sources. \n Identify and implement data quality checks and monitoring mechanisms to ensure data accuracy and consistency. \n Build and maintain large-scale analytical databases to support data-driven decision-making using Python. \n Work closely with cross-functional teams, including data scientists and software engineers, to design and implement complex data models and analytical solutions. \n Develop software tools and frameworks to automate the data pipeline and data processing workflows using Apache Spark, Hardoop, Apache Kafka etc. \n Analyze and optimize database performance, identify and resolve technical issues, and perform troubleshooting as necessary. \n Communicate effectively with non-technical stakeholders to gather requirements, convey technical information, and present data-driven insights. \n \n Requirements \n \n 5+ years of experience leveraging Python in a professional setting, a SaaS environment strongly preferred  \n Strong expertise in building and optimizing large-scale data pipelines using modern data technologies such as Apache Spark, Hadoop, or Apache Kafka. \n Experience with cloud-based data warehousing solutions such as Google BigQuery, Amazon Redshift, or Snowflake. \n Proficient in SQL, Python, or Java and data modeling principles. \n Solid knowledge of database systems and concepts, including SQL, NoSQL, and distributed databases. \n Understanding of data security and privacy principles and experience working with regulated data (e.g., PII, PHI). \n Good understanding of software engineering principles and experience with software development best practices, including code versioning, testing, and deployment. \n Excellent problem-solving and analytical skills, with a drive for continuous improvement and learning. \n \n Benefits \n \n Medical, Dental & Vision (Inclusive of domestic partnerships) \n Employer Paid Life Insurance & Employee/Spouse/Child Supplemental life \n Voluntary Short/Long Term Disability Insurance \n 401k (Roth/Traditional) \n A generous PTO plan that celebrates your commitment and seniority (including paid Bereavement/Jury Duty, etc) \n Above market annual bonuses \n \n Keeper Security, Inc. is an equal opportunity employer and participant in the U.S. Federal E-Verify program. We celebrate diversity and are committed to creating an inclusive environment for all employees.  Classification: Exempt", "cleaned_desc": " Identify and implement data quality checks and monitoring mechanisms to ensure data accuracy and consistency. \n Build and maintain large-scale analytical databases to support data-driven decision-making using Python. \n Work closely with cross-functional teams, including data scientists and software engineers, to design and implement complex data models and analytical solutions. \n Develop software tools and frameworks to automate the data pipeline and data processing workflows using Apache Spark, Hardoop, Apache Kafka etc. \n Analyze and optimize database performance, identify and resolve technical issues, and perform troubleshooting as necessary. \n Communicate effectively with non-technical stakeholders to gather requirements, convey technical information, and present data-driven insights.   \n Requirements \n \n 5+ years of experience leveraging Python in a professional setting, a SaaS environment strongly preferred  \n Strong expertise in building and optimizing large-scale data pipelines using modern data technologies such as Apache Spark, Hadoop, or Apache Kafka. \n Experience with cloud-based data warehousing solutions such as Google BigQuery, Amazon Redshift, or Snowflake.   Proficient in SQL, Python, or Java and data modeling principles. \n Solid knowledge of database systems and concepts, including SQL, NoSQL, and distributed databases. \n Understanding of data security and privacy principles and experience working with regulated data (e.g., PII, PHI). \n Good understanding of software engineering principles and experience with software development best practices, including code versioning, testing, and deployment. \n Excellent problem-solving and analytical skills, with a drive for continuous improvement and learning. \n ", "techs": ["python", "apache spark", "hadoop", "apache kafka", "google bigquery", "amazon redshift", "snowflake", "sql", "java", "nosql"]}, "308bb112e80090ea": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "codebuild", "codedeploy", "python", "postgresql"]}, "b550de8ce9c720b5": {"terms": ["data engineer"], "salary_min": 63813.51, "salary_max": 80802.125, "title": "eDiscovery Data Engineer (Processing Specialist)", "company": "Contact Government Services, LLC", "desc": "Contact Discovery Services, one of the leading boutique eDiscovery vendors headquartered in Washington, DC, has an outstanding opportunity for an entry-level eDiscovery Data Engineer to join team. This position will be run from our Washington DC location with potential opportunity for travel, remote work, or shifted hour schedule. This position is ideal for someone early in their technology career. The qualified individual should possess a technical background, as well as be highly motivated and reliable. The selected candidate will be expected to: \n \n  Work closely with project management and other team members on completing complex projects in a fast pace, deadline driven environment \n  Learn and complete electronically stored information (ESI) processing and document productions in accordance with industry standards and client specifications \n  Consistently adhere to standard operating procedures \n  Perform quality checks on work product prior to delivering to the client \n  Assist in developing, documenting, and refining procedures to accomplish discovery process requirements \n  Additional duties assigned by manager including general IT functions \n \n \n  EXPERIENCE & QUALIFICATIONS: \n \n  B.S. or B.A. degree, preferably information technology, computer science, or other related fields \n  Ability to communicate effectively and tactfully in both verbally and in written format \n  Ability to work extended hours when necessary to ensure client deadlines are met \n  Ability to demonstrate superior organizational skills with an acute attention to detail \n  Ability to work effectively under pressure in time sensitive situations \n  Ability to work well in a team environment, as well as independently \n  Ability to prioritize multiple projects with similar deadlines \n  Ability to troubleshoot/problem solve and communicate results to a team \n \n \n  PREFERRED BUT NOT REQUIRED: \n \n  Interest / Ability to work alternative hours such as Mid or Late Shift \n  Prior experience in data processing (Relativity, Nuix Workstation, LAW Prediscovery, Venio One, iPro eCapture, etc.) \n  A basic understanding of digital forensic preservation functions and tools (Cellebrite UFED Physical Analyzer, AccessData FTK Imager, etc.) \n  Familiarity with scripting, programming, coding, and/or database languages (SQL, VB, HTML, Access) \n \n \n  ABOUT CONTACT: \n  Contact Discovery Services delivers best in class service to many Fortune 100/500 companies. Our growth is driven by investing in people and technologies, ensuring our customers have access to the highest level of customer service and the most qualified resources. Our team members are comprised of talented engineers, analysts, and project managers from all walks of life. We concentrate on engineering \u201coutside-the-box\u201d solutions to help organize discovery so our clients can focus on developing case strategy and leaving the heavy lifting to us. Employees are encouraged to actively participate in the development of new ideas, technology, and processes to ensure our customers receive the highest level of service. We offer competitive benefits, work schedule flexibility, and coordinate various company activities throughout the year. If you are looking to be part of an exciting, fast paced environment then we want to hear from you. EOE \n   \n Pw1eHm8e9E", "cleaned_desc": "  Interest / Ability to work alternative hours such as Mid or Late Shift \n  Prior experience in data processing (Relativity, Nuix Workstation, LAW Prediscovery, Venio One, iPro eCapture, etc.) \n  A basic understanding of digital forensic preservation functions and tools (Cellebrite UFED Physical Analyzer, AccessData FTK Imager, etc.) \n  Familiarity with scripting, programming, coding, and/or database languages (SQL, VB, HTML, Access) \n \n ", "techs": ["relativity", "nuix workstation", "law prediscovery", "venio one", "ipro ecapture", "cellebrite ufed physical analyzer", "accessdata ftk imager", "sql", "vb", "html", "access"]}, "72c4d49cab9d1468": {"terms": ["data engineer"], "salary_min": 105000.0, "salary_max": 150000.0, "title": "Senior/Principal Electronic Data Capture (EDC) Engineer", "company": "Takeda Pharmaceutical", "desc": "By clicking the \u201cApply\u201d button, I understand that my employment application process with Takeda will commence and that the information I provide in my application will be processed in line with Takeda\u2019s Privacy Notice and Terms of Use. I further attest that all information I submit in my employment application is true to the best of my knowledge. \n \n  Job Description \n \n \n  Are you looking for a patient-focused, innovation-driven company that will inspire you and empower you to shine? Join us as a Electronic Data Capture (EDC) Engineer (Senior/Principal) \u2013 Alta Petens based remotely. \n \n  At Takeda, we are transforming the pharmaceutical industry through our R&D-driven market leadership and being a values-led company. To do this, we empower our people to realize their potential through life-changing work. Certified as a Global Top Employer, we offer stimulating careers, encourage innovation, and strive for excellence in everything we do. We foster an inclusive, collaborative workplace, in which our global teams are united by an unwavering commitment to deliver  Better Health and a Brighter Future  to people around the world. \n \n  Here, you will be a vital contributor to our inspiring, bold mission. \n \n  Objectives: \n  As the EDC Engineer you will work on EDC activities and oversee delivery of systems and documentation to support Takeda Clinical trials. You will work with Takeda study teams to develop eCRF specifications, build and/or oversee implementation of Case Report Forms (eCRFs) for clinical trials. The EDC Engineer will manage and oversee EDC system configuration, dictionaries, and integrations. The EDC Engineer operates in compliance with Takeda SOPs and processes while working with Data Management and Standards Teams to enhance existing processes. The EDC Engineer will understand Clinical Data Acquisition Standards Harmonization (CDASH) and Study Data Tabulation Model (SDTM) standards and concepts while considering EDC platform best practices. The EDC Engineer will continue developing new skills associated with EDC technologies. \n \n  Key Accountabilities \n \n  Create eCRF specifications, design, program, and validate clinical trial setup \n  Review edit check specifications and program edit checks at the trial level \n  Setup different instances of trial URL (eg: UAT, production, testing etc.,) \n  Configure and maintain user accounts for study teams and site users \n  Setup and manage blinded and unblinded study configurations \n  Perform and document functional testing of all EDC design components \n  Setup, configure, and validate integration modules within the EDC ecosystem such as coding, IRT, eCOA, safety system, local labs etc. \n  Work closely with EDC vendors to understand system enhancements and limitations \n  Ability to identify and troubleshoot database design and maintenance issues \n  Prepare, test, and implement post-production changes as per study needs while ensuring data integrity \n  Archive and retire the study URL after database lock \n  Excellent written and verbal communication skills and interpersonal relationship skills including negotiating and relationship management skills with ability to drive achievement of objectives \n  Monitor general progress to ensure clinical programming milestones and deliverables are met with quality for all concurrent projects \n  Partner with appropriate team members to establish technology standards and best practices \n  Adhere to and support business process SOPs. \n  Oversee system delivery life cycle in collaboration with appropriate partners including Clinical Operations, Clinical Supplies, IT, and Quality organizations \n  Support adoption of new capabilities and business process \n  Collaborate with standards team in creating standard EDC libraries for study level consumption \n  Assist data management with CRO oversight of EDC Builds \n  Provide SME expertise to study teams having site entry and/or bug issues in Production \n  Work closely with data engineers and data managers on study level integrations and deliveries \n  Assist in technology vendor oversight activities. \n  Partner with appropriate team members, technology vendors, and CRO partners to avoid and resolve risks. \n  Confirm archival and inspection readiness of all Clinical Technology Trial Master File (TMF) documents \n  Participate in preparing job function for submission readiness \n  Track study deliverables and escalate any risk(s) for major data management deliverables \n  Adaptable to new ways of working using technology to accelerate clinical trial setup \n \n \n  Education and Experience Requirements: \n \n  Bachelor's degree or related experience. \n  Knowledge of drug development process. \n  Minimum of 10+ years\u2019 experience in Data Management, Programming, Clinical IT, or other Clinical Research related fields. \n  Hands-on experience with at least one EDC system (e.g.: Medidata Rave, Inform, IBM Clinical, Veeva etc.). Veeva is preferred \n  Understanding of CQL/SQL, Python, and/or C# is preferred \n  Experience integrating the EDC database with other clinical trial modules (e.g.: lab, safety, IRT, coding etc.) \n  Understanding of industry standard technologies to support Clinical Development needs (e.g., CTMS, IRT, eCOA, SAS, R or Python, Data Warehouses, SharePoint) \n \n \n  This position is currently classified as \u201cremote\u201d in accordance with Takeda\u2019s Hybrid and Remote Work policy. \n \n  Base Salary Range:  $105,000 to $150,000 based on candidate professional experience level. Employees may also be eligible for Short Term and Long-Term Incentive benefits as well. Employees are eligible to participate in Medical, Dental, Vision, Life Insurance, 401(k), Charitable Contribution Match, Holidays, Personal Days & Vacation, Tuition Reimbursement Program and Paid Volunteer Time Off. \n \n  The final salary offered for this position may take into account a number of factors including, but not limited to, location, skills, education, and experience. \n \n \n  EEO Statement \n  Takeda is proud in its commitment to creating a diverse workforce and providing equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, parental status, national origin, age, disability, citizenship status, genetic information or characteristics, marital status, status as a Vietnam era veteran, special disabled veteran, or other protected veteran in accordance with applicable federal, state and local laws, and any other characteristic protected by law. \n \n  Locations \n  Massachusetts - Virtual\n  \n  Worker Type \n  Employee\n  \n  Worker Sub-Type \n  Regular\n  \n  Time Type \n  Full time \n  #LI-Remote", "cleaned_desc": "", "techs": ""}, "993a2c39c6637697": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "ce83cb33f26b16c0": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "etl", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "31ebada2a40e6379": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "0eb76c7db8b684c1": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "python", "postgresql"]}, "1c5e4510e26f2669": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql", "sql"]}, "4e1bb417d0ac2bde": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "python", "postgresql"]}, "fca86c1080dd9448": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "python", "postgresql", "sql", "codepipeline", "codebuild", "codedeploy"]}, "6239ec5dbcdce56c": {"terms": ["data engineer"], "salary_min": 130000.0, "salary_max": 160000.0, "title": "Senior Data Engineer (AWS)", "company": "Emergent Software", "desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems \n Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy \n Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: \n \n Application (5 minutes) \n Online Assessment (40 minutes) \n Initial Phone Interview (30-45 minutes) \n 2-3 Interviews with the Client \n Job Offer! \n \n Job Type: Full-time \n Pay: $130,000.00 - $160,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Work Location: Remote", "cleaned_desc": "** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \n Responsibilities: \n \n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \n Maintain data pipelines & data systems   Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \n \n Requirements: \n \n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \n Experienced with AWS services and AWS-native CI/CD tools \n Experienced designing data pipelines & data solutions \n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy   Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \n Ability to work independently and integrate with other team members \n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \n Excellent communication skills (written and verbal) \n \n Our Vetting Process \n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position: ", "techs": ["aws", "etl", "iam", "glue", "lambda", "s3", "athena", "sns", "sqs", "dynamodb", "rds", "emr", "ecs", "route 53", "redshift", "jenkins", "git", "aws codepipeline", "aws codebuild", "aws codedeploy", "python", "postgresql"]}, "6f3f781c8f9c8424": {"terms": ["data engineer"], "salary_min": 61.73, "salary_max": 70.61, "title": "Data Engineer 2", "company": "TalentBurst, Inc.", "desc": "Data Engineer 2     Location: Centennial, CO - 80155 (REMOTE)   Duration: 12+ months     **w2 acceptable**       A day in the life...  \" Designs and develops analytical solutions using in-house and modern technologies for business.  \" Collaborate with technical program management, data analysts, data scientist and business partners to build scalable and durable solutions that drive business outcomes and reduce friction and time to delivery for our users.  \" Collaborate and partner with other tech teams and contribute at team level.  \" Understands the architecture and network principles to be followed.  \" Able to understand data concepts, data models, data structures and adhere to data principles.  \" Design and develop data processing pipelines using Spark, Flink, Kafka, Presto, etc.  \" Work autonomously and closely with other team members.  \" Demonstrate knowledge of information technology concepts issues, trends and best practices as they relate to the discipline/practice  \" Demonstrate knowledge of specific needs of a major line of business and the related set of applications, systems or functions  \" Owns responsibility of code reviews and drives accountability in following software engineering principles.  \" Follows and maintains good data ecosystem principles, quality and security while building data solutions.  \" Understanding of CI/CD pipeline, Test coverage, and unit testing     Good to have:  \" 4+ years of professional experience in practice area  \" 4+ years of experience in at least one modern programming language (e.g. Java, Python, Scala)  \" 3+ years of experience with Databases, SQL, data modeling, data mining, and automated engineering solutions.  \" Experience with cloud environments (AWS or other equivalent cloud providers)  \" Experience with distributed systems (Kubernetes, AWS EMR, etc.)  \" Experience in developing near real-time data processing solutions using Kafka, Flink, Spark Streaming  \" Experience with Spring boot framework  \" Strive to deliver results, make quality technical decisions, manage complexity, and drive a vision while being customer focus.  \" Experience with standard industry practices for engineering and operational excellence  \" Bachelor's or master's degree in CS, Engineering or equivalent practical experience    TB_EN", "cleaned_desc": "", "techs": ""}, "018e5330f26cd3f8": {"terms": ["data engineer"], "salary_min": 150000.0, "salary_max": 300000.0, "title": "Big Data Software Engineer", "company": "Strategic Employment", "desc": "Our client, a startup SaaS company with a suite of big data and cloud solutions, is seeking to hiring a Big Data Engineer to join their growing team. Ideal candidates will bring 4+ years of professional software engineering experience with an emphasis on big data. \n \n \n Term:  Full Time/Direct Hire \n \n \n Location:  Remote (or hybrid in Bay Area) \n \n \n Ideal Qualifications: \n \n \n 4+ years of experience in a big data-focused software engineering role \n Experience developing scalable ETL pipelines \n Expert object-oriented programming skills \n Experience with SQL and Snowflake is strongly desired \n Bachelor's degree in computer science or similar field \n We are unable to sponsor at this time.* \n  indsepps1hi", "cleaned_desc": "", "techs": ""}, "bba9a4c3a7da6f49": {"terms": ["machine learning engineer"], "salary_min": 29.4, "salary_max": 65.0, "title": "Machine Learning Engineer", "company": "Sira Consulting Inc", "desc": "Job Description \n Key Responsibilities: \n \n Fine-tune and serve LLMs to help build best-in-class solutions on NLP and Generative AI \n Stay up-to-date with recent advancements in NLP and large language models, applying novel techniques and methodologies to improve our models. \n Be curious with new industry learnings, startups utilizing LLMs in novel ways, and other ways the real world is utilizing LLMs. \n Conduct experiments and benchmarking to assess the performance of various model architectures and optimize hyperparameters. \n Apply, discover and research techniques to optimize existing LLM training and serving, as well as improving the model quality. \n Generate comprehensive client corpus data from various systems for LLM use cases \n Build and maintain production ready ML pipelines \n Build and maintain a production-ready platform to run models in parallel and A/B test \n Prepare and preprocess data in collaboration with the data engineering team \n \n Required Job Qualifications \n Required: \n \n Bachelor\u2019s degree or equivalent experience in MIS, Computer Science, Mathematics, Machine Learning, Data science or related field \n 5+ years of experience in Machine learning related field including prior lead experience \n Hands-on experience in developing models, in optimizing model training and tuning and in deploying Large Language Models such as GPT, LLaMA, BERT, or Transformer-based architectures with strong knowledge in tokenization and embeddings. \n Knowledge on Natural Language Processing, Information Retrieval, Machine Comprehension, Question Answering/Conversational AI, Reinforcement Learning and Inference \n 2+ plus years of experience in developing deep learning models as a Data Scientist and experience in building products backed up ML /AI using NLP algorithms \n Advanced in-depth knowledge of Predictive Analytics, Statistical modeling, advanced mathematics, data integration concepts and tools \n Strong organizational, analytical, critical thinking and leadership skills \n Demonstrated leadership on mid-large-scale project impacting strategic partners \n Background in Machine Learning frameworks such as TensorFlow, SparkML or Keras, Scikit Learn \n Proficiency in Scala, Python or an equivalent language \n Familiarity with Statistical Machine Learning models, Machine Learning concepts like Natural Language Processing, Image Processing, Recommendation Systems, etc. \n Deep knowledge of math, probability, statistics, and algorithms, Machine Learning Development Lifecycle, Model monitoring, Data Exploration, Bias detection/removal, Explainable AI \n Experience with AWS DevOps tools (e.g. Sagemaker, Lambda) \n Experience with CI/CD tools (e.g. github, bamboo) and best practices \n \n Preferred: \n \n Advanced degree or equivalent in Mathematics, Statistics, MIS, Computer Science, Machine Learning, Data science or related field \n Leadership experience \n Understanding of Thrivent\u2019s Technology capabilities \n Experience with Databricks \n \n Job Type: Contract \n Pay: $29.40 - $65.00 per hour \n Benefits: \n \n Paid time off \n \n Compensation package: \n \n Bonus opportunities \n Performance bonus \n \n Schedule: \n \n 8 hour shift \n Monday to Friday \n \n Experience: \n \n Python: 1 year (Preferred) \n SQL: 1 year (Preferred) \n \n Work Location: Remote", "cleaned_desc": "Job Description \n Key Responsibilities: \n \n Fine-tune and serve LLMs to help build best-in-class solutions on NLP and Generative AI \n Stay up-to-date with recent advancements in NLP and large language models, applying novel techniques and methodologies to improve our models. \n Be curious with new industry learnings, startups utilizing LLMs in novel ways, and other ways the real world is utilizing LLMs. \n Conduct experiments and benchmarking to assess the performance of various model architectures and optimize hyperparameters. \n Apply, discover and research techniques to optimize existing LLM training and serving, as well as improving the model quality. \n Generate comprehensive client corpus data from various systems for LLM use cases \n Build and maintain production ready ML pipelines \n Build and maintain a production-ready platform to run models in parallel and A/B test   Prepare and preprocess data in collaboration with the data engineering team \n \n Required Job Qualifications \n Required: \n \n Bachelor\u2019s degree or equivalent experience in MIS, Computer Science, Mathematics, Machine Learning, Data science or related field \n 5+ years of experience in Machine learning related field including prior lead experience \n Hands-on experience in developing models, in optimizing model training and tuning and in deploying Large Language Models such as GPT, LLaMA, BERT, or Transformer-based architectures with strong knowledge in tokenization and embeddings. \n Knowledge on Natural Language Processing, Information Retrieval, Machine Comprehension, Question Answering/Conversational AI, Reinforcement Learning and Inference \n 2+ plus years of experience in developing deep learning models as a Data Scientist and experience in building products backed up ML /AI using NLP algorithms \n Advanced in-depth knowledge of Predictive Analytics, Statistical modeling, advanced mathematics, data integration concepts and tools   Strong organizational, analytical, critical thinking and leadership skills \n Demonstrated leadership on mid-large-scale project impacting strategic partners \n Background in Machine Learning frameworks such as TensorFlow, SparkML or Keras, Scikit Learn \n Proficiency in Scala, Python or an equivalent language \n Familiarity with Statistical Machine Learning models, Machine Learning concepts like Natural Language Processing, Image Processing, Recommendation Systems, etc. \n Deep knowledge of math, probability, statistics, and algorithms, Machine Learning Development Lifecycle, Model monitoring, Data Exploration, Bias detection/removal, Explainable AI \n Experience with AWS DevOps tools (e.g. Sagemaker, Lambda) \n Experience with CI/CD tools (e.g. github, bamboo) and best practices \n \n Preferred: \n ", "techs": ["llms", "nlp", "generative ai", "model architectures", "hyperparameters", "client corpus data", "ml pipelines", "mis", "computer science", "mathematics", "machine learning", "data science", "gpt", "llama", "bert", "transformer-based architectures", "tokenization", "embeddings", "natural language processing", "information retrieval", "machine comprehension", "question answering/conversational ai", "reinforcement learning", "inference", "deep learning models", "data scientist", "nlp algorithms", "predictive analytics", "statistical modeling", "advanced mathematics", "data integration concepts", "tools", "tensorflow", "sparkml", "keras", "scikit learn", "scala", "python", "statistical machine learning models", "machine learning concepts", "image processing", "recommendation systems", "math", "probability", "statistics", "algorithms", "machine learning development lifecycle", "model monitoring", "data exploration", "bias detection/removal", "explainable ai", "aws devops tools", "sagemaker", "lambda", "ci/cd tools", "github", "bamboo"]}, "592e1a25ecc9719f": {"terms": ["machine learning engineer"], "salary_min": 130100.0, "salary_max": 180000.0, "title": "Software Engineer, Home Lending", "company": "Upstart Network, Inc.", "desc": "About Upstart \n  Upstart is a leading AI lending marketplace partnering with banks and credit unions to expand access to affordable credit. By leveraging Upstart's AI marketplace, Upstart-powered banks and credit unions can have higher approval rates and lower loss rates across races, ages, and genders, while simultaneously delivering the exceptional digital-first lending experience their customers demand. More than two-thirds of Upstart loans are approved instantly and are fully automated. \n  Upstart is a digital-first company, which means that most Upstarters can live and work anywhere in the U.S. We also have offices in San Mateo, California; Columbus, Ohio; and Austin, Texas. \n  Most Upstarters join us because they connect with our mission of enabling access to effortless credit based on true risk. If you are energized by the impact you can make at Upstart, we\u2019d love to hear from you! \n \n The Team: \n  We\u2019re a small team building our own business. Our first product, launched in summer \u201823, is a home equity line of credit (HELOC) for homeowners. Our next product is a home loan (mortgage) for homebuyers. It will be the most complex product Upstart has built to date, and we need more talented, creative engineers to help us make it happen! \n  Why Mortgage?  This industry is the largest category of consumer credit in the US, with over $2 trillion in originations last year. This is where Upstart can make its biggest impact. \n  Why us?  The market has left behind millions of Americans with imperfect credit or difficult-to-document income. Serving these customers is Upstart\u2019s bread and butter, and it\u2019s core to our mission. Because we use machine learning to assess creditworthiness versus a fixed formula more than a decade old (FICO), we\u2019re able to extend loans to prospective homeowners that our competitors just can\u2019t. We\u2019ve proven this model with personal loans and auto loans, and now we want to bring this access to where families feel it the most:  where they live. \n  We want our team to reflect the people we\u2019re serving. To serve customers at the fringes, we need people who  think  at the fringes\u2014weird, quirky, out-of-the-box do-ers who reason from first principles and not from the status quo. We leave our ego at the door and crave being proven wrong. We're a high-empathy team, and we succeed as a  team  first, individuals second. \n  As a software engineer on the Home Lending team, you\u2019ll help build and grow Upstart\u2019s 2 newest products: HELOC and Mortgage. You\u2019ll work closely with cross-functional partners, working across the whole stack. You're a self-starter that's able to leverage tools in your toolbelt to pave your own path forward to progress and deliver work with quality, testability, resiliency, and maintainability in mind. You may currently specialize on the frontend (React) or backend (Kotlin, but you're looking to get your hands into both areas of the stack. You're eager to learn and grow as an individual but also enjoy helping your team come along for the ride. \n  How you\u2019ll make an impact: \n  Beyond leveraging our existing tech (AWS, Kafka, Spring Boot, Next.js, Kubernetes, Kotlin, React), here\u2019s how you\u2019ll make an impact: \n \n Proactively search out ways to make an impact beyond the Jira board \n Turn feature-specific tasks into opportunities to up-level the team\u2019s codebase and product \n Know the product intimately enough to propose changes to requirements when we need to find a middle path \n Propose and deliver new ideas that achieve meaningful improvements to our team\u2019s metrics \n Contribute in a meaningful way to quality improvements within our team \n \n What we\u2019re looking for: \n \n Minimum requirements: \n    \n 3+ years professional experience in fullstack software \n 2+ years proficiency in Kotlin, Java, or C#. (Our backend is in Kotlin, but Java and C# are similar enough that you can learn on the job.) \n Some hands-on experience with a JavaScript framework like React \n Some hands-on experience with SQL databases and domain modeling \n \n \n \n Preferred qualifications: \n    \n Excellent written and verbal communication. As an engineer, a big part of your job will be representing your projects in cross-functional, cross-team, or even cross-company conversations. And you\u2019ll need to tailor your communication style to fit these different audiences\u2019 needs. \n Preference for face-time and synchronous collaboration. We\u2019re a distributed team of remote workers (from coast to coast), and while we predominantly build asynchronously, we prefer facetime when collaborating in a tight feedback loop. You\u2019ll need to be comfortable with a few hours of Zoom per day (e.g. mob programming) to excel in this role. \n Comfortable challenging authority. We want engineers who can push back on their managers when they think we\u2019re going down the wrong path. Related: you thrive in environments where you are expected to think like an owner and make decisions like one. \n   \n \n Travel requirements  As a digital first company, the majority of your work can be accomplished remotely. The majority of our employees can live and work anywhere in the U.S but are encouraged to to still spend high quality time in-person collaborating via regular onsites. The in-person sessions\u2019 cadence varies depending on the team and role; most teams meet once or twice per quarter for 2-4 consecutive days at a time. \n  What you'll love: \n \n Competitive Compensation (base + bonus & equity) \n Comprehensive medical, dental, and vision coverage with Health Savings Account contributions from Upstart \n 401(k) with 100% company match up to $4,500 and immediate vesting and after-tax savings \n Employee Stock Purchase Plan (ESPP) \n Life and disability insurance \n Generous holiday, vacation, sick and safety leave \n Supportive parental, family care, and military leave programs \n Annual wellness, technology & ergonomic reimbursement programs \n Social activities including team events and onsites, all-company updates, employee resource groups (ERGs), and other interest groups such as book clubs, fitness, investing, and volunteering \n Catered lunches + snacks & drinks when working in offices \n \n  #LI-REMOTE \n  #LI-Associate \n \n \n \n At Upstart, your base pay is one part of your total compensation package. The anticipated base salary for this position is expected to be within the below range. Your actual base pay will depend on your geographic location\u2013with our \u201cdigital first\u201d philosophy, Upstart uses compensation regions that vary depending on location. Individual pay is also determined by job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. \n  In addition, Upstart provides employees with target bonuses, equity compensation, and generous benefits packages (including medical, dental, vision, and 401k). \n \n \n     United States | Remote - Anticipated Base Salary Range\n    \n \n     $130,100\u2014$180,000 USD\n    \n \n \n \n Upstart is a proud Equal Opportunity Employer. We are dedicated to ensuring that underrepresented classes receive better access to affordable credit, and are just as committed to embracing diversity and inclusion in our hiring practices. We celebrate all cultures, backgrounds, perspectives, and experiences, and know that we can only become better together. \n  If you require reasonable accommodation in completing an application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please email  candidate_accommodations@upstart.com \n  https://www.upstart.com/candidate_privacy_policy", "cleaned_desc": "", "techs": ""}, "63075caff1301397": {"terms": ["machine learning engineer"], "salary_min": 63931.477, "salary_max": 80951.49, "title": "Software Developer", "company": "DCS Corp", "desc": "Job Description \n \n  Infoscitex, a DCS company, is seeking a part time, experienced Software Developer to support development of new sensor data processing/management technologies and associated software infrastructure leveraging data governance and data standards initiatives. As a Software Developer, you will have the opportunity to work on projects which directly support Department of Defense efforts world-wide. You will also be joining a growing team of engineers, scientists, and subject matter experts in Dayton, OH, truly dedicated to improving the efficiency and effectiveness of our Airmen. \n \n \n Essential Job Functions: \n  Capture requirements, design, develop, extend, demonstrate, and deliver software products. \n Receive low fidelity mock up and quickly produce demon proof of concept for feedback from senior team members. \n Research and implement concepts from academia. \n Apply software development and testing best practices. \n Use an agile development process. \n Collaborate with mixed contractor and government teams. \n Help provide documentation, integration, and training support to users. \n \n \n Requirements: \n  Due to the sensitivity of customer requirements, U.S. Citizenship is required. \n BS degree in Computer Science, Computer Engineering, or equivalent field, and 2 years of experience, or a Master\u2019s degree, or actively working toward one, in a similar field. \n Applicants selected will be subject to a U.S. Government background investigation and must meet eligibility requirements for access to classified information; must be eligible for a U.S Government DOD Secret level clearance. \n Experience with JavaScript, Python and Java development in either Windows or Linux. \n Experience with both front-end and back-end web development using React, HTML, CSS. \n Experience creating UI/UX mock up with Astro Space UX Design System. \n Comfortable with version control software (Git) and software development tools (Jira, Gitlab, Github, BitBucket, etc.). \n Excellent written and verbal English communications skills. \n Ability to work well in team environments. \n \n \n Company Information: \n  Infoscitex (IST) is an employee owned organization with a reputation for agile and efficient development of technology solutions for U.S. Defense, Aerospace, Human Factors, and Security markets. We offer opportunities to work in a dynamic environment, with state of the art simulation technologies, simulation integration experts, machine learning specialists, and military Subject Matter Experts (SME); and continue to provide innovative solutions with our multidisciplinary teams formed with exceptional employees.", "cleaned_desc": " Experience with both front-end and back-end web development using React, HTML, CSS. \n Experience creating UI/UX mock up with Astro Space UX Design System. \n Comfortable with version control software (Git) and software development tools (Jira, Gitlab, Github, BitBucket, etc.). \n Excellent written and verbal English communications skills. \n Ability to work well in team environments. ", "techs": ["react", "html", "css", "astro space ux design system", "git", "jira", "gitlab", "github", "bitbucket"]}, "7ab0b5ef518cb301": {"terms": ["machine learning engineer"], "salary_min": 166000.0, "salary_max": 190000.0, "title": "Ruby Engineer - Dispute Management", "company": "Sift", "desc": "About the team: \n  At Sift, we enhance trust and safety in the digital world with our AI-driven technology platform. Our products deliver payment protection, ensure content integrity, protect account defense, and manage payment disputes for businesses worldwide. Sift provides consumer-facing companies an end-to-end solution that reduces revenue lost to chargebacks using a holistic dispute management system, powered by machine learning, and which also helps businesses unlock potential revenue. \n  The Dispute Management engineering team handles all data integrations in Sift's Dispute Management business. This team is responsible for all incoming and outgoing data integrations, and ingesting streams of customer data to be used in the dispute process. \n  What we\u2019re looking for: \n  As a software engineer, you will work closely with product managers, data scientists and other engineering teams to deliver projects on time with high quality. You are an engineer who is excited about building scalable distributed ETL (Extract, Transform and Load) systems. We are looking for an engineer who is passionate about building high quality software with high test coverage, observability, security and reliability in mind. \n  What you\u2019ll do: \n \n  Get Sift done. Work closely on a cross functional team to ship great code regularly. \n  Win as one team. Work collaboratively with other bright minded engineers on a cross functional team. \n  Ever better. Be proud of your skills and talents, but alway be willing to learn something new, accept feedback, or welcome a new opinion. \n  Take end-to-end ownership. Our engineers build, deploy and maintain their code \n  Gather responsibility. Lead out to make meaningful contributions to a product designed to make the internet safe for everyone \n  Be nice. Work is better with great people. \n \n  What would make you a strong fit: \n \n  1+ years of experience with Ruby with or without Rails \n  A passion for scalable, 24x7 backend web services \n  Love for clean, maintainable code \n  Experience with Postgres, RabbitMQ, Sidekiq, Docker and AWS \n  Self-motivated with a strong sense of accountability \n  You thrive in ambiguous environments where you get to work directly with stakeholders and make decisions that have a huge impact on the business \n  You value being part of a great team and realize that being positive and helpful is as valuable as having great skill. At Sift \u201cWe win as one team\u201d \n  Bachelor's Degree in CS, Math, Engineering, Hard science(s), or comparable industry experience \n \n  A little about us: \n  Sift is the leading innovator in Digital Trust & Safety. Hundreds of disruptive, forward-thinking companies like Airbnb, Zillow, and Twitter trust Sift to deliver outstanding customer experience while preventing fraud and abuse. \n  The Sift engine powers Digital Trust & Safety by helping companies stop fraud before it happens. But it\u2019s not just another anti-fraud platform: Sift enables businesses to tailor experiences to each customer according to the risk they pose. That means fraudsters experience friction, but honest users do not. By drawing on insights from our global network of customers, Sift allows businesses to scale, win, and thrive in the digital era. \n  Benefits and Perks: \n \n  Competitive total compensation package \n  401k plan \n  Medical, dental and vision coverage \n  Wellness reimbursement \n  Education reimbursement \n  Flexible time off \n \n \n  Let\u2019s Build It Together \n  At Sift, we are intentionally building a diverse, equitable, and inclusive workplace. We believe that diversity drives innovation, equity is a fundamental right, and inclusion is a basic human need. We envision a place where all Sifties feel secure sharing their authentic selves and diverse experiences with their teams, their customers, and their community \u2013 ultimately using this empowerment and authenticity to build trust and create a safer Internet. \n  This document provides transparency around the way in which Sift handles personal data of job applicants:  https://sift.com/recruitment-privacy \n  Compensation Range: $166K - $190K", "cleaned_desc": "", "techs": ""}, "c7a27e0ad8c1789f": {"terms": ["machine learning engineer"], "salary_min": 173000.0, "salary_max": 241000.0, "title": "Senior Software Engineer - Oculus Studios Central Technology", "company": "Oculus", "desc": "Oculus Studios is expanding the Central Technology team (OSCT) focusing on efforts for the First Party Game Studios (Armature Studios, Beat Games, BigBox VR, Camouflaj, Downpour Interactive, Ready At Dawn Studios, Sanzaru Games, Twisted Pixel, Supernatural).Within OSCT, we help our Studios take advantage of the economies of scale that Meta provides. We are looking for engineers who can work effectively within Meta\u2019s vast infrastructure and are excited by the chance to help Studios make video games.We collaborate to build core systems and shared technologies for the Studios. Working with partners across Meta, we assist in migrating and onboarding our partner studios with common Meta supported solutions.\n  \n \n \n Senior Software Engineer - Oculus Studios Central Technology Responsibilities:    \n \n Help define and establish Meta's infrastructure approach for game studios - working across multi-year effort \n  Find common needs by working across the first party game studios collecting requirements and proposing solutions to gain buy-in. Build shipping features as part of a 1st Party product and launch to consumers. Learn from those launches and bring back institutional knowledge to apply to the next 1st Party engagement. \n  Build custom solutions that align with the Metaverse (e.g. Avatars, Common Identity, Integrity) that are delivered to consumers by launching games that use those features. \n  Work as a full-stack developer and gain familiarity in multiple coding languages as needed: e.g. CPP, C#, Python, Hack, JavaScript, React \n  Be or become familiar with multiple game engines: i.e. Unity, Unreal, proprietary engine \n  Be or become familiar with multiple 3rd Party Game technologies (i.e. AWS GameLift, Unity Vivox) \n  Gain familiarity of alternate hosting environments (i.e. cloud/corporate/lab) and help determine where and how they should be used \n  Help onboard new game studios engineers to Meta infra and internal tools, while bringing empathy for game development back to core Meta teams \n  Partner with other teams within Meta to figure out best approaches to migrate game studio services while bringing the Meta focuses of reliability, scalability, security to the game studio space \n  Support game studios during break/fix scenarios and be part of the escalation path \n  Assist game studios in building, scaling, and optimizing new features for their existing/future titles \n \n \n \n \n Minimum Qualifications:   \n \n  Experience in learning new approaches, coding languages, and Meta specific solutions \n  5+ years of experience in developing backend services for games and/or enterprise (large scale distributed environments) \n  Experienced with C/C++ or C \n  Broad knowledge of games and game development and/or enterprise scale infrastructure \n  Experienced with Python and / or PowerShell \n  Experienced collaborating with team members across multiple disciplines to bring products to life \n \n \n \n \n Preferred Qualifications:   \n \n  Experience with General Frameworks: GraphQL, Graph API, Ent Framework \n  Shipped AAA titles or Large enterprise solution \n  Experience working in major game engines (i.e. Unreal, Unity) \n  Experience as a Full-Stack developer in a large scale distributed environment \n  Experience with backend security, privacy, scalability, and persistence storage design paradi \n  Experience in building user facing tools using languages such as Hack, JavaScript, React, etc. \n  Experience building supportable solutions that include proactive alerting, monitoring, and product metrics \n  Experience with major cloud providers (AWS, Azure, etc.) \n  Experience working with distributed computing platforms \n  Bachelor's Degree in Computer Science or Engineering \n \n \n \n \n \n \n \n  Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment. \n   \n  Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.", "cleaned_desc": " \n Minimum Qualifications:   \n \n  Experience in learning new approaches, coding languages, and Meta specific solutions \n  5+ years of experience in developing backend services for games and/or enterprise (large scale distributed environments) \n  Experienced with C/C++ or C \n  Broad knowledge of games and game development and/or enterprise scale infrastructure \n  Experienced with Python and / or PowerShell \n  Experienced collaborating with team members across multiple disciplines to bring products to life \n   \n \n \n Preferred Qualifications:   \n \n  Experience with General Frameworks: GraphQL, Graph API, Ent Framework \n  Shipped AAA titles or Large enterprise solution \n  Experience working in major game engines (i.e. Unreal, Unity) \n  Experience as a Full-Stack developer in a large scale distributed environment \n  Experience with backend security, privacy, scalability, and persistence storage design paradi    Experience in building user facing tools using languages such as Hack, JavaScript, React, etc. \n  Experience building supportable solutions that include proactive alerting, monitoring, and product metrics \n  Experience with major cloud providers (AWS, Azure, etc.) \n  Experience working with distributed computing platforms \n  Bachelor's Degree in Computer Science or Engineering \n \n \n \n \n ", "techs": ["learning new approaches", "coding languages", "and meta specific solutions", "c/c++ or c", "python", "powershell", "graphql", "graph api", "ent framework", "unreal", "unity", "hack", "javascript", "react", "aws", "azure"]}, "c351b8c3fc457f20": {"terms": ["machine learning engineer"], "salary_min": 195858.61, "salary_max": 248000.64, "title": "Staff Software Engineer, Ads Infra", "company": "Pinterest", "desc": "About Pinterest : \n  Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. \n  Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences. \n  Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our  PinFlex  landing page to learn more. \n \n  Pinterest is one of the fastest growing online advertising platforms and our continued success depends on rapidly scaling our core revenue-generating systems. Specifically, we need 10X the scale of our campaign management, ad delivery, and machine learning platforms, while enabling developers inside Pinterest and external advertisers to build and iterate rapidly on new features. We are looking for multiple staff engineers to initiate, design, and build the next-gen version of key infra components in our monetization ecosystem, such as modernizing an end-to-end ML platform serving over hundreds of use cases making billions of predictions per second, and redesigning our catalog ingestion and ads delivery systems to become one of the leading advertising platforms in the world. These roles are exciting, because you will be able to lean on your deep infra knowledge to redesign systems to handle a much bigger scale, while also having the chance to work with very experienced engineers and cross-functional partners. \n  What you'll do: \n \n Re-architect core catalog, ads indexing and serving infrastructure to achieve greater scalability, freshness, performance, and reliability, using data storage, streaming processing, and information retrieval technologies such as MySQL, TiDB, Flink, and HNSW. \n Modernize the ML ecosystem for the entire Pinterest Ads product, replacing a hodgepodge of out-of-date ML models with a unified, modern, and privacy-first ML stack with Pytorch, Spark, Iceberg, and GPU based serving. \n Collaborate with cross-functional teams to define problems and drive solutions. \n Work with a strong team of engineers and provide technical guidance and mentorship. \n \n What we're looking for: \n \n 6+ years of relevant industry experience with distributed systems, transactional datastores, and systems programming. \n Experience in building and owning large scale high performance infrastructure powering ads, recommendation, search, or other consumer facing applications. \n Experience solving end-user problems and envisioning solutions to improve their productivity. \n Proficiency in Java, C++, or Python. \n \n This position is not eligible for relocation assistance. \n  #LI-CL5 \n  #LI-REMOTE \n \n \n  At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. \n  Information regarding the culture at Pinterest and benefits available for this position can be found  here . \n \n  US based applicants only \n \n    $207,500\u2014$311,200 USD\n   \n \n \n  Our Commitment to Diversity: \n  Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.", "cleaned_desc": " \n 6+ years of relevant industry experience with distributed systems, transactional datastores, and systems programming. \n Experience in building and owning large scale high performance infrastructure powering ads, recommendation, search, or other consumer facing applications. \n Experience solving end-user problems and envisioning solutions to improve their productivity. \n Proficiency in Java, C++, or Python. \n \n This position is not eligible for relocation assistance. ", "techs": ["java", "c++", "python"]}, "5c684cc7e946cbbe": {"terms": ["machine learning engineer", "mlops"], "salary_min": 140000.0, "salary_max": 200000.0, "title": "DevOps Engineer", "company": "Spresso", "desc": "At Spresso, our mission is to use data to deliver better business outcomes to various industries around the world. We optimize decision-making with tools we\u2019ve built spanning a decade of first-hand experience in e-commerce. What started as an end-to-end platform for the e-commerce retailers is now a world-class suite of SaaS products powered by advanced analytics and machine learning. Spresso brings unique data and machine learning capabilities to retailers globally. \n Our Engineering team is a brilliant cultivator of technology powering our world class platform & SaaS modules spanning everything from Catalog, Orders & Fulfillment, and Personalization. Being part of Spresso\u2019s Engineering team means you\u2019ll work with wicked-smart individuals from all over the world who contribute as engineers, product managers, designers, and data scientists. Every day our Engineering team innovates in the e-commerce space with the latest technologies. We\u2019re excited to welcome engineers who are ready for a challenge and know how to think outside the box! \n The DevOps team values an always-learning approach to technology. We want people who already know they don\u2019t know it all, and who are willing to work closely and honestly with team members to collectively and objectively find the best solutions to problems. Technological humility - the desire to build what\u2019s correct over being the person who\u2019s correct - is a trait we value highly. \n As a  DevOps Engineer , you\u2019ll be helping to scale out and improve our large and growing GCP infrastructure and helping us manage our applications running in Docker and Kubernetes. Most of the software we run is Node.js, with a heavy reliance on MongoDB. Python, Go, and Java, which are also utilized for various services, as are several other data stores, including Redis, Postgres, Snowflake, and Elasticsearch. \n You Will: \n \n Work collaboratively with the team to build out and improve our large and growing GCP infrastructure \n Improve and optimize our cloud infrastructure with Terraform \n Help automate and streamline our operations \n Troubleshoot and resolve issues in our dev, staging and production environments \n Support Software Engineers with building, testing, and deploying their applications \n Be on an on-call rotation with the rest of the Engineering team \n Be a critical part of not just the technology team but to the company as an excellent problem solver. \n \n Requirements: \n \n Good programming skills \n Experience with Kubernetes \n Experience with GCP components & pitfalls (AWS or Azure also acceptable) \n Experience with an IaaC tool (Terraform) \n Experience with a CI/CD tool such as Jenkins \n Experience writing Dockerfiles for apps in multiple languages \n Experience writing scripts to automate repetitive tasks \n Disaster recovery experience, especially if self-inflicted \n Experience with MongoDB performance analysis, debugging, and/or data modeling \n Familiarity with cloud networking - Firewalls, NAT, VPN, network peering \n Experience running Node.js apps under load \n Ability to debug system problems like running out of memory, inodes, or ephemeral ports \n Knowledge of distributed systems (formal theory or learned firsthand) \n Experience using a Log Aggregation Platform. \n \n Job Type: Full-time \n Pay: $140,000.00 - $200,000.00 per year \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n Life insurance \n Paid time off \n Parental leave \n Vision insurance \n \n Work Location: Remote", "cleaned_desc": " Good programming skills \n Experience with Kubernetes \n Experience with GCP components & pitfalls (AWS or Azure also acceptable) \n Experience with an IaaC tool (Terraform) \n Experience with a CI/CD tool such as Jenkins \n Experience writing Dockerfiles for apps in multiple languages \n Experience writing scripts to automate repetitive tasks \n Disaster recovery experience, especially if self-inflicted   Experience with MongoDB performance analysis, debugging, and/or data modeling \n Familiarity with cloud networking - Firewalls, NAT, VPN, network peering \n Experience running Node.js apps under load \n Ability to debug system problems like running out of memory, inodes, or ephemeral ports \n Knowledge of distributed systems (formal theory or learned firsthand) \n Experience using a Log Aggregation Platform. \n \n Job Type: Full-time ", "techs": ["good programming skills", "kubernetes", "gcp components & pitfalls", "aws", "azure", "iaac tool (terraform)", "ci/cd tool (jenkins)", "dockerfiles", "scripting", "disaster recovery", "mongodb performance analysis", "debugging", "data modeling", "cloud networking", "firewalls", "nat", "vpn", "network peering", "running node.js apps under load", "system debugging", "distributed systems", "log aggregation platform"]}, "b4c88e4d61bd83bf": {"terms": ["machine learning engineer"], "salary_min": 80000.0, "salary_max": 95000.0, "title": "Support Development Engineer", "company": "EnergyHub", "desc": "EnergyHub empowers utilities and their customers to create a clean, distributed energy future. Our customers are real utilities managing the real electrical grid supporting the normal lives of millions of real people. Our mission is to keep the grid running reliably while enabling more renewable energy and reducing reliance on fossil fuel. \n  We are establishing a Support Development Engineer position to assist Engineering as we scale. In this position you will work closely with Engineering, Internal Stakeholders, and Clients to identify root causes of technical issues, configure and update data attributes that Engineering has not yet built UI tooling for, configure data streams between EnergyHub and our OEM or Utility partners, and identify and implement technical solutions to mitigate or correct known issues. EnergyHub is a growing organization, and as the first Support Development Engineer, you will have an outsized impact on defining this role within the company. \n  You should apply for this job if you are passionate about addressing climate change, and want to leverage your strong technical integration, technical communication, and problem solving skills to do so. \n  Main Responsibilities: \n \n Configure automated file delivery via SFTP, email, or other mechanisms. \n Create, update, and maintain Python scripts for custom data implementations, data cleanup, or process improvements \n Debug issues in Frontend (React/Javascript) and Backend (Java) code, and contribute bug-fix PR's to the codebase \n Experience with identifying and debugging issues using tools like AWS Cloudwatch, Sentry.io, and Datadog. \n Experience working with 3rd Party API's using Postman or Nightingale \n Experience with low-code platforms like Retool or Zapier \n \n What we look for: \n \n Minimum of 3 years experience in a technical role \n Strong organizational skills and an eye for calling out areas for improvement \n Comfortable working with internal teams as well as external partners \n Experience with a scripting language such as Python. \n Experience or familiarity with Fullstack web development \n Experience with the AWS ecosystem (and/or other Azure/GCP/etc.) \n A comprehension of code versioning concepts and experience with tools like git. \n Ability to combine information gained from multiple sources to identify root causes \n Enjoy being part of a team; helping and learning from others. \n \n The salary range for this position is $80,000-$95,000. Base pay offered may vary depending on location, job-related knowledge, skills and experience. \n  Why work for EnergyHub? \n \n Collaborate with outstanding people : Our employees work hard, do great work, and enjoy collaborating and learning from each other. \n Make an immediate impact : New employees can expect to be given real responsibility for bringing new technologies to the marketplace. You are empowered to perform as soon as you join the team! \n Gain well rounded experience : EnergyHub offers a diverse and dynamic environment where you will get the chance to work directly with executives and develop expertise across multiple areas of the business. \n Work with the latest technologies : You'll gain exposure to a broad spectrum of IoT, SaaS and machine learning obstacles, including distributed fault-tolerance, device control optimization, and process modeling to support scalable interaction with disparate downstream APIs. \n Be part of something important:  Help create the future of how energy is produced and consumed. Make a positive impact on our climate. \n Focus on fun : EnergyHub places high value on our team culture. Happy hours and holiday parties are important to us, but what's also important is how our employees feel every single day. \n \n Company Information \n  EnergyHub is a growing enterprise software company that works with the most forward-thinking companies in smart energy. Our platform lets consumers turn their smart thermostats, electric cars, water heaters, and other products into virtual power plants that keep the grid stable and enable higher penetration of solar and wind power. We work on technology that already provides energy and cost savings to millions of people through partnerships with the most innovative companies in the Internet of Things. \n  Company Benefits \n  EnergyHub offers a generous benefits package including 100% paid medical for employees and a 401(k) with employer match. We offer a casual environment, the flexibility to set your own schedule, a fully stocked fridge and pantry, free Citi Bike membership, secure bike rack, gym subsidy, paid parental leave, and an education assistance program. \n  EnergyHub is an Equal Opportunity Employer \n  In connection with your application, we collect information that identifies, reasonably relates to or describes you (\"Personal Information\"). The categories of Personal Information that we may collect include your name, government-issued identification number(s), email address, mailing address, other contact information, emergency contact information, employment history, educational history, and demographic information. We collect and use those categories of Personal Information about you for human resources and other business management purposes, including identifying and evaluating you as a candidate for potential or future employment or future positions, recordkeeping in relation to recruiting and hiring, conducting analytics, and ensuring compliance with applicable legal requirements and Company policies.", "cleaned_desc": " Debug issues in Frontend (React/Javascript) and Backend (Java) code, and contribute bug-fix PR's to the codebase \n Experience with identifying and debugging issues using tools like AWS Cloudwatch, Sentry.io, and Datadog. \n Experience working with 3rd Party API's using Postman or Nightingale \n Experience with low-code platforms like Retool or Zapier \n \n What we look for: \n   Minimum of 3 years experience in a technical role \n Strong organizational skills and an eye for calling out areas for improvement \n Comfortable working with internal teams as well as external partners \n Experience with a scripting language such as Python. \n Experience or familiarity with Fullstack web development \n Experience with the AWS ecosystem (and/or other Azure/GCP/etc.) \n A comprehension of code versioning concepts and experience with tools like git. ", "techs": ["react", "javascript", "java", "aws cloudwatch", "sentry.io", "datadog", "postman", "nightingale", "retool", "zapier", "python", "fullstack web development", "aws ecosystem", "azure", "gcp", "git"]}, "b952ae4095e9fbe1": {"terms": ["machine learning engineer"], "salary_min": 179444.97, "salary_max": 227217.31, "title": "Manager II, Engineering", "company": "Pinterest", "desc": "About Pinterest : \n  Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. \n  Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences. \n  Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our  PinFlex  landing page to learn more. \n \n  As the Engineering Manager of the Advertising at Scale team, you'll be leading a large team that is responsible for many of the key surfaces that enable a user to create an ad campaign on Pinterest. We are a user centric team laser focused this year on building foundational tools and experiences to best serve our enterprise clients, with a focus on creating and editing campaigns at scale. We aim to accomplish this by making significant changes in the surfaces that create an ad on Pinterest based on UX research, machine learning, and experimentation. Our goal is to make successful advertising on Pinterest easy to use, understand, and execute. \n  The ideal candidate should have experience leading teams that work across the web technology stack, be driven about leading a highly productive engineering team through its roadmap, and be passionate about helping each member of their team grow. \n  What you'll do: \n \n Managing a team of full-stack engineers \n Keep the team accountable for hitting business goals and driving meaningful impact \n Foster a highly productive team that releases features on roadmap on time while maintaining high technical quality \n Propose technical projects to improve quality to include in the roadmap \n Work closely with Product and Design on executing roadmap, setting technical direction and delivering value \n Coordinate closely with XFN partners on multiple partner teams that the team interfaces with \n Work with PM and tech leads to estimate scope of work, define release schedules, and track progress. \n Mentor and develop engineers at various levels of seniority \n \n What we're looking for: \n \n 2+ years of experience as an engineering manager (perf cycles, managing up/out, 10 ppl) \n 5+ years of software engineering experience as a hands on engineer \n Experience leading a team of engineers through a significant feature or product launch in collaboration with Product and Design \n Experience working being on the front lines for advertiser issues: handling support tickets while launching new features on the roadmap \n Track record of developing high quality software in an automated build and deployment environment \n Experience working with both frontend and backend technologies \n Ability to wear different hats (Product/Design/Data Analyst) to ensure team success \n Well versed in agile development methodologies \n Ability to operate in a fast changing environment / comfortable with ambiguity \n Experience working on a legacy codebase a plus \n \n This position is not eligible for relocation assistance. \n  #LI-REMOTE \n  #LI-NB1 \n \n \n  At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. \n  Information regarding the culture at Pinterest and benefits available for this position can be found  here . \n \n  US based applicants only \n \n    $135,150\u2014$278,000 USD\n   \n \n \n  Our Commitment to Diversity: \n  Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.", "cleaned_desc": "", "techs": ""}, "2d008d516b7a30dc": {"terms": ["machine learning engineer"], "salary_min": 206236.02, "salary_max": 261140.77, "title": "Staff Software Engineer, Ads ML Infrastructure", "company": "Pinterest", "desc": "About Pinterest : \n  Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. \n  Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences. \n  Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our  PinFlex  landing page to learn more. \n \n  Pinterest is one of the fastest growing online advertising platforms and our continued success depends on rapidly scaling our core revenue-generating systems. Specifically, we need 10X the scale of our campaign management, ad delivery, and machine learning platforms, while enabling developers inside Pinterest and external advertisers to build and iterate rapidly on new features. We are looking for senior staff engineers to initiate, design, and build the next-gen version of key infra components in our monetization ecosystem, such as modernizing an end-to-end ML platform serving over hundreds of use cases making billions of predictions per second, and redesigning our catalog ingestion and ads delivery systems to become one of the leading advertising platforms in the world. These roles are exciting, because you will be able to lean on your deep infra knowledge to redesign systems to handle a much bigger scale, while also having the chance to work with very experienced engineers and cross-functional partners. \n \n \n  What you'll do: \n \n Re-architect core catalog, ads indexing and serving infrastructure to achieve greater scalability, freshness, performance, and reliability, using data storage, streaming processing, and information retrieval technologies such as MySQL, TiDB, Flink, and HNSW.   \n Modernize the ML ecosystem for the entire Pinterest Ads product, replacing a hodgepodge of out-of-date ML models with a unified, modern, and privacy-first ML stack with Pytorch, Spark, Iceberg, and GPU based serving.   \n Collaborate with cross-functional teams to define problems and drive solutions.   \n Work with a strong team of engineers and provide technical guidance and mentorship. \n \n \n   \n What we're looking for: \n \n 8+ years of relevant industry experience with distributed systems, transactional datastores, and systems programming.   \n Experience in building and owning large scale high performance infrastructure powering ads, recommendation, search, or other consumer facing applications.   \n Experience solving end-user problems and envisioning solutions to improve their productivity.   \n Proficiency in Java, C++, or Python. \n \n \n   \n \n \n At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. \n  Information regarding the culture at Pinterest and benefits available for this position can be found  here . \n \n  US based applicants only \n \n    $135,150\u2014$278,000 USD\n   \n \n \n  Our Commitment to Diversity: \n  Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.", "cleaned_desc": " \n  What you'll do: \n \n Re-architect core catalog, ads indexing and serving infrastructure to achieve greater scalability, freshness, performance, and reliability, using data storage, streaming processing, and information retrieval technologies such as MySQL, TiDB, Flink, and HNSW.   \n Modernize the ML ecosystem for the entire Pinterest Ads product, replacing a hodgepodge of out-of-date ML models with a unified, modern, and privacy-first ML stack with Pytorch, Spark, Iceberg, and GPU based serving.   \n Collaborate with cross-functional teams to define problems and drive solutions.   \n Work with a strong team of engineers and provide technical guidance and mentorship. ", "techs": ["mysql", "tidb", "flink", "hnsw", "pytorch", "spark", "iceberg", "gpu"]}, "798e41a6de951019": {"terms": ["machine learning engineer"], "salary_min": 148068.0, "salary_max": 174197.0, "title": "Senior Product Manager, Member & Clinician Experience", "company": "Virta Health", "desc": "Virta Health is on a mission to transform diabetes care and reverse the type 2 diabetes epidemic. Current treatment approaches aren\u2019t working\u2014over half of US adults have either type 2 diabetes or prediabetes. Virta is changing this by helping people reverse type 2 diabetes through innovations in technology, personalized nutrition, and virtual care delivery reinvented from the ground up. We have raised over $350 million from top-tier investors, and partner with the largest health plans, employers, and government organizations to help their employees and members restore their health and live diabetes-free. Join us on our mission to reverse diabetes in 100M. \n \n  We\u2019re looking for a product manager who is passionate about understanding member and clinician needs and building technology that has a direct, transformative impact on their lives. As a Senior Product Manager in our Virta Program Group, you will be responsible for evolving core aspects of our member and clinician applications to improve our member experience and health outcomes at scale. You will develop the vision, strategy, and roadmap for your products. You will manage the direction and execution of a cross functional product development team, and play a critical role in setting company-wide strategy to scale our treatment with clinical and operational department leads. This is one of the highest impact positions at Virta - the decisions you make and products you build will have a direct impact on both the patient experience and Virta\u2019s business. \n \n  Responsibilities \n  As a Senior Product Manager in our Program Group, you will be responsible for evolving core aspects of our member and clinician applications to improve our member experience and health outcomes at scale: \n \n  You will develop the vision, strategy, and roadmap for your products \n  Develop a deep understanding of our members and clinicians and their unmet needs, desires, and pain points through qualitative primary research, analytics, and controlled software experiments \n  Define the product direction for a team comprising designers, software engineers, and Applied AI/ML engineers; work cross-functionally with clinical and operational leadership \n  Translate your vision into compelling initiatives and product requirements and stir excitement across the organization to execute on them \n  Coordinate execution on highly strategic initiatives across a diverse group of experts from domains such as: Behavioral Health, Clinical Excellence, our Provider Group, AI & Machine Learning, Infosec/Legal, Design, and Analytics \n  Take ownership of our product\u2019s contributions to improve critical patient-facing metrics related to engagement, health outcomes, and scalability of our treatment \n  Invest in improving our culture and practice of product management at Virta \n \n \n  90 Day Plan \n  Within your first 90 days at Virta, we expect you will do the following: \n \n  Launch 1 new software initiative to improve member and clinician experience and health outcomes \n  Engage in user research and quantitative analysis to make data informed decisions on your product\u2019s direction \n  Develop a strong understanding of our north star metrics at Virta, and formulate the appropriate metrics for your team in partnership with leadership \n  Establish strong relationships across our leadership team, partnering departments, and adjacent product development groups \n  Begin to set the strategy for your product development team, and contribute to the broader strategy for our entire patient experience. \n \n \n  Must-Haves \n \n  Demonstrated experience and success building and scaling consumer grade product experiences \n  8+ years of experience in a product management, general management, or related product development role. \n  Strong organizational, leadership, and communication skills \n  Nuanced ability to balance the long term investments with short term wins \n  Extreme ownership, bias to action, and know-how to succeed in ambiguity \n  Demonstrated ability to lead without established authority \n  Experience in healthcare \n \n \n  Values-driven culture \n  Virta\u2019s company values drive our culture, so you\u2019ll do well if: \n \n  You put  people first  and take care of yourself, your peers, and our patients equally \n  You have a strong sense of  ownership  and take initiative while empowering others to do the same \n  You prioritize positive  impact  over busy work \n  You have  no ego  and understand that everyone has something to bring to the table regardless of experience \n  You appreciate  transparency  and promote trust and empowerment through open access of information \n  You are  evidence-based  and prioritize data and science over seniority or dogma \n  You  take risks and rapidly iterate \n \n \n  Is this role not quite what you're looking for? Join our Talent Community and follow us on Linkedin to stay connected! \n \n  As part of your duties at Virta, you may come in contact with sensitive patient information that is governed by HIPAA. Throughout your career at Virta, you will be expected to follow Virta's security and privacy procedures to ensure our patients' information remains strictly confidential. Security and privacy training will be provided. \n \n  Virta has a location based compensation structure. Starting pay will be based on a number of factors and commensurate with qualifications & experience. For this role, the compensation range is $148,068 to $174,197. Information about Virta\u2019s benefits is on our Careers page at:  https://www.virtahealth.com/careers . \n  Compensation Range: $148.1K - $174.2K", "cleaned_desc": "", "techs": ""}, "56005c23d13b5523": {"terms": ["machine learning engineer", "mlops"], "salary_min": 126639.08, "salary_max": 160353.3, "title": "SENIOR PYTHON/GO ENGINEER POSITION WITH KNOWLEDGE OF DEVOPS FUNCTIONS", "company": "Innovecs", "desc": "Required Skills \n \n   DevOps:  strong \n \n \n   Python:  middle \n \n \n   Clouds:  strong \n \n \n \n  Are you passionate about bridging the gap between development and operations? Join us as a DevOps Engineer and help us streamline our software development and deployment processes. We expect you to have experience developing highly-distributed systems and that you have worked with protocols, network, and storage concepts. You will work in a dynamic work environment and get endless opportunities for growth and development. Sounds interesting? If so, we\u2019d be delighted to receive your resume! \n  About Innovecs \n  We are a global digital transformation tech company with over a decade of history. Let numbers speak for ourselves: 13 locations on a world map, 60+ projects, and 100+ clients served. You will be able to work with us in Fintech, Healthtech, Supply Chain, Logistics, High Tech, Trading and Banking, Warehouse Management and Automation, Blockchain, Data, and AI from our offices worldwide or remotely. Based on our policies, we work as solid vendors representing only the final result. Individually, we may be engineers, testers, designers, product managers, and more. But together, we're one team. Due to our flat structure, you can bring your ideas and approaches to life and figure out all your objectives within the team or even become a part of the core team because, for us, people are always above all. \n  Our value to you: \n \n People first, then processes.  Whatever the projects you work on, you\u2019ll face the team\u2019s openness and loyalty, whether you talk to the Middle Engineer or Delivery Director, because people relationship is our foremost principle. \n \n \n \n \n We reject micromanagement.  You are responsible for your results, and you choose whether to grow vertically or horizontally. \n \n \n \n \n Your versatile growth matters to us.  Get ready shine as an expert via technical articles, public conferences, lectures, and thought leadership \n \n \n \n \n Knowledge sharing.  We discuss our wins and failures at all-hands events; share our experiences on mentorship programs, and grow professionally guided by a personal development plan. \n \n \n \n \n Technical quality is the key to success.  We value and encourage our experts who contribute to the development of products due to well-being and learning programs, because the high quality of our work is #1 priority. \n \n \n \n \n Reliable workplace.  You\u2019ll get attractive compensation pegged to the U.S. dollar. \n \n \n \n \n Support on all stages.  From the preparation programs for global certification (AWS, ISTQB, Unity, Scrum, etc.) to paid maternity/paternity leaves and your manager\u2019s reliable backing. \n \n \n \n \n Sense of belonging.  We have 13 communities where you will enjoy your hobbies with like-minded people. From cooking to mindfulness \u2014 practice what you love and explore yourself. \n \n \n \n \n Free sports classes whenever you want.  Running, chess, yoga, or online workouts with fitness coaches \u2013 choose yours and take it away. \n \n \n \n \n Healthcare and well-being.  Private health insurance, sick leave, vacation, and holidays. \n \n \n \n \n English classes.  Take your speech and writing to a new level. \n \n \n \n \n Celebrate the lifestyle.  Virtual and offline social activities and occasional gifts. \n \n \n \n \n Requirements 4-5 years with Python/Go 2-3 years as a DevOps Experience with Terraform and k8s Upper-Intermediate English \n \n Responsibilities Design and build infrastructure both on-premises and in the cloud to run our regression tests and in-house services; Deploy and manage orchestration systems to schedule maintenance jobs, ensure our services are available, and auto-scale when testing demands grow; Implement monitoring and alerting systems around the various DevOps operations; Build a CI system to allow teams to run their pipelines on our storage product and on various other projects; Proactively work with a variety of infrastructure providers, from cloud providers like AWS to virtual machines and physical servers; Review the security state of our infrastructure and act to improve it.", "cleaned_desc": "", "techs": ""}, "8d2c9fdab07d4f95": {"terms": ["machine learning engineer"], "salary_min": 120000.0, "salary_max": 170000.0, "title": "Tech Lead - Back-End Software Engineer, Java (US Remote)", "company": "LeanTaaS", "desc": "We are a growth stage company that creates software solutions combining lean principles, predictive and prescriptive analytics, and machine learning to transform hospital and infusion center operations. More than 180 health systems and over 685 hospitals across 49 states rely on our award-winning products to increase patient access, decrease wait times, and reduce healthcare delivery costs. We have raised more than $300 million from top-tier investors such as Bain Capital, Insight Partners, and Goldman Sachs. We have been named among the top 100 AI companies in the world.\n  \n \n \n  Please note that while this role is listed as available for remote, we are currently employing in the following states:  \n AK, AZ, CA, CO, CT, DC, FL, GA, IL, IN, KS, LA, MD, MA, MI, MO, MT, NH, NJ, NC, OH, OR, PA, SC, TN, TX, UT, VA, WA, WI \n   \n . If your state is not listed, we may not be able to proceed with your application. We have offices in Santa Clara, CA and Charlotte, NC for employees who prefer to work regularly or occasionally from an office. \n \n \n   Join us to work on developing iQueue for Operating Rooms - maximize healthcare capacity with AI and prescriptive analytics.\n  \n \n   You will be a part of a small, close-knit, cross-functional team that includes product managers, backend engineers, frontend engineers, data scientists and designers. We are on a mission to understand customer needs and build thoughtful solutions for hard problems.\n  \n \n   You have to be comfortable with ambiguity - healthcare data can be messy and challenging and requires patience, tenacity, and reverse engineering skill. But the satisfaction that results when customers love our features is priceless.\n  \n \n \n  WHAT YOU\u2019LL DO \n \n  Collaborate with Product Managers, Designers and Data Scientists to understand customers problems and develop elegant solutions for them \n  Produce thoughtful design documents and readable, performant, automated testing-friendly code \n  Take ownership of the code base and continuously improving it \n  Perform design and code reviews and resolve production issues to ensure that the service is running smoothly at all times \n  Make the team stronger by sharing your knowledge via documentation, blogs, tech talks, and other means \n \n  WHAT YOU\u2019LL BRING \n \n  5+ years of professional experience working on production RESTful web services in cloud environments (AWS or GCP) \n  Strong Java skills \n  Solid experience with SQL and database performance \n  Sense of ownership and ability to get things done - with curiosity, passion, and entrepreneurial spirit \n \n  BONUS POINTS IF YOU HAVE \n \n  Experience working with serverless technologies like AWS lambdas, API Gateways and SQS \n  Experience in developing GraphQL APIs \n  Experience working on healthcare products \n \n  WHAT YOU'LL GET \n \n  Intellectual and emotional satisfaction of solving tough operational problems in healthcare while improving patient access and saving lives! \n  Competitive compensation package that includes base salary, annual bonus, and stock options \n  401(k) Match \n  Comprehensive healthcare benefits \n  Generous Paid Time Off and Parental Leave \n  Monthly reimbursement for Skill Building \n  Monthly reimbursement for Wellness, Transportation, and/or Home Office \n  Education Reimbursement for select courses/programs \n \n \n   The offered base salary will reflect careful consideration of a number of factors, including your location, skills and qualifications, prior relevant experience, internal equity and market conditions. This range may be modified in the future.\n  \n \n \n  Vaccination policy \n \n \n   We have an obligation to protect our employees, our customers, and the patients of our customers. Therefore employees are required to be vaccinated to work from the office, attend in-person company events, or to travel on behalf of the company.\n  \n \n \n  LeanTaaS is an equal opportunity employer committed to promoting an inclusive work environment free of discrimination and harassment. We value diversity, inclusion, and aim to provide a sense of belonging for everyone. All qualified applicants for employment will be considered without regard to race, color, sex, gender identity, gender expression, religion, age, national origin or ancestry, citizenship, physical or mental disability, medical condition, family care status, marital status, domestic partner status, sexual orientation, genetic information, military or veteran status, or any other basis protected by federal, state or local laws. If you require assistance during the application process, please reach out to accommodations@leantaas.com. LeanTaaS will reasonably accommodate qualified individuals with disabilities to the extent required by applicable law.\n  \n \n \n  Please note: LeanTaaS is not accepting agency resumes at this time, and we are not responsible for any fees related to unsolicited resumes. Thank you.", "cleaned_desc": "  Make the team stronger by sharing your knowledge via documentation, blogs, tech talks, and other means \n \n  WHAT YOU\u2019LL BRING \n \n  5+ years of professional experience working on production RESTful web services in cloud environments (AWS or GCP) \n  Strong Java skills \n  Solid experience with SQL and database performance \n  Sense of ownership and ability to get things done - with curiosity, passion, and entrepreneurial spirit \n \n  BONUS POINTS IF YOU HAVE \n \n  Experience working with serverless technologies like AWS lambdas, API Gateways and SQS \n  Experience in developing GraphQL APIs ", "techs": ["aws", "gcp", "java", "sql", "aws lambdas", "api gateways", "sqs", "graphql apis"]}, "f2302550880efc13": {"terms": ["machine learning engineer", "mlops"], "salary_min": 126256.04, "salary_max": 159868.28, "title": "Senior MLOps Engineer \u2013 Federal Marketing", "company": "Dell Technologies", "desc": "Senior MLOps Engineer \u2013 Federal Marketing \n  Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand-new methodologies, tools, statistical methods, and models. What\u2019s more, we are in collaboration with leading academics, industry experts, and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data. \n  Join us to do the best work of your career and make a profound social impact as a Machine Learning Operations Engineer (MLOps) on our Customer Growth Marketing Team in Round Rock, Texas, Hopkinton, Massachusetts, or  Remote United States (U.S.) \n  What you\u2019ll achieve \n  This position is focused on our U.S. Federal Marketing segment. As a Senior MLOps Engineer on a growing team, you will bring in your industry experience to manage the machine-learning (ML) lifecycle at scale. \n  You will: \n \n  Collaborate with our MLOps Engineering, Data Engineering, and IT teams to support the technical roadmap and world-class engineering practices \n  Play a critical role in growing and maturing our marketing capabilities with machine learning at its core \n  Work collaboratively across diverse teams and with different stakeholders \n \n  Take the first step toward your dream career  Every Dell Technologies team member brings something unique to the table. Here\u2019s what we are looking for in this role: \n  Essential Requirements  \n \n U.S. Citizen residing in the U.S. \n  Bachelor\u2019s Degree in Computer Science, Engineering, Information Technology, or equivalent professional experience \n  5+ years of related professional software-engineering or infrastructure engineering experience in enterprise-scale environments \n  3+ years supporting CI/CD systems for continuous testing and deployment of machine learning models or scalable software/services \n  Experience with automation, monitoring, logging, and troubleshooting in production \n \n  Desired Requirements \n \n  Experience with U.S. Federal industry \n  Experience with Python and containerization technologies, such as Docker \n \n  Who we are \n  We believe that each of us has the power to make an impact. That\u2019s why we put our team members at the center of everything we do. If you\u2019re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we\u2019re looking for you.    Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us.    Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. Read the full Equal Employment Opportunity Policy here. \n  Job ID: R227328\n   Dell\u2019s Flexible & Hybrid Work Culture    At Dell Technologies, we believe our best work is done when flexibility is offered.    We know that freedom and flexibility are crucial to all our employees no matter where you are located and our flexible and hybrid work style allows team members to have the freedom to ideate, be innovative, and drive results their way. To learn more about our work culture, please visit our locations page.", "cleaned_desc": "  Bachelor\u2019s Degree in Computer Science, Engineering, Information Technology, or equivalent professional experience \n  5+ years of related professional software-engineering or infrastructure engineering experience in enterprise-scale environments \n  3+ years supporting CI/CD systems for continuous testing and deployment of machine learning models or scalable software/services \n  Experience with automation, monitoring, logging, and troubleshooting in production \n ", "techs": ["none"]}, "0a163077a1e5e6bf": {"terms": ["machine learning engineer"], "salary_min": 100448.805, "salary_max": 127190.57, "title": "IT Consultant", "company": "Capgemini", "desc": "Duration : 4 months \n \n  Job Description: \n \n \n \n  Role Description: Software Engineers apply the principles of software engineering to the design, development, maintenance, testing, and evaluation of the software and systems work to the specified requirements. \n \n \n \n \n  This role is focused on the support and management of the enterprise's CMDB. Primary work will include the support of incidents and requests related to the CMDB, as well as some analysis/development in managing the health of the CMDB. \n \n \n  Job description: \n \n \n \n  Apply current principles of software engineering to design, develop, maintain, configure and test software and systems to ensure they work to the specified requirements \n  Performs incident triage, including determining potential impact and identifying remediation's \n  Analyzes current systems & integrations, and works establish automation where possible \n  Using software development skills & knowledge, fully support the Configuration Management practices with the enterprise, including work with the Configuration Management Database (CMDB), Configuration Items (CI's), and infrastructure & application discovery; all of this needs to be done with the Service Management framework using ITIL practices \n  Build strong relationships with peers and Customers across functions through collaborative engineering operations and initiatives \n \n \n  Skills Required: \n \n \n \n  Can performs a range of assignments related to Service Management, especially Configuration Management and Configuration Management Database (CMDB) \n  Knowledge and understanding of software engineering architectures, system/software designs, and system deployments \n  Knowledge and understanding of System Administration (configurations, installations, patch management, server maintenance, etc.) and Network Management (firewalls, proxies, IP management, routing, DNS). \n  Knowledge and understanding of integration and communication protocols between applications, databases, and technology platforms. \n  Knowledge and understanding of coding principles developing independent modules based on the given specs in choice of scripts (Powershell, Shell, Perl), and hand-on experience with programming language like Java, Python, etc. \n  Uses prescribed guidelines or policies in analyzing situations \n  Works well within a team environment, and can work independently to get work accomplished when provide generally information about the solutions needed (requiring a moderate level of guidance and direction on a regular basis) \n \n \n  The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job. \n \n  A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients\u2019 opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.", "cleaned_desc": "  Knowledge and understanding of integration and communication protocols between applications, databases, and technology platforms. \n  Knowledge and understanding of coding principles developing independent modules based on the given specs in choice of scripts (Powershell, Shell, Perl), and hand-on experience with programming language like Java, Python, etc. \n  Uses prescribed guidelines or policies in analyzing situations \n  Works well within a team environment, and can work independently to get work accomplished when provide generally information about the solutions needed (requiring a moderate level of guidance and direction on a regular basis) \n \n \n  The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job. \n ", "techs": ["integration and communication protocols", "applications", "databases", "technology platforms", "coding principles", "scripts", "powershell", "shell", "perl", "programming language", "java", "python", "guidelines", "policies", "team environment", "independent work", "solutions", "capgemini freelancer gateway", "software platform", "machine learning", "artificial intelligence"]}, "62a6fa2bfad6cf82": {"terms": ["machine learning engineer"], "salary_min": 96998.11, "salary_max": 122821.22, "title": "Platform Engineer", "company": "Capgemini", "desc": "Duration : 12 months \n \n  Job Description: \n \n \n \n  Role Description: The DevOps SRE engineer is responsible for supporting development and deployment of AmFam Group's application portfolio. You will be a part of our DevOps SRE team and report to the product owner responsible for DevOps process improvement, reliability, and troubleshooting efforts for the organization. \n \n \n  Duties for this role include, but are not limited to: \n \n \n \n  ServiceNow Incident and Problem Response \n  Automation and orchestration of DevOps pipeline tools \n  Improvement of existing processes and observability of those processes \n  CI/CD Pipeline Management and Troubleshooting \n \n \n  Candidate Requirements: \n \n  Associate's degree in software engineering, information technology, computer science, or other related fields. \n  4+ years of appropriate experience \n  Proven hands-on experience with DevOps, building CI/CD using Gitlab, Jenkins, Artifactory, Kubernetes, Docker, and AWS \n  Experience with administration of Linux operating systems (Ubuntu, RHEL) \n  Experience with python \n  Experience with java application servers, including JBoss and Tomcat \n  Experience in troubleshooting and problem analysis \n \n \n  It is ideal if candidates also have experience in the following, but not required: \n \n  Experience working in the insurance/financial industry \n  Experience with additional languages such as Perl, PowerShell, or BASH scripting \n  Exposure to governance frameworks such as PCI, HIPAA \n  Experience in Kubernetes within AWS and GCP \n  A passion for all things DevOps \n  A willingness to teach and learn alongside the team \n  Strong documentation writing \n \n \n  The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job. \n \n  A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients\u2019 opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.", "cleaned_desc": "  Experience with administration of Linux operating systems (Ubuntu, RHEL) \n  Experience with python \n  Experience with java application servers, including JBoss and Tomcat \n  Experience in troubleshooting and problem analysis \n \n \n  It is ideal if candidates also have experience in the following, but not required: \n    Experience working in the insurance/financial industry \n  Experience with additional languages such as Perl, PowerShell, or BASH scripting \n  Exposure to governance frameworks such as PCI, HIPAA \n  Experience in Kubernetes within AWS and GCP \n  A passion for all things DevOps \n  A willingness to teach and learn alongside the team \n  Strong documentation writing \n ", "techs": ["linux operating systems (ubuntu", "rhel)", "python", "java application servers (jboss", "tomcat)", "troubleshooting", "problem analysis", "perl", "powershell", "bash scripting", "kubernetes", "aws", "gcp", "devops", "documentation writing"]}, "74893546a99fdd11": {"terms": ["machine learning engineer"], "salary_min": 96575.36, "salary_max": 122285.92, "title": "MSP - NOC Engineer | Philippines", "company": "My It Crew", "desc": "PLEASE NOTE:  This is a 3-Day weekend position. We are looking for someone to work Saturday, Sunday, and Monday from Midnight until Noon Eastern Standard Time (New York, USA Time Zone). This is a 3/12 36-hour shift; however, we add an additional 4 hours to your paycheck weekly to make up the difference of 40 hours. \n \n  Ready to get off the IT machine and come be part of a team where you are more than a cog in the wheel? My IT Crew is the place where everyone gets a voice and new ideas are welcomed. Sound like this could be your new home? Keep reading\u2026. \n \n \n  My IT Crew has been a leader in the Managed Service provider space since 2016. We strive for excellence and achieve it. \n \n Our clients love us, and our satisfaction surveys show it at a consistent 97% \n We are the team who always \u201chas the cool stuff\u201d, we adopt new tools to make us more efficient and allow us to help our clients with best-in-class support. \n No more asking everyone for help, it's right at your fingertips. Having documentation that's actually useful? That's us! \n We Listen. Our culture and collaborative meetings and weekly team-led training will help YOU drive yourself and the company forward. \n \n \n \n  We are expanding our operations and seeking to grow our team. We have an opening for a NOC Engineer. What does a NOC Engineer do? \n \n \n  The Network Operations Center (NOC) Engineer is responsible for monitoring our client\u2019s infrastructure and responding to incidents and outage situations. The NOC Engineer will analyze problems, perform troubleshooting and incident response, communicate with clients, and drive all problems to resolution. \n \n \n  NOC Engineer Duties and Responsibilities \n \n Be Fanatically awesome all the time! \n Provide technical excellence for systems, software, and hardware. \n Document/Update internal Kb\u2019s, SOP\u2019s, Wiki\u2019s. \n Monitor network administration and maintenance operations. \n Resolve network incidents to ensure the smooth running of the network operation center. \n Apply advanced consulting skills, extensive technical expertise, and full industry knowledge. \n Develop innovative solutions to complex problems. \n Support problem management resolutions by tracking system-wide problems on the operational networks and work with engineering support, including network engineering, systems engineering, catapult operations, and desktop engineering to see problem troubleshooting through to resolution. \n \n \n \n  NOC Engineer Requirements and Qualifications \n \n Strong working knowledge of IT. \n Advanced technical skills in e-learning and mobile training. \n Specialized knowledge of specific industries. \n Strong teaching and mentoring experience. \n Consistently demonstrate the MyITcrew Core Values and meet standard metrics. \n Current Microsoft, Azure or cloud computing, CCNA certifications or equivalent \n A minimum of 3+ Years experience in support and engineering of Windows on-premises and cloud environments", "cleaned_desc": "", "techs": ""}, "8f210fe4dde34cce": {"terms": ["machine learning engineer"], "salary_min": 200000.0, "salary_max": 200000.0, "title": "Sr. Systems Engineer", "company": "Extreme Networks", "desc": "Extreme Networks Named to Computerworld\u2019s 2023 List of Best Places to Work in IT! \n \n \n   Over 50,000 customers globally trust our end-to-end, cloud-driven networking solutions and rely on our top-rated services and support to accelerate their digital transformation efforts and deliver progress like never before and with double digit growth year over year, no provider is better positioned to deliver better outcomes on scale, than Extreme.\n  \n \n   We believe in \n   \u201cwalking the walk\u201d  of our strong core values which enable us to successfully advance together. Diversity and Inclusion is a vital part of our values and beliefs, and we\u2019re proud to foster an environment where every Extreme employee can thrive.\n  \n \n   Come become part of something big with us! We are a global leader, with hubs in North America, South America, Asia Pacific, Europe, and the Middle East.\n  \n \n \n  The Systems Engineer will be an integral member supporting the Sales Account Executive team in a Pre-Sales Capacity. He or she will support Strategic Major Accounts. Travel to customer sites presenting Proof of Concepts; contribute in the design, planning, and implementation and training on the Extreme Networks architecture. We seek individuals who are dynamic, flexible and willing/able to work outside of their comfort zone.\n  \n \n   Assist Pre-Sales team in the design, planning, and implementation of a Secure Network Solution Engineer\n  \n \n   Provide consultative support for pre-sales, partners and customers on Extreme Networks Solutions\n  \n \n   Present at trade shows, seminars, and oversee internal demonstration facilities, etc.\n  \n \n   Prepare and deliver customer presentations to all levels of management\n  \n \n   Provide educational training on leading edge technologies/Extreme Networks products to presales.\n  \n \n   Effectively use Wi-Fi test and planning tools. I.e. Ekahau, AirMagnet, NetScout.\n  \n \n   Travel and work the unique schedules in Strategic Major Accounts.\n  \n \n   Work independently, coordinate and communicate with all cross functional groups\n  \n \n  Requirements \n \n \n   5+ years relevant experience\n  \n \n   Expert in networking technologies with competitive Network industry and experience\n  \n \n   Proven ability to effectively influence customer decisions and displace competition\n  \n \n   Thorough knowledge of security design features and ability to implement and perform analysis of network and systems security design\n  \n \n   Hands-on experience using network toolsets, for wired, wireless and security products.\n  \n \n   Demonstrated skills in troubleshooting and problem resolution in a network environment\n  \n \n   Keen ability to understand and analyze customer requirements and position the Extreme Networks Solutions effectively\n  \n \n   Ability to effectively demonstrate Extreme Networks products and Solutions\n  \n \n   Demonstrated ability to manage complex customer interactions in difficult circumstances and control customer expectations in order to maintain satisfied customers\n  \n \n   Excellent inter-personal, technical presentation communication and proven team player\n  \n \n   Ability and willingness to travel within region up to 60%\n  \n \n   B.S. in Computer Science or related field, or equivalent work experience\n  \n \n   Industry Wireless Certifications are encouraged. (CWTS\u00ae, CWNA\u00ae, CWDP\u00ae, CWAP\u00ae, etc.)\n  \n \n  In depth knowledge of two or more of the following networking technologies \n \n \n   Understanding of Network Access Control (NAC), Radius, 802.1x, Active Directory, LDAP, etc.\n  \n \n   Security principles including: Firewalls, VPNs and Intrusion Prevention\n  \n \n   Cloud technologies; AWS, Azure, GCP\n  \n \n   Wireless, VPN, SDN, SaaS\n  \n \n   Ipv4/v6 Routing Protocols (RIPv1/v2/v3, OSPFv2/v3, VRRP, MPLS, BGP, PIM, etc.)\n  \n \n   Layer2 Switching (802.1Q, 802.1D, 801.x)\n  \n \n \n  Salary 200K on a 70/30 Split\n  \n \n  Extreme Networks, Inc. (EXTR) creates effortless networking experiences that enable all of us to advance. We push the boundaries of technology leveraging the powers of machine learning, artificial intelligence, analytics, and automation. Over 50,000 customers globally trust our end-to-end, cloud-driven networking solutions and rely on our top-rated services and support to accelerate their digital transformation efforts and deliver progress like never before. For more information, visit Extreme's website or follow us on Twitter, LinkedIn, and Facebook.\n  \n \n \n  We encourage people from underrepresented groups to apply. Come Advance with us! In keeping with our values, no employee or applicant will face discrimination/harassment based on: race, color, ancestry, national origin, religion, age, gender, marital domestic partner status, sexual orientation, gender identity, disability status, or veteran status. Above and beyond discrimination/harassment based on \u201cprotected categories,\u201d Extreme Networks also strives to prevent other, subtler forms of inappropriate behavior (e.g., stereotyping) from ever gaining a foothold in our organization. Whether blatant or hidden, barriers to success have no place at Extreme Networks.", "cleaned_desc": "  \n \n   Demonstrated ability to manage complex customer interactions in difficult circumstances and control customer expectations in order to maintain satisfied customers\n  \n \n   Excellent inter-personal, technical presentation communication and proven team player\n  \n \n   Ability and willingness to travel within region up to 60%\n  \n \n   B.S. in Computer Science or related field, or equivalent work experience\n  \n \n   Industry Wireless Certifications are encouraged. (CWTS\u00ae, CWNA\u00ae, CWDP\u00ae, CWAP\u00ae, etc.)\n  \n \n  In depth knowledge of two or more of the following networking technologies \n \n \n   Understanding of Network Access Control (NAC), Radius, 802.1x, Active Directory, LDAP, etc.\n  ", "techs": ["cwts\u00ae", "cwna\u00ae", "cwdp\u00ae", "cwap\u00ae"]}, "9cb8da6568298f38": {"terms": ["machine learning engineer"], "salary_min": 104300.0, "salary_max": 149000.0, "title": "Software Engineering Manager - Data Applications", "company": "EarthOptics", "desc": "EarthOptics is a growing AgTech company developing tools that help farmers more sustainably manage their soil, increase yields and feed the growing world. We are seeking a  Software Engineering Manager  to join our team. As a  S oftware Engineering Manager  you will join our growing engineering team and have people management and individual contributor responsibilities. As a member you will be responsible for leading our Data Applications team, as well as participate in the software development lifecycle. You will work closely with our technical product management, operations, and sales teams to plan and develop scalable software solutions for data collection, visualization, delivery for our customers and internal operations teams. \n \n  A Day in the Life\u2026 \n  The Software Engineering Manager for Data Applications is the manager of a cross functional agile team responsible for creating all user-facing front and back end applications for data ingestion and delivery. They are responsible for driving alignment and technical decision making with the members of the data application team, the technical product management team, and principal engineers. This includes people management responsibilities such as mentoring, training and progress reviews as well as technical management responsibilities such as design and code reviews. The Software Engineering Manager is also expected to participate in the software development life cycle, taking on individual contributor responsibilities and helping the team achieve its goals. This could include backend web-app system development, frontend app development, or cloud infrastructure development. It is expected that this role will have a significant individual contributor responsibility as well. \n \n  We seek motivated individuals who are eager to gain new skills in a fast-paced environment, and work closely and collaboratively with a team. We are committed to building a diverse team of employees from various backgrounds, educational histories, and industries. We have a competitive compensation package including robust healthcare benefits, paid time off and meaningful equity options grants. Even if you do not meet all of the requirements below, we encourage you to apply - we\u2019d love to hear from you! Learn more at www.EarthOptics.com. \n \n  To learn more about working at EarthOptics, visit: https://www.themuse.com/profiles/earthoptics \n \n  Job Type:  Full-Time, Exempt \n  Work Location:  This remote role welcomes candidates anywhere in the continental US. \n  Travel:  Travel may be required, approximately 10% of the time \n  Compensation:  $104,300-$149,000. Actual pay will be adjusted based on candidate\u2019s experience and geographic location. \n \n \n  Duties & Responsibilities: \n \n Hire and train new members of the data applications team. \n Assist with new employee onboarding for the data applications team. \n Perform reviews and assessments for members of the data applications team. \n Works closely with the technical product management team and SVP of engineering to ensure resources are allocated in order to be successful with our development milestones \n Work closely with the technical product management team to drive alignment between engineering tasks and product goals and set achievable milestones for delivery. \n Conducts code reviews to provide guidance on engineering best practices and compliance with development procedure and takes feedback from those reviews \n Participate in the software development lifecycle as an individual contributor doing backend, frontend or cloud infrastructure development \n Work with Principal Engineers to ensure technical alignment between engineering teams. \n Participate in and lead scrum ceremonies for the data applications team. \n Help design and implement our customer-facing web-based APIs, user management systems, and web applications for gathering and displaying data \n \n \n \n  What we are looking for: \n \n Masters Degree in Computer Engineering, Computer Science, or equivalent professional experience \n A proven track record developing fast, reliable, scalable distributed web services. \n \n \n 5-8 years of software engineering experience with 3+ years of experience in leading software development teams \n \n \n Experience with applications deployed using Amazon Web Services (AWS). \n Demonstrate a working knowledge of RESTful API design, the HTTP stack and Browser limitations \n Experience with Python, Rust and/or Golang for backend development \n Proficiency with Typescript \n Passion for providing software engineering best practices (e.g. unit testing, code reviews, design documentation, and continuous integration). \n Understanding of good UI/UX practices \n Familiarity with container orchestration (Kubernetes, ECS, or similar). \n Experience with common CI/CD methods and tools (Jenkins, Bamboo, TravisCI, etc) \n \n We are interested in every qualified candidate who is eligible to work in the continental United States. However, we are not able to sponsor or transfer visas at this time. \n \n \n  What would make you stand out: \n \n Background in geospatial data handling \n Understanding of machine learning and AI concepts \n Experience with cloud native geospatial systems \n \n \n \n  What we can offer: \n \n Medical, Dental & Vision Insurance - 80% funded by EarthOptics for benefit-eligible employees and dependents \n Company paid LTD, STD and Life Insurance \n Meaningful equity option grants \n 401k Plan + Employer Match up to 4% \n Paid Time Off \n Parental Leave \n Professional Development \n Referral Program \n Flexible Schedule & Hybrid/Remote Work Environment \n \n \n \n  EarthOptics is an Equal Opportunity Employer and prohibits Discrimination and Harassment of any kind. All employment decisions are based on business needs, job requirements and individual qualifications without regard to race, color, religion, gender or gender identity, national origin, age, marital, parental, veteran or disability status.", "cleaned_desc": " \n  Duties & Responsibilities: \n \n Hire and train new members of the data applications team. \n Assist with new employee onboarding for the data applications team. \n Perform reviews and assessments for members of the data applications team. \n Works closely with the technical product management team and SVP of engineering to ensure resources are allocated in order to be successful with our development milestones \n Work closely with the technical product management team to drive alignment between engineering tasks and product goals and set achievable milestones for delivery. \n Conducts code reviews to provide guidance on engineering best practices and compliance with development procedure and takes feedback from those reviews \n Participate in the software development lifecycle as an individual contributor doing backend, frontend or cloud infrastructure development \n Work with Principal Engineers to ensure technical alignment between engineering teams. \n Participate in and lead scrum ceremonies for the data applications team. \n Help design and implement our customer-facing web-based APIs, user management systems, and web applications for gathering and displaying data \n   \n \n  What we are looking for: \n \n Masters Degree in Computer Engineering, Computer Science, or equivalent professional experience \n A proven track record developing fast, reliable, scalable distributed web services. \n \n \n 5-8 years of software engineering experience with 3+ years of experience in leading software development teams \n \n \n Experience with applications deployed using Amazon Web Services (AWS). \n Demonstrate a working knowledge of RESTful API design, the HTTP stack and Browser limitations \n Experience with Python, Rust and/or Golang for backend development   Proficiency with Typescript \n Passion for providing software engineering best practices (e.g. unit testing, code reviews, design documentation, and continuous integration). \n Understanding of good UI/UX practices \n Familiarity with container orchestration (Kubernetes, ECS, or similar). \n Experience with common CI/CD methods and tools (Jenkins, Bamboo, TravisCI, etc) \n \n We are interested in every qualified candidate who is eligible to work in the continental United States. However, we are not able to sponsor or transfer visas at this time. \n \n \n  What would make you stand out: \n \n Background in geospatial data handling \n Understanding of machine learning and AI concepts \n Experience with cloud native geospatial systems ", "techs": ["restful api design", "http stack", "browser limitations", "python", "rust", "golang", "typescript", "amazon web services (aws)", "container orchestration (kubernetes", "ecs)", "common ci/cd methods and tools (jenkins", "bamboo", "travisci)", "geospatial data handling", "machine learning", "ai concepts", "cloud native geospatial systems"]}, "89e16492329f9d24": {"terms": ["machine learning engineer"], "salary_min": 105673.17, "salary_max": 133805.78, "title": "ServiceNow Developer", "company": "Capgemini", "desc": "Duration : 12 months \n \n  Job Description: \n \n \n \n  As a ServiceNow Developer, you will be a engineer working to develop, design and own technical solutions on the ServiceNow platform. \n  You will provide best-practices but creative application development expertise to deliver robust solutions to our internal business and technical clients utilizing the ServiceNow platform (among the ITSM, ITOM, CMDB and HRSD) \n  Overall responsibility for the successful planning, execution, monitoring, control, and closure of all technical tasks related to configuration and implementation of all aspects of the ServiceNow platform. \n  Efficient and effective throughput of delivering solutions prioritized against a roadmap and Agile release schedule. \n  Provide software coding and customization for all aspects of the platform: i.e. UI form layout and design, client/server scripting, workflows, business rules, proper platform security/administration, data design, custom webservice integrations, by working with product managers and other development team members. \n  Coordinate, document and solve complex technical data and transformation issues. \n  Obtain and analyze/interpret business requirements to create sustainable solutions in ServiceNow. \n  Answer \"how to\" technical configuration questions upon critical issues, as well as debugging. \n \n \n  The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job. \n \n  A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients\u2019 opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.", "cleaned_desc": "  Efficient and effective throughput of delivering solutions prioritized against a roadmap and Agile release schedule. \n  Provide software coding and customization for all aspects of the platform: i.e. UI form layout and design, client/server scripting, workflows, business rules, proper platform security/administration, data design, custom webservice integrations, by working with product managers and other development team members. \n  Coordinate, document and solve complex technical data and transformation issues. ", "techs": ["ui form layout and design", "client/server scripting", "workflows", "business rules", "platform security/administration", "data design", "custom webservice integrations"]}, "d6b96403e4859ecd": {"terms": ["machine learning engineer"], "salary_min": 100050.86, "salary_max": 126686.69, "title": "Senior Detection Engineer", "company": "Computer World Services", "desc": "The mission of the OFR is to support the Financial Stability Oversight Council (FSOC) in promoting financial stability by: collecting data on behalf of FSOC; providing such data to FSOC and member agencies; standardizing the types and formats of data reported and collected; performing applied research and essential long-term research; developing tools for risk measurement and monitoring; performing other related services; making the results of the activities of the OFR available to financial regulatory agencies; and assisting such member agencies in determining the types of formats of data authorized to be collected by such member agencies.  \n The Senior Detection Engineer provides services required to integrate existing security and network tools, platforms, and threat intel feeds in a modern hybrid (cloud/on-premises) environment to implement zero-trust security policies. This includes the ability to detect and respond to vulnerabilities and security incidents, alert on incidents and vulnerabilities for action, and enable active threat hunting capabilities. Identify gaps in security operations capabilities and/or visibility and provide recommendations to improve existing tools or new technology. Develop and automate highly complex detection playbooks that fully integrate and utilize the security and network tools. This role is highly technical and requires a solid understanding of offensive network security principles and how to translate these to protect and defend enterprise environments. This engineer will support the deployment of new network security technologies that will enhance the overall network security capabilities of the organization. This includes, but is not limited to tuning detection systems to minimize false positives, enhancing detection policies such as using adversarial models / TTPs (Tools, Techniques & Procedures) and threat intelligence to build the latest/greatest detections. Implement User Behavior Monitoring that leverages machine learning and artificial intelligence techniques to detect anomalous user actions and help combat advanced threats. The engineer is expected to perform mild penetration testing to test defenses, and work closely with the different technology teams to harden all systems, building deception ecosystems (breadcrumbs to trap servers), as well as identifying required data sources, and architect/engineer pipelines to get the data needed to make any/all detections. The engineer should be proficient in collecting multiple sources of network/security data under a single visibility platform with the ability to correlate events, preferably using machine learning / artificial intelligence, to identify indicators of compromise. Develop relevant dashboards, stoplight charts, monitors, metrics, and diagrams to assist operators in assessing the current security state of the enterprise. \n Key Tasks and Responsibilities \n \n Configure, set policy and tune platforms to increase detection capabilities and scope, and eliminate noise and false positives: \n \n \n EDR/IDS/IPS \n \n \n NDR/Network \n \n \n Identity Provider (IdP) authentication policies \n \n \n Email defense platforms \n \n \n Integration of threat intelligence feeds with security policy enforcement points \n \n \n SIEM and XDR detections \n \n \n Security orchestration, automation, and response (SOAR) playbook development \n \n \n Apply knowledge of monitoring, analyzing, detecting and responding to cyber events to develop clever, efficient methods and technology to detect all types of threat and to weaponize our threat hunting capabilities \n \n \n Develop strategies for network security operations, which includes detecting new threats as they emerge, including those from the most sophisticated threat actors; enhancing our detection and investigation capabilities with threat correlations and intelligence; integrating situational awareness of system intrusions; enhancing ticket data, and automate mitigation of threats \n \n \n Develop and implement strategies to secure and monitor new technologies, platforms, or services \n \n \n Develop and deploy deception technology (e.g., honeypots, honey hashes) across both enterprise and cloud environments. This includes planting breadcrumbs on endpoints/services, and then laying trap servers/services to detect their use \n \n \n Work with Technology teams to develop architecture that utilizes integrations between network, host, and security tools and intelligence feeds that fuses defenses and detections together to increase the effectiveness of policy enforcement. This includes not only endpoints and email; but networks, Identity Providers, cloud (e.g., IAM), Access/Authentication/Authorization, Zero Trust, jumphosts/bastion, firewalls, etc. \n \n \n Identify data sources required, and then architect/engineer pipelines to get the data needed to make any/all detections, preferably on a cloud-based, single platform / single pane of glass \n \n \n Perform security testing to identify security strengths and weaknesses, to assess the effectiveness of existing controls, and to recommend remedial action \n \n \n Evaluate modern, state of the art network security technologies, work with proof-of-concepts and evaluate them for adoption.  \n \n \n Train and educate engineers about new detection/response tactics and technologies \n \n \n Document specifications, playbooks and detections - not as an afterthought, but through the whole process and polished to share externally \n \n \n Work with developers to build security automation workflows, enrichments, and mitigations \n \n \n Gather, analyze, and assess the current and future threat landscape, and assist in providing leadership with a realistic overview of risks and threats in the enterprise environment \n \n \n Evaluate policies and procedures and recommend updates to management as appropriate \n \n  Job Requirements: \n \n Education & Experience \n \n Wide, deep and practical knowledge of TTPs (Tools, Techniques & Procedures) used by attackers across all varieties of attack surfaces - Authentication, Authorization, Networks, AD, Zero Trust, Firewalls, Applications, etc.) \n \n \n Solid experience with offensive TTPs, penetration testing and working with/on purple teams to harden detection capabilities \n \n \n Note we are not looking for individuals who only have experience with OWASP/application testing and network perimeter assessments; but the whole gambit of technologies and vectors, such as OS Escalations, Active Directory, Appliances, SaaS, Networks, Zero Trust, ATO, etc.) \n \n \n Mastery of security tools such as endpoint protection/EDR, SIEM, IPS/IDS, HIDS/NIDS, Networking, firewalls, WAFs, edge/endpoint security, DNS security, cryptography, layered security, defense in depth practices, vulnerability scanning, malware analysis tools, networking tool for full packet analysis, data encryption, data loss prevention, DDoS prevention, etc. \n \n \n Experience building detections for enterprise environments, which must include systems outside of a SIEM \n \n \n Deep understanding of Active Directory architecture and features; as well as the attacks and defenses (e.g., NTLM/Kerberos, escalation paths, multiple kinds of Kerberoasting, ADCS vectors, etc.) \n \n \n Experience securing and monitoring cloud-based IdPs such as Okta/Azure AD \n \n \n Scripting capabilities (e.g., bash, powershell, python) \n \n \n SOAR and playbook automation experience \n \n \n Linux/Unix OS and Windows administration knowledge \n \n \n Collaborative mindset that thrives in fast paced environments \n \n \n Excellent verbal and written communication skills including the ability to author and present materials ranging from detailed technical specifications to high-level concepts for senior audiences \n \n \n Preferred College Degree in Computer Science or related \n \n \n Must have 10+ years of related experience \n \n Certifications \n \n Preference given for CEH, CISM, GIAC, GCIH, GSLC, GICSP, GSEC, CEH, GWAP, CompTIA Net+, CompTIA A+, CompTIA Security+, CASP CE, SEC+, CISSP, CISSLP, Splunk Core, OSCP, etc. \n \n Security Clearance \n \n Public Trust \n \n \n Must be US Citizen \n \n Other (Travel, Work Environment, DoD 8570 Requirements, Administrative Notes, etc.) \n \n This is a remote/work from home role \n \n \n \n \n \n \n \n  Get job alerts by email. \n   Sign up now!  Join Our Talent Network! \n  \n \n Job Snapshot \n \n Employee Type  Full-Time \n   \n \n Location  United States of America (Remote) \n   \n \n Job Type  Engineering, Government, Information Technology \n   \n \n Experience  Not Specified \n   \n \n Date Posted  08/21/2023 \n   \n \n Job ID  4018/2854/18341", "cleaned_desc": " \n Develop strategies for network security operations, which includes detecting new threats as they emerge, including those from the most sophisticated threat actors; enhancing our detection and investigation capabilities with threat correlations and intelligence; integrating situational awareness of system intrusions; enhancing ticket data, and automate mitigation of threats \n \n \n Develop and implement strategies to secure and monitor new technologies, platforms, or services \n \n \n Develop and deploy deception technology (e.g., honeypots, honey hashes) across both enterprise and cloud environments. This includes planting breadcrumbs on endpoints/services, and then laying trap servers/services to detect their use \n \n \n Work with Technology teams to develop architecture that utilizes integrations between network, host, and security tools and intelligence feeds that fuses defenses and detections together to increase the effectiveness of policy enforcement. This includes not only endpoints and email; but networks, Identity Providers, cloud (e.g., IAM), Access/Authentication/Authorization, Zero Trust, jumphosts/bastion, firewalls, etc. \n \n \n Identify data sources required, and then architect/engineer pipelines to get the data needed to make any/all detections, preferably on a cloud-based, single platform / single pane of glass \n \n \n Perform security testing to identify security strengths and weaknesses, to assess the effectiveness of existing controls, and to recommend remedial action \n \n \n Evaluate modern, state of the art network security technologies, work with proof-of-concepts and evaluate them for adoption.  \n \n \n Train and educate engineers about new detection/response tactics and technologies \n \n \n Document specifications, playbooks and detections - not as an afterthought, but through the whole process and polished to share externally \n \n \n Work with developers to build security automation workflows, enrichments, and mitigations \n   \n Gather, analyze, and assess the current and future threat landscape, and assist in providing leadership with a realistic overview of risks and threats in the enterprise environment \n \n \n Evaluate policies and procedures and recommend updates to management as appropriate \n \n  Job Requirements: \n \n Education & Experience \n \n Wide, deep and practical knowledge of TTPs (Tools, Techniques & Procedures) used by attackers across all varieties of attack surfaces - Authentication, Authorization, Networks, AD, Zero Trust, Firewalls, Applications, etc.) \n \n \n Solid experience with offensive TTPs, penetration testing and working with/on purple teams to harden detection capabilities \n \n \n Note we are not looking for individuals who only have experience with OWASP/application testing and network perimeter assessments; but the whole gambit of technologies and vectors, such as OS Escalations, Active Directory, Appliances, SaaS, Networks, Zero Trust, ATO, etc.) \n \n \n Mastery of security tools such as endpoint protection/EDR, SIEM, IPS/IDS, HIDS/NIDS, Networking, firewalls, WAFs, edge/endpoint security, DNS security, cryptography, layered security, defense in depth practices, vulnerability scanning, malware analysis tools, networking tool for full packet analysis, data encryption, data loss prevention, DDoS prevention, etc. \n \n \n Experience building detections for enterprise environments, which must include systems outside of a SIEM \n \n \n Deep understanding of Active Directory architecture and features; as well as the attacks and defenses (e.g., NTLM/Kerberos, escalation paths, multiple kinds of Kerberoasting, ADCS vectors, etc.) \n \n \n Experience securing and monitoring cloud-based IdPs such as Okta/Azure AD \n   \n Scripting capabilities (e.g., bash, powershell, python) \n \n \n SOAR and playbook automation experience \n \n \n Linux/Unix OS and Windows administration knowledge \n \n \n Collaborative mindset that thrives in fast paced environments \n \n \n Excellent verbal and written communication skills including the ability to author and present materials ranging from detailed technical specifications to high-level concepts for senior audiences \n \n \n Preferred College Degree in Computer Science or related \n \n \n Must have 10+ years of related experience \n \n Certifications \n \n Preference given for CEH, CISM, GIAC, GCIH, GSLC, GICSP, GSEC, CEH, GWAP, CompTIA Net+, CompTIA A+, CompTIA Security+, CASP CE, SEC+, CISSP, CISSLP, Splunk Core, OSCP, etc. \n \n Security Clearance \n \n Public Trust \n \n ", "techs": ["network security operations", "threat correlations", "threat intelligence", "situational awareness", "endpoint security", "cloud environments", "deception technology", "honeypots", "honey hashes", "trap servers", "architecture integration", "network tools", "host tools", "security tools", "intelligence feeds", "policy enforcement", "data pipelines", "security testing", "modern security technologies", "proof-of-concepts", "engineer training", "documentation", "security automation", "threat landscape analysis", "policy evaluation", "ttp knowledge", "offensive ttps", "penetration testing", "purple teams", "security tools", "endpoint protection", "siem", "ips/ids", "hids/nids", "firewalls", "wafs", "dns security", "cryptography", "vulnerability scanning", "data encryption", "ddos prevention", "enterprise detections", "active directory architecture", "cloud-based idps", "scripting capabilities", "soar automation", "linux/unix administration", "windows administration", "communication skills", "computer science degree", "it certifications", "public trust security clearance"]}, "afc6aa883e7cc7a4": {"terms": ["machine learning engineer"], "salary_min": 135200.0, "salary_max": 205700.0, "title": "Sr. Product Manager, Search AI/ML", "company": "eBay", "desc": "Looking for a company that inspires passion, courage and creativity, where you can be on the team shaping the future of global commerce? Want to shape how millions of people buy, sell, connect, and share around the world? If you\u2019re interested in joining a purpose driven community that is dedicated to crafting an ambitious and inclusive work environment, join eBay \u2013 a company you can be proud to be with. \n \n  eBay\u2019s Search organization innovates at the heart of ecommerce, with an ambitious goal of redefining ecommerce search. We are looking for a top notch Senior Product Manager to join Search\u2019s Content, Item and Inventory understanding team. \n \n  The ideal candidate should have prior experience growing online commerce products and will be responsible for leading the definition, design and delivery of science heavy (e.g. NLP, embeddings etc) products and features that help deconstruct complex pieces of information provided by the sellers to craft optimized experiences for buyers using search on eBay. The candidate should be comfortable dealing with ambiguity and should have experience with defining requirements, driving cross team collaboration and exec level communication, as well as working with product managers, engineers, applied researchers, and analytics teams to define the solutions. The candidate should be adept at data driven decision making and with analyzing huge amounts of data. \n \n  Responsibilities: \n \n  Own product strategy and roadmap for Content Science - improving understanding the various facets of a listing (title, attributes, images, description etc.) \n  Data driven product prioritization with a balance of near-term deliverables and longer-term investment in technology platforms \n  Optimally communicate strategies & trade-offs up to senior leaders and down to development teams \n  Putting the customer first, understand market specific nuances and identify science algorithm improvements to address unique search specific needs \n  Driving collaboration across multiple teams to ensure teams are aligned and are working towards solving customer problems using the product built \n  Drive A/B testing to identify launch candidates that improve metrics \n \n \n  Qualifications:  \n \n 5+ years of strong track record of product management experience. Ideally have previous experience building customer centric platform products delivering values to both sides of a market place (buyers and sellers) \n  Machine Learning experience \n  A proven track record of building new, innovative, and engaging customer experiences informed by customer data from various sources (market research, A/B testing, user research) \n  A customer and Search Engine obsession with a passion for everything related to technology, and AI driven innovation to build great experiences for the customers \n  Strong quantitative skills with a data first mindset; ability to use data and metrics to back up assumptions, recommendations, and drive actions \n  Technically savvy (prefer a technical role in past, or Computer Science Degree) \n  Bachelor\u2019s Degree required; Master\u2019s Degree preferred \n \n \n  Benefits are an essential part of your total compensation for the work you do every day. Whether you\u2019re single, in a growing family, or nearing retirement, eBay offers a variety of comprehensive and competitive benefit programs to meet your needs. Including maternal & paternal leave, paid sabbatical, and plans to help ensure your financial security today and in the years ahead because we know feeling financially secure during your working years and through retirement is important. \n \n  Here at eBay, we love creating opportunities for others by connecting people from widely diverse backgrounds, perspectives, and geographies. So, being diverse and inclusive isn\u2019t just something we strive for, it is who we are, and part of what we do each and every single day. We want to ensure that as an employee, you feel eBay is a place where, no matter who you are, you feel safe, included, and that you have the opportunity to bring your unique self to work. To learn about eBay\u2019s Diversity & Inclusion click here: https://www.ebayinc.com/company/diversity-inclusion/ \n  #LI-ML2 \n \n  The pay range for this position at commencement of employment in California, Washington, or New York is expected in the range below.  $135,200 - $205,700\n   Base pay offered may vary depending on multiple individualized factors, including location, skills, and experience. The total compensation package for this position may also include other elements, including a target bonus and restricted stock units (as applicable) in addition to a full range of medical, financial, and/or other benefits (including 401(k) eligibility and various paid time off benefits, such as PTO and parental leave). Details of participation in these benefit plans will be provided if an employee receives an offer of employment. \n  If hired, employees will be in an \u201cat-will position\u201d and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors. \n \n  eBay Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, veteran status, and disability, or other legally protected status. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talent@ebay.com. We will make every effort to respond to your request for disability assistance as soon as possible. View our accessibility info to learn more about eBay's commitment to ensuring digital accessibility for people with disabilities. For more information see: EEO is the Law Poster and EEO is the Law Poster Supplement. \n \n  Jobs posted with location as \"Remote - United States (Excludes: HI, NM)\" excludes residents of Hawaii and New Mexico.", "cleaned_desc": "", "techs": ""}, "1c4861b7ab3a6d9c": {"terms": ["machine learning engineer"], "salary_min": 131690.95, "salary_max": 166750.08, "title": "Senior Data Scientist", "company": "Jerry", "desc": "We'd love to hear from you if you like: \n \n  Making a big impact with a Forbes Top Startup Employer \n  Working on products that have traction (40X revenue growth in 4 years | #1 rated app in the insurance comparison category) \n  Solving problems in a huge market ($2T market size) \n  Working closely with serial entrepreneurs and seasoned leaders who have scaled companies like Robinhood, Amazon, LinkedIn, Wayfair, SoFi, Microsoft, etc. \n \n \n  About the opportunity: \n  We are looking for a Senior Data Scientist to join our team! Helping everyday, hard working Americans save time and money on their cars and creating a world class experience is what drives every decision we make as a company. Since launching our mobile app in 2019, we have amassed over 4M customers, expanded our product offerings to multiple categories and scaled our team 10X. Our data team fuels all of our business and product decisions through delivering analytical insights and building advanced models. \n \n  Reporting to our VP of Business Operations and Analytics, you will leverage data to drive growth and retention for our core insurance product. You will perform analytical deep dives, develop and analyze experiments, build predictive models, and make recommendations that inform our product roadmap. Working with a brilliant team of product managers, product designers, software engineers, and key business leaders, you will play a big role in accelerating our growth and taking our customer experience to the next level. \n \n  How you will make an impact: \n \n  Partner closely with our product managers, software engineers, product designers, and key business leaders to drive user growth and retention for our core insurance product  \n Design, run, and analyze A/B experiments on new and existing features; extract key insights, share learnings and make recommendations on next steps \n  Build key reports, dashboards, and predictive models to monitor the performance of our insurance business, and communicate analytical outcomes to our teams \n  Transform and refine raw production data for analytical needs \n  Continually improve our data governance and data consistency standards within our database \n  Work with data engineering team on data tracking, integrity, and security as needed \n  Work with other data scientists to evolve, optimize and integrate machine learning models \n \n \n  Who you are: \n \n  Intellectually curious: You're not satisfied with surface level insights. You dive deep to understand how systems work, why people behave in certain ways and are intrinsically motivated to uncover root causes for issues or underlying reasons behind decisions. \n  Data-driven: You're extremely analytical and live in data. At the same time, you're confident enough to make decisions when the data is limited. \n  Creative problem-solver: No challenge is too complex, no issue is too hard, and \u201cno\u201d is not an acceptable answer. \n  Strong communicator: Able to drive alignment and communicate effectively to different audiences. \n \n \n  Ideal profile: \n \n  Bachelor\u2019s degree in Mathematics, Statistics, Economics, Computer Science or a related quantitative discipline \n  3+ years experience as a data scientist or product analyst in a consumer-facing web or mobile app environment \n  Experience designing and implementing A/B tests, and analyzing user experience \n  Hands-on experience with SQL (advanced proficiency) \n \n \n  Jerry is proud to be an Equal Employment Opportunity employer. We prohibit discrimination based on race, religion, color, national origin, sex, pregnancy, reproductive health decisions or related medical conditions, sexual orientation, gender identity, gender expression, age, veteran status, disability, genetic information, or other characteristics protected by applicable local, state or federal laws.  \n \n Jerry is committed to providing reasonable accommodations for individuals with disabilities in our job application process. If you need assistance or an accommodation due to a disability, please contact us at  recruiting@getjerry.com \n \n  About Jerry: \n  Jerry is America\u2019s first and only AllCar app. We are redefining and radically improving how people manage owning a car, one of their most expensive and time-consuming assets. \n \n  Backed by artificial intelligence and machine learning, Jerry simplifies and automates owning and maintaining a car while providing personalized services for all car owners' needs. We spend every day innovating and improving our AI-powered app to provide the best possible experience for our customers. From car insurance and financing to maintenance and safety, Jerry does it all. \n \n  We are the #1 rated and most downloaded app in our category with a 4.7 star rating in the App Store. We have more than 4 million customers \u2014 and we\u2019re just getting started. \n \n  Jerry was founded in 2017 by serial entrepreneurs and has raised more than $242 million in financing. \n \n  Join our team and work with passionate, curious and egoless people who love solving real-world problems. Help us build a revolutionary product that\u2019s disrupting a massive market.", "cleaned_desc": " \n \n  Ideal profile: \n \n  Bachelor\u2019s degree in Mathematics, Statistics, Economics, Computer Science or a related quantitative discipline \n  3+ years experience as a data scientist or product analyst in a consumer-facing web or mobile app environment \n  Experience designing and implementing A/B tests, and analyzing user experience \n  Hands-on experience with SQL (advanced proficiency) \n \n ", "techs": ["sql"]}, "c19ab8ab8df86379": {"terms": ["machine learning engineer"], "salary_min": 135222.66, "salary_max": 171222.0, "title": "Senior Software Engineer, Perception", "company": "Stack AV", "desc": "About \n  With customers at its core, Stack AV is focused on revolutionizing the way businesses transport goods, designing solutions to alleviate long-standing issues that have plagued the trucking industry including driver shortages, lagging efficiency in uptime per vehicle, overarching safety concerns, high operating costs, and elevated emission levels. By building safe and efficient autonomous trucking solutions, Stack AV is creating better and smarter supply chains for its partners, improving business outcomes for its customers, delivering goods to end-users faster, and ultimately moving the trucking industry forward. \n \n  What We're Looking for: \n  We are looking for people who are passionate about delivering self-driving (L4) products that make the way we move safer, faster, and more efficient. We seek mission-driven, highly skilled people with deep experience in fast-paced, rapidly growing, tech development environments. We are looking for strong Computer Vision/Machine Learning engineers to develop, train, and deploy a state-of-the-art perception pipeline for our self-driving vehicles. We seek engineers with strong foundations coupled with a practical, product-focused mindset to produce a highly performant, real-time, safety-critical application. \n  What Success Looks Like: \n \n Work in a team to design and develop state-of-the-art perception systems for self-driving vehicle systems. \n Contribute to the technical direction of the team, and work cross-functionally to develop safe systems. \n Write software for perception models for real-time, resource-constrained applications. \n Develop and evaluate multimodal perception approaches for pushing performance and improving robustness. \n Identify bottlenecks and limitations in system performance, and develop novel perception components to unlock new perception capabilities and reliability. \n Drive experimentation, design, and iteration exercises, and align stakeholders with strong presentation and communication skills. \n Experience with architecting, training, and deploying Deep Learning Models. \n Experience with building perception systems using lidar, camera and/or radar data. \n Knowledge and experience of advanced probabilistic algorithms, real-time algorithms, and machine learning approaches. \n Experience with approaches to one or more perception problems, such as multi-object detection, panoptic segmentation, temporal detection, and tracking. \n Strong experience in software engineering and algorithm design. \n Fluent in Python. \n Experience with C++ or CUDA a plus. \n \n \n \n  We are proud to be an equal-opportunity workplace. We believe that diverse teams produce the best ideas and outcomes. We are committed to building a culture of inclusion, entrepreneurship, and innovation across gender, race, age, sexual orientation, religion, disability, and identity. \n  Please Note:  Pursuant to its business activities and use of technology, Stack AV complies with all applicable U.S. national security laws, regulations, and administrative requirements, which can restrict Stack AV's ability to employ certain persons in certain positions pursuant to a range of national security-related requirements. As such, this position may be contingent upon Stack AV verifying a candidate's residence, U.S. person status, and/or citizenship status. This position may also involve working with software and technologies subject to U.S. export control regulations. Under these regulations, it may be necessary for Stack AV to obtain a U.S. government export license prior to releasing its technologies to certain persons. If Stack AV determines that a candidate's residence, U.S. person status, and/or citizenship status will require a license, prohibit the candidate from working in this position, or otherwise be subject to national security-related restrictions, Stack AV expressly reserves the right to either consider the candidate for a different position that is not subject to such restrictions, on whatever terms and conditions Stack AV shall establish in its sole discretion, or, in the alternative, decline to move forward with the candidate's application.", "cleaned_desc": " Drive experimentation, design, and iteration exercises, and align stakeholders with strong presentation and communication skills. \n Experience with architecting, training, and deploying Deep Learning Models. \n Experience with building perception systems using lidar, camera and/or radar data. \n Knowledge and experience of advanced probabilistic algorithms, real-time algorithms, and machine learning approaches.   Experience with approaches to one or more perception problems, such as multi-object detection, panoptic segmentation, temporal detection, and tracking. \n Strong experience in software engineering and algorithm design. \n Fluent in Python. \n Experience with C++ or CUDA a plus. ", "techs": ["drive experimentation", "design", "and iteration exercises", "align stakeholders", "architect", "train", "deploy deep learning models", "build perception systems", "use lidar", "camera", "radar data", "advanced probabilistic algorithms", "real-time algorithms", "machine learning approaches", "multi-object detection", "panoptic segmentation", "temporal detection", "tracking", "software engineering", "algorithm design", "python", "c++", "cuda."]}, "879944eea9412250": {"terms": ["machine learning engineer"], "salary_min": 90000.0, "salary_max": 120000.0, "title": "Advanced Manufacturing Engineer- CNC Machining & Grinding", "company": "CW Bearing", "desc": "Our Success Formula: Global Network \u2013 Local Service. \n \n \n \n  CW Bearing USA, Inc. is a medium-sized manufacturing company poised for growth over the next few years. We are a globally operational, privately-owned company based in Ningbo, on China's east coast. The company was founded in 1984 by Hu Xiangen.\n  \n \n \n  We specialize in the development and production of a wide range of premium quality ball bearings for electrical motors, gearboxes, power tools and the automotive industry. We also specialize in ball nuts and ball screws for automotive and industrial customers.\n  \n \n \n  CIXING GROUP CO. LTD. is one of the 10 largest producers of ball bearings in China. We possess a worldwide sales and distribution network and have offices in Asia, Europe, North America and Latin America.\n  \n \n \n  Our ultra-modern production facilities enable us to serve both global customers and mid-sized companies. Regardless of the size of the delivery \u2013 small lots or serial production \u2013 we never lose sight of our customers' requirements. The establishment of good working partnerships is our highest priority.\n  \n \n \n We offer competitive wages, excellent benefits, 401k with annual employer match, flexible schedules (for applicable positions) clean temperature-controlled manufacturing facility, and much more! \n \n \n \n  We are currently seeking:\n  \n \n \n \n \n Advanced Manufacturing Engineer \u2013 CNC Machining & Grinding \n \n \n Reports to Engineering Manager \n \n \n Classification: Salaried Exempt \n \n \n Work From Home Status: Does Not Apply \n \n \n \n  Job Summary:\n  \n \n  The Manufacturing Engineer will develop turning & milling processes by studying product requirements; researching, designing, modifying, and testing manufacturing methods and equipment; conferring with equipment vendors.\n  \n \n  Supervisory Responsibilities:\n  \n \n  None\n  \n \n \n  Duties & Responsibilities:\n  \n \n Develops grinding, machining & assembly processes by studying product requirements; researching, designing, modifying, and testing manufacturing methods and equipment; conferring with equipment vendors. \n Develops and maintains CNC programs for grinding, turning/milling & honing/super finishing machines. \n Help senior engineers through complete APQP process, installs and initial start-up of equipment. \n Creates and maintains process routings, process drawings, CNC programs, and tooling lists. \n Improves manufacturing efficiency by analyzing and planning workflow, space requirements, and equipment layout. \n Improves grinding efficiency and cycle time by redesigning tooling/fixtures, develop new grinding wheels and dressing wheels by researching, testing various feed rates/depth of cuts, and taking direction from senior engineers. \n Develop plant layouts, work instructions, process flow layout, set-up procedures while adhering to safety, quality and productivity procedures and policies. \n Handle automation with robotic materials and parts. \n Development and management of process FMEA's and other process documentation. Troubleshooting of processes and equipment. \n Develop, monitor, and report production measurables and productivity incentive plans. Support senior engineers and or lead refurbishing and repurposing of equipment. \n Use Solidworks to create process models/process drawings. \n Create ballooned drawings & inspection standards for part manufacturing. \n Develop standard procedures to improve manufacturing processes and to reduce overall cost. \n Communicate with customers & suppliers to understand and determine product specifications. \n Assures product and process quality by designing testing methods; testing finished product and process capabilities; establishing standards; confirming manufacturing processes. \n Work with product development team to manufacture prototypes and work with product engineers to assure manufacturability. \n Ensure the consideration of internal process standards and lessons learned, make sure that experiences and lessons learned are communicated back to the organization. \n Completes design and development projects by training and guiding technicians. \n Complies to All safety regulations outlined by the company, customer, and government regulations. \n Communicates and interacts with all departments to achieve company goals and initiatives. \n Travel 5-10% \n \n \n \n  Required Skills & Abilities:\n  \n \n Knowledge of Six Sigma Concepts preferred \n Thorough understanding of setting up processes and BOM's \n Familiarity with automotive specifications and GD&T \n Ability to communicate with all levels of personnel including customers & suppliers \n Experience in bearings, half-shaft components and power steering components manufacturing a plus \n Experience working in a mass production environment with tight tolerances. \n Hands on experience using Solid works, AutoCAD, or Inventor \n Bilingual: English & Chinese (a plus) \n \n \n \n  Education & Experience:\n  \n \n Bachelor's Degree in Manufacturing Engineering, Industrial Engineering, Production Engineering, or a related field preferred \n 2+ years of experience in centerless grinding, automated assembly processes, and CNC machining processes \n 3+ years of work experience in automotive manufacturing environment (excluding internships). \n 5+ years experience with APQP, SPC, FMEA, & 8D Problem Solving \n 5+ years experience planning, creating and interpreting timelines \n 5-10 years experience process design and development including materials, drawings, industry specifications, equipment, tooling, gaging and testing \n 5-10 years experience product and process validation including industry standard requirements \n \n \n \n  Physical Requirements\n  \n \n  Positions at CW may require the following: alternating between sitting and standing; climbing stairs, ladders, scaffolding or ramps; crouching/stooping; driving; near/far visual acuity; fine motor manipulation; gross motor manipulation; hearing; keyboarding; kneeling; lifting/carrying; moving objects; peripheral visual acuity; pushing or pulling; reaching overhead or below; repetitive task performance; sitting; speaking; standing; using foot or leg controls; walking.\n  \n \n \n  Environmental Requirements\n  \n \n  Our manufacturing facility is a climate-controlled environment. There is no long-term exposure to dangerously loud noise or extreme temperatures.\n  \n \n \n  Cognitive Requirements\n  \n \n  Cognitive abilities include executing tasks independently; learning and/or memorizing tasks; maintaining concentration/focus on tasks; working with/in close proximity to, other people.\n  \n \n \n  Attendance Requirements\n  \n \n  All hourly team members are expected to report to work and be ready to work at their scheduled start time. It is required that they adhere to and follow the attendance policy(ies).\n  \n \n \n  All salaried team members are expected to perform their duties/responsibilities in a timely manner. Although they may not have set schedules, they are expected to either be at work on a schedule agreed upon by their manager or be available via phone/text/Teams if working remotely (Remote work does not apply to CWM positions).\n  \n \n \n All CW entities are equal opportunity employers. \n \n \n \n  Employee Referral Eligible - Level 2\n  \n  PI229826154", "cleaned_desc": " Thorough understanding of setting up processes and BOM's \n Familiarity with automotive specifications and GD&T \n Ability to communicate with all levels of personnel including customers & suppliers \n Experience in bearings, half-shaft components and power steering components manufacturing a plus \n Experience working in a mass production environment with tight tolerances. \n Hands on experience using Solid works, AutoCAD, or Inventor \n Bilingual: English & Chinese (a plus) \n \n \n \n  Education & Experience:\n  \n \n Bachelor's Degree in Manufacturing Engineering, Industrial Engineering, Production Engineering, or a related field preferred \n 2+ years of experience in centerless grinding, automated assembly processes, and CNC machining processes \n 3+ years of work experience in automotive manufacturing environment (excluding internships). \n 5+ years experience with APQP, SPC, FMEA, & 8D Problem Solving \n 5+ years experience planning, creating and interpreting timelines \n 5-10 years experience process design and development including materials, drawings, industry specifications, equipment, tooling, gaging and testing \n 5-10 years experience product and process validation including industry standard requirements \n \n \n \n  Physical Requirements\n  \n \n  Positions at CW may require the following: alternating between sitting and standing; climbing stairs, ladders, scaffolding or ramps; crouching/stooping; driving; near/far visual acuity; fine motor manipulation; gross motor manipulation; hearing; keyboarding; kneeling; lifting/carrying; moving objects; peripheral visual acuity; pushing or pulling; reaching overhead or below; repetitive task performance; sitting; speaking; standing; using foot or leg controls; walking.\n  \n ", "techs": ["solid works", "autocad", "inventor"]}, "4d49f1837523cf91": {"terms": ["machine learning engineer"], "salary_min": 122495.32, "salary_max": 155106.38, "title": "Infrastructure Automation Engineer", "company": "Metropolis", "desc": "The Company \n  Metropolis develops advanced computer vision and machine learning technology that makes mobile commerce remarkable. Our platform is already deployed in hundreds of mobility facilities and industries with billions in opportunity. We're building the digital pipes through which the future of mobile commerce will move. \n \n \n  The Role \n  We are seeking an Infrastructure Automation Engineer to will help shape the direction of Technical Operations by introducing automation practices to scale our server and networking infrastructure, while ensuring our environment works optimally in even the most extreme conditions. We are looking for an individual with an engineering mindset, a decisive nature, and a preference for simple solutions to keeping our systems reliable. \n \n \n  Responsibilities \n \n Drive forward the adoption of infrastructure as code across all technology teams   \n Design, develop, and maintain automation frameworks and scripts to support scalable and distributed hardware provisioning, network configuration and software deployments.   \n Collaborate with hardware and ML teams to identify opportunities for automation and drive the implementation of automated processes.   \n Implement configuration management tools such as Ansible to automate provisioning, configuration, and deployment of infrastructure and applications.   \n Work with and mentor technology teams on best practices related to building and maintaining Infrastructure as Code tools   \n Troubleshoot and resolve issues related to automation tools, scripts, and infrastructure configurations.   \n Ensure security best practices are integrated into automation solutions, including compliance with company policies and industry standards.   \n Monitor and optimize automated processes, identifying areas for improvement and implementing enhancements.   \n Participate in code reviews, provide constructive feedback, and adhere to established coding standards.   \n Stay up-to-date with industry trends and emerging technologies in automation, DevOps, and cloud computing \n \n \n   \n Requirements \n \n  You have a passion for automation! \n   Proven experience as an Automation Engineer or similar role.   \n Proficiency in automation tools like Ansible, Puppet, Chef, Salt, or similar technologies.   \n Proficiency in networking concepts   \n Strong scripting skills with languages such as Python or Bash   \n Experience with version control systems.   \n Familiarity with cloud platforms such as AWS.   \n Knowledge of containerization technologies like Docker and orchestration with Kubernetes is a plus.   \n Experience automating across Linux and Networking infrastructure   \n Understanding of CI/CD principles and tools like Jenkins, GitLab CI/CD, or CircleCI.   \n Excellent problem-solving skills and attention to detail.   \n Effective communication and teamwork abilities.   \n You are not afraid to challenge consensus.   \n You seek out opportunities to leave your comfort zone and enjoy frequent change.   \n Seattle preferred; open to other office locations (NY, LA) or remote for exceptional candidates", "cleaned_desc": " Proficiency in networking concepts   \n Strong scripting skills with languages such as Python or Bash   \n Experience with version control systems.   \n Familiarity with cloud platforms such as AWS.   \n Knowledge of containerization technologies like Docker and orchestration with Kubernetes is a plus.   \n Experience automating across Linux and Networking infrastructure   \n Understanding of CI/CD principles and tools like Jenkins, GitLab CI/CD, or CircleCI.   ", "techs": ["networking concepts", "python", "bash", "version control systems", "aws", "docker", "kubernetes", "linux", "jenkins", "gitlab ci/cd", "circleci"]}, "f34a31a3067c82c2": {"terms": ["mlops"], "salary_min": 112000.0, "salary_max": 150000.0, "title": "REMOTE - AWS DevOps Engineer", "company": "First American Financial Corporation", "desc": "Who We Are \n \n  Come join First American's Digital Title Group, newly formed to re-imagine and digitize the title search and examination process through Big Data, AI, document automation and modern, cloud-native application development. As a market leading title insurance company, powered by the nation's largest and most complete property information, ownership and recorded document database, First American is committed to advancing title automation and removing friction from the real estate closing process. Our modern title decisioning solutions create certainty and speed through data and analytics, delivered to real estate agents, lenders, title agents and homebuyers. Join a team that puts its People First! Since 1889, First American (NYSE: FAF) has held an unwavering belief in its people. They are passionate about what they do, and we are equally passionate about fostering an environment where all feel welcome, supported, and empowered to be innovative and reach their full potential. Our inclusive, people-first culture has earned our company numerous accolades, including being named to the Fortune 100 Best Companies to Work For\u00ae list for eight consecutive years. We have also earned awards as a best place to work for women, diversity and LGBTQ+ employees, and have been included on more than 50 regional best places to work lists. First American will always strive to be a great place to work, for all. For more information, please visit www.careers.firstam.com. \n \n  What We Do \n \n  Our modern title decisioning solutions create certainty and speed through data and analytics, delivered to real estate agents, lenders, title agents and homebuyers. We're looking for AWS DevOps Engineers interested in transforming our industry by solving cutting-edge problems with modern technologies, want the benefit of working for an established real estate insurance leader and seek a culture awarded as a Fortune 100 Best Companies to Work! \n \n  What you'll do: \n \n  Work collaboratively with solution architects and engineering teams to define infrastructure and deployment requirements \n  Create, maintain, and improve CI/CD build and release pipelines in AWS environment \n  Build and deploy automation, monitoring, and analysis solutions \n  Provision, configure and maintain AWS cloud infrastructure defined as code \n  Troubleshoot problems across a wide array of services and functional areas \n \n \n  Knowledge and Skills \n \n  Experience with Docker, git, and deploying applications in AWS using Terraform \n  Willingness and ability to learn/use a wide variety of open source technologies and tools \n  Experience with one or more AWS SDKs and/or CLI \n  Experience using bash and python to produce automation scripts and generate reports \n  Experience with IAM roles/policies/permissions and security groups \n  Serverless orchestration, especially Lambdas \n  Solid foundation of networking and Linux administration \n \n \n  Education \n \n  Bachelor's degree in Computer Science (or related field) or equivalent combination of education and experience \n \n \n  Typical Range of Experience \n \n  2+ years of directly related experience \n \n    **Open to remote candidates**    Pay Range: $112,000 - $150,000 annually \n \n  This hiring range is a good faith and reasonable estimate of the salary range of possible compensation at the time of the posting, and is subject to change. The actual compensation offered will be determined by various factors, which may include a candidate\u2019s education, training, experience, and geographic location. \n \n  #LI-EF1  #LI-REMOTE \n \n  What We Offer \n \n  By choice, we don\u2019t simply accept individuality \u2013 we embrace it, we support it, and we thrive on it! Our People First Culture celebrates diversity, equity and inclusion not simply because it\u2019s the right thing to do, but also because it\u2019s the key to our success. We are proud to foster an authentic and inclusive workplace For All. You are free and encouraged to bring your entire, unique self to work. First American is an equal opportunity employer in every sense of the term. \n \n  Based on eligibility, First American offers a comprehensive benefits package including medical, dental, vision, 401k, PTO/paid sick leave and other great benefits like an employee stock purchase plan.", "cleaned_desc": " \n  Work collaboratively with solution architects and engineering teams to define infrastructure and deployment requirements \n  Create, maintain, and improve CI/CD build and release pipelines in AWS environment \n  Build and deploy automation, monitoring, and analysis solutions \n  Provision, configure and maintain AWS cloud infrastructure defined as code \n  Troubleshoot problems across a wide array of services and functional areas \n \n \n  Knowledge and Skills   \n  Experience with Docker, git, and deploying applications in AWS using Terraform \n  Willingness and ability to learn/use a wide variety of open source technologies and tools \n  Experience with one or more AWS SDKs and/or CLI \n  Experience using bash and python to produce automation scripts and generate reports \n  Experience with IAM roles/policies/permissions and security groups \n  Serverless orchestration, especially Lambdas \n  Solid foundation of networking and Linux administration \n ", "techs": ["docker", "git", "terraform", "aws sdks", "cli", "bash", "python", "iam roles/policies/permissions", "security groups", "lambdas"]}, "4d0c17d39834faec": {"terms": ["mlops"], "salary_min": 100843.49, "salary_max": 127690.336, "title": "DevOps Engineer (Remote)", "company": "KLDiscovery", "desc": "KLDiscovery, a leading global provider of electronic discovery, information governance and data recovery services, is currently seeking a Development Operations Engineer for an exciting new opportunity.  \n Come help us develop the future of our infrastructure! We are rapidly growing our team to transition out of infrastructure management systems into fully automated pipelines! This team will be designing and implementing a process to build, deploy, run, and support large-scale eDiscovery applications in a wide variety of infrastructure models.  \n Remote, work from home opportunity.  \n Responsibilities  \n \n Develop automated processes for the application build pipeline  \n Develop automated processes for the application release/deploy pipeline \n    \n As-needed deployments to development environments  \n Scheduled deployments to internal validation environments  \n Deployments to all production environments  \n \n Develop automated processes and procedures for initial environment configuration, including infrastructure automation  \n Utilize, communicate, and enforce coding standards  \n Seek and participate in personal development opportunities to maintain a detailed knowledge of industry standards, engineering best practices, and business needs  \n Provide Technical Support - bug fixes and implement enhancements to applications in Production within defined SLA  \n Work in a team or with multiple organizational departments in a dynamic and rapidly changing environment  \n Adhere to development processes and workflows  \n \n Qualifications  \n \n BS in Computer Science, Engineering, or related scientific field preferred  \n 3 + years of experience in software development or QA experience  \n Experience working in both Windows and Linux environments  \n Experience with continuous integration platforms preferred  \n Experience with the following tools and technologies is preferred but not required: \n    \n Azure Cloud  \n Terraform  \n Ansible  \n Azure DevOps  \n SonarQube  \n BurpSuite  \n Git  \n Docker  \n \n Experience with version control systems and work management systems such as Azure DevOps or other tools preferred  \n Capable of problem identification and resolutions  \n Ability to learn from others and adapt to standards  \n Ability to express complex technical concepts effectively, both verbally and in writing  \n Understands various programming languages including C#, SQL, etc.  \n Knowledge of client/server and internet systems architectures  \n Ability to work in a team or with multiple organizational departments in a dynamic and rapidly changing environment  \n Ability to pass a background check upon offer  \n \n This position does not qualify for VISA sponsorship  \n This position operates under International Traffic in Arms Regulations (ITAR) and therefore, any person hired must demonstrate with verifiable documentation that he/she is either (i) a U.S. Citizen; (ii) ac active Green Card Holder; pr (iii) a \"Protected Person\" as defined by 8 U.S.C. 1324(b)(a)(3).  \n Why You will Love Working for KLD  \n \n Generous paid time off, that offers various time off options to help employees maintain a work-life balance, such as vacation, paid sick leave, parental leave, paid jury leave and more!  \n Comprehensive health, dental, vision and supplemental benefits package that includes life insurance, short- and long-term disability, to promote the health of our employees.  \n Remote-friendly, flexible working culture, where you can apply to work from a number of global locations.  \n A focus on continuous professional development through various training and education reimbursement programs.  \n A diverse and inclusive workplace where we all learn, grow, and achieve the greatest heights\u2026together.  \n A surrounding team of mission-driven individuals who genuinely love what they do.  \n Equity incentives and company bonus programs; that way, we all share in the success of KLDiscovery.  \n Free, fun, interactive and incentivized global wellness program that promotes the wellbeing of our employees plus offers a wide range of perks and discounts!  \n FREE Employee Assistance Program (EAP) because we all could use a little help and support every now and then.  \n 401(k) with employer match to help our employees achieve financial success.  \n KLD supports the communities where our employees live and offers a paid community service day for employees to volunteer with what resonates with them.  \n To keep our furry, 4-legged family members healthy, KLD employees can opt in for Pet Insurance.  \n \n Our Cultural Values  \n Entrepreneurs at heart, we are a customer first team sharing one goal and one vision. We seek team members who are:  \n \n Humble - No one is above another; we all work together to meet our clients\u2019 needs and we acknowledge our own weaknesses  \n Hungry - We all are driven internally to be successful and to continually expand our contribution and impact  \n Smart - We use emotional intelligence when working with one another and with clients  \n \n Our culture shapes our actions, our products, and the relationships we forge with our customers.  \n Who We Are  \n KLDiscovery provides technology-enabled services and software to help law firms, corporations, government agencies and consumers solve complex data challenges. The company, with offices in 25 locations across 16 countries, is a global leader in delivering best-in-class eDiscovery, information governance and data recovery solutions to support the litigation, regulatory compliance, internal investigation and data recovery and management needs of our clients.  \n Serving clients for over 30 years, KLDiscovery offers data collection and forensic investigation, early case assessment, electronic discovery and data processing, application software and data hosting for web-based document reviews, and managed document review services. In addition, through its global Ontrack Data Recovery business, KLDiscovery delivers world-class data recovery, email extraction and restoration, data destruction and tape management.  \n KLDiscovery has been recognized as one of the fastest growing companies in North America by both Inc. Magazine (Inc. 5000) and Deloitte (Deloitte\u2019s Technology Fast 500) and CEO Chris Weiler has been honored as a past Ernst & Young Entrepreneur of the Year\u2122. Additionally, KLDiscovery is an Orange-level Relativity Best in Service Partner, a Relativity Premium Hosting Partner and maintains ISO/IEC 27001 Certified data centers.  \n KLDiscovery is an Equal Opportunity Employer.  \n Texas PI# A04094801  \n #LI-AK1  \n #LI-Remote", "cleaned_desc": " Provide Technical Support - bug fixes and implement enhancements to applications in Production within defined SLA  \n Work in a team or with multiple organizational departments in a dynamic and rapidly changing environment  \n Adhere to development processes and workflows  \n \n Qualifications  \n \n BS in Computer Science, Engineering, or related scientific field preferred  \n 3 + years of experience in software development or QA experience  \n Experience working in both Windows and Linux environments  \n Experience with continuous integration platforms preferred  \n Experience with the following tools and technologies is preferred but not required: \n    \n Azure Cloud  \n Terraform  \n Ansible    Azure DevOps  \n SonarQube  \n BurpSuite  \n Git  \n Docker  \n \n Experience with version control systems and work management systems such as Azure DevOps or other tools preferred  \n Capable of problem identification and resolutions  \n Ability to learn from others and adapt to standards  \n Ability to express complex technical concepts effectively, both verbally and in writing  \n Understands various programming languages including C#, SQL, etc.  \n Knowledge of client/server and internet systems architectures  \n Ability to work in a team or with multiple organizational departments in a dynamic and rapidly changing environment  \n Ability to pass a background check upon offer  \n ", "techs": ["azure cloud", "terraform", "ansible", "azure devops", "sonarqube", "burpsuite", "git", "docker", "c#", "sql"]}, "76550118e6e34b90": {"terms": ["mlops"], "salary_min": 117402.33, "salary_max": 148657.52, "title": "DevOps Delivery Lead", "company": "Integra Connect", "desc": "Company Summary: \n We are a value-based, precision medicine company specializing in solutions for providers, patients, life science and EMS companies. With the emergence of value-based care, Integra Connect\u2019s mission is to help specialty care providers succeed both clinically and financially. We accomplish this through a comprehensive offering of technology and services, unified by our cloud-based platform, combined with unmatched industry expertise. We are looking for like-minded individuals committed to making a difference in healthcare. Come join our growing team! \n Position Summary: \n The DevOps Delivery Lead is responsible for driving successful outcomes of all DevOps-related projects and operational work. They should be very familiar with DevOps best practices, following agile development practices, ensuring that Jira tickets and stories have adequate detail and requirements, facilitating meetings, communicating the status of projects, and managing ticket queues, all while focusing on resource management and allocation. Lead would be acting as a two-way conduit between stakeholders and DevOps team while balancing the team\u2019s workload between operational support and projects. \n Responsibilities: \n \n Manage Jira tickets and stories, ensure they are adequately detailed with requirements, and organize and distribute work to DevOps team resources based on skillsets and available capacity \n Ensure that Jira, Confluence, and Aha are properly linked and kept up to date. \n Work across business and functional teams to understand macro level business priorities and blockers to properly set expectations with stakeholders and prioritize DevOps work appropriately. \n Ensure that project and operational work is properly scoped and communicated timelines are agreed upon, adhered to, and met. \n Act as an escalation point for stakeholders requesting DevOps status updates on open items. Track and communicate blockers or risks to leadership and stakeholders in a timely manner. \n Analyze and track DevOps team resources, throughput, and performance and suggest ways to improve team delivery. \n \n Qualifications: \n \n 5+ years experience in a Delivery Lead role. \n 4-year degree in computer science or related field, or equivalent experience. \n 5+ years experience with Agile methodology. \n 3+ years experience with DevOps best practices. \n Mastery in written/verbal skills along with task, project, and program management. \n Strong understanding of contemporary engineering processes (e.g. agile, scrum, Kanban). \n Strong conceptual understanding of DevOps CI/CD, Infrastructure as Code, Testing, Cloud Orchestration, and Management. \n Mastery of JIRA, and Confluence is required; experience in Aha! or another road-mapping tool is a plus. \n \n Benefits:  Integra Connect, LLC provides a comprehensive benefits plan. \n \n Medical/Dental/Vision Insurance beginning the 1st of the month following your date of hire \n Paid Time Off \n 401k with employer match \n Paid Holidays and Floating Holiday \n \n Job Type: Full-time \n Benefits: \n \n 401(k) \n Dental insurance \n Health insurance \n Paid time off \n Retirement plan \n Vision insurance \n \n Schedule: \n \n Monday to Friday \n \n Application Question(s): \n \n Will you now, or in the future, require sponsorship for employment visa status (e.g. H-1B visa status)? \n \n Experience: \n \n Azure: 1 year (Preferred) \n AWS: 1 year (Preferred) \n Kubernetes: 1 year (Preferred) \n \n Work Location: Remote", "cleaned_desc": " Analyze and track DevOps team resources, throughput, and performance and suggest ways to improve team delivery. \n \n Qualifications: \n \n 5+ years experience in a Delivery Lead role. \n 4-year degree in computer science or related field, or equivalent experience. \n 5+ years experience with Agile methodology. \n 3+ years experience with DevOps best practices. \n Mastery in written/verbal skills along with task, project, and program management. \n Strong understanding of contemporary engineering processes (e.g. agile, scrum, Kanban). \n Strong conceptual understanding of DevOps CI/CD, Infrastructure as Code, Testing, Cloud Orchestration, and Management. ", "techs": ["devops", "agile methodology", "devops best practices", "engineering processes", "ci/cd", "infrastructure as code", "testing", "cloud orchestration", "management"]}, "5039d3a5e00a515b": {"terms": ["mlops"], "salary_min": 97500.0, "salary_max": 176250.0, "title": "DevOps Engineer / DHMSM Virtual Health", "company": "Leidos", "desc": "Description   \n The Leidos Partnership for Defense Health is seeking a  DevOps Infrastructure Engineer  to work remotely and support its $4.6B single award DHMSM IDIQ contract, known as MHS GENESIS. This highly visible program provides modernization of Electronic Health Records (EHR) capabilities for the Department of Defense, supporting nearly 10 million service members and their families worldwide. While every healthcare system is addressing quality, user experience and cost, the MHS GENESIS program has the added challenge of delivering these objectives within the DOD security environment, at the DOD\u2019s global scale, all while supporting the Defense Health Agency (DHA)\u2019s vision of a medically ready force which is essential to our national defense. \n \n  WHAT YOU WILL BE DOING \n \n  The  DevOps Infrastructure Engineer  will join the Virtual Health team as a key team member responsible for the team\u2019s SecDevOps toolsets used across all projects. Your efforts will enable the entire team\u2019s implementation, deployment, and sustainment of new capabilities for the portfolio of virtual health solutions. Primary responsibilities include: \n \n  Understand and administer IdAM solutions (Jump Cloud, KeyCloak, etc.) supporting access. \n  Configure and maintain user accounts and network access to the pSDO environment to include supporting requests for new/modifications to existing Jira and Confluence project tools. \n  Provide build and operations engineering support for the virtual health solution hosting environments in AWS GovCloud. \n  Test, Plan, and Deploy updates, as needed, to the pSDO environment. \n  Support resolution of operational issues and employ best practice in troubleshooting and resolving tickets received from the user community. \n  Support the scheduling and execution of pipeline build and deployment activities (GitLab) within the project cloud computing environments. \n  Coordinate with Tiered engineering team and COTS vendor engineering, as required, to resolve build and operational tickets. \n  Follow DHMSM established Change Management (CM) and Release Management (RM) practices. \n  Contribute to the design and use of Infrastructure as Code (analyze, design, build, test, deploy, support) for large scale infrastructure implementations. \n  Assist in the introduction of new capabilities, the enhancement, or troubleshooting of existing capabilities; Ability to understand/learn new technologies when encountered. \n  Integrate commercial products with semi-custom developed solutions to meet functional requirements while working within the guidelines of the client\u2019s infrastructure. \n  Communicate with Product Team Owners, application development and integration teams, network teams, and application security teams. \n  Work independently within a matrixed organization. \n \n \n  REQUIRED Qualifications \n \n  US Citizen with  EXISTING  ADP1/IT1 Public Trust and completed Tier 5 background investigation. Federal Government requirement. \n  BS Degree and 6+ years of prior relevant experience (4-6+ years of hands-on fulltime Cloud experience highly desirable) \n  Support 24/7 operations team, call back support as required. \n  Experience with cloud infrastructure, architecture, and implementation \n  AWS GovCloud Architect or DevOps certification \n  Implementation experience with: \n \n  AWS GovCloud services use, configuration, operational use (EC2, Lambda, EKS, ECS, etc.) \n  Large enterprise AWS Kubernetes (EKS) implementation experience \n  Infrastructure as Code (IaC) design, deployment, migration, and sustainment (Terraform, Ansible, GitLab pipeline automation, etc) \n  DevSecOps Engr experience, service mgmt., container to cluster migration \n  GitLab (CI/CD pipeline) \n  Jira, Confluence \n  Linux (RHEL or Debian/Ubuntu) \n  Windows Server (2016, 2019) \n \n  Strong verbal and written communications skills, ability to communicate technical concepts. \n  Demonstrated self-starter and team contributor with strong analytical and problem-solving skills. \n  Understanding of software development (including testing) methodologies, and cross functional experience throughout life-cycle phases. \n \n \n  PREFERRED Qualifications \n \n  4-6+ years of hands-on fulltime Cloud experience highly desirable \n  Implementation experience with: \n \n  Twilio audio/video services, Twilio SMS text services, Pexip video \n  SIP trunk integration \n  MongoDB \n  Elasticache \n  Docker \n  Scripting: BASH/shell, Python, or Golang (Go) \n  OracleDB \n  Atlassian Jira and Confluence, Remedy &/or ServiceNow queue administration \n  Jump Cloud / KeyCloak OIDC \n  HashiCorp applications (Packer, Terraform, Consul, Vault) \n \n  Large scale infrastructure implementation and sustainment \n  Cybersecurity issue resolution, STIG application \n \n \n  Pay Range:  Pay Range $97,500.00 - $176,250.00\n  \n  The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law. \n  #Remote", "cleaned_desc": "  Coordinate with Tiered engineering team and COTS vendor engineering, as required, to resolve build and operational tickets. \n  Follow DHMSM established Change Management (CM) and Release Management (RM) practices. \n  Contribute to the design and use of Infrastructure as Code (analyze, design, build, test, deploy, support) for large scale infrastructure implementations. \n  Assist in the introduction of new capabilities, the enhancement, or troubleshooting of existing capabilities; Ability to understand/learn new technologies when encountered. \n  Integrate commercial products with semi-custom developed solutions to meet functional requirements while working within the guidelines of the client\u2019s infrastructure. \n  Communicate with Product Team Owners, application development and integration teams, network teams, and application security teams. \n  Work independently within a matrixed organization. \n \n \n  REQUIRED Qualifications \n \n  US Citizen with  EXISTING  ADP1/IT1 Public Trust and completed Tier 5 background investigation. Federal Government requirement. \n  BS Degree and 6+ years of prior relevant experience (4-6+ years of hands-on fulltime Cloud experience highly desirable)    Support 24/7 operations team, call back support as required. \n  Experience with cloud infrastructure, architecture, and implementation \n  AWS GovCloud Architect or DevOps certification \n  Implementation experience with: \n \n  AWS GovCloud services use, configuration, operational use (EC2, Lambda, EKS, ECS, etc.) \n  Large enterprise AWS Kubernetes (EKS) implementation experience \n  Infrastructure as Code (IaC) design, deployment, migration, and sustainment (Terraform, Ansible, GitLab pipeline automation, etc) \n  DevSecOps Engr experience, service mgmt., container to cluster migration \n  GitLab (CI/CD pipeline) \n  Jira, Confluence \n  Linux (RHEL or Debian/Ubuntu) \n  Windows Server (2016, 2019)   \n  Strong verbal and written communications skills, ability to communicate technical concepts. \n  Demonstrated self-starter and team contributor with strong analytical and problem-solving skills. \n  Understanding of software development (including testing) methodologies, and cross functional experience throughout life-cycle phases. \n \n \n  PREFERRED Qualifications \n \n  4-6+ years of hands-on fulltime Cloud experience highly desirable \n  Implementation experience with: \n \n  Twilio audio/video services, Twilio SMS text services, Pexip video \n  SIP trunk integration ", "techs": ["terraform", "ansible", "gitlab", "aws govcloud", "ec2", "lambda", "eks", "ecs", "kubernetes", "gitlab ci/cd pipeline", "jira", "confluence", "linux", "rhel", "debian/ubuntu", "windows server", "twilio", "pexip", "sip trunk integration"]}, "9e62e5059301ba66": {"terms": ["mlops"], "salary_min": 40.0, "salary_max": 42.0, "title": "DBA with DevOps", "company": "Q1 Technologies", "desc": "Job description Below \n \n Minimum of 10+ years of professional experience in software development \n Experience as a Database Administrator, with expertise in managing and optimizing databases. \n Strong knowledge of database management systems, such as Oracle, MySQL, PostgreSQL, or MongoDB. \n Proficiency in scripting languages like SQL, Python, or PowerShell. \n Familiarity with DevOps principles, tools, and methodologies. \n Experience with configuration management tools like Ansible, Chef, or Puppet. \n Understanding of CI/CD pipelines and version control systems (e.g., Git). \n Knowledge of performance monitoring and optimization techniques for databases. \n Excellent problem-solving and analytical skills. \n Strong communication and collaboration abilities to work effectively in cross-functional teams. \n \n Experience Required \n \n Database Administrator (DBA) with a strong understanding of DevOps principles and practices. \n Deep knowledge of database management systems, excellent problem-solving skills. \n Expertise in implementing and maintaining databases in a DevOps environment. \n Manage and optimize our organization's databases while collaborating with development and operations teams to ensure efficient and scalable database solutions. \n \n Roles & Responsibilities \n \n Design, implement, and maintain databases, ensuring data integrity, security, and high availability. \n Perform routine database maintenance tasks, such as backups, recovery, and optimization. \n Monitor database performance, identify bottlenecks, and recommend improvements. \n Troubleshoot and resolve database issues, working closely with development and operations teams. \n Collaborate with development and operations teams to establish and enhance DevOps processes for database management. \n Implement automated deployment and configuration management for databases using tools like Ansible, Chef, or Puppet. \n Contribute to the development of CI/CD pipelines, ensuring smooth integration of database changes. \n Work with version control systems and release management tools to track and manage database changes. \n Analyze database performance metrics and proactively identify opportunities for optimization. \n Optimize database configurations, queries, and indexing strategies to improve overall system performance. \n Collaborate with development teams to design efficient database schemas and query patterns. \n Stay up to date with emerging technologies and best practices in database performance optimization. \n Implement and enforce database security measures, including access controls and data encryption. \n Ensure compliance with relevant data protection regulations and industry standards. \n Perform regular security assessments and vulnerability testing on databases. \n Contribute to disaster recovery planning and execute recovery procedures when necessary. \n \n Job Type: Contract \n Salary: $40.00 - $42.00 per hour \n Experience: \n \n MySQL: 4 years (Required) \n Database administration: 7 years (Preferred) \n DevOps: 5 years (Required) \n \n Work Location: Remote", "cleaned_desc": "Job description Below \n \n Minimum of 10+ years of professional experience in software development \n Experience as a Database Administrator, with expertise in managing and optimizing databases. \n Strong knowledge of database management systems, such as Oracle, MySQL, PostgreSQL, or MongoDB. \n Proficiency in scripting languages like SQL, Python, or PowerShell. \n Familiarity with DevOps principles, tools, and methodologies. \n Experience with configuration management tools like Ansible, Chef, or Puppet. \n Understanding of CI/CD pipelines and version control systems (e.g., Git).   Knowledge of performance monitoring and optimization techniques for databases. \n Excellent problem-solving and analytical skills. \n Strong communication and collaboration abilities to work effectively in cross-functional teams. \n \n Experience Required \n \n Database Administrator (DBA) with a strong understanding of DevOps principles and practices. \n Deep knowledge of database management systems, excellent problem-solving skills. \n Expertise in implementing and maintaining databases in a DevOps environment.   Implement automated deployment and configuration management for databases using tools like Ansible, Chef, or Puppet. \n Contribute to the development of CI/CD pipelines, ensuring smooth integration of database changes. \n Work with version control systems and release management tools to track and manage database changes. \n Analyze database performance metrics and proactively identify opportunities for optimization. \n Optimize database configurations, queries, and indexing strategies to improve overall system performance. \n Collaborate with development teams to design efficient database schemas and query patterns. \n Stay up to date with emerging technologies and best practices in database performance optimization. \n Implement and enforce database security measures, including access controls and data encryption. \n Ensure compliance with relevant data protection regulations and industry standards. ", "techs": ["oracle", "mysql", "postgresql", "mongodb", "sql", "python", "powershell", "ansible", "chef", "puppet", "git"]}, "2235d793853a64d6": {"terms": ["mlops"], "salary_min": 156000.0, "salary_max": 205000.0, "title": "REMOTE - Sr. DevOps Engineer (AWS)", "company": "First American Financial Corporation", "desc": "Who We Are \n \n  Come join First American's Digital Title Group, newly formed to re-imagine and digitize the title search and examination process through Big Data, AI, document automation and modern, cloud-native application development. As a market leading title insurance company, powered by the nation's largest and most complete property information, ownership and recorded document database, First American is committed to advancing title automation and removing friction from the real estate closing process. Our modern title decisioning solutions create certainty and speed through data and analytics, delivered to real estate agents, lenders, title agents and homebuyers. Join a team that puts its People First! Since 1889, First American (NYSE: FAF) has held an unwavering belief in its people. They are passionate about what they do, and we are equally passionate about fostering an environment where all feel welcome, supported, and empowered to be innovative and reach their full potential. Our inclusive, people-first culture has earned our company numerous accolades, including being named to the Fortune 100 Best Companies to Work For\u00ae list for eight consecutive years. We have also earned awards as a best place to work for women, diversity and LGBTQ+ employees, and have been included on more than 50 regional best places to work lists. First American will always strive to be a great place to work, for all. For more information, please visit www.careers.firstam.com. \n \n  What We Do \n \n  Our modern title decisioning solutions create certainty and speed through data and analytics, delivered to real estate agents, lenders, title agents and homebuyers. We're looking for Senior AWS DevOps Engineers interested in transforming our industry by solving cutting-edge problems with modern technologies, want the benefit of working for an established real estate insurance leader and seek a culture awarded as a Fortune 100 Best Companies to Work! \n \n  What you'll do: \n \n  Work collaboratively with solution architects and engineering teams to define infrastructure and deployment requirements \n  Create, maintain, and improve CI/CD build and release pipelines in AWS environment \n  Provide development, operational, and database support in AWS and GitHub environments \n  Define and deploy infrastructure components using pipelines and IaC tooling \n \n \n  Knowledge and Skills \n \n  Experience with GitHub, GitHub Actions CI/CD, pipeline automation, and application deployment within AWS \n  Experience with IAM roles/policies/permissions and security groups \n  Experience orchestrating and maintaining AWS infrastructure using Terraform (VPC, EC2, RDS, Security Groups, IAM, ECS, CodeDeploy, CloudFront, EventBridge, SNS, SQS, Lambda, S3, EMR, MWAA, Glue) \n  Experience using containerization technologies such as Docker, ECS, ECR, Fargate \n  Serverless orchestration, especially Lambdas \n  Experience with one or more AWS SDKs and/or CLI \n  Solid foundation of networking and Linux administration \n \n \n  Education \n \n  Bachelor's degree in Computer Science (or related field) or equivalent combination of education and experience \n \n \n  Typical Range of Experience \n \n  5+ years of directly related experience \n \n \n  **Open to remote candidates** \n \n  Pay Range: $156,000 - $205,000 annually \n \n  This hiring range is a good faith and reasonable estimate of the salary range of possible compensation at the time of the posting, and is subject to change. The actual compensation offered will be determined by various factors, which may include a candidate\u2019s education, training, experience, and geographic location. \n \n  #LI-EF1  #LI-REMOTE \n \n  What We Offer \n \n  By choice, we don\u2019t simply accept individuality \u2013 we embrace it, we support it, and we thrive on it! Our People First Culture celebrates diversity, equity and inclusion not simply because it\u2019s the right thing to do, but also because it\u2019s the key to our success. We are proud to foster an authentic and inclusive workplace For All. You are free and encouraged to bring your entire, unique self to work. First American is an equal opportunity employer in every sense of the term. \n \n  Based on eligibility, First American offers a comprehensive benefits package including medical, dental, vision, 401k, PTO/paid sick leave and other great benefits like an employee stock purchase plan.", "cleaned_desc": " \n  Work collaboratively with solution architects and engineering teams to define infrastructure and deployment requirements \n  Create, maintain, and improve CI/CD build and release pipelines in AWS environment \n  Provide development, operational, and database support in AWS and GitHub environments \n  Define and deploy infrastructure components using pipelines and IaC tooling \n \n \n  Knowledge and Skills \n    Experience with GitHub, GitHub Actions CI/CD, pipeline automation, and application deployment within AWS \n  Experience with IAM roles/policies/permissions and security groups \n  Experience orchestrating and maintaining AWS infrastructure using Terraform (VPC, EC2, RDS, Security Groups, IAM, ECS, CodeDeploy, CloudFront, EventBridge, SNS, SQS, Lambda, S3, EMR, MWAA, Glue) \n  Experience using containerization technologies such as Docker, ECS, ECR, Fargate \n  Serverless orchestration, especially Lambdas \n  Experience with one or more AWS SDKs and/or CLI \n  Solid foundation of networking and Linux administration \n \n ", "techs": ["github", "github actions ci/cd", "aws", "iam", "terraform", "vpc", "ec2", "rds", "security groups", "ecs", "codedeploy", "cloudfront", "eventbridge", "sns", "sqs", "lambda", "s3", "emr", "mwaa", "glue", "docker", "ecr", "fargate", "lambdas", "aws sdks", "cli", "networking", "linux administration"]}, "107fca5c271e336e": {"terms": ["mlops"], "salary_min": 148130.08, "salary_max": 187565.7, "title": "Senior Devops Engineer", "company": "Alpaca", "desc": "Your Role: \n  We are looking for a senior DevOps engineer who will design, develop and manage our large scale cloud-hosted systems. Our devops team takes care of the infrastructure layer of our services, and works closely with other engineering teams to continuously improve and extend our systems and services. \n  Things You Get To Do: \n \n Design, maintain and expand our infrastructure (maintained as IaaC) \n Oversee systems and resources for alerting, logging, backups, disaster recovery, and monitoring \n Work jointly with other software engineering teams in building fault-tolerant & resilient services that are in line with our infrastructure's best practices \n Improve the performance and durability of our CI/CD pipelines \n Think with IT security in your mind all the time \n You may be asked to be on-call to assist with engineering projects that are timely in nature \n \n Who You Are (Must-Haves): \n \n You have 5+ years of experience in DevOps or a similar role \n You have significant experience with Kubernetes in production environments \n You have hands-on experience with a cloud provider (we use GCP) by maintaining production applications in the cloud \n You have a deep understanding of Docker \n You are familiar with modern distributed logging, monitoring, and metrics systems \n You have deep experience building and scaling continuous integration and continuous delivery pipelines \n You have deep familiarity with Git for distributed source code version control \n You have excellent communication skills and value good documentation and writing \n Strong understanding and comfort with POSIX Operating Systems (Linux/BSD, etc) \n Experience with RDBMS, message brokers (Redpanda, Kafka) \n \n Who You Might Be (Nice-to-Haves): \n \n You have experience with Elasticsearch and Kibana and/or Grafana Loki \n You have experience with Prometheus (including Prometheus Alerts) and Grafana \n You are familiar with distributed tracing \n Experience with multi-region high availability \n Experience with Golang \n Strong Postgres scaling and performance tuning experience (including overseeing the management of both self-managed instances and cloud-managed instances) \n FinTech experience \n Experience building a local development environment using docker-composite or Tilt \n \n Our Tech & Infrastructure Stack: \n \n Our services are running on Google Cloud Platform via Kubernetes \n Colocated systems in close proximity to financial hubs with cross-connects to data partners and market execution venues \n Golang \n Postgres \n RabbitMQ, Kafka/Redpanda \n ArgoCD \n \n \n  How We Take Care of You: \n \n Competitive Salary & Stock Options \n Benefits: Health benefits start on day 1. In the US this includes Medical, Dental, Vision. In Canada, this includes supplemental health care. Internationally, this includes a stipend value to offset medical costs. \n New Hire Home-Office Setup: One-time USD $500 \n Monthly Stipend: USD $150 per month via a Brex Card \n Work with awesome hard working people, super smart and cool clients and innovative partners from around the world \n \n Alpaca is proud to be an equal opportunity workplace dedicated to pursuing and hiring a diverse workforce.", "cleaned_desc": " You have excellent communication skills and value good documentation and writing \n Strong understanding and comfort with POSIX Operating Systems (Linux/BSD, etc) \n Experience with RDBMS, message brokers (Redpanda, Kafka) \n \n Who You Might Be (Nice-to-Haves): \n \n You have experience with Elasticsearch and Kibana and/or Grafana Loki \n You have experience with Prometheus (including Prometheus Alerts) and Grafana \n You are familiar with distributed tracing \n Experience with multi-region high availability   Experience with Golang \n Strong Postgres scaling and performance tuning experience (including overseeing the management of both self-managed instances and cloud-managed instances) \n FinTech experience \n Experience building a local development environment using docker-composite or Tilt \n \n Our Tech & Infrastructure Stack: \n \n Our services are running on Google Cloud Platform via Kubernetes \n Colocated systems in close proximity to financial hubs with cross-connects to data partners and market execution venues \n Golang ", "techs": ["elasticsearch", "kibana", "grafana loki", "prometheus", "grafana", "distributed tracing", "golang", "postgres", "docker-compose", "tilt", "google cloud platform", "kubernetes"]}, "f514ca69ccda4437": {"terms": ["mlops"], "salary_min": 97500.0, "salary_max": 176250.0, "title": "DevOps Engineer / DHMSM Virtual Health", "company": "Leidos", "desc": "Description   \n The Leidos Partnership for Defense Health is seeking a  DevOps Infrastructure Engineer  to work remotely and support its $4.6B single award DHMSM IDIQ contract, known as MHS GENESIS. This highly visible program provides modernization of Electronic Health Records (EHR) capabilities for the Department of Defense, supporting nearly 10 million service members and their families worldwide. While every healthcare system is addressing quality, user experience and cost, the MHS GENESIS program has the added challenge of delivering these objectives within the DOD security environment, at the DOD\u2019s global scale, all while supporting the Defense Health Agency (DHA)\u2019s vision of a medically ready force which is essential to our national defense. \n \n  WHAT YOU WILL BE DOING \n \n  The  DevOps Infrastructure Engineer  will join the Virtual Health team as a key team member responsible for the team\u2019s SecDevOps toolsets used across all projects. Your efforts will enable the entire team\u2019s implementation, deployment, and sustainment of new capabilities for the portfolio of virtual health solutions. Primary responsibilities include: \n \n  Understand and administer IdAM solutions (Jump Cloud, KeyCloak, etc.) supporting access. \n  Configure and maintain user accounts and network access to the pSDO environment to include supporting requests for new/modifications to existing Jira and Confluence project tools. \n  Provide build and operations engineering support for the virtual health solution hosting environments in AWS GovCloud. \n  Test, Plan, and Deploy updates, as needed, to the pSDO environment. \n  Support resolution of operational issues and employ best practice in troubleshooting and resolving tickets received from the user community. \n  Support the scheduling and execution of pipeline build and deployment activities (GitLab) within the project cloud computing environments. \n  Coordinate with Tiered engineering team and COTS vendor engineering, as required, to resolve build and operational tickets. \n  Follow DHMSM established Change Management (CM) and Release Management (RM) practices. \n  Contribute to the design and use of Infrastructure as Code (analyze, design, build, test, deploy, support) for large scale infrastructure implementations. \n  Assist in the introduction of new capabilities, the enhancement, or troubleshooting of existing capabilities; Ability to understand/learn new technologies when encountered. \n  Integrate commercial products with semi-custom developed solutions to meet functional requirements while working within the guidelines of the client\u2019s infrastructure. \n  Communicate with Product Team Owners, application development and integration teams, network teams, and application security teams. \n  Work independently within a matrixed organization. \n \n \n  REQUIRED Qualifications \n \n  US Citizen with  EXISTING  ADP1/IT1 Public Trust and completed Tier 5 background investigation. Federal Government requirement. \n  BS Degree and 6+ years of prior relevant experience (4-6+ years of hands-on fulltime Cloud experience highly desirable) \n  Support 24/7 operations team, call back support as required. \n  Experience with cloud infrastructure, architecture, and implementation \n  AWS GovCloud Architect or DevOps certification \n  Implementation experience with: \n \n  AWS GovCloud services use, configuration, operational use (EC2, Lambda, EKS, ECS, etc.) \n  Large enterprise AWS Kubernetes (EKS) implementation experience \n  Infrastructure as Code (IaC) design, deployment, migration, and sustainment (Terraform, Ansible, GitLab pipeline automation, etc) \n  DevSecOps Engr experience, service mgmt., container to cluster migration \n  GitLab (CI/CD pipeline) \n  Jira, Confluence \n  Linux (RHEL or Debian/Ubuntu) \n  Windows Server (2016, 2019) \n \n  Strong verbal and written communications skills, ability to communicate technical concepts. \n  Demonstrated self-starter and team contributor with strong analytical and problem-solving skills. \n  Understanding of software development (including testing) methodologies, and cross functional experience throughout life-cycle phases. \n \n \n  PREFERRED Qualifications \n \n  4-6+ years of hands-on fulltime Cloud experience highly desirable \n  Implementation experience with: \n \n  Twilio audio/video services, Twilio SMS text services, Pexip video \n  SIP trunk integration \n  MongoDB \n  Elasticache \n  Docker \n  Scripting: BASH/shell, Python, or Golang (Go) \n  OracleDB \n  Atlassian Jira and Confluence, Remedy &/or ServiceNow queue administration \n  Jump Cloud / KeyCloak OIDC \n  HashiCorp applications (Packer, Terraform, Consul, Vault) \n \n  Large scale infrastructure implementation and sustainment \n  Cybersecurity issue resolution, STIG application \n \n \n  Pay Range:  Pay Range $97,500.00 - $176,250.00\n  \n  The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law. \n  #Remote", "cleaned_desc": "  Coordinate with Tiered engineering team and COTS vendor engineering, as required, to resolve build and operational tickets. \n  Follow DHMSM established Change Management (CM) and Release Management (RM) practices. \n  Contribute to the design and use of Infrastructure as Code (analyze, design, build, test, deploy, support) for large scale infrastructure implementations. \n  Assist in the introduction of new capabilities, the enhancement, or troubleshooting of existing capabilities; Ability to understand/learn new technologies when encountered. \n  Integrate commercial products with semi-custom developed solutions to meet functional requirements while working within the guidelines of the client\u2019s infrastructure. \n  Communicate with Product Team Owners, application development and integration teams, network teams, and application security teams. \n  Work independently within a matrixed organization. \n \n \n  REQUIRED Qualifications \n \n  US Citizen with  EXISTING  ADP1/IT1 Public Trust and completed Tier 5 background investigation. Federal Government requirement. \n  BS Degree and 6+ years of prior relevant experience (4-6+ years of hands-on fulltime Cloud experience highly desirable)    Support 24/7 operations team, call back support as required. \n  Experience with cloud infrastructure, architecture, and implementation \n  AWS GovCloud Architect or DevOps certification \n  Implementation experience with: \n \n  AWS GovCloud services use, configuration, operational use (EC2, Lambda, EKS, ECS, etc.) \n  Large enterprise AWS Kubernetes (EKS) implementation experience \n  Infrastructure as Code (IaC) design, deployment, migration, and sustainment (Terraform, Ansible, GitLab pipeline automation, etc) \n  DevSecOps Engr experience, service mgmt., container to cluster migration \n  GitLab (CI/CD pipeline) \n  Jira, Confluence \n  Linux (RHEL or Debian/Ubuntu) \n  Windows Server (2016, 2019)   \n  Strong verbal and written communications skills, ability to communicate technical concepts. \n  Demonstrated self-starter and team contributor with strong analytical and problem-solving skills. \n  Understanding of software development (including testing) methodologies, and cross functional experience throughout life-cycle phases. \n \n \n  PREFERRED Qualifications \n \n  4-6+ years of hands-on fulltime Cloud experience highly desirable \n  Implementation experience with: \n \n  Twilio audio/video services, Twilio SMS text services, Pexip video \n  SIP trunk integration ", "techs": ["terraform", "ansible", "gitlab pipeline automation", "aws govcloud services (ec2", "lambda", "eks", "ecs)", "kubernetes (eks)", "gitlab (ci/cd pipeline)", "jira", "confluence", "linux (rhel or debian/ubuntu)", "windows server (2016", "2019)", "twilio audio/video services", "twilio sms text services", "pexip video", "sip trunk integration"]}, "59225606eba41ae7": {"terms": ["mlops"], "salary_min": 117777.68, "salary_max": 149132.78, "title": "Mid-Level DevOps Engineer", "company": "RevaComm", "desc": "Headquartered in Honolulu, Hawaii with remote locations across the United States, RevaComm is a leader in Agile Software Development, User-Centered Design, and DevSecOps. As an enterprise digital transformation company, we transform organizational challenges into powerful digital capabilities through fresh experiences and great technology. Grounded by the company's core values, our approach brings together digital business strategists and architects, software engineers, user experience designers, and project managers to create sustainable solutions for customers while surprising and delighting their users. \n  RevaComm DevOps Engineers are responsible for designing and implementing DevOps solutions on various cloud services to automate, secure, and test applications that RevaComm builds and deploys. The DevOps Engineer should be proficient with cloud infrastructure and deployments as well as containerization, CI/CD, automated scaling and security in relation to DevOps as well as be able to understand large data architectures and IT systems. \n  Core Responsibilities: \n \n  5+ Years of DevOps experience with cloud systems and architectures (AWS, Google Cloud or Microsoft Azure) \n \n \n  A deep understanding of Agile and DevSecOps methodologies \n \n \n  Extensive experience with tools such as ArgoCD, Gitlab, Helm, and Kustomize \n  Development experience with Kubernetes, Docker, and Helm \n  Excellent communication skills as interacting with stakeholders is a large part of this role. \n  Comfortable evaluating COTS and FOSS products \n  Experience with upgrades, patching, monitoring, and CVE fixes \n \n  Preferred Qualifications: \n \n  Bachelor's degree or higher \n  CompTIA Security+ certification \n  Additional security clearance is required \n  Familiarity with Keycloak, Atlassian products, Carvel tool suite, Sonatype products, \n  Ability to work on high performing teams supporting multiple projects and personnel \n \n \n  Strong oral and written communication skills \n \n \n  Knowledgeable of industry standards and best practices \n  Eager to learn new technologies and gain expert-level understanding of the latest approaches (auto-scaled cloud-based systems, serverless architectures, cloud/on-premises hybrid solutions) \n \n \n  Aware of software development risks, with an appreciation for what is technically feasible \n \n \n  Able to maintain a high level of coding, testing, and engineering standards \n \n \n  Able to be flexible and adaptable if changes in priorities occur \n \n \n  Self-starters and fast-learners who are able to independently research and resolve issues using appropriate resources \n \n \n  Comfortable working with multi-disciplinary, multicultural, and geographically dispersed teams \n \n  Benefits: \n  As a Full-time RVCM employee, you can also expect these additional benefits: \n \n  Fully remote work within the U.S. \n  Exemplary Medical, Dental, and Vision coverage \n  Health Care and Dependent Care Savings Accounts \n  Group Term Life Insurance auto-enrollment (for employees who participate in health insurance coverage with RVCM) \n  401(k) with company matching after 1 year of service. Accounts are 100% vested immediately \n  15 days of PTO for new hires. PTO increases by 1 day for every year of service up to a max of 25 days \n  11 Paid Holidays \n  Fully Paid Holiday Break (Last week of the calendar year) \n  A company-provided laptop plus a $1,000 Home Office Bonus upon hire; $500 per year thereafter to purchase additional equipment to improve productivity at home. \n  Paid parental leave for the care of a newborn or adopted child which is compensated at 100% of the employee's regular, straight-time weekly pay. \n  Healthcare FSA (Flexible Spending Account) for eligible medical expenses \n  Dependent Care FSA (Flexible Spending Account) for eligible dependent care expenses \n  Continuous Education & Training: Including financial support for acquiring certifications \n  Mentorship Programs \n \n  We believe in providing a safe space for all members of the RVCM 'ohana to grow and thrive. Our employees come from a broad spectrum of backgrounds; each with their own unique story. Diversity, Equity, and Inclusion are at the heart of who we are, and everyone should feel valued and free to bring their most authentic self to work - without fear, without judgment. Creating this environment is important, not only for our organization but also for our customers and our communities. \n  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran. \n  Relocation Expenses Not Covered", "cleaned_desc": "Headquartered in Honolulu, Hawaii with remote locations across the United States, RevaComm is a leader in Agile Software Development, User-Centered Design, and DevSecOps. As an enterprise digital transformation company, we transform organizational challenges into powerful digital capabilities through fresh experiences and great technology. Grounded by the company's core values, our approach brings together digital business strategists and architects, software engineers, user experience designers, and project managers to create sustainable solutions for customers while surprising and delighting their users. \n  RevaComm DevOps Engineers are responsible for designing and implementing DevOps solutions on various cloud services to automate, secure, and test applications that RevaComm builds and deploys. The DevOps Engineer should be proficient with cloud infrastructure and deployments as well as containerization, CI/CD, automated scaling and security in relation to DevOps as well as be able to understand large data architectures and IT systems. \n  Core Responsibilities: \n \n  5+ Years of DevOps experience with cloud systems and architectures (AWS, Google Cloud or Microsoft Azure) \n \n \n  A deep understanding of Agile and DevSecOps methodologies \n \n \n  Extensive experience with tools such as ArgoCD, Gitlab, Helm, and Kustomize \n  Development experience with Kubernetes, Docker, and Helm \n  Excellent communication skills as interacting with stakeholders is a large part of this role.    Comfortable evaluating COTS and FOSS products \n  Experience with upgrades, patching, monitoring, and CVE fixes \n \n  Preferred Qualifications: \n \n  Bachelor's degree or higher \n  CompTIA Security+ certification \n  Additional security clearance is required \n  Familiarity with Keycloak, Atlassian products, Carvel tool suite, Sonatype products, \n  Ability to work on high performing teams supporting multiple projects and personnel \n \n \n  Strong oral and written communication skills ", "techs": ["honolulu", "hawaii", "agile software development", "user-centered design", "devsecops", "cloud services", "automation", "security", "cloud infrastructure", "deployments", "containerization", "ci/cd", "scaling", "large data architectures", "it systems", "argocd", "gitlab", "helm", "kustomize", "kubernetes", "docker", "communication skills", "cots", "foss products", "upgrades", "patching", "monitoring", "cve fixes", "bachelor's degree", "comptia security+ certification", "security clearance", "keycloak", "atlassian products", "carvel tool suite", "sonatype products", "high performing teams"]}, "d45706316adf17da": {"terms": ["mlops"], "salary_min": 133000.0, "salary_max": 210000.0, "title": "Engagement Director, Energy Sector - Remote", "company": "EPAM Systems", "desc": "Are you ready to lead change in the dynamic world of energy?\n  \n \n We are seeking an accomplished Engagement Director  to join our innovative team in the Energy Sector. If you're passionate about driving transformation, fostering meaningful relationships, and achieving impactful results, this role is your gateway to making a significant difference in the industry.\n  \n  Req.#523591663\n  \n  RESPONSIBILITIES\n  \n  Build trusted partner relationships with senior client stakeholders in the Energy sector \n  Be a thought leader, evangelize and drive collaborative, value-centered approaches to defining and solving problems \n  Align with Business teams and Senior Leadership to co-develop / support account plans and strategies for delivery, growth, and client satisfaction \n  Drive revenue within the client/account portfolio by identifying opportunities, solutions, launching, and orchestrating a wide range of data & analytics engagements \n  Serve as a principal consultant leading complex transformational initiatives \n  Own and actively contribute to the strategic deliverables, such as vision & mission statements, assessment results, solution visions, strategies, plans & roadmaps \n  Orchestrate pre-sales activities and deal closure process \n  Your performance scorecard will include \n \n  Impact on Client Value and Business Growth \u2013 Achieve target revenue goals \n  Impact on Operating the Business \u2013 Achieve target billable utilization \n  Impact on Our People and Team(s) \u2013 Contribute to EPAM Data & Analytics Practice initiatives \n \n \n  REQUIREMENTS\n  \n  Experience in IT or management consulting in the Oil, Gas & Energy industry \n  Proven track record of launching and managing complex technology initiatives or large-scale programs (50+ people) working with senior business and IT stakeholders \n  Demonstrated success in developing and growing client relationships \n  Strong problem solver and creative thinker with gravitas \n  Ability to operate at the strategic level, yet being close enough to the details to add value to clients and be a real support to your team \n  Ability to manage customer expectations, present and explain solutions and project deliverables to senior stakeholders \n  Ability to lead teams while tackling high levels of uncertainty, big number of stakeholders, and tight timelines \n  Ability to facilitate and drive strategy definition \n  Ability to challenge and influence data solution design and technology selection for enterprise data platforms, warehouses, data lakes, and data science platforms \n  Desire to learn & expand your comfort zone \n  Entrepreneurial spirit \n \n  NICE TO HAVE\n  \n  Experience building data and analytics organizations (center of excellence, data engineering pools, data science teams) \n  Awareness of cloud and data technologies and trends with practical experience with one or more major data and analytics technologies \n  Experience in end-to-end data and analytics project delivery (BigData, Cloud DWH, Advanced Analytics / MLOps, etc.) \n \n  BENEFITS\n  \n  Medical, Dental and Vision Insurance (Subsidized) \n  Health Savings Account \n  Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) \n  Short-Term and Long-Term Disability (Company Provided) \n  Life and AD&D Insurance (Company Provided) \n  Employee Assistance Program \n  Unlimited access to LinkedIn learning solutions \n  Matched 401(k) Retirement Savings Plan \n  Paid Time Off \n  Legal Plan and Identity Theft Protection \n  Accident Insurance \n  Employee Discounts \n  Pet Insurance \n  Employee Stock Purchase Program \n \n  ABOUT EPAM\n  \n  EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential \n \n  ADDITIONAL\n  \n  This posting includes a good faith range of the salary EPAM would reasonably expect to pay the selected candidate. The range provided reflects base salary only. Individual compensation offers within the range are based on a variety of factors, including, but not limited to: geographic location, experience, credentials, education, training; the demand for the role; and overall business and labor market considerations. Most candidates are hired at a salary within the range disclosed. Salary range: $133k - $210k. In addition, the details highlighted in this job posting above are a general description of all other expected benefits and compensation for the position", "cleaned_desc": "  Proven track record of launching and managing complex technology initiatives or large-scale programs (50+ people) working with senior business and IT stakeholders \n  Demonstrated success in developing and growing client relationships \n  Strong problem solver and creative thinker with gravitas \n  Ability to operate at the strategic level, yet being close enough to the details to add value to clients and be a real support to your team \n  Ability to manage customer expectations, present and explain solutions and project deliverables to senior stakeholders \n  Ability to lead teams while tackling high levels of uncertainty, big number of stakeholders, and tight timelines \n  Ability to facilitate and drive strategy definition \n  Ability to challenge and influence data solution design and technology selection for enterprise data platforms, warehouses, data lakes, and data science platforms \n  Desire to learn & expand your comfort zone \n  Entrepreneurial spirit \n \n  NICE TO HAVE\n  ", "techs": ["none"]}, "09f8358c25435b9c": {"terms": ["mlops"], "salary_min": 133000.0, "salary_max": 210000.0, "title": "Engagement Director, Energy Sector - Remote", "company": "EPAM Systems", "desc": "Are you ready to lead change in the dynamic world of energy?\n  \n \n We are seeking an accomplished Engagement Director  to join our innovative team in the Energy Sector. If you're passionate about driving transformation, fostering meaningful relationships, and achieving impactful results, this role is your gateway to making a significant difference in the industry.\n  \n  Req.#523591663\n  \n  RESPONSIBILITIES\n  \n  Build trusted partner relationships with senior client stakeholders in the Energy sector \n  Be a thought leader, evangelize and drive collaborative, value-centered approaches to defining and solving problems \n  Align with Business teams and Senior Leadership to co-develop / support account plans and strategies for delivery, growth, and client satisfaction \n  Drive revenue within the client/account portfolio by identifying opportunities, solutions, launching, and orchestrating a wide range of data & analytics engagements \n  Serve as a principal consultant leading complex transformational initiatives \n  Own and actively contribute to the strategic deliverables, such as vision & mission statements, assessment results, solution visions, strategies, plans & roadmaps \n  Orchestrate pre-sales activities and deal closure process \n  Your performance scorecard will include \n \n  Impact on Client Value and Business Growth \u2013 Achieve target revenue goals \n  Impact on Operating the Business \u2013 Achieve target billable utilization \n  Impact on Our People and Team(s) \u2013 Contribute to EPAM Data & Analytics Practice initiatives \n \n \n  REQUIREMENTS\n  \n  Experience in IT or management consulting in the Oil, Gas & Energy industry \n  Proven track record of launching and managing complex technology initiatives or large-scale programs (50+ people) working with senior business and IT stakeholders \n  Demonstrated success in developing and growing client relationships \n  Strong problem solver and creative thinker with gravitas \n  Ability to operate at the strategic level, yet being close enough to the details to add value to clients and be a real support to your team \n  Ability to manage customer expectations, present and explain solutions and project deliverables to senior stakeholders \n  Ability to lead teams while tackling high levels of uncertainty, big number of stakeholders, and tight timelines \n  Ability to facilitate and drive strategy definition \n  Ability to challenge and influence data solution design and technology selection for enterprise data platforms, warehouses, data lakes, and data science platforms \n  Desire to learn & expand your comfort zone \n  Entrepreneurial spirit \n \n  NICE TO HAVE\n  \n  Experience building data and analytics organizations (center of excellence, data engineering pools, data science teams) \n  Awareness of cloud and data technologies and trends with practical experience with one or more major data and analytics technologies \n  Experience in end-to-end data and analytics project delivery (BigData, Cloud DWH, Advanced Analytics / MLOps, etc.) \n \n  BENEFITS\n  \n  Medical, Dental and Vision Insurance (Subsidized) \n  Health Savings Account \n  Flexible Spending Accounts (Healthcare, Dependent Care, Commuter) \n  Short-Term and Long-Term Disability (Company Provided) \n  Life and AD&D Insurance (Company Provided) \n  Employee Assistance Program \n  Unlimited access to LinkedIn learning solutions \n  Matched 401(k) Retirement Savings Plan \n  Paid Time Off \n  Legal Plan and Identity Theft Protection \n  Accident Insurance \n  Employee Discounts \n  Pet Insurance \n  Employee Stock Purchase Program \n \n  ABOUT EPAM\n  \n  EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential \n \n  ADDITIONAL\n  \n  This posting includes a good faith range of the salary EPAM would reasonably expect to pay the selected candidate. The range provided reflects base salary only. Individual compensation offers within the range are based on a variety of factors, including, but not limited to: geographic location, experience, credentials, education, training; the demand for the role; and overall business and labor market considerations. Most candidates are hired at a salary within the range disclosed. Salary range: $133k - $210k. In addition, the details highlighted in this job posting above are a general description of all other expected benefits and compensation for the position", "cleaned_desc": "  Proven track record of launching and managing complex technology initiatives or large-scale programs (50+ people) working with senior business and IT stakeholders \n  Demonstrated success in developing and growing client relationships \n  Strong problem solver and creative thinker with gravitas \n  Ability to operate at the strategic level, yet being close enough to the details to add value to clients and be a real support to your team \n  Ability to manage customer expectations, present and explain solutions and project deliverables to senior stakeholders \n  Ability to lead teams while tackling high levels of uncertainty, big number of stakeholders, and tight timelines \n  Ability to facilitate and drive strategy definition \n  Ability to challenge and influence data solution design and technology selection for enterprise data platforms, warehouses, data lakes, and data science platforms \n  Desire to learn & expand your comfort zone \n  Entrepreneurial spirit \n \n  NICE TO HAVE\n  ", "techs": ["none of the information provided in the text includes specific tools or technologies."]}, "4663cc28f97b1b53": {"terms": ["mlops"], "salary_min": 101658.85, "salary_max": 128722.76, "title": "DevOps Engineer", "company": "Network Innovations U.S. Government", "desc": "Job title:  DevOps Engineer \n Department:  Software Development \n Location:  Frederick, MD Office (Hybrid/Remote work) \n Reports to:  Senior DevOps Manager \n About the Company: \n Network Innovation U.S. Government delivers leading-edge satellite, wireless & terrestrial telecommunications infrastructure, bandwidth, & turnkey network mobility solutions to the U.S. Department of Defense, Federal Agencies, Coalition Forces, Local & State Government Agencies, NGOs, Government Contractors, & commercial sector clients worldwide. We are a hardware agnostic solution integrator & has experience designing & implementing unique communication & networking solutions. Recently awarded several contracts, Network Innovation U.S. Government is positioned for rapid growth and advancement in Satellite Communication Enterprise Architectures within the DoD and with Commercial Network Providers. \n This team primarily works remotely, but will meet periodically at our office in Frederick, MD. \n Job purpose: \n Network Innovations is looking for a DevOps Engineer to help support the development of our Satellite Communications Network Management products and services participating as a team member using the Agile software development life cycle. \n The engineer\u2019s responsibilities include automation of software build, test and deployment systems and infrastructure. Management of various development, test, staging, demo environments (code deployment, backups, data refreshes). Development of automated tests and management of the test automation as part of the CI/CD pipeline. Deployment/management of software into Network Innovations\u2019 production Managed Service environment as well as customer production environments. DevOps may be asked to participate in the development of network device simulations to aide in the testing of devices that are difficult to acquire or may not be readily available. DevOps will be required to help in the installation and configuration of satellite hardware and networking equipment as needed for the development of our products and services. \n If you are very \u201chands-on\u201d, love to learn, are a great problem solver, are not afraid to tackle the unknown (and make mistakes), and have a passion for building solid, reliable process...we\u2019d like to meet you. \n Duties and responsibilities: \n \n Actively participate in an Agile Scrum development process (DevOps is part of the development team for sure!). \n Manage and administer Linux development systems and related infrastructure. \n Support the build, deployment, and test of large-scale web applications for satellite communications and network service provisioning. \n Maintain development, test, staging and production environments by ensuring code installations are correct, databases are refreshed/restored to the proper level, and infrastructure is configured to allow communications to the correct equipment. \n Develop, schedule, and regularly monitor automated tests to ensure key features/workflows are working post-deployment of new code (automated integration test and smoke tests). \n Maintain and execute database seeding scripts to stage (or reset) scenarios for demonstrations of the software features. \n Ensure proper isolation of various networks such that environments are not talking across one another. \n Manage JIRA/support cases for DevOps related work assignments and requests for help. \n Effectively communicate progress on assignments and indication of any roadblocks, challenges, or need for help/clarification. \n Review, test and troubleshoot code you author as well as other team members\u2019 code \n Document software defects promptly and clearly and address open software defects when assigned. \n Support demos for internal customers and potential clients \n Document build, CI/CD processes infrastructure and code required to obtain automation and measurement of processes. \n \n Qualifications: \n \n Work experience (5+ years professional experience) as a DevOps Engineer or similar role \n Proficient in CM, Artifact repositories, Build (Git, JFrog/Nexus) \n Knowledge of CI/CD deployment tools (Puppet, Jenkins ,Chef, Ansible) \n Experience working in an Agile software development methodology \n Experience with database monitoring, tuning, back-up and recovery (SQL database as well as NoSQL databases) \n Proficient in scripting languages (I.e. python, shell, etc) \n General understanding of web application development (interaction between web browsers and backend services) \n General understanding of REST-based services \n Excellent time management skills \n Excellent problem-solving abilities \n Excellent written and verbal communications skills \n \n Preferred Skill Set and Domain Knowledge (not required) \n \n Bachelor\u2019s Degree in Engineering or Computer Science (or equivalent) \n Git repositories and JIRA ticketing and work flow. \n Java-based micro service frameworks (Spring Boot). \n OpenSource Databases (Postgres, MySQL, ElasticSearch) \n REST Service development and utilization (Jersey/JAX-RS) \n Proficient in Linux, \u201ccontainerized\u201d application environments (i.e. Docker), and container management like K8s (storage, networking, disk, application installation and troubleshooting) \n Entity frameworks to persist data (Hibernate, JPA) \n Network Configuration and Communications protocols including: Router, Switch, and modem configuration and Serial/HTTP Protocols to configure devices \n Managed Network Services domain knowledge is a plus! \n Satellite Communications domain knowledge is a plus! \n Experience with customers / stakeholders in the Armed Forces (DoD). \n \n Working conditions: \n \n This is primary a remote position. \n This position requires that the candidate, when working remotely, effectively uses communications devices to provide status and support to their team and organization. \n This position will require meetings (and general availability) during normal business hours for team collaboration, project status, and customer demonstrations and presentations. \n When working remotely, the incumbent must have a work environment free from distractions/interruptions and the ability to provide a private environment in which business sensitive information can be shared over voice and video collaboration. \n When working remotely, the incumbent must have access to high-speed internet connectivity for unencumbered video conferencing and screen share collaboration. \n The position is primarily remote, however, there may be occasions to be \u2018in the office\u2019 or travel to customer locations for group meetings and/or demonstrations. \n \n Clearance requirements: \n \n Must be a US Citizen and have at a minimum a Secret clearance or be willing and able to obtain a security clearance. \n \n Physical requirements: \n \n Sitting and using a computer for extended periods of time. \n \n Disclaimer Statement : This job description lists the essential functions of the position and is not intended to include every job duty and responsibility specific to a position. An employee may be required to perform other related duties not listed above provided that such duties are characteristic of that classification. \n Job Type: Full-time \n Benefits: \n \n 401(k) \n 401(k) matching \n Dental insurance \n Employee assistance program \n Flexible spending account \n Health insurance \n Health savings account \n Life insurance \n Paid time off \n Vision insurance \n \n Experience level: \n \n 5 years \n \n Schedule: \n \n 8 hour shift \n Day shift \n Monday to Friday \n \n Ability to commute/relocate: \n \n Frederick, MD 21704: Reliably commute or planning to relocate before starting work (Preferred) \n \n Application Question(s): \n \n Are you eligible to obtain a US DoD security clearance? \n \n Experience: \n \n DevOps Engineer: 5 years (Required) \n \n Security clearance: \n \n Secret (Preferred) \n \n Work Location: Hybrid remote in Frederick, MD 21704", "cleaned_desc": " Review, test and troubleshoot code you author as well as other team members\u2019 code \n Document software defects promptly and clearly and address open software defects when assigned. \n Support demos for internal customers and potential clients \n Document build, CI/CD processes infrastructure and code required to obtain automation and measurement of processes. \n \n Qualifications: \n \n Work experience (5+ years professional experience) as a DevOps Engineer or similar role \n Proficient in CM, Artifact repositories, Build (Git, JFrog/Nexus) \n Knowledge of CI/CD deployment tools (Puppet, Jenkins ,Chef, Ansible) \n Experience working in an Agile software development methodology \n Experience with database monitoring, tuning, back-up and recovery (SQL database as well as NoSQL databases) \n Proficient in scripting languages (I.e. python, shell, etc) \n General understanding of web application development (interaction between web browsers and backend services) \n General understanding of REST-based services \n Excellent time management skills \n Excellent problem-solving abilities \n Excellent written and verbal communications skills \n \n Preferred Skill Set and Domain Knowledge (not required) \n \n Bachelor\u2019s Degree in Engineering or Computer Science (or equivalent) ", "techs": ["review", "test and troubleshoot code", "document software defects", "support demos", "document build", "cm", "artifact repositories", "build", "git", "jfrog/nexus", "ci/cd deployment tools", "puppet", "jenkins", "chef", "ansible", "agile software development methodology", "database monitoring", "tuning", "back-up and recovery", "scripting languages", "python", "shell", "web application development", "rest-based services", "time management", "problem-solving abilities", "written and verbal communications skills", "bachelor's degree in engineering or computer science."]}, "801776f8efb7e3ca": {"terms": ["mlops"], "salary_min": 154846.12, "salary_max": 196069.69, "title": "Principal LLM Data Scientist", "company": "ELI , LLC", "desc": "MUST SEE FOR ANY DATA SCIENTIST THAT IS SUPER PASSIONATE ABOUT WORKING WITH LLMs AND CREATING A BETTER LIFE FOR OTHER'S CARE! \n ** REMOTE - AMAZING COMP PACKAGE (to include stocks & bonus) - WORK WITH INCREDIBLY SMART PEOPLE WHO ARE CHANGING HEALTHCARE** \n Our client is revolutionizing healthcare by delivering value-based care to their patients. Their total care model empowers doctors to prioritize the patient's well-being over fees preserving the autonomy of local physicians while establishing enduring partnerships. As a Principal Data Scientist , you will play a pivotal role in addressing complex healthcare challenges, ranging from enhancing medical diagnoses to identifying medication adherence risks. \n Key Responsibilities: \n \n Modeling Expertise : Develop, train, and deploy predictive models utilizing deep learning and large language models (LLMs)/generative architectures to address diverse healthcare use cases. \n Team Leadership : Lead project teams, provide mentorship to junior data scientists, and collaborate closely with clinical and operational stakeholders across the company to drive innovation. \n MLOps Implementation : Construct production-quality pipelines for model training and inference, utilizing the MLOps stack on AWS to ensure seamless integration. \n Data Utilization : Work with extensive clinical datasets sourced from company's data lake, spanning various data sources and formats. \n Research and Innovation : Stay abreast of the latest advancements in machine learning and other technologies relevant to value-based care. Identify strategic opportunities for publishing based on project achievements. \n Work Environment : What more can you ask for - REMOTE!! Your home office. \n \n Qualifications: \n \n Educational Background : Possess an advanced degree (PhD, MS) in a quantitative field, such as machine learning, statistics, economics, population health, epidemiology, or computer science. \n Deep Learning Experience : Demonstrate a minimum of 5 years of experience in training deep learning models, with a preference for hands-on experience with large language models (LLMs). \n Technical Proficiency : Exhibit expert proficiency in Python and SQL, essential for effective data analysis and queries. \n Production Experience : Have a minimum of 5 of experience working in a software production environment or a similar research background. Should be able to show documents or publications as to your research. \n \n Job Type: Full-time \n Benefits: \n \n 401(k) \n 401(k) matching \n Dental insurance \n Employee assistance program \n Employee discount \n Flexible schedule \n Flexible spending account \n Health insurance \n Health savings account \n Life insurance \n Paid time off \n Parental leave \n Professional development assistance \n Referral program \n Retirement plan \n Tuition reimbursement \n Vision insurance \n \n Schedule: \n \n Monday to Friday \n \n Supplemental pay types: \n \n Bonus opportunities \n \n Experience: \n \n Python: 5 years (Required) \n SQL: 5 years (Required) \n LLM: 5 years (Required) \n production or research environment: 5 years (Required) \n completed Master's or PhD: 1 year (Required) \n AWS: 1 year (Preferred) \n \n License/Certification: \n \n Live in US and area Green Card holder or Citizen (Required) \n \n Work Location: Remote", "cleaned_desc": "MUST SEE FOR ANY DATA SCIENTIST THAT IS SUPER PASSIONATE ABOUT WORKING WITH LLMs AND CREATING A BETTER LIFE FOR OTHER'S CARE! \n ** REMOTE - AMAZING COMP PACKAGE (to include stocks & bonus) - WORK WITH INCREDIBLY SMART PEOPLE WHO ARE CHANGING HEALTHCARE** \n Our client is revolutionizing healthcare by delivering value-based care to their patients. Their total care model empowers doctors to prioritize the patient's well-being over fees preserving the autonomy of local physicians while establishing enduring partnerships. As a Principal Data Scientist , you will play a pivotal role in addressing complex healthcare challenges, ranging from enhancing medical diagnoses to identifying medication adherence risks. \n Key Responsibilities: \n \n Modeling Expertise : Develop, train, and deploy predictive models utilizing deep learning and large language models (LLMs)/generative architectures to address diverse healthcare use cases. \n Team Leadership : Lead project teams, provide mentorship to junior data scientists, and collaborate closely with clinical and operational stakeholders across the company to drive innovation. \n MLOps Implementation : Construct production-quality pipelines for model training and inference, utilizing the MLOps stack on AWS to ensure seamless integration. \n Data Utilization : Work with extensive clinical datasets sourced from company's data lake, spanning various data sources and formats. \n Research and Innovation : Stay abreast of the latest advancements in machine learning and other technologies relevant to value-based care. Identify strategic opportunities for publishing based on project achievements. \n Work Environment : What more can you ask for - REMOTE!! Your home office. \n   Qualifications: \n \n Educational Background : Possess an advanced degree (PhD, MS) in a quantitative field, such as machine learning, statistics, economics, population health, epidemiology, or computer science. \n Deep Learning Experience : Demonstrate a minimum of 5 years of experience in training deep learning models, with a preference for hands-on experience with large language models (LLMs). \n Technical Proficiency : Exhibit expert proficiency in Python and SQL, essential for effective data analysis and queries. \n Production Experience : Have a minimum of 5 of experience working in a software production environment or a similar research background. Should be able to show documents or publications as to your research. \n \n Job Type: Full-time \n Benefits: \n \n 401(k) \n 401(k) matching ", "techs": ["deep learning", "large language models (llms)", "mlops stack", "aws", "python", "sql", "401(k)"]}, "1100a2cf4254ef01": {"terms": ["mlops"], "salary_min": 84286.625, "salary_max": 106725.66, "title": "DevOps/Site Reliability Engineer", "company": "CertiPath Inc", "desc": "Want to energize your career?  Looking for stability with the freedom to innovate? At CertiPath you can have both while making the world more secure and simplifying access processes. We are seeking a \n     DevOps/Site Reliability Engineer  to support a talented team of dedicated professionals who create and deploy cutting-edge solutions to our clients. Join our team, make a difference, and crush your goals at CertiPath.\n    \n \n \n  I\u2019ve never heard of CertiPath. What do you do? \n \n \n \n  We are the experts in high-assurance digital identity verification and management services. We are an established organization with a 19-year track record of delivering on our promises with the drive and entrepreneurial spirit of a start-up. CertiPath is focused on bringing facility and network access management for federal agencies into the 21st century.\n    \n \n \n  What will the DevOps/Site Reliability Engineer \n do? \n \n \n \n  You\u2019ll be a key member of CertiPath\u2019s DevOps and Site Reliability Engineering (SRE) team, with day-to-day responsibility for continuous integration/continuous development (CI/CD) pipelines, environment management, critical network and host-level infrastructures, and cybersecurity infrastructures. You\u2019ll be key to the delivery of both general availability and customer-specific software that meet CertiPath\u2019s standards for design, functional capability, scalability and performance, user experience and usability, quality, and cybersecurity. In day-to-day work, you will proactively engage with team members, work in a fast-paced environment with a heavy focus on optimizing continuous integration and CI/CD pipeline deployments and automation, and take a hands-on approach to secure agile development processes.\n    \n \n \n  What does a typical day look like for the DevOps/Site Reliability Engineer? \n \n \n \n  Although no two days will be the same, you will:\n    \n \n \n \n  Actively engage as part of the CertiPath SRE team in building more robust systems and resilient infrastructures, identifying key performance indicators (KPIs), addressing reliability, redundancy, security, operational monitoring, and automation wherever practical and in support of identified business needs \n  Build or improve existing CI/CD pipelines and CertiPath\u2019s development, staging, and production environments to support Software Engineering (SWE) activities \n  Scale systems sustainably and improve existing systems by pushing for changes that improve reliability \n  Act as point of contact to support services, go-live activities, and IT infrastructure-related Tier IV production support processes, which may include system design consulting, developing software platforms and frameworks, capacity planning, and launch reviews \n  Continuously measure, monitor, and manage end-to-end availability, efficiency, and performance of critical services, build automation, applications, and systems to prevent problem recurrence \n  Independently work and learn new skills \n  Work cross-functionally with CertiPath\u2019s software engineering, product quality, and product support teams to architect and implement scalable solutions while operating within customer and regulatory expectations \n  Maintain current knowledge of relevant technologies, and perform research on alternate and emerging technologies \n  Exhibit excellent written and verbal communication skills with the ability to present complex technical information in a clear and concise manner to a variety of audiences \n \n \n \n  What qualifications do you look for? \n \n \n \n  You might be the DevOps/Site Reliability Engineer we\u2019re looking for if you have:\n    \n \n \n \n  Bachelor's Degree in Computer Science or related technical discipline, or the equivalent combination of education, technical certifications or training, and work experience \n \n \n  3+ years of experience as an SRE or DevOps engineer \n  Able to obtain a US Government clearance \n  Experience working with other engineers and applying strong software deployment and operations practices \n  Ability to make reasonable trade-offs based on time and resource constraints in the interest of our business and our customers \n  Experience working in Microsoft Windows server and desktop environments \n  Experience with best-of-class code repository and build automation tools, preferably Git and Jenkins \n  Experience supporting .NET application development (troubleshooting/code reviews & CI/CD) \n  Experience with relational databases such as MySQL/MariaDB, Postgres, and Microsoft SQL Server \n  Experience configuring, hardening, and monitoring web servers such as Apache, Nginx, and IIS \n  Proficiency with one or more scripting languages such as Python, Perl, PowerShell, or Bash \n  Possess excellent analytical skills, technical aptitude, and a proven ability to consistently solve complex problems \n  Experience building, operating, and scaling shared infrastructure across multiple clients with varying needs, and deploying products across global markets and user bases \n  Experience managing hypervisors in a virtualization environment \n  Experience with network storage protocols \n \n \n  We\u2019re extra impressed by folks with:\n    \n \n  Linux systems administration, configuration, troubleshooting and automation \n  Experience building CI/CD pipelines using tools like Jenkins, Terraform, Ansible, Puppet, Sonar, etc. \n  Familiarity with application development, containerization and deployment using Docker and Kubernetes in production cloud environments using tools such as Terraform, ArgoCD, DevSpace, and CloudFormation \n  Experience with deployments leveraging AWS technologies such as EKS, Lambda, Fargate, EC2, Aurora RDS, S3, KMS, and ElastiCache \n  Experience with Node and Python application development and packaging \n  Experience working with secure SDLC technologies and DevSecOps methodologies, including automating vulnerability and code quality scanning \n  Deploying client-authenticated TLS solutions in a cloud environment \n  Experience deploying, security hardening, and managing Microsoft Active Directory and Microsoft PKI services \n  Proficiency with cloud monitoring tools such as Nagios, Prometheus, Loki, Grafana, and Kibana \n  Experience in a Network Operations Center or datacenter environment, including server and infrastructure, cable management, and configuration best practices for lights-out system management \n  Experience with secure network management, administration, and configuration \n  Experience deploying and administering relational databases \n  Experience configuring and maintaining backup and DR solutions for virtual and bare metal systems \n  Experience working with public key infrastructure (PKI) standards and/or physical access control system (PACS) technologies \n  Experience building, deploying, and supporting FISMA and FedRAMP authorized production environments \n  Advanced certifications in agile methodologies, systems design and architecture, information security, or cybersecurity", "cleaned_desc": "  Act as point of contact to support services, go-live activities, and IT infrastructure-related Tier IV production support processes, which may include system design consulting, developing software platforms and frameworks, capacity planning, and launch reviews \n  Continuously measure, monitor, and manage end-to-end availability, efficiency, and performance of critical services, build automation, applications, and systems to prevent problem recurrence \n  Independently work and learn new skills \n  Work cross-functionally with CertiPath\u2019s software engineering, product quality, and product support teams to architect and implement scalable solutions while operating within customer and regulatory expectations \n  Maintain current knowledge of relevant technologies, and perform research on alternate and emerging technologies \n  Exhibit excellent written and verbal communication skills with the ability to present complex technical information in a clear and concise manner to a variety of audiences \n \n \n \n  What qualifications do you look for? \n \n \n \n  You might be the DevOps/Site Reliability Engineer we\u2019re looking for if you have:\n    \n \n   \n  Bachelor's Degree in Computer Science or related technical discipline, or the equivalent combination of education, technical certifications or training, and work experience \n \n \n  3+ years of experience as an SRE or DevOps engineer \n  Able to obtain a US Government clearance \n  Experience working with other engineers and applying strong software deployment and operations practices \n  Ability to make reasonable trade-offs based on time and resource constraints in the interest of our business and our customers \n  Experience working in Microsoft Windows server and desktop environments \n  Experience with best-of-class code repository and build automation tools, preferably Git and Jenkins \n  Experience supporting .NET application development (troubleshooting/code reviews & CI/CD) \n  Experience with relational databases such as MySQL/MariaDB, Postgres, and Microsoft SQL Server \n  Experience configuring, hardening, and monitoring web servers such as Apache, Nginx, and IIS \n  Proficiency with one or more scripting languages such as Python, Perl, PowerShell, or Bash \n  Possess excellent analytical skills, technical aptitude, and a proven ability to consistently solve complex problems \n  Experience building, operating, and scaling shared infrastructure across multiple clients with varying needs, and deploying products across global markets and user bases \n  Experience managing hypervisors in a virtualization environment    Experience with network storage protocols \n \n \n  We\u2019re extra impressed by folks with:\n    \n \n  Linux systems administration, configuration, troubleshooting and automation \n  Experience building CI/CD pipelines using tools like Jenkins, Terraform, Ansible, Puppet, Sonar, etc. \n  Familiarity with application development, containerization and deployment using Docker and Kubernetes in production cloud environments using tools such as Terraform, ArgoCD, DevSpace, and CloudFormation \n  Experience with deployments leveraging AWS technologies such as EKS, Lambda, Fargate, EC2, Aurora RDS, S3, KMS, and ElastiCache \n  Experience with Node and Python application development and packaging \n  Experience working with secure SDLC technologies and DevSecOps methodologies, including automating vulnerability and code quality scanning \n  Deploying client-authenticated TLS solutions in a cloud environment \n  Experience deploying, security hardening, and managing Microsoft Active Directory and Microsoft PKI services \n  Proficiency with cloud monitoring tools such as Nagios, Prometheus, Loki, Grafana, and Kibana \n  Experience in a Network Operations Center or datacenter environment, including server and infrastructure, cable management, and configuration best practices for lights-out system management \n  Experience with secure network management, administration, and configuration ", "techs": ["jenkins", "git", "mysql/mariadb", "postgres", "microsoft sql server", "apache", "nginx", "iis", "python", "perl", "powershell", "bash", "linux", "terraform", "ansible", "puppet", "sonar", "docker", "kubernetes", "aws", "eks", "lambda", "fargate", "ec2", "aurora rds", "s3", "kms", "elasticache", "node", "active directory", "pki", "nagios", "prometheus", "loki", "grafana", "kibana."]}, "b925ba55edff80fe": {"terms": ["mlops"], "salary_min": 130000.0, "salary_max": 140000.0, "title": "Senior DevOps/ Cloud Engineer", "company": "E-business International INC", "desc": "Job Title : Senior Dev Ops Engineer \n Location : 100% Remote \n Duration : Full Time \n Summary: \n Direct Client is looking for a  SENIOR DEV-OPS ENGINEER  in a Remote role who will apply their AWS DevOps experience architecting for multi-region environments, scripting, and automation (infrastructure as code), with a deep understanding of infrastructure architecture in AWS SaaS, PaaS, and IaaS. \n Responsibilities: \n \n Collaborate with Engineering Leadership to shape the vision of our next-generation platform and deployment pipeline; \n Ensure objectives and development plans are established and reviewed continuously throughout the year \n Lead technical design decisions across teams to ensure solutions meet business requirements and to ensure data and architectural designs are documented, consistent, maintainable, flexible, and cost-effective \n Maintain and expand Software Development skills, including identifying and evaluating new technologies that could benefit our platform \n \n Qualifications: \n \n REQUIRED: Bachelor\u2019s Degree in Computer Science, or technical/business discipline is \n 7+ years of overall experience in automation frameworks and supporting tools; writing code to automate the management of Infrastructure services; testing and deploying infrastructure templates with CI/CD automation \n 5+ years of experience in system architecture, scalability, reliability, and performance in a SaaS environment; including experience in testing frameworks (JEST, Cypress) \n Experience implementing AWS core services, including EC2, S3, RDS, Lambda, ECS, VPCs, SNS, SQS, etc. \n Strong knowledge of Infrastructure as Code (IaC) tools, such as Laravel Vapor, Terraform, and CloudFormation \n Experience in programming with at least one modern language such as PHP, Python, Javascript \n Experience in integrating static/dynamic code analysis framework and tools to minimize security risks \n Extensively used source code management systems like Bitbucket or, GitHub \n Experience in monitoring and mogging systems; Disaster Recovery (DR) and High Availability (HA) strategy in AWS using native services \n Understand HIPAA/HI-Tech/SOC requirements for Healthcare Software and ensure compliance \n Familiarity with collaboration tools like Jira, Confluence \n AWS certifications in related field is a big plus \n \n Job Types: Full-time, Permanent \n Salary: $130,000.00 - $140,000.00 per year \n Benefits: \n \n 401(k) \n 401(k) matching \n Dental insurance \n Health insurance \n \n Experience level: \n \n 10 years \n \n Schedule: \n \n 8 hour shift \n Day shift \n Monday to Friday \n \n Application Question(s): \n \n Only USC and GC can apply \n How many years of experience do you have in automation frameworks and supporting tools; writing code to automate the management of Infrastructure services; testing and deploying infrastructure templates with CI/CD automation? \n How many years of experience do you have in years of experience in system architecture, scalability, reliability, and performance in a SaaS environment; including experience in testing frameworks (JEST, Cypress)? \n How many years of experience you have in implementing AWS core services, including EC2, S3, RDS, Lambda, ECS, VPCs, SNS, SQS, etc.? \n How about your knowledge in Infrastructure as Code (IaC) tools, such as Laravel Vapor, Terraform, and CloudFormation? \n How many years of experience do you have in programming with at least one modern language such as PHP, Python, Javascript \n Comemnt about your Understanding of HIPAA/HI-Tech/SOC requirements for Healthcare Software and ensure compliance \n \n Education: \n \n Master's (Required) \n \n Experience: \n \n Laravel Vapor: 5 years (Required) \n AWS: 5 years (Required) \n DevOps: 10 years (Required) \n \n Work Location: Remote", "cleaned_desc": " \n REQUIRED: Bachelor\u2019s Degree in Computer Science, or technical/business discipline is \n 7+ years of overall experience in automation frameworks and supporting tools; writing code to automate the management of Infrastructure services; testing and deploying infrastructure templates with CI/CD automation \n 5+ years of experience in system architecture, scalability, reliability, and performance in a SaaS environment; including experience in testing frameworks (JEST, Cypress) \n Experience implementing AWS core services, including EC2, S3, RDS, Lambda, ECS, VPCs, SNS, SQS, etc. \n Strong knowledge of Infrastructure as Code (IaC) tools, such as Laravel Vapor, Terraform, and CloudFormation \n Experience in programming with at least one modern language such as PHP, Python, Javascript \n Experience in integrating static/dynamic code analysis framework and tools to minimize security risks \n Extensively used source code management systems like Bitbucket or, GitHub \n Experience in monitoring and mogging systems; Disaster Recovery (DR) and High Availability (HA) strategy in AWS using native services \n Understand HIPAA/HI-Tech/SOC requirements for Healthcare Software and ensure compliance \n Familiarity with collaboration tools like Jira, Confluence \n AWS certifications in related field is a big plus   \n Schedule: \n \n 8 hour shift \n Day shift \n Monday to Friday \n \n Application Question(s): \n \n Only USC and GC can apply \n How many years of experience do you have in automation frameworks and supporting tools; writing code to automate the management of Infrastructure services; testing and deploying infrastructure templates with CI/CD automation? \n How many years of experience do you have in years of experience in system architecture, scalability, reliability, and performance in a SaaS environment; including experience in testing frameworks (JEST, Cypress)? \n How many years of experience you have in implementing AWS core services, including EC2, S3, RDS, Lambda, ECS, VPCs, SNS, SQS, etc.? ", "techs": ["bachelor\u2019s degree in computer science", "laravel vapor", "terraform", "cloudformation", "php", "python", "javascript", "bitbucket", "github", "jira", "confluence", "aws certifications"]}, "818132163ee251b5": {"terms": ["mlops"], "salary_min": 100000.0, "salary_max": 135000.0, "title": "REMOTE- Mid-Level DevOps Engineer (BP)", "company": "Sigma Defense [SOLUTE]", "desc": "Sigma Defense Systems  is a leading technology company serving the Department of Defense (DoD), providing tactical communications systems and services for digital modernization since 2006. Through our acquisitions of SOLUTE in January 2022 and Sub U Systems in May 2022, we have expanded our software and communications hardware solutions to better support JADC2, C5ISR, SATCOM, and DEVSECOPS for customers in the Army, Navy, Air Force, Marine Corps, and Space Force. Through a combination of hardware, software, and industry expertise, we provide a complete portfolio of solutions and services that accelerates information collection and sharing for faster decision making and better mission outcomes. \n  We are a company of innovative professionals thriving in a highly motivating work environment that fosters creativity and independent thinking. If you are a motivated individual with a desire to support our service men and women, now is a great time to join Sigma Defense! \n  Why would you work for us? Quite simply, the work we do is meaningful and stimulating. We promote initiative and independent thought; we encourage direct client engagement to ensure we are delivering what the customer wants; and our engineers and scientists are working on cutting-edge projects that move the state-of-the-art closer to the people who need them. If you're looking for technical challenges and an opportunity to take a leadership role in an environment that encourages you to excel, then WE are your destination. \n  Want to know what it's like to work at Sigma?  Find out what our employees are saying. \n  Discover Your Next Mission with Sigma Defense  - Find and follow us at Sigma Defense Systems LLC: Overview | LinkedIn and visit Sigma Defense | A Leading Technology Company for more information.  \n Equal Opportunity Employer/Veterans/Disabled:  Sigma Defense Systems is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability. \n \n \n \n Sigma Defense is seeking a platform engineer who is enthusiastic about learning and contributing ideas to multiple teams. As a member of the team, the  Mid-Level DevSecOps Engineer  will help accelerate software modernization efforts through platform engineering in unique environments and help instill good DevSecOps practices across the industry. An ideal candidate will bring flexible and humble mindset to the team, and willingness to take on new and unique challenges. As this is a fully remote position, embracing the remote-first culture with the understanding of responsibility for intentional communication as part of daily activity is critical for success. \n  Requirements \n \n Must be a citizen of the United States. \n At minimum 2+ years of experience in DevOps role.  \n Candidate must have experience in a customer service capacity as they will be working directly with customers.  \n Excellent interpersonal and communication skills are needed as providing follow-ups and reports to the customer is required.  \n Candidate must have a customer-first attitude. \n \n Personnel Clearance Level: \n \n Candidate must possess or have the ability to obtain an active, DoD issued Secret security clearance.  A clearance will be sponsored for the right candidate, but employment is contingent upon their ability to successfully obtain a clearance.   \n Candidates with an active DoD issued Secret security clearance, or higher, are preferred. \n \n Education Requirements: \n \n  High School Diploma or GED \n \n  Software/Programs Experience: \n \n Kubernetes experience is critical (operational and developmental.) \n GitOps \n Terraform > VMWare, Azure, AWS \n Helm or Kustomize \n Agile \n Docker \n Linux \n \n Candidate Differentiators: \n \n \n Familiarity with code security and static analysis tools. \n Experience with Git-based infrastructure as code development processes. \n Familiarity with Agile Manifesto. \n Experience documenting processes and workflows with attention to detail. \n Experience with:\n    \n Gitlab CI/CD, Terraform, ArgoCD, Sonarqube, Anchore, Fortify, or, \n a generalized background in CI/CD, static code analysis, and Git infrastructure. \n \n \n Essential Job Duties (Not All-Inclusive): \n \n Contribute to daily Standup meetings. \n Development and maintenance of Infrastructure as Code. \n Development and maintenance of Helm and Kustomize deployments. \n Contribute to individual and team growth, skills, and culture. \n Contribute ideas for continuous improvement. \n \n Salary Range:  $100,000 - $135,000 annually.  \n Benefits \n \n Dental and Vision Insurance \n Medical Insurance to Include an HSA Plan and HRA Plan Which Features a $6,000 Health Reimbursement \n Life and A&D coverage \n Employee Assistance Program (EAP) \n 401(k) Plan with Company Matching Contributions \n 160 Hours of Paid Time Off (PTO) with Carry-Over up to 240 hours \n 12 (Floating) Holidays \n Educational Assistance \n Highly Competitive Salary \n Flexible Schedule", "cleaned_desc": " \n Kubernetes experience is critical (operational and developmental.) \n GitOps \n Terraform > VMWare, Azure, AWS \n Helm or Kustomize \n Agile \n Docker \n Linux \n \n Candidate Differentiators: \n \n \n Familiarity with code security and static analysis tools. \n Experience with Git-based infrastructure as code development processes. ", "techs": ["kubernetes", "gitops", "terraform", "vmware", "azure", "aws", "helm", "kustomize", "agile", "docker", "linux", "code security and static analysis tools", "git-based infrastructure as code development processes"]}, "metadata": {"keywords": ["data science", "data analyst", "data engineer", "machine learning engineer", "mlops"], "locations": ["remote"], "time_ran": "15:00:17-09-09-23", "num_jobs": 7479, "timings": {"start_drivers": 44.324976682662964, "find_job_ids": 439.4420838356018, "get_job_descs": 174.21183109283447}, "models": {"classifier": {"clf": "data/classifier_models/job_desc_classifier_v1.0.pkl", "tfidf": "data/classifier_models/job_desc_tfidf_vectorizerv1.0.pkl"}, "NER": "gpt-3.5-turbo"}}}