{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were no techs in 9db0afe11e3a01c3 (term: data science)\n",
      "There were no techs in 9db0afe11e3a01c3 (term: data analyst)\n",
      "There were no techs in 9db0afe11e3a01c3 (term: data engineer)\n",
      "There were no techs in 72ab352f5ca4ab31 (term: mlops)\n",
      "There were no techs in 7e48c4e07c4d7ce2 (term: mlops)\n",
      "There were no techs in 940d20ee4c5e7c69 (term: data science)\n",
      "There were no techs in 940d20ee4c5e7c69 (term: machine learning engineer)\n",
      "There were no techs in 3e3c45e519024fa4 (term: mlops)\n",
      "There were no techs in 1d0964ec4fc36ff2 (term: data science)\n"
     ]
    }
   ],
   "source": [
    "test = defaultdict(Counter)\n",
    "with open(fr\"data/p-raw_data-17-09-23-small.json\") as f:\n",
    "    data = json.load(f)\n",
    "    for key in list(data.keys()):\n",
    "        if key.startswith(\"metadata\"):\n",
    "            continue\n",
    "        else:\n",
    "            for term in data[key]['terms']:\n",
    "                # print(term)\n",
    "                # if term in test: #test[term] already exists, keep counting\n",
    "                #     continue\n",
    "                # else:\n",
    "                #     test[term] = 0 #init the search term to the test dict then keep counting\n",
    "                \n",
    "                # print(test[term])\n",
    "                try:\n",
    "                    for tech in data[key]['techs'][0].split(\"\\n\"):\n",
    "                        clean_tech = tech.strip()\n",
    "                        clean_tech = clean_tech.replace(\"-\", \"\")\n",
    "                        if tech in test[term]:\n",
    "                            test[term][clean_tech] += 1\n",
    "                        else:\n",
    "                            test[term][clean_tech] = 1\n",
    "                except IndexError:\n",
    "                    print(f\"There were no techs in {key} (term: {term})\")\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'b376020741b70ef0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Zade\\Desktop\\PythonStuff\\jobReqs\\jobReqs\\check_results.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/check_results.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data[\u001b[39m'\u001b[39;49m\u001b[39mb376020741b70ef0\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mtechs\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'b376020741b70ef0'"
     ]
    }
   ],
   "source": [
    "data['b376020741b70ef0']['techs'][0].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data engineer',\n",
       " 'data science',\n",
       " 'machine learning engineer',\n",
       " 'mlops',\n",
       " 'data analyst']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tensorflow', 'python', 'monitor system performance', 'sql']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test['data science'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine learning'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['c69e4efd30d464dd']['techs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anomaly detection'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['c69e4efd30d464dd']['techs'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tech: machine learning\n",
      "Tech: anomaly detection\n",
      "Tech: prediction of p1s\n",
      "Tech: capacity planning\n",
      "Tech: data analytics\n",
      "Tech: azure\n",
      "Tech: cloudera cdp\n",
      "Tech: ansible\n",
      "Tech: puppet\n",
      "Tech: terraform\n",
      "Tech: jenkins\n",
      "Tech: docker\n",
      "Tech: kubernetes\n",
      "Tech: continuous integration\n",
      "Tech: continuous deployment\n",
      "Tech: jenkins\n",
      "Tech: spark\n",
      "Tech: kafka\n",
      "Tech: rabbitmq\n",
      "Tech: impala\n",
      "Tech: kudu\n",
      "Tech: redis\n",
      "Tech: hue\n",
      "Tech: kerberos\n",
      "Tech: tableau\n",
      "Tech: grafana\n",
      "Tech: mariadb\n",
      "Tech: prometheus\n",
      "Tech: hadoop systems administration\n",
      "Tech: hadoop/big data ecosystem\n",
      "Tech: hive\n",
      "Tech: spark streaming\n",
      "Tech: hdfs\n",
      "Tech: yarn\n",
      "Tech: hbase\n",
      "Tech: ci/cd pipelines\n",
      "Tech: native cloud\n",
      "Tech: azure\n",
      "Tech: aws\n",
      "Tech: sentry\n",
      "Tech: ranger\n",
      "Tech: ldap\n",
      "Tech: kerberos kdc\n",
      "Tech: linux systems administration.\n"
     ]
    }
   ],
   "source": [
    "for tech in data['c69e4efd30d464dd']['techs']:\n",
    "    print(f\"Tech: {tech}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine learning',\n",
       " 'anomaly detection',\n",
       " 'prediction of p1s',\n",
       " 'capacity planning',\n",
       " 'data analytics',\n",
       " 'azure',\n",
       " 'cloudera cdp',\n",
       " 'ansible',\n",
       " 'puppet',\n",
       " 'terraform',\n",
       " 'jenkins',\n",
       " 'docker',\n",
       " 'kubernetes',\n",
       " 'continuous integration',\n",
       " 'continuous deployment',\n",
       " 'jenkins',\n",
       " 'spark',\n",
       " 'kafka',\n",
       " 'rabbitmq',\n",
       " 'impala',\n",
       " 'kudu',\n",
       " 'redis',\n",
       " 'hue',\n",
       " 'kerberos',\n",
       " 'tableau',\n",
       " 'grafana',\n",
       " 'mariadb',\n",
       " 'prometheus',\n",
       " 'hadoop systems administration',\n",
       " 'hadoop/big data ecosystem',\n",
       " 'hive',\n",
       " 'spark streaming',\n",
       " 'hdfs',\n",
       " 'yarn',\n",
       " 'hbase',\n",
       " 'ci/cd pipelines',\n",
       " 'native cloud',\n",
       " 'azure',\n",
       " 'aws',\n",
       " 'sentry',\n",
       " 'ranger',\n",
       " 'ldap',\n",
       " 'kerberos kdc',\n",
       " 'linux systems administration.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['c69e4efd30d464dd']['techs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Machine Learning, Anomaly detection and Prediction of P1s \\n  Capacity planning \\n  Data analytics and deriving Actionable Business Insights \\n \\n \\n  What you get to do in this role \\n \\n  Responsible for deploying, production monitoring, maintaining and supporting of Big Data infrastructure, Applications on ServiceNow Cloud and Azure environments. \\n  Architect and drive the end-end Big Data deployment automation from vision to delivering the automation of Big Data foundational modules (Cloudera CDP), prerequisite components and Applications leveraging Ansible, Puppet, Terraform, Jenkins, Docker, Kubernetes to deliver end-end deployment automation across all ServiceNow environments. \\n  Automate Continuous Integration / Continuous Deployment (CI/CD) data pipelines for applications leveraging tools such as Jenkins, Ansible, and Docker. \\n  Performance tuning and troubleshooting of various Hadoop components and other data analytics tools in the environment: HDFS, YARN, Hive, HBase. Spark, Kafka, RabbitMQ, Impala, Kudu, Redis, Hue, Kerberos, Tableau, Grafana, MariaDB, and Prometheus. \\n  Provide production support to resolve critical Big Data pipelines and application issues and mitigating or minimizing any impact on Big Data applications. Collaborate closely with Site Reliability Engineers (SRE), Customer Support (CS), Developers, QA and System engineering teams in replicating complex issues leveraging broad experience with UI, SQL, Full-stack and Big Data technologies.    Responsible for enforcing data governance policies in Commercial and Regulated Big Data environments. \\n \\n \\n  Qualifications \\n  To be successful in this role you have: \\n \\n  4+ years experience in Hadoop Systems Administration with Cloudera \\n  Deep understanding of Hadoop/Big Data Ecosystem. \\n  Good knowledge in Querying and analyzing large amounts of data on Hadoop HDFS using Hive and Spark Streaming and working on systems like HDFS, YARN, Hive, HBase. Spark, Kafka, RabbitMQ, Impala, Kudu, Redis, Hue, Tableau, Grafana, MariaDB, and Prometheus. \\n  Experience supporting CI/CD pipelines on Cloudera on Native cloud and Azure/AWS environments \\n  Experience securing Hadoop stack with Sentry, Ranger, LDAP, Kerberos KDC \\n  Strong Linux Systems Administration skills '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['c69e4efd30d464dd']['cleaned_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
