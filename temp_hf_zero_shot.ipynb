{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must ensure that the labels used are the cleaned ones after check_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zade\\Desktop\\PythonStuff\\jobReqs\\jobReqs\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fr\"data/p-raw_data-22-10-23.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sap bi', 'sap basis', 'sap bi roles']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['0aa33d3c84db9848']['techs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0ff5fd9855b6a27d ['analytical thinking', 'communication skills', 'curiosity', 'prototype culture', 'problem solving', 'collaboration', 'technical solutions', 'templates', 'data integration', 'data science', 'data analytics', 'data engineering', 'data visualization', 'best practices', 'data sources', 'feature and model evaluation', 'data transformation', 'data cleansing', 'data organization', 'data standardization', 'machine learning', 'neural networks', 'data visualization', 'statistical exploratory data analysis', 'analytics programming languages', 'frameworks', 'visualization tools', 'complex data analysis', 'time management', 'relational databases', 'advanced sql techniques', 'feature discovery', 'statistical modeling', 'machine learning algorithms', 'natural language processing technologies', 'deep learning technologies', 'python data analysis frameworks', 'data visualization tools', 'imbalanced data', 'git version control', 'automl tools', 'nosql databases', 'graph databases', 'json', 'protocol buffers', 'windows operating systems', 'cloud environment', 'aws']\n",
      "1fcbce8d8b3782e6 ['cloudformation', 'python', 'git', 'linux', 'docker', 'cicd']\n",
      "bd3d3b81ebfe258f ['azure', 'aws', 'gcp', 'snowflake', 'databricks', 'tensorflow', 'pytorch', 'scikit-learn', 'mlflow', 'python', 'sql', 'linux']\n",
      "680d271d91230548 ['product management', 'ai', 'product team', 'roadmap', 'customer empathy', 'customer needs', 'cross-functional product initiatives', 'strategic thinking', 'problem solving', 'logical structuring', 'ambiguity', 'hypotheses', 'ai integration in customer support']\n",
      "5f03dde9bb640d7c ['powershell', 'python', 'amazon web services (ec2', 'cloud formation', 'codedeploy', 'lambda functions', 'salt)', '.net framework', 'c#', 'azure', 'pipelines', 'devops services', 'sql server', 'oracle', 'postgres', 'multidimensional databases', 'network protocols', 'network security', 'ssl', 'pkix', 'routing', 'load balancing']\n",
      "90ca40677b46e6ea ['mayo clinic', 'u.s. news & world report', 'google cloud platform', 'gcp shared services', 'google healthcare api', 'big query', 'hl7 fhir store', 'jenkins', 'github actions', 'azure pipelines', 'jira', 'github', 'sharepoint', 'azure boards', 'apache spark', 'hive', 'airflow', 'kafka', 'gcp dataflow', 'google bigquery', 'fhir apis', 'vertex ai', 'kubernetes', 'docker', 'cloud dataflow', 'cloud storage', 'pub/sub', 'cloud composer', 'python', 'java', 'spark', 'airflow', 'kafka', 'rdbms', 'unix shell scripting', 'json', 'xml', 'jenkins', 'git', 'powerbi', 'looker', 'tableau', 'deltalake', 'gcp dataplex']\n",
      "32dd969adaa2f18f ['analytics programming languages', 'frameworks', 'visualization tools', 'relational databases', 'advanced sql techniques', 'feature discovery', 'engineering', 'statistical modeling', 'classification and regression machine learning algorithms', 'natural language processing technologies', 'deep learning technologies', 'python data analysis frameworks', 'pandas', 'seaborn/matplotlib', 'jupyter', 'keras/tensorflow', 'imbalanced data', 'git version control', 'automl tools', 'pycaret', 'nosql databases', 'graph databases', 'json', 'protocol buffers', 'windows operating systems', 'linux', 'virtual machines', 'cloud environment', 'aws']\n",
      "41d68c99e5711dd6 ['sql', 'python', 'excel', 'tableau', 'power bi', 'microstrategy', 'big query', 'azure sql', 'ssms']\n",
      "8a2de8bffa0e147b ['tableau', 'power bi', 'six sigma belt certification', 'pmp certification', 'lean', 'six sigma']\n",
      "f6a2f4a5b893251b ['distributed computing', 'ml development lifecycle', 'machine learning', 'models', 'cloud infrastructure', 'data pipelines', 'ml frameworks', 'xgboost', 'pytorch', 'aws sagemaker', 'cloud computing services', 'aws', 'software engineering teams', 'data-intensive multi-line business environment', 'data pipelines', 'data architecture', 'distributed computing infrastructure/platforms']\n",
      "743e7da975150baa ['mayo clinic', 'u.s. news & world report', 'oracle reporting and analytics (r&a) solutions', 'otbi', 'bi publisher', 'oracle analytics', 'fusion analytics warehouse (faw)', 'information technology', 'finance', 'hr', 'supply chain', 'harwick project implementation partner', 'oracle fusion', 'r&a modules', 'vendor tools', 'end-to-end r&a solutions', 'regulatory compliance', 'faqs', 'data governance', 'reference data management', 'data standards', 'sql', 'oracle cloud financial (fin)', 'hr (hcm) system', 'supply chain (scm) modules', 'oracle data lineage', 'fusion analytics warehouse (faw)', 'oracle autonomous data warehouse (adw)', 'epm', 'oracle analytics cloud (oac)', 'oracle data integrator (odi)']\n",
      "b6322345d39699fd ['python', 'r', 'scala', 'sql', 'snowflake', 'databricks', 'mlflow', 'neo4j', 'airflow']\n",
      "33ea0afe08e72de9 ['databricks e2', 'mlflow', 'python', 'r', 'scala', 'snowflake', 'sql', 'snowflake', 'neo4j', 'airflow', 'nlp']\n",
      "77fdd56440089809 ['google forms', 'microsoft forms', 'qualtrics', 'ms visio', 'lucidchart', 'omnigraffle', 'miro', 'powerpoint', 'google slides', 'sharepoint']\n",
      "973ba5800f26b081 ['surveys', 'focus group interviews', 'observational checklists', 'cleaning', 'joining', 'updating', 'refining', 'quality assurance activities', 'analysis of data', 'linking experience data', 'key business insights', 'presentations', 'survey design', 'measurement best practices', 'sampling methods', 'descriptive statistical techniques', 'predictive statistical techniques', 'data stories/narratives', 'visualizations', 'drawing inferences', 'key findings', 'recommendations', 'microsoft office', 'attitudes', 'behaviors', 'engagement surveys', 'exit surveys', 'project management', 'attention to detail', 'verbal communication skills', 'written communication skills', 'microsoft excel', 'statistical programming languages (r', 'python)', 'business intelligence/visualization tools (power bi', 'tableau)']\n",
      "9f88fc03766cb72e ['data analysis tools', 'software', 'quantitative data sets', 'qualitative data sets', 'models', 'publications', 'regulations', 'guidelines', 'business rules', 'reporting requirements', 'standards', 'data output', 'trends', 'patterns', 'business concepts', 'statistical concepts', 'actuarial concepts', 'credibility', 'simulation', 'frequency', 'severity']\n",
      "6efe3e4de9ab2fef ['java', 'python', 'scala', 'go', 'apache hadoop', 'spark', 'snowflake', 'aws', 'datadog', 'sql', 'jenkins', 'databricks']\n",
      "ce672ce8edd2c985 ['kotlin', 'java', 'scala', 'kafka', 'azure service bus', 'zeromq', 'mysql', 'cassandra']\n",
      "1ddd50c44d4f551d ['git', 'docker', 'container management', 'git based development workflows', 'devops platforms', 'ci infrastructure', 'c++', 'python', 'ansible', 'container orchestration', 'task automation', 'software observability technologies', 'jira', 'scalability', 'monitoring', 'security', 'performance engineering', 'production-grade software', 'deployments at scale', 'startup experience']\n",
      "6c7526d0e31f8099 ['java', 'spring boot', 'spring security', 'spring cloud', 'aws', 'paas', 'laas', 'saas', 'node js', 'angular', 'react js', 'webassembly', 'javascript', 'typescript', 'jquery', 'hibernate', 'git', 'uml', 'oracle', 'mysql', 'domain driven design (ddd)', 'test driven development (tdd)', 'continuous integration (cl)', 'cloud computing', 'gaia', 'jules', 'j2ee', 'web services', 'soap', 'rest', 'api gateway development', 'hadoop', 'python', 'express js', 'vue js', 'mango db', 'docker', 'kubernetes', 'artificial intelligence (al)', 'machine learning (ml)', 'service now', 'datadog', 'splunk', 'openshift', 'xlr', 'groovy pipelines', 'dtc', 'ibm mq', 'kafka', 'ibm data power', 'jenkins', 'glassbox', 'git', 'atlassian jira', 'sailpoint', 'fiserv/firstdata apis', 'transmit (security orchestrator)', 'threat metrix', 'lexisnexis products', 'pki', 'cryptography', 'http(s)', 'ssl/tls', 'certificates']\n",
      "38d6168912e94b9e ['airflow', 'glue', 'spark/emr', 'kinesis', 'kafka', 'lambda', 'api gateway', 'terraform', 'github', 'aws', 'unix', 'python', 'sql', 'aws sagemaker', 'jira']\n",
      "b2ed92266dbaa124 ['aws', 'python', 'perl', 'shell-scripting', 'troubleshooting', 'analytical skills', 'performance monitors', 'custom scripts', 'tuning techniques', 'team collaboration', 'high-availability systems', 'networking', 'security', 'application load balancing', 'communication skills', 'interpersonal skills.']\n",
      "69f4c84cd9749821 ['java', 'linux systems', 'shell scripting', 'javascript', 'python', 'hadoop', 'nosql databases', 'aws', 'unified platform (up) big data platform', 'json', 'xml', 'csv formats', 'agile software development methodologies', 'jira', 'confluence', 'github enterprise']\n",
      "d75c8f1633b38759 ['azure devops (ado)', 'github enterprise', 'gitlab', 'terraform', 'ansible', 'lambda', 'eks', 'windows server', 'gitflow', 'trunk based development']\n",
      "241a80b3883d2e12 ['adobe analytics™ clickstream', 'doubleclick™', 'python', 'sql', 'mlops', 'pandas', 'airflow', 'git™', 'bigquery', 'ml engine', 'jira', 'confluence']\n"
     ]
    }
   ],
   "source": [
    "for key in data:\n",
    "    if key.startswith('meta'):\n",
    "        continue\n",
    "    else:\n",
    "        if len(data[key]['techs']) > 5:\n",
    "            print(key, data[key]['techs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_jd = data['b6322345d39699fd']['cleaned_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Perform testing and quality control on analytic models to ensure a smooth production release using Python, R, Scala, etc.  \\n Conducting research, requirements design and validation of new Analytic products.  \\n Properly document code, artifact description, implementation, and use cases  \\n Provides data analytic support to client and leadership teams with reports, analytics, and with other ad hoc requests.  \\n Understand the current reporting process and work out ways to optimize and enhance regular reports.  \\n Coordinate with different functional teams to help implement models and monitor outcomes  \\n Assists modelers with data-related questions to ensure models run efficiently and use the correct data  \\n Triages, investigates, and helps resolve data issues  \\n Customer and end user support and communication  \\n Creation of training videos for the Analytic platform  \\n \\n \\n Qualifications \\n \\n \\n Required Qualifications:    \\n \\n Bachelor's degree with 0-2 years of experience OR HS diploma with 6-8 years of experience  \\n Demonstrated experience with Programming in Python, R, SnowFlake, DataBricks  \\n Experience with SQL  \\n Database experience  \\n Analytics experience  \\n Requirements design and validation experience  \\n Must be a US citizen and able to obtain a Public Trust security clearance  \\n \\n \\n Preferred Qualifications:  \\n \\n \\n Demonstrated experience with Programming Databricks E2, MLFlow, Neo4j, geopspatial model development, Airflow  \\n Demonstrated experience with NLP  \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_jd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word based tokenization:\n",
    "Similar words have different meanings.  \n",
    "Vocab can end up large, very heavy model - large # weights.  can ignore words to limit amount.  e.g. take top 10,000 most common words could be done.  OOV words are lost.\n",
    "Not great for me as we want it to learn previously unknown techs.\n",
    "\n",
    "Character based tokenization:\n",
    "num. characters is fairly low, maybe around 256 for all letters, numbers, special characters in english.  chinese could get up to 20,000 but we will focus on english\n",
    "OOV tokens are less frequent, can tokenize misspelt words.  \n",
    "but characters hold less info than words themselves in english.  must use lots of tokens at once, lowers the amount of context that can be used at once.\n",
    "Can still be decent, not like I need a LOT of context for mine.\n",
    "\n",
    "Subword based tokenization:\n",
    "Middle ground between word and character based.  \n",
    "Freq. words should not be split int osmaller subwords.\n",
    "rare words should be decomposed into meaningful subwords\n",
    "\n",
    "Other techs exist.  e.g. BERT uses WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer pipeline\n",
    "\n",
    "\n",
    "Raw text --> tokens --> special tokens (e.g. start of sentence, etc) --> input IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = autoTokenizer.from_pretrained(\"model to use\")\n",
    "tokens = tokenizer.tokenize(\"text example\")\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens) \n",
    "\n",
    "is the same as\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model to use\")\n",
    "inputs = tokenizer(\"text example\")\n",
    "\n",
    "decode them with tokenizer.decode(inputs) # where inputs is something like [635, 37586, 2342, ...], our tokenized text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch inputs together\n",
    "\n",
    "Inputs will not usually have the exact same length.  Must make this so for the tensors as they must all be rectangular.  Do this by padding and truncating.  Truncating is only done when sentence length is longer than max length allowed by model, as otherwise you lose info.\n",
    "\n",
    "Models are pretrained with a specific character by model.  Find with:\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token_id # this is the character used by the model to pad.\n",
    "\n",
    "Attention layers look at padding tokens.  To fix this, tell attention layer to ignore tokens marked with the pad_token_id.  Do this via an attention mask.  \n",
    "All done behind the scenes by the tokenizer with padding=True\n",
    "\n",
    "For facebook/bart-large-mnli the max model length is 1024.\n",
    "\n",
    "##For batch tokenization, can pad to the max batch size rather than the max length in the entire dataset saving a lot of time/cost!\n",
    "This is called dynamic padding. Do this via a  DataCollatorWithPadding(tokenizer)\n",
    "Note this may not work on all accelerators.  Never works on a TPU.\n",
    "\n",
    "Can do a lot of this together with the .to_tf_dataset() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning with keras best with a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
