{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import openai\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from random import sample, seed\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_ner = nlp.add_pipe(\"ner\", name=\"custom_ner\")\n",
    "custom_ner.add_label(\"TECHNOLOGY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before putting anything in to the openai api\n",
    "Set up a binary classifier to identify if a given paragraph has any relevance to the task.  I.e. if it contains any technologies/skills that I might want to take out.  \n",
    "### First step is to grab a bunch of paragraphs from job descriptions, label a few hundred as relevant or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_desc_list = []\n",
    "with open(fr\"data/raw_data-09-09-23.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_desc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(data.keys()):\n",
    "    if key.startswith(\"metadata\"):\n",
    "        continue\n",
    "    else:\n",
    "        job_desc_list.append(data[key]['desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_desc_list)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not many jobs in total here, randomly sample half of them to label. set seed for reproducability\n",
    "seed(10) \n",
    "jobs_to_label = sample(job_desc_list, int(len(job_desc_list)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(jobs_to_label[85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_to_nths(text, nths):\n",
    "    \"Splits the text roughly into 'n'ths e.g. if n=10, to 10ths\"\n",
    "    n_nlines = text.count('\\n')\n",
    "    num_to_split = n_nlines // nths #number of newlines to split at to get roughly 10ths in each text output\n",
    "    split_text = re.findall(\"\\n\".join([\"[^\\n]+\"]*num_to_split), text)\n",
    "    return [(x.strip(), \" \") for x in split_text]  # leave a space after each one for a 0/1 label for binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Overview: \\n  \\n   Amyx is seeking a Zero Trust Security, Orchestration, Automation and Response (SOAR) Cyber Security Engineer (CSE) to support our TRANSCOM J6 SISO contract located at Scott AFB, IL\\n  \\n \\n   In this role, the SOAR CSE will bring the background and experience collecting and analyzing data, and developing/implementing automated solutions that trigger worflows, alerts and other actions based on the results of data analysis. Data collected and analyzed will be retrieved from tools, applications and devices that comprise on-premises and cloud-based network environments. The ideal candidate will work collaboratively with our team and our customer to assess existing data collection and automation capabilities, examine other opportunities for automation, and increase the value of collected such that it can be used to further automation capabilities and provide additional insight into the overall security posture of network environments and applications running in those environments. With regards to Zero Trust, this position will require the candidate to become the resident expert of the \"Visibility & Analytics\" pillar of capabilities and activities that are a key part of overall zero trust efforts.\\n   Responsibilities: \\n  \\n Determine and document the data that can be exported/extracted from the tools and applications that comprise a zero trust network architecture \\n Conduct research on and become familiar with a variety of Security, Orchestration, Automation and Response (SOAR) solutions \\n Understand how machine learning and artificial intelligence tools and processes can be used to enhance automated decision making \\n Identify and implement opportunities to enrich data with the inclusion of relevant data from approved external sources \\n Understand the input and output of in-place and planned Security Information and Event Management (SIEM) tools and how such data might be used to trigger automation actions and workflows \\n Develop the actions and workflows that occur when data indicates conditions have been met that trigger the action or workflow \\n Understand how raw data formats can be extracted from other tools, parsed and converted so it can be ingested and used by SOAR solutions \\n Recommend SOAR solutions that provide the most benefit to the overall zero trust goals and objectives and enhance patch management/incident response capabilities \\n Develop, test and deploy data filters and algorithms that correlate incidents and events with data from different data sources \\n Develop, test and deploy data filters and algorithms that identify high-severity concerns identified through data analysis and prioritizes them for remediation/mitigation \\n Develop and implement data mapping strategies to ensure ingested data is translated into the fields and expected formats required by SOAR tools and supports up-channel reporting \\n Develop, test, and deploy data filters and algorithms that identify trends and anomalies within data sets \\n Use SOAR tools to ingest and analyze user and device behavioral data to highlight behaviors that are outside of the norm \\n Work with security analysts to identify points of interest that may be analyzed to enhance SIEM and incident response capabilities  \\n Develop data-driven charts, graphs and infographics that illustrate the status of high-interest and high-severity subjects \\n Examine/Explore new sources of useful raw data to include queries of endpoint agents, API calls, device and application logs, etc... \\n \\n \\n \\n  Technology used:\\n  \\n \\n   Zero Trust, NiFi, Elastic, Splunk, Kafka\\n   Qualifications: \\n  \\n   Required:\\n  \\n \\n BS degree in one of the following: computer engineering, computer science, IT, cyber security, or a related field.  \\n Relevant years of experience may be used in substitution for situations where the candidate does not have the required Bachelr\\'s degree.  \\n Possess a Certified Information Systems Security Professional (CISSP) or similar IA III certification and \\n Possess DoD 8570/8140 CSSP Infrastructure Support certification \\n Active Secret Clearance. \\n At least 5 years of experience in Data science, cybersecurity, information technology, or related fields.  \\n Expertise in large-scale data collection, transformation, correlation, and analysis. \\n Experience developing, implementing and fine-tuning automation workflows/playbooks.  \\n Knowledge of EO 14028, Federal, DoD, and other Zero Trust efforts and initiatives.  \\n Experience identifying trends, anomalies, risk and issues of concern within data sets.  \\n Excellent communication, collaboration, and problem-solving skills.  \\n Ability to work independently and as part of a team \\n Understanding of key IT/Cybersecurity concepts (network security, security operations & administration, incident response & recovery, identity and access management, vulnerability management, zero trust). \\n Experience with SIEM, SOAR and endpoint protection tools. \\n Understanding of Intrusion Detection and Prevention Systems (IDS, IPS) and Data Loss Prevention (DLP). \\n Understanding of Unclassified and Classified Government networks. \\n Experience supporting RMF/NIST/FedRAMP accreditation and assessment of systems. \\n Experience implementing STIG requirements for Microsoft and Linux operating systems, services and applications. \\n Familiarity with DevSecOps concepts and approaches. \\n Experience with data technologies such as NiFi, Elastic, Splunk and Kafka. \\n \\n \\n \\n  Desired:\\n  \\n \\n BS degree and at least 10 years cybersecurity experience  \\n Demonstrated knowledge and understanding of the USTRANSCOM mission \\n \\n \\n \\n  Benefits include:\\n  \\n \\n  Medical, Dental, and Vision Plans (PPO & HSA options available) \\n  Flexible Spending Accounts (Health Care & Dependent Care FSA) \\n  Health Savings Account (HSA) \\n  401(k) with matching contributions \\n  Roth \\n  Qualified Transportation Expense with matching contributions \\n  Short Term Disability \\n  Long Term Disability \\n  Life and Accidental Death & Dismemberment \\n  Basic & Voluntary Life Insurance \\n  Wellness Program \\n  PTO \\n  11 Holidays \\n  Professional Development Reimbursement \\n \\n \\n \\n  Please contact talent@amyx.com with any questions!\\n  \\n \\n \\n  Amyx is an Equal Opportunity employer. Amyx is committed to providing equal employment opportunity to all job seekers. Every qualified applicant receives focused consideration for employment and no one is discriminated against on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status. In addition to federal law requirements, Amyx complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Equal Opportunity Employer- Minorities/Females/Veterans/Individuals with Disabilities/Sexual Orientation/Gender Identity. Amyx is an E-Verify employer.\\n   \\n  Amyx proudly and proactively takes affirmative action to advance employment of individuals who are minorities, women, protected veterans and individuals with disabilities.\\n   \\n \\n \\n \\n Physical Demands \\n \\n  Employee needs to be able to sit at a workstation for extended periods; use hand(s) to handle or feel objects, tools, or controls; reach with hands and arms; talk and hear. Most positions require ability to work on desktop or laptop computer for extended periods of time reading, reviewing/analyzing information, and providing recommendations, summaries and/or reports in written format. Must be able to effectively communicate with others verbally and in writing. Employee may be required to occasionally lift and/or move moderate amounts of weight, typically less than 20 pounds. Regular and predictable attendance is essential.'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_desc_list[82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_split_texts = []\n",
    "for text in jobs_to_label:\n",
    "    all_split_texts.append(split_text_to_nths(text, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns list of lists into just one big list.\n",
    "# print(list(chain.from_iterable(all_split_texts)))\n",
    "# label 136 job descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fr\"data/classifier_data/labelled_01.txt\", encoding='utf8') as f:\n",
    "    data = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = [(textstr, int(label.strip())) for textstr, label in list(eval(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Collaborate closely with cross-functional teams including data scientists, product managers, and software developers to understand requirements and objectives for building a benchmark chat application. \\n Leverage your in-depth knowledge of the Google Cloud Platform and associated tools to design, develop, and deploy scalable and reliable solutions. \\n Utilize Google Gen AI App Builder to create a chat application that showcases the capabilities of AI-powered conversation models. \\n Work with existing data sets to ensure consistent and relevant input for the chat application, ensuring optimal performance and benchmarking accuracy.',\n",
       "  1),\n",
       " ('Qualifications and Skills: \\n \\n Data engineer or software engineer who must be hands on with GCP platform and tools, and understand Gen AI technologies (Google Gen AI App Builder). \\n Use Google Gen AI App Builder to build benchmark chat based on same data sets \\n Deep understanding of General Artificial Intelligence (Gen AI) technologies, particularly in the context of natural language processing and conversation models.',\n",
       "  1),\n",
       " ('Prior experience with AI-powered Chabot development and integration is highly desirable \\n Job Type: Contract \\n Experience level:',\n",
       "  1),\n",
       " ('8 years \\n \\n Schedule: \\n \\n 8 hour shift', 0),\n",
       " ('Experience: \\n \\n GCP: 6 years (Preferred) \\n Natural language processing: 4 years (Preferred)',\n",
       "  1),\n",
       " ('By clicking the “Apply” button, I understand that my employment application process with Takeda will commence and that the information I provide in my application will be processed in line with Takeda’s Privacy Notice and Terms of Use. I further attest that all information I submit in my employment application is true to the best of my knowledge. \\n \\n  Job Description \\n \\n \\n  Are you looking for a patient-focused, innovation-driven company that will inspire you and empower you to shine? Join us as a Electronic Data Capture (EDC) Engineer (Senior/Principal) – Alta Petens based remotely. \\n \\n  At Takeda, we are transforming the pharmaceutical industry through our R&D-driven market leadership and being a values-led company. To do this, we empower our people to realize their potential through life-changing work. Certified as a Global Top Employer, we offer stimulating careers, encourage innovation, and strive for excellence in everything we do. We foster an inclusive, collaborative workplace, in which our global teams are united by an unwavering commitment to deliver  Better Health and a Brighter Future  to people around the world. \\n \\n  Here, you will be a vital contributor to our inspiring, bold mission. \\n \\n  Objectives: \\n  As the EDC Engineer you will work on EDC activities and oversee delivery of systems and documentation to support Takeda Clinical trials. You will work with Takeda study teams to develop eCRF specifications, build and/or oversee implementation of Case Report Forms (eCRFs) for clinical trials. The EDC Engineer will manage and oversee EDC system configuration, dictionaries, and integrations. The EDC Engineer operates in compliance with Takeda SOPs and processes while working with Data Management and Standards Teams to enhance existing processes. The EDC Engineer will understand Clinical Data Acquisition Standards Harmonization (CDASH) and Study Data Tabulation Model (SDTM) standards and concepts while considering EDC platform best practices. The EDC Engineer will continue developing new skills associated with EDC technologies. \\n \\n  Key Accountabilities',\n",
       "  0),\n",
       " ('Create eCRF specifications, design, program, and validate clinical trial setup \\n  Review edit check specifications and program edit checks at the trial level \\n  Setup different instances of trial URL (eg: UAT, production, testing etc.,) \\n  Configure and maintain user accounts for study teams and site users \\n  Setup and manage blinded and unblinded study configurations \\n  Perform and document functional testing of all EDC design components \\n  Setup, configure, and validate integration modules within the EDC ecosystem such as coding, IRT, eCOA, safety system, local labs etc. \\n  Work closely with EDC vendors to understand system enhancements and limitations \\n  Ability to identify and troubleshoot database design and maintenance issues \\n  Prepare, test, and implement post-production changes as per study needs while ensuring data integrity \\n  Archive and retire the study URL after database lock \\n  Excellent written and verbal communication skills and interpersonal relationship skills including negotiating and relationship management skills with ability to drive achievement of objectives \\n  Monitor general progress to ensure clinical programming milestones and deliverables are met with quality for all concurrent projects \\n  Partner with appropriate team members to establish technology standards and best practices',\n",
       "  0),\n",
       " ('Adhere to and support business process SOPs. \\n  Oversee system delivery life cycle in collaboration with appropriate partners including Clinical Operations, Clinical Supplies, IT, and Quality organizations \\n  Support adoption of new capabilities and business process \\n  Collaborate with standards team in creating standard EDC libraries for study level consumption \\n  Assist data management with CRO oversight of EDC Builds \\n  Provide SME expertise to study teams having site entry and/or bug issues in Production \\n  Work closely with data engineers and data managers on study level integrations and deliveries \\n  Assist in technology vendor oversight activities. \\n  Partner with appropriate team members, technology vendors, and CRO partners to avoid and resolve risks. \\n  Confirm archival and inspection readiness of all Clinical Technology Trial Master File (TMF) documents \\n  Participate in preparing job function for submission readiness \\n  Track study deliverables and escalate any risk(s) for major data management deliverables \\n  Adaptable to new ways of working using technology to accelerate clinical trial setup',\n",
       "  0),\n",
       " (\"Education and Experience Requirements: \\n \\n  Bachelor's degree or related experience. \\n  Knowledge of drug development process. \\n  Minimum of 10+ years’ experience in Data Management, Programming, Clinical IT, or other Clinical Research related fields. \\n  Hands-on experience with at least one EDC system (e.g.: Medidata Rave, Inform, IBM Clinical, Veeva etc.). Veeva is preferred \\n  Understanding of CQL/SQL, Python, and/or C# is preferred \\n  Experience integrating the EDC database with other clinical trial modules (e.g.: lab, safety, IRT, coding etc.) \\n  Understanding of industry standard technologies to support Clinical Development needs (e.g., CTMS, IRT, eCOA, SAS, R or Python, Data Warehouses, SharePoint) \\n \\n \\n  This position is currently classified as “remote” in accordance with Takeda’s Hybrid and Remote Work policy. \\n \\n  Base Salary Range:  $105,000 to $150,000 based on candidate professional experience level. Employees may also be eligible for Short Term and Long-Term Incentive benefits as well. Employees are eligible to participate in Medical, Dental, Vision, Life Insurance, 401(k), Charitable Contribution Match, Holidays, Personal Days & Vacation, Tuition Reimbursement Program and Paid Volunteer Time Off.\",\n",
       "  1),\n",
       " ('The final salary offered for this position may take into account a number of factors including, but not limited to, location, skills, education, and experience. \\n \\n \\n  EEO Statement \\n  Takeda is proud in its commitment to creating a diverse workforce and providing equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, parental status, national origin, age, disability, citizenship status, genetic information or characteristics, marital status, status as a Vietnam era veteran, special disabled veteran, or other protected veteran in accordance with applicable federal, state and local laws, and any other characteristic protected by law. \\n \\n  Locations \\n  Massachusetts - Virtual\\n  \\n  Worker Type \\n  Employee\\n  \\n  Worker Sub-Type \\n  Regular',\n",
       "  0),\n",
       " ('Extreme Networks Named to Computerworld’s 2023 List of Best Places to Work in IT! \\n \\n \\n   Over 50,000 customers globally trust our end-to-end, cloud-driven networking solutions and rely on our top-rated services and support to accelerate their digital transformation efforts and deliver progress like never before and with double digit growth year over year, no provider is better positioned to deliver better outcomes on scale, than Extreme.\\n  \\n \\n   We believe in \\n   “walking the walk”  of our strong core values which enable us to successfully advance together. Diversity and Inclusion is a vital part of our values and beliefs, and we’re proud to foster an environment where every Extreme employee can thrive.\\n  \\n \\n   Come become part of something big with us! We are a global leader, with hubs in North America, South America, Asia Pacific, Europe, and the Middle East.\\n  \\n \\n \\n  The Systems Engineer will be an integral member supporting the Sales Account Executive team in a Pre-Sales Capacity. He or she will support Strategic Major Accounts. Travel to customer sites presenting Proof of Concepts; contribute in the design, planning, and implementation and training on the Extreme Networks architecture. We seek individuals who are dynamic, flexible and willing/able to work outside of their comfort zone.\\n  \\n \\n   Assist Pre-Sales team in the design, planning, and implementation of a Secure Network Solution Engineer\\n  \\n \\n   Provide consultative support for pre-sales, partners and customers on Extreme Networks Solutions',\n",
       "  0),\n",
       " ('Present at trade shows, seminars, and oversee internal demonstration facilities, etc.\\n  \\n \\n   Prepare and deliver customer presentations to all levels of management\\n  \\n \\n   Provide educational training on leading edge technologies/Extreme Networks products to presales.\\n  \\n \\n   Effectively use Wi-Fi test and planning tools. I.e. Ekahau, AirMagnet, NetScout.\\n  \\n \\n   Travel and work the unique schedules in Strategic Major Accounts.\\n  \\n \\n   Work independently, coordinate and communicate with all cross functional groups\\n  \\n \\n  Requirements',\n",
       "  1),\n",
       " ('5+ years relevant experience\\n  \\n \\n   Expert in networking technologies with competitive Network industry and experience\\n  \\n \\n   Proven ability to effectively influence customer decisions and displace competition\\n  \\n \\n   Thorough knowledge of security design features and ability to implement and perform analysis of network and systems security design\\n  \\n \\n   Hands-on experience using network toolsets, for wired, wireless and security products.\\n  \\n \\n   Demonstrated skills in troubleshooting and problem resolution in a network environment\\n  \\n \\n   Keen ability to understand and analyze customer requirements and position the Extreme Networks Solutions effectively\\n  \\n \\n   Ability to effectively demonstrate Extreme Networks products and Solutions',\n",
       "  0),\n",
       " ('Demonstrated ability to manage complex customer interactions in difficult circumstances and control customer expectations in order to maintain satisfied customers\\n  \\n \\n   Excellent inter-personal, technical presentation communication and proven team player\\n  \\n \\n   Ability and willingness to travel within region up to 60%\\n  \\n \\n   B.S. in Computer Science or related field, or equivalent work experience\\n  \\n \\n   Industry Wireless Certifications are encouraged. (CWTS®, CWNA®, CWDP®, CWAP®, etc.)\\n  \\n \\n  In depth knowledge of two or more of the following networking technologies \\n \\n \\n   Understanding of Network Access Control (NAC), Radius, 802.1x, Active Directory, LDAP, etc.',\n",
       "  1),\n",
       " (\"Security principles including: Firewalls, VPNs and Intrusion Prevention\\n  \\n \\n   Cloud technologies; AWS, Azure, GCP\\n  \\n \\n   Wireless, VPN, SDN, SaaS\\n  \\n \\n   Ipv4/v6 Routing Protocols (RIPv1/v2/v3, OSPFv2/v3, VRRP, MPLS, BGP, PIM, etc.)\\n  \\n \\n   Layer2 Switching (802.1Q, 802.1D, 801.x)\\n  \\n \\n \\n  Salary 200K on a 70/30 Split\\n  \\n \\n  Extreme Networks, Inc. (EXTR) creates effortless networking experiences that enable all of us to advance. We push the boundaries of technology leveraging the powers of machine learning, artificial intelligence, analytics, and automation. Over 50,000 customers globally trust our end-to-end, cloud-driven networking solutions and rely on our top-rated services and support to accelerate their digital transformation efforts and deliver progress like never before. For more information, visit Extreme's website or follow us on Twitter, LinkedIn, and Facebook.\",\n",
       "  1),\n",
       " ('Responsibilities \\n \\n  As a Statistical Programmer, the candidate will support Mayo Clinic research by providing high-value analytical collaborations in the form of data manipulations, analysis and reporting. Current Statistical Programmer needs are focused in various clinical departments such as urology, nephrology, obstetrics & gynecology, oncology, and bioinformatics. \\n  Key roles and responsibilities include the following: \\n \\n Interact and collaborate within multidisciplinary teams, under the direction of project team leads \\n Familiarity with programming languages and techniques (e.g., SAS, R) \\n Contribute to project deliverables by critically reviewing and preparing data for data processing, analysis, and/or summarization \\n Demonstrate strengths in organization, documentation, logical and systematic thinking, written and oral communication \\n Perform tasks with strong problem-solving skills, with high attention to detail focusing on delivering high quality results \\n Manage multiple tasks with concurrent deadlines \\n Work independently and remotely as well as in a team environment \\n Seek educational opportunities and share knowledge within teams to enhance professional development',\n",
       "  1),\n",
       " (\"Qualifications \\n \\n  Visa sponsorship is not available for this position; Must be a US citizen, refugee or asylee. \\n  A minimum of a Bachelor's degree with a major in statistics, biostatistics, bioinformatics, mathematics, computer science, data science or quantitative degree relevant to the current needs is required. \\n  Required to be considered: A complete application includes a cover letter, resume, and academic transcripts. \\n  Preferred qualifications include the following: \\n \\n  Applied experience and/or coursework in programming (e.g., SAS, R, Python), data capture systems, data management, statistical and/or bioinformatics analysis \\n  Commitment to customer service \\n  Basic knowledge of human physiology and/or medical terminology \\n  Interest in professional growth and continuing education \\n  GPA of 3.0 or greater\",\n",
       "  1),\n",
       " ('Exemption Status \\n \\n  Exempt\\n  \\n \\n Compensation Detail \\n \\n  $61,131.20 - $91,707.20 / year; Education, experience and tenure may be considered along with internal equity when job offers are extended.\\n  \\n \\n Benefits Eligible',\n",
       "  0),\n",
       " ('Yes\\n  \\n \\n Schedule \\n \\n  Full Time\\n  \\n \\n Hours/Pay Period \\n \\n  80\\n  \\n \\n Schedule Details',\n",
       "  0),\n",
       " ('Monday-Friday, 8:00am-5:00pm (with some flexibility)\\n  \\n \\n Weekend Schedule \\n \\n  As needed\\n  \\n \\n International Assignment \\n \\n  No',\n",
       "  0),\n",
       " ('Description   \\n The Leidos Defense Group currently has an opening for a Defense Property Accountability System (DPAS) Business Analyst. This role will support the DPAS program through Material Management, Warehouse Management, Property Accountability or Maintenance and Utilization applications, processes, and associated interfaces. Duties includes both management and monitoring of the existing applications and processes, as well as partnering with the development teams to guide future business direction, enhancements, and optimization. \\n \\n  The analyst will work with stakeholders to gather and document key functional and technical requirements, assist in development of the business case, and collaborate with internal development teams to deliver technology solutions that meet the business need and deliver value. \\n \\n  This role will be located with our DPAS team in Mechanicsburg, PA in a Hybrid/Remote capacity requiring on-site and off-site support with expectation of one or more days in the office as needed and up-to 25% travel! \\n \\n  Primary Responsibilities',\n",
       "  0),\n",
       " ('Assists and supports the development and implementation for new and significant improvements to major processes through collaboration with colleagues. \\n  Develop a thorough understanding of business processes, best practices, and can support technology & documentation. \\n  Work and collaborate closely with key stakeholders to document the key business goals and objectives for a new or existing process. \\n  Prepares detailed requirement specifications in the form of user stories or functional requirement documents for the development teams. \\n  Participation in the agile software development process as a proxy for the process owner role to help facilitate progress, answer any questions, and provide more detailed requirements as necessary. \\n  Collaborate with development teams to produce any required documentation or communications that may be necessary in support of the project including project status, project roadmap, release plan, etc. \\n  Work closely with Q/A Analyst to assist with developing test cases/scenarios and detailed acceptance criteria.',\n",
       "  1),\n",
       " ('Support project delivery by working with the user community to document and coordinate user acceptance testing and training plans. \\n  Assist the Help Desk with problem and incident management. \\n  Assist with end user training as necessary. \\n \\n \\n  Basic Qualifications \\n \\n  Bachelors’ Degree in Information Technology, Logistics, or related field with two or more years of relevant experience. Additional years of relevant experience, training, and/or certifications may be considered in lieu of degree requirement.',\n",
       "  0),\n",
       " ('US Citizen: Currently possessing or ability to successfully obtain and maintain a DoD Secret Security Clearance upon hire. \\n  Working knowledge of Windows based Desktop/Laptop Computers. \\n  Experience with Microsoft Office products to include Microsoft Word, Excel, PowerPoint, and Outlook. \\n  Demonstrated problem-solving skills. \\n  Demonstrated critical skills such as collaboration, strong written and verbal communication skills, and analytical capability. Self-motivated, with the ability to prioritize, meet deadlines, and manage changing priorities. \\n  Ability to be flexible and work hard, in a fast-paced environment with changing priorities.',\n",
       "  1),\n",
       " ('Preferred Qualifications \\n \\n  Experience with Department of Defense Supply Systems, DLMS processing or MILS processing. \\n  DPAS experience \\n \\n \\n  LMS2023',\n",
       "  0),\n",
       " ('At Spresso, our mission is to use data to deliver better business outcomes to various industries around the world. We optimize decision-making with tools we’ve built spanning a decade of first-hand experience in e-commerce. What started as an end-to-end platform for the e-commerce retailers is now a world-class suite of SaaS products powered by advanced analytics and machine learning. Spresso brings unique data and machine learning capabilities to retailers globally. \\n Our Engineering team is a brilliant cultivator of technology powering our world class platform & SaaS modules spanning everything from Catalog, Orders & Fulfillment, and Personalization. Being part of Spresso’s Engineering team means you’ll work with wicked-smart individuals from all over the world who contribute as engineers, product managers, designers, and data scientists. Every day our Engineering team innovates in the e-commerce space with the latest technologies. We’re excited to welcome engineers who are ready for a challenge and know how to think outside the box! \\n The DevOps team values an always-learning approach to technology. We want people who already know they don’t know it all, and who are willing to work closely and honestly with team members to collectively and objectively find the best solutions to problems. Technological humility - the desire to build what’s correct over being the person who’s correct - is a trait we value highly. \\n As a  DevOps Engineer , you’ll be helping to scale out and improve our large and growing GCP infrastructure and helping us manage our applications running in Docker and Kubernetes. Most of the software we run is Node.js, with a heavy reliance on MongoDB. Python, Go, and Java, which are also utilized for various services, as are several other data stores, including Redis, Postgres, Snowflake, and Elasticsearch. \\n You Will: \\n \\n Work collaboratively with the team to build out and improve our large and growing GCP infrastructure \\n Improve and optimize our cloud infrastructure with Terraform',\n",
       "  1),\n",
       " ('Help automate and streamline our operations \\n Troubleshoot and resolve issues in our dev, staging and production environments \\n Support Software Engineers with building, testing, and deploying their applications \\n Be on an on-call rotation with the rest of the Engineering team \\n Be a critical part of not just the technology team but to the company as an excellent problem solver. \\n \\n Requirements:',\n",
       "  0),\n",
       " ('Good programming skills \\n Experience with Kubernetes \\n Experience with GCP components & pitfalls (AWS or Azure also acceptable) \\n Experience with an IaaC tool (Terraform) \\n Experience with a CI/CD tool such as Jenkins \\n Experience writing Dockerfiles for apps in multiple languages \\n Experience writing scripts to automate repetitive tasks \\n Disaster recovery experience, especially if self-inflicted',\n",
       "  1),\n",
       " ('Experience with MongoDB performance analysis, debugging, and/or data modeling \\n Familiarity with cloud networking - Firewalls, NAT, VPN, network peering \\n Experience running Node.js apps under load \\n Ability to debug system problems like running out of memory, inodes, or ephemeral ports \\n Knowledge of distributed systems (formal theory or learned firsthand) \\n Experience using a Log Aggregation Platform. \\n \\n Job Type: Full-time',\n",
       "  1),\n",
       " ('Pay: $140,000.00 - $200,000.00 per year \\n Benefits: \\n \\n 401(k) \\n Dental insurance \\n Health insurance \\n Life insurance \\n Paid time off',\n",
       "  0),\n",
       " ('Looking for a company that inspires passion, courage and creativity, where you can be on the team shaping the future of global commerce? Want to shape how millions of people buy, sell, connect, and share around the world? If you’re interested in joining a purpose driven community that is dedicated to crafting an ambitious and inclusive work environment, join eBay – a company you can be proud to be with. \\n \\n  eBay’s Search organization innovates at the heart of ecommerce, with an ambitious goal of redefining ecommerce search. We are looking for a top notch Senior Product Manager to join Search’s Content, Item and Inventory understanding team. \\n \\n  The ideal candidate should have prior experience growing online commerce products and will be responsible for leading the definition, design and delivery of science heavy (e.g. NLP, embeddings etc) products and features that help deconstruct complex pieces of information provided by the sellers to craft optimized experiences for buyers using search on eBay. The candidate should be comfortable dealing with ambiguity and should have experience with defining requirements, driving cross team collaboration and exec level communication, as well as working with product managers, engineers, applied researchers, and analytics teams to define the solutions. The candidate should be adept at data driven decision making and with analyzing huge amounts of data. \\n \\n  Responsibilities:',\n",
       "  1),\n",
       " ('Own product strategy and roadmap for Content Science - improving understanding the various facets of a listing (title, attributes, images, description etc.) \\n  Data driven product prioritization with a balance of near-term deliverables and longer-term investment in technology platforms \\n  Optimally communicate strategies & trade-offs up to senior leaders and down to development teams \\n  Putting the customer first, understand market specific nuances and identify science algorithm improvements to address unique search specific needs \\n  Driving collaboration across multiple teams to ensure teams are aligned and are working towards solving customer problems using the product built \\n  Drive A/B testing to identify launch candidates that improve metrics',\n",
       "  0),\n",
       " ('Qualifications:  \\n \\n 5+ years of strong track record of product management experience. Ideally have previous experience building customer centric platform products delivering values to both sides of a market place (buyers and sellers) \\n  Machine Learning experience \\n  A proven track record of building new, innovative, and engaging customer experiences informed by customer data from various sources (market research, A/B testing, user research)',\n",
       "  0),\n",
       " ('A customer and Search Engine obsession with a passion for everything related to technology, and AI driven innovation to build great experiences for the customers \\n  Strong quantitative skills with a data first mindset; ability to use data and metrics to back up assumptions, recommendations, and drive actions \\n  Technically savvy (prefer a technical role in past, or Computer Science Degree) \\n  Bachelor’s Degree required; Master’s Degree preferred \\n \\n \\n  Benefits are an essential part of your total compensation for the work you do every day. Whether you’re single, in a growing family, or nearing retirement, eBay offers a variety of comprehensive and competitive benefit programs to meet your needs. Including maternal & paternal leave, paid sabbatical, and plans to help ensure your financial security today and in the years ahead because we know feeling financially secure during your working years and through retirement is important.',\n",
       "  0),\n",
       " ('Here at eBay, we love creating opportunities for others by connecting people from widely diverse backgrounds, perspectives, and geographies. So, being diverse and inclusive isn’t just something we strive for, it is who we are, and part of what we do each and every single day. We want to ensure that as an employee, you feel eBay is a place where, no matter who you are, you feel safe, included, and that you have the opportunity to bring your unique self to work. To learn about eBay’s Diversity & Inclusion click here: https://www.ebayinc.com/company/diversity-inclusion/ \\n  #LI-ML2 \\n \\n  The pay range for this position at commencement of employment in California, Washington, or New York is expected in the range below.  $135,200 - $205,700\\n   Base pay offered may vary depending on multiple individualized factors, including location, skills, and experience. The total compensation package for this position may also include other elements, including a target bonus and restricted stock units (as applicable) in addition to a full range of medical, financial, and/or other benefits (including 401(k) eligibility and various paid time off benefits, such as PTO and parental leave). Details of participation in these benefit plans will be provided if an employee receives an offer of employment. \\n  If hired, employees will be in an “at-will position” and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors.',\n",
       "  0),\n",
       " ('BNL Consulting is actively seeking a contractor with experience as a Public Health Informatics Business Analyst. This position will be remote and will be a 1099 contract. \\n Role Description : \\n You will assist in meeting business needs requiring IT based solutions for public health including: identifying and analyzing business needs, creating functional and technical specifications/requirements documentation, defining scope and objectives, making recommendations for solutions or improvements to business processes that can be accomplished through new technology or alternative uses of existing technology, researching business requirements and documenting the relationships between the components of the application system (i.e., end users, business processes, data, applications, and devices), and translating business requirements into application requirements to parallel overall business strategies. Experience with public health organizations and specifically with public health informatics is strongly preferred. \\n Required Experience: \\n - Four plus years of experience with business process analysis \\n - Four plus years of experience with requirements analysis and development \\n - Four plus years of working with stakeholders to understand the current business processes and the outcomes and impact they are looking to see from business process changes \\n - Four plus years of working with public health data, especially as part of a public health informatics team \\n The must-haves:',\n",
       "  0),\n",
       " ('-Minimum of Bachelor of Science or Bachelor of Arts degree - preferably in Healthcare, Business, or related discipline \\n - Experience with Public Health, especially with Public Health Informatics \\n - Business Analyst Certification from PMI (PMI Professional in Business Analysis (PMI-PBA)) \\n -Experience working with laboratory public health data \\n -4 years of experience acting as a business analyst \\n -Experience analyzing and developing requirements for new systems and enhancements for existing systems \\n -Experience evaluating business processes, anticipate requirements, uncover areas for improvement, and use analytical skills to gather, document, and optimize the requirements \\n -Experience conducting discovery, workflow analysis, or landscape analysis to understand current and future state, needs and capabilities \\n -Experience meeting with clients, internal business Subject Matter Experts (SMEs), operations analysts, and/or members of the development team to gather, define and document system requirements',\n",
       "  0),\n",
       " ('The nice-to-haves: \\n - Strong problem-solving skills \\n - Good self-starter/self-manager \\n - Good communication skills (verbal and written) \\n About Us: \\n Founded in 2007, BNL Consulting began by designing and implementing small to enterprise-level business intelligence (BI) systems. At our core, we’re still dedicated to providing SAS-centric services, but over the past decade, we’ve expanded our offerings to include custom analytics platforms, highly interactive user experiences, seamless enterprise integration solutions, and secure cloud architecture. While the relationships we cultivate with our clients give us a wide spectrum of experience, we have an especially successful track record of providing solutions to the Public Sector, Sports and Entertainment industries, and the Health and Life Sciences fields. For more information on us, please visit our website at www.bnlconsulting.com. \\n Job Type: Contract \\n Pay: $60.00 - $65.00 per hour \\n Experience level:',\n",
       "  0),\n",
       " ('4 years \\n \\n Schedule: \\n \\n 8 hour shift \\n \\n Education:', 0),\n",
       " (\"Bachelor's (Required) \\n \\n Experience: \\n \\n Business process analysis: 4 years (Required) \\n Requirements analysis: 4 years (Required) \\n Public Health Data: 2 years (Required) \\n \\n License/Certification:\",\n",
       "  0),\n",
       " ('Overview: \\n  \\n   Amyx is seeking a Zero Trust Security, Orchestration, Automation and Response (SOAR) Cyber Security Engineer (CSE) to support our TRANSCOM J6 SISO contract located at Scott AFB, IL\\n  \\n \\n   In this role, the SOAR CSE will bring the background and experience collecting and analyzing data, and developing/implementing automated solutions that trigger worflows, alerts and other actions based on the results of data analysis. Data collected and analyzed will be retrieved from tools, applications and devices that comprise on-premises and cloud-based network environments. The ideal candidate will work collaboratively with our team and our customer to assess existing data collection and automation capabilities, examine other opportunities for automation, and increase the value of collected such that it can be used to further automation capabilities and provide additional insight into the overall security posture of network environments and applications running in those environments. With regards to Zero Trust, this position will require the candidate to become the resident expert of the \"Visibility & Analytics\" pillar of capabilities and activities that are a key part of overall zero trust efforts.\\n   Responsibilities: \\n  \\n Determine and document the data that can be exported/extracted from the tools and applications that comprise a zero trust network architecture \\n Conduct research on and become familiar with a variety of Security, Orchestration, Automation and Response (SOAR) solutions \\n Understand how machine learning and artificial intelligence tools and processes can be used to enhance automated decision making \\n Identify and implement opportunities to enrich data with the inclusion of relevant data from approved external sources \\n Understand the input and output of in-place and planned Security Information and Event Management (SIEM) tools and how such data might be used to trigger automation actions and workflows \\n Develop the actions and workflows that occur when data indicates conditions have been met that trigger the action or workflow \\n Understand how raw data formats can be extracted from other tools, parsed and converted so it can be ingested and used by SOAR solutions \\n Recommend SOAR solutions that provide the most benefit to the overall zero trust goals and objectives and enhance patch management/incident response capabilities \\n Develop, test and deploy data filters and algorithms that correlate incidents and events with data from different data sources \\n Develop, test and deploy data filters and algorithms that identify high-severity concerns identified through data analysis and prioritizes them for remediation/mitigation \\n Develop and implement data mapping strategies to ensure ingested data is translated into the fields and expected formats required by SOAR tools and supports up-channel reporting \\n Develop, test, and deploy data filters and algorithms that identify trends and anomalies within data sets',\n",
       "  1),\n",
       " (\"Use SOAR tools to ingest and analyze user and device behavioral data to highlight behaviors that are outside of the norm \\n Work with security analysts to identify points of interest that may be analyzed to enhance SIEM and incident response capabilities  \\n Develop data-driven charts, graphs and infographics that illustrate the status of high-interest and high-severity subjects \\n Examine/Explore new sources of useful raw data to include queries of endpoint agents, API calls, device and application logs, etc... \\n \\n \\n \\n  Technology used:\\n  \\n \\n   Zero Trust, NiFi, Elastic, Splunk, Kafka\\n   Qualifications: \\n  \\n   Required:\\n  \\n \\n BS degree in one of the following: computer engineering, computer science, IT, cyber security, or a related field.  \\n Relevant years of experience may be used in substitution for situations where the candidate does not have the required Bachelr's degree.  \\n Possess a Certified Information Systems Security Professional (CISSP) or similar IA III certification and \\n Possess DoD 8570/8140 CSSP Infrastructure Support certification\",\n",
       "  1),\n",
       " ('Active Secret Clearance. \\n At least 5 years of experience in Data science, cybersecurity, information technology, or related fields.  \\n Expertise in large-scale data collection, transformation, correlation, and analysis. \\n Experience developing, implementing and fine-tuning automation workflows/playbooks.  \\n Knowledge of EO 14028, Federal, DoD, and other Zero Trust efforts and initiatives.  \\n Experience identifying trends, anomalies, risk and issues of concern within data sets.  \\n Excellent communication, collaboration, and problem-solving skills.  \\n Ability to work independently and as part of a team \\n Understanding of key IT/Cybersecurity concepts (network security, security operations & administration, incident response & recovery, identity and access management, vulnerability management, zero trust). \\n Experience with SIEM, SOAR and endpoint protection tools. \\n Understanding of Intrusion Detection and Prevention Systems (IDS, IPS) and Data Loss Prevention (DLP). \\n Understanding of Unclassified and Classified Government networks. \\n Experience supporting RMF/NIST/FedRAMP accreditation and assessment of systems. \\n Experience implementing STIG requirements for Microsoft and Linux operating systems, services and applications. \\n Familiarity with DevSecOps concepts and approaches. \\n Experience with data technologies such as NiFi, Elastic, Splunk and Kafka. \\n \\n \\n \\n  Desired:',\n",
       "  1),\n",
       " ('BS degree and at least 10 years cybersecurity experience  \\n Demonstrated knowledge and understanding of the USTRANSCOM mission \\n \\n \\n \\n  Benefits include:\\n  \\n \\n  Medical, Dental, and Vision Plans (PPO & HSA options available) \\n  Flexible Spending Accounts (Health Care & Dependent Care FSA) \\n  Health Savings Account (HSA) \\n  401(k) with matching contributions \\n  Roth \\n  Qualified Transportation Expense with matching contributions \\n  Short Term Disability \\n  Long Term Disability \\n  Life and Accidental Death & Dismemberment \\n  Basic & Voluntary Life Insurance',\n",
       "  0),\n",
       " ('Wellness Program \\n  PTO \\n  11 Holidays \\n  Professional Development Reimbursement \\n \\n \\n \\n  Please contact talent@amyx.com with any questions!\\n  \\n \\n \\n  Amyx is an Equal Opportunity employer. Amyx is committed to providing equal employment opportunity to all job seekers. Every qualified applicant receives focused consideration for employment and no one is discriminated against on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status. In addition to federal law requirements, Amyx complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Equal Opportunity Employer- Minorities/Females/Veterans/Individuals with Disabilities/Sexual Orientation/Gender Identity. Amyx is an E-Verify employer.\\n   \\n  Amyx proudly and proactively takes affirmative action to advance employment of individuals who are minorities, women, protected veterans and individuals with disabilities.\\n   \\n \\n \\n \\n Physical Demands',\n",
       "  0),\n",
       " ('Overview: \\n  \\n  THE COMPANY \\n \\n \\n \\n  Phastar is a multiple award-winning global biometric Contract Research Organization (CRO) that is accredited as an outstanding company to work for by Best Companies. We partner with pharmaceutical, biotechnology and medical device organizations to provide the expertise and processes to manage and deliver on time, quality biostatistics, programming, data management and data science services. With offices across the UK, US, Germany, Denmark, Kenya, Australia, India, China and Japan, Phastar is the second largest specialized biometrics provider globally, and the largest in the UK.\\n  \\n \\n \\n  Our unique approach to data analysis, “The Phastar Discipline”, has led us to build a reputation for outstanding quality. With this as our core focus, we’re looking for talented individuals who share our passion for quality and technical expertise to join our team.\\n  \\n \\n \\n  WHY PHASTAR',\n",
       "  0),\n",
       " ('Accredited as an outstanding company to work for, Phastar is committed to employee engagement, workplace satisfaction and ensuring a healthy work-life balance. We offer flexible working, part-time hours, involvement in developing company-wide initiatives, structured training and development plans, and a truly supportive, fun and friendly environment.\\n  \\n \\n \\n  What’s more, when you join our team, Phastar will plant a tree in your honour, as one of our Environmental, Social and Governance (ESG) initiatives. So, not only would you get your dream job, you’ll also be helping to save the planet!\\n  \\n \\n \\n  THE ROLE',\n",
       "  0),\n",
       " ('Demand for our Functional Service Provision is growing, and we are looking for an experienced Senior Data Scientist to join our FSP team.\\n  \\n \\n \\n  This is a fantastic opportunity to work for a growing CRO that is recognized for its continuous learning and development opportunities, whilst also gaining direct experience of working within a pharmaceutical environment.\\n   Responsibilities: \\n  \\n Collaborate with Payer and Epidemiology teams to maximise the value derived from large observational research data \\n  Deliver analyses of data from EMR, claims and primary observational data required by TA RWE strategies \\n  Support the development of IVS strategies and selection of optimised contact models for prioritised markets through analysis of RWD \\n  Provide scientific guidance on the application of Real World Evidence and observational research data to address issues across the Oncology and Biopharmaceuticals business units \\n  Provide technical input, options and directions to strategic decisions made by the client observational study teams on study design, data partner selection and best practices in RWE data utilization \\n  Support technical teams to provide access to analytical tools and develop visual analytics to enable self-serving applications for end customers \\n  Provide clear technical input, options, and direction to strategic decisions on RWE platform and capability build \\n  Provide support for strategic decisions on client Medical Evidence and Observational Research external collaborations in the US and other markets',\n",
       "  0),\n",
       " ('Assist in building a capability that becomes a source of sustained competitive advantage for the client in identifying, acquiring, integrating and mining diverse RW data from multiple geographic and healthcare system sources to support evidence generation and real-world studies \\n  Evaluate and assess strengths and weaknesses of external RW data sources, and potential partners for advancing the data strategy for specific therapeutic areas \\n  Maintain a strong insight into the capabilities of potential external partners in RWE, especially for US and emerging markets \\n  Qualifications: \\n  \\n Experience in real-world evidence and familiarity with health economics/epidemiology \\n  Expertise in methods development and application using statistical languages such as R/Matlab/SAS/SQL/Hadoop/Python \\n  Preferred to have experience in advanced visualisation and visual analytics of routinely collected healthcare data \\n  MSc or PhD in Data Science or related/advanced analytical degree field i.e. Biostatistics/Statistics, Epidemiology, Public Health \\n  Experience working within a clinical trials environment (CRO, pharma or academia) \\n  Excellent written and verbal communication skills \\n \\n \\n \\n  APPLY NOW',\n",
       "  1),\n",
       " ('With the world’s eyes focused on clinical trial data, this is a fantastic time to join an award-winning specialized biometric CRO that is renowned for its technical expertise, outstanding quality and cutting-edge data science techniques. We offer flexible working, part-time hours, structured training and development plans, continuous learning opportunities, and a competitive salary and benefits package. We’re committed to ensuring our employees achieve a healthy work-life balance, within a supportive, fun and friendly working environment.\\n  \\n \\n \\n  Should you feel that you have the right skill set and motivations for this position, please apply! Please note that we are considering candidates located anywhere in the US or Canada as this role can be carried out remotely.\\n  \\n \\n \\n  Phastar is committed to the principles and practices of equal opportunities and to encouraging the establishment of a diverse workforce. It is our policy to employ individuals on the basis of their suitability for the work to be performed and their potential for development, regardless of age, sex, race, colour, nationality, ethnic or national origin, disability, marital status, pregnancy or maternity, sexual orientation, gender reassignment, religion, or belief. This includes creating a culture that fully reflects our commitment to equal opportunities for all.',\n",
       "  0),\n",
       " ('The mission of the OFR is to support the Financial Stability Oversight Council (FSOC) in promoting financial stability by: collecting data on behalf of FSOC; providing such data to FSOC and member agencies; standardizing the types and formats of data reported and collected; performing applied research and essential long-term research; developing tools for risk measurement and monitoring; performing other related services; making the results of the activities of the OFR available to financial regulatory agencies; and assisting such member agencies in determining the types of formats of data authorized to be collected by such member agencies.  \\n The Senior Detection Engineer provides services required to integrate existing security and network tools, platforms, and threat intel feeds in a modern hybrid (cloud/on-premises) environment to implement zero-trust security policies. This includes the ability to detect and respond to vulnerabilities and security incidents, alert on incidents and vulnerabilities for action, and enable active threat hunting capabilities. Identify gaps in security operations capabilities and/or visibility and provide recommendations to improve existing tools or new technology. Develop and automate highly complex detection playbooks that fully integrate and utilize the security and network tools. This role is highly technical and requires a solid understanding of offensive network security principles and how to translate these to protect and defend enterprise environments. This engineer will support the deployment of new network security technologies that will enhance the overall network security capabilities of the organization. This includes, but is not limited to tuning detection systems to minimize false positives, enhancing detection policies such as using adversarial models / TTPs (Tools, Techniques & Procedures) and threat intelligence to build the latest/greatest detections. Implement User Behavior Monitoring that leverages machine learning and artificial intelligence techniques to detect anomalous user actions and help combat advanced threats. The engineer is expected to perform mild penetration testing to test defenses, and work closely with the different technology teams to harden all systems, building deception ecosystems (breadcrumbs to trap servers), as well as identifying required data sources, and architect/engineer pipelines to get the data needed to make any/all detections. The engineer should be proficient in collecting multiple sources of network/security data under a single visibility platform with the ability to correlate events, preferably using machine learning / artificial intelligence, to identify indicators of compromise. Develop relevant dashboards, stoplight charts, monitors, metrics, and diagrams to assist operators in assessing the current security state of the enterprise. \\n Key Tasks and Responsibilities \\n \\n Configure, set policy and tune platforms to increase detection capabilities and scope, and eliminate noise and false positives: \\n \\n \\n EDR/IDS/IPS \\n \\n \\n NDR/Network \\n \\n \\n Identity Provider (IdP) authentication policies \\n \\n \\n Email defense platforms \\n \\n \\n Integration of threat intelligence feeds with security policy enforcement points \\n \\n \\n SIEM and XDR detections \\n \\n \\n Security orchestration, automation, and response (SOAR) playbook development \\n \\n \\n Apply knowledge of monitoring, analyzing, detecting and responding to cyber events to develop clever, efficient methods and technology to detect all types of threat and to weaponize our threat hunting capabilities',\n",
       "  0),\n",
       " ('Develop strategies for network security operations, which includes detecting new threats as they emerge, including those from the most sophisticated threat actors; enhancing our detection and investigation capabilities with threat correlations and intelligence; integrating situational awareness of system intrusions; enhancing ticket data, and automate mitigation of threats \\n \\n \\n Develop and implement strategies to secure and monitor new technologies, platforms, or services \\n \\n \\n Develop and deploy deception technology (e.g., honeypots, honey hashes) across both enterprise and cloud environments. This includes planting breadcrumbs on endpoints/services, and then laying trap servers/services to detect their use \\n \\n \\n Work with Technology teams to develop architecture that utilizes integrations between network, host, and security tools and intelligence feeds that fuses defenses and detections together to increase the effectiveness of policy enforcement. This includes not only endpoints and email; but networks, Identity Providers, cloud (e.g., IAM), Access/Authentication/Authorization, Zero Trust, jumphosts/bastion, firewalls, etc. \\n \\n \\n Identify data sources required, and then architect/engineer pipelines to get the data needed to make any/all detections, preferably on a cloud-based, single platform / single pane of glass \\n \\n \\n Perform security testing to identify security strengths and weaknesses, to assess the effectiveness of existing controls, and to recommend remedial action \\n \\n \\n Evaluate modern, state of the art network security technologies, work with proof-of-concepts and evaluate them for adoption.  \\n \\n \\n Train and educate engineers about new detection/response tactics and technologies \\n \\n \\n Document specifications, playbooks and detections - not as an afterthought, but through the whole process and polished to share externally \\n \\n \\n Work with developers to build security automation workflows, enrichments, and mitigations',\n",
       "  1),\n",
       " ('Gather, analyze, and assess the current and future threat landscape, and assist in providing leadership with a realistic overview of risks and threats in the enterprise environment \\n \\n \\n Evaluate policies and procedures and recommend updates to management as appropriate \\n \\n  Job Requirements: \\n \\n Education & Experience \\n \\n Wide, deep and practical knowledge of TTPs (Tools, Techniques & Procedures) used by attackers across all varieties of attack surfaces - Authentication, Authorization, Networks, AD, Zero Trust, Firewalls, Applications, etc.) \\n \\n \\n Solid experience with offensive TTPs, penetration testing and working with/on purple teams to harden detection capabilities \\n \\n \\n Note we are not looking for individuals who only have experience with OWASP/application testing and network perimeter assessments; but the whole gambit of technologies and vectors, such as OS Escalations, Active Directory, Appliances, SaaS, Networks, Zero Trust, ATO, etc.) \\n \\n \\n Mastery of security tools such as endpoint protection/EDR, SIEM, IPS/IDS, HIDS/NIDS, Networking, firewalls, WAFs, edge/endpoint security, DNS security, cryptography, layered security, defense in depth practices, vulnerability scanning, malware analysis tools, networking tool for full packet analysis, data encryption, data loss prevention, DDoS prevention, etc. \\n \\n \\n Experience building detections for enterprise environments, which must include systems outside of a SIEM \\n \\n \\n Deep understanding of Active Directory architecture and features; as well as the attacks and defenses (e.g., NTLM/Kerberos, escalation paths, multiple kinds of Kerberoasting, ADCS vectors, etc.) \\n \\n \\n Experience securing and monitoring cloud-based IdPs such as Okta/Azure AD',\n",
       "  1),\n",
       " ('Scripting capabilities (e.g., bash, powershell, python) \\n \\n \\n SOAR and playbook automation experience \\n \\n \\n Linux/Unix OS and Windows administration knowledge \\n \\n \\n Collaborative mindset that thrives in fast paced environments \\n \\n \\n Excellent verbal and written communication skills including the ability to author and present materials ranging from detailed technical specifications to high-level concepts for senior audiences \\n \\n \\n Preferred College Degree in Computer Science or related \\n \\n \\n Must have 10+ years of related experience \\n \\n Certifications \\n \\n Preference given for CEH, CISM, GIAC, GCIH, GSLC, GICSP, GSEC, CEH, GWAP, CompTIA Net+, CompTIA A+, CompTIA Security+, CASP CE, SEC+, CISSP, CISSLP, Splunk Core, OSCP, etc. \\n \\n Security Clearance \\n \\n Public Trust',\n",
       "  1),\n",
       " ('Must be US Citizen \\n \\n Other (Travel, Work Environment, DoD 8570 Requirements, Administrative Notes, etc.) \\n \\n This is a remote/work from home role \\n \\n \\n \\n \\n \\n \\n \\n  Get job alerts by email. \\n   Sign up now!  Join Our Talent Network! \\n  \\n \\n Job Snapshot \\n \\n Employee Type  Full-Time \\n   \\n \\n Location  United States of America (Remote) \\n   \\n \\n Job Type  Engineering, Government, Information Technology \\n   \\n \\n Experience  Not Specified',\n",
       "  0),\n",
       " ('AMSURG is a leading national healthcare provider, focused on acquiring, developing and operating ambulatory surgery centers (ASCs) in partnership with physicians throughout the United States. We operate and hold ownership in more than 250 ASCs in 34 states and the District of Columbia with medical specialties ranging from gastroenterology to ophthalmology and orthopedics. In 1992, AMSURG launched as a pioneer in the ambulatory surgery center healthcare sector, originating a physician partnership model that endures today. In the three decades since, our company has become an innovator, advocate and source for patients to find high quality care in their communities. \\n  In this role, the Senior Data Engineer’s primary job responsibilities involve preparing data for analytical or operational uses. The specific tasks handled by the Senior Data Engineers include, but not limited to, guiding/executing the work of all Data Engineer resources, both individually and collective, building data pipelines to pull together information from different source systems; integrating, consolidating, and cleansing data; and structuring it for use in individual analytics applications. The Senior Data Engineer often works as part of an analytics team, in a team lead and individual contributor role, providing data aggregations to executives, business analysts and other end users for more basic types of analysis to aid in ongoing operations. \\n \\n \\n  RESPONSIBILITIES \\n \\n Design, construct, install, test and maintain highly scalable data management systems. \\n Ensure systems meet business requirements and industry practices. \\n Build high-performance algorithms, prototypes, predictive models, and proof of concepts.',\n",
       "  0),\n",
       " ('Research opportunities for data acquisition and new uses for existing data \\n Lead and Develop data set processes for data modeling, mining, and production. \\n Direct and integrate new data management technologies and software engineering tools into existing structures. \\n Employ a variety of techniques and tools to marry systems together. \\n Recommend and execute ways to improve data reliability, efficiency, and quality. \\n Manage projects, resources, customer expectations, and business priorities to achieve customer satisfaction. \\n Collaborate with data architects, modelers, and IT team members on project goals.',\n",
       "  0),\n",
       " ('QUALIFICATIONS \\n \\n Experience in Azure Data Factory and SSIS \\n Extensive experience with Microsoft SQL Server \\n Advanced level user of Microsoft Office products. Advanced/power user of Excel \\n Advanced knowledge of relational database principles including SQL and MS-Office products \\n Excellent quantitative and analytical skills as well as the ability to translate findings into meaningful information appropriate to the audience/stakeholder. \\n High level of comfort with many types of data including financial, quality, clinic, and security.',\n",
       "  1),\n",
       " ('Relational database training and data modeling skills. Must demonstrate a history of project management, technology investigation, technology implementation and technology oversight in various capacities. \\n Ability to be a self-starter that can provide leadership. \\n Comprehensive understanding of the Agile Development process. \\n Presentation and PowerPoint skills, with a demonstrated ability to tell a data story to executive leadership. \\n Strong ability to understand and analyze user requirements as they relate to organizational goals and objectives. \\n Strong attention to detail with the ability to work under deadlines and switch quickly and comfortably between projects as business needs dictate. \\n Superior written and oral communication skills. \\n Strong interpersonal skills with the ability to effectively collaborate across teams. \\n Strong work ethic and ability to work autonomously in a high production environment.',\n",
       "  1),\n",
       " ('Ability to work independently and prioritize work appropriately. \\n Ability to work under tight deadlines leading multiple projects and switch quickly and comfortably between projects as business needs dictate. \\n Manage and mentor team members. \\n \\n Education/Experience \\n \\n Bachelor’s degree from an accredited college or university with a minimum of six (6) years previous data management/ETL experience required. \\n Highly proficient in database management and query tools, including SQL and/or other query languages, required',\n",
       "  1),\n",
       " ('THE ROLE \\n  Your role stands at the intersection of state-of-the-art data management and the effort to make positive changes in educational equity and racial justice in high schools nationwide.  As a key member of Stand’s Technology team at a moment of tremendous need and opportunity , you will be responsible for leading and supporting data acquisition, integrity, analysis, and maintenance. As part of a major organizational strategic initiative, you will build a structure for campaigns and elections that drives decisions. Through your dedication and passionate approach to the role, you get the opportunity to work with colleagues who not only care deeply about justice and equity but who are bold and strategic and take responsibility for achieving substantial progress toward our mission every year. \\n  This is a remote position reporting to the Chief Technology Officer.  \\n THE ORGANIZATION \\n  Stand for Children is a unique catalyst for  education equity  and  racial justice  to create a  brighter future for us all.  \\n RESPONSIBILITIES: \\n  Work closely with business owners, the business user community, the technology team, and other program, state, and national teams to:',\n",
       "  0),\n",
       " ('Serve as a single-service, builder, provider and technical expert  for all data and reporting needs for the enterprise at Stand. Must be comfortable as a lead and only data person for the team. \\n  Provide technical support in solving data and reporting problems  working with business units and/or other members of the tech team. \\n  Write, develop/code, modify, and enhance reports using Tableau  including the interpretation of data, data visualization, dashboard, reporting, and information to support organization metrics. \\n  Provide ad-hoc analysis and trending  to identify data anomalies and opportunities. \\n  Lead & participate in project teams  to analyze/clarify business processes, business rules and define functional requirements for data and reporting needs. \\n  Provide estimates on work effort  and proposed technical solutions to business problems. \\n  Build and Support electoral data analytics , big data, BI and visualization to support campaign staff on engagement strategy and tactics. \\n  Build and Support ETL , data transformation of big data from the Voter File to/from the marketing engagement system.',\n",
       "  1),\n",
       " ('Lead and Serve  as a data gatekeeper to ensure data consistency, integrity, and quality.  \\n Establish and maintain policies and procedures  for data ownership and governance. \\n  Train business leaders & end users . \\n  Work remotely in collaboration  with other technical team members and business community \\n \\n  QUALIFICATIONS: \\n \\n  Passionate commitment to Stand for Children’s mission.',\n",
       "  0),\n",
       " ('8-10 years of direct experience  in data analysis, BI, data manipulation including ETL and API and report writing. \\n  Experience in Tableau is required . Proven ability to connect to/from other systems’ data sources. Proficient in data prep, and transformation to build complex data visualization and dashboards in Tableau to “tell the story”. \\n  Experience in pulling data from large enterprise systems . \\n  Experience with Salesforce and/or CRM and ability to connect Salesforce database from Tableau. \\n  Technical understanding of Database structure and data model . Experience working with SQL databases, data stores, data warehouses, and big data. \\n  Proven technical skills  in Excel mastery, Query/SQL, Report writing, Report publishing, Data visualization (dashboard, metrics, graphs), and Ad-hoc analysis of data for BI for trends, issues, and opportunities. \\n  Written, verbal, and presentation  communication skills to a wide audience. Experience effectively translating technical information and instructions to less technically-minded colleagues.  \\n Proven ability to quickly adopt and apply new technologies  and Creative problem-solving.',\n",
       "  1),\n",
       " ('Ability to  manage competing & time-sensitive priorities . \\n  Exceptional customer service orientation  with strong interpersonal skills; a focus on rapport-building, collaboration, listening, asking questions, and delivering information in a timely and friendly manner. \\n \\n  STARTING SALARY RANGE:  $89,858- $112,322 annually commensurate with experience. Generous benefits. \\n  Stand for Children consists of two separate entities. Stand for Children, Inc., a grassroots membership organization is exempt under section 501(c)(4) of the Internal Revenue Code, and Stand for Children Leadership Center, a leadership development organization is exempt under section 501(c)(3) \\n  Stand for Children and Stand for Children Leadership Center provide equal employment opportunity (EEO)  to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression or marital status. We are committed to a diverse and culturally inclusive workplace in which our  differences broaden our awareness, enrich our daily experiences, and contribute to our collective strength .  We predominantly partner with low-income communities of color. Therefore, though race and other legally protected characteristics are never used to make final hiring decisions, we place a particular focus on recruiting staff members who share the backgrounds of the communities we serve.',\n",
       "  0),\n",
       " ('Techno-functional Business Analyst \\n Location:  Bay Area (Condition is to work on-site at an office of the client in the Bay area) \\n Contract Length:  +6 months (extendable) \\n Skills:',\n",
       "  0),\n",
       " ('Oracle ERP - Order Management experience \\n Service contracts background \\n Oracle EBS experience',\n",
       "  1),\n",
       " ('Job Types: Full-time, Contract \\n Pay: $90.00 - $95.00 per hour \\n Schedule:',\n",
       "  0),\n",
       " ('8 hour shift \\n Monday to Friday', 0),\n",
       " ('Experience: \\n \\n Oracle ERP: 5 years (Required) \\n Oracle EBS: 4 years (Required)',\n",
       "  1),\n",
       " ('Service contracts background: 5 years (Required) \\n Business analyst: 10 years (Required) \\n \\n Work Location: In person',\n",
       "  0),\n",
       " ('Title:  Senior Data Engineer - Cloud Infrastructure and Video \\n  Department:  BIDS \\n  Reports to:  Chief Data Scientist \\n \\n  About Sports Info Solutions \\n  Pioneers in the Sports Data Industry \\n  SIS was founded on the belief that decision making in sports could be improved and that we could help teams win more games through the use of better data, analytics, and technology. \\n  That belief has been validated repeatedly since our founding in 2002 as we continue to revolutionize the way the game is played, both on and off the field. \\n  Company overview \\n  Our mission is to enrich and optimize the decision-making process for sports teams, sportsbooks, and sports fans. \\n  We are proud to be a leader in collecting, analyzing and distributing the deepest data sets and insights to professional sports teams across the MLB, NBA and NFL. \\n  We are now doubling down on what’s made us successful by further advancing our data, technology, insights and partners as we drive forward the next innovations in Sports Data and Analytics.',\n",
       "  0),\n",
       " ('Position overview \\n  Sports Info Solutions (SIS) is looking for a new team member to fill a full time position in our BIDS department as Senior Data Engineer. \\n  We seek a skilled and experienced Data Engineer to join our dynamic and growing team at Sports Info Solutions. As a Senior Data Engineer, you will be crucial in building and maintaining our cloud-based data infrastructure, designing and optimizing data lakes for structured and semi-structured data, and ensuring seamless data integration and preparation for machine learning training and inference. \\n \\n  This position is considered remote. \\n \\n  What you’ll do as  Senior Data Engineer  on the team at SIS: \\n \\n  Includes (but is not limited to):',\n",
       "  0),\n",
       " ('Design, develop, and maintain pipelines optimized for video ingestion, processing, and management, utilizing cloud-based services for scalability and reliability. \\n  Collaborate closely with data scientists and computer vision experts to understand the requirements for building deep learning models and implement video preprocessing and transformation steps. \\n  Leverage your expertise in cloud infrastructure, e.g., AWS (preferred), GCP, or Azure, to create scalable and efficient data solutions for CV applications. \\n  Lead the development of pipelines to preprocess and augment video data, ensuring compatibility with deep learning frameworks. \\n  Build and maintain data pipelines that enable real-time or near-real-time video data processing for inference and analytics. \\n  Collaborate with cross-functional teams to deliver CV insights and predictions to clients successfully. \\n \\n \\n  Why work with SIS? \\n  We believe in making sports better through data, analysis and insights. For that reason, we have an incredible team of technologists, scouts, analysts, and operators helping our partners win more games. \\n  It is our ultimate vision to create an unparalleled platform of sporting data and insights, through best-in-class technology, products and partnerships. \\n  We believe in a flexible, energetic, enjoyable working environment where we band together as teammates to do great things. We are committed to creating a diverse environment, working in a collaborative, team-centric environment.',\n",
       "  1),\n",
       " (\"Qualifications \\n  If you possess the following, you are well on your way to making an impact at SIS: \\n \\n  Bachelor's degree in Computer Science, Engineering, a related field, or equivalent work experience; advanced degree is a plus. \\n  Proven experience designing and implementing data pipelines for video ingest and management in a cloud environment. \\n  Strong understanding of cloud-based services and infrastructure and their video processing, content delivery, and storage integration. \\n  Strong programming skills in languages such as Python (preferred), Rust, C++, or Java. \\n  Desire and ability to write maintainable, tested code. \\n  Knowledge of video codecs, formats, and processing techniques for efficient video data handling. \\n  Familiarity with machine learning concepts and experience preparing data for training and inference purposes. \\n  Familiarity with computer vision concepts and practices, including data preprocessing, augmentation, and annotation.\",\n",
       "  1),\n",
       " ('Strong problem-solving skills and ability to work collaboratively in a fast-paced environment. \\n  Excellent communication skills to convey complex concepts to technical and non-technical stakeholders. \\n \\n \\n  EEO commitment \\n  SIS provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, gender, national origin, age, veteran status, military status, disability, gender identity, sexual orientation, genetic information, or any other characteristic protected by law. In addition to federal law requirements, SIS complies with applicable state and local laws governing nondiscrimination in employment in every location where the company operates. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. \\n  Abilities required \\n  These physical demands are representative of the physical requirements necessary for an employee to successfully perform the essential functions of the job. Reasonable accommodation can be made to enable people with disabilities to perform the described essential functions of the job. \\n  While performing the responsibilities of the job, the employee is required to talk and hear. The employee is often required to sit and use their hands and fingers, to handle or feel. The employee is occasionally required to stand, walk, reach with arms and hands, climb or balance, and to stoop, kneel, crouch or crawl. Vision abilities required by this job include close vision, including intensive computer usage. \\n  Additional info \\n  Sponsorship is not available for this position. Applicants must be currently authorized to work in the United States on a full-time basis.',\n",
       "  0),\n",
       " ('Overview: \\n  \\n  Build the Future \\n \\n \\n   Could your creative thinking build the future? As a VP of Data & AI Platform Innovation at McGraw Hill, make a difference for learners and educators across the world. Our team needs individuals with new ideas who connect with people in innovative ways.\\n  \\n \\n \\n  What is this role about?',\n",
       "  0),\n",
       " ('The VP of Data & AI Platform Innovation will lead the Enterprise Data & Analytics (D&A) Data Engineering and ML Engineering Teams for McGraw Hill and will report directly to the Head of Enterprise Data & Analytics (D&A). This role will initially focus on leading the design and delivery, cross-functionally, of data & AI platforms we need to deliver on our major digital transformation initiatives at McGraw Hill. As these D&A platforms go live, this role will lead the teams responsible for operations and support of our modernized D&A platforms. This role will empower our products to leverage data, AI, and information assets as a true competitive advantage for McGraw Hill.\\n  \\n \\n \\n  What can you expect from the position? \\n \\n \\n  Modernized Data & AI Platform Architecture must be one of your “superpowers”. \\n  Provide strategic insights and direction to Chief Product Officer(s) to inform how future product strategy is delivered with modernized and/or net new data platforms AND source data systems (Oracle, SFDC, etc.); must provide our Chief Product Officer(s) with business cases/justification for your recommended path forward. \\n  Responsible for partnering with cross-organizational business lines and functions to establish the framework for and execute the systems transformation, both data platforms and source data systems, ensuring cross-functional alignment with our IT (GTS) and Product Development (DPG) teams \\n  Must drive design and delivery of technical solutions with a focus on business simplification by increasing operational efficiency and reducing manual processes, while supporting financial reporting requirements \\n  Assess and define what metadata/data design requirements are needed to ensure product data & AI strategy can be delivered as part of broader product strategy; must call out gaps in data ownership for CPO(s) to ensure CPO(s) are able to work with appropriate senior leadership to assign data ownership and close gaps.',\n",
       "  1),\n",
       " ('Efficiently assess enterprise systems, processes, operations, data, AI and financial reporting to document areas of impact for remediation \\n  Utilize business process techniques to map which business processes can be improved through technology, and then define the correct KPIs to measure targeted improvement of end-to-end business processes as we progress with our digital transformation. \\n  Assesses risk potentials and discover potential problems before they occur, and flag for CPO(s) to ensure CPO(s) can incorporate into broader communications and product strategy planning. \\n  Identify and address resource constraints. \\n  Contribute to business cases to secure funding. \\n  Create and monitor project plans. \\n  Write clear and well-structured business requirements and documentation. \\n  Implement automation where there is opportunity. \\n  Generate organizational confidence and buy-in by raising awareness, committing to accuracy, and operating with precision.',\n",
       "  0),\n",
       " ('We are looking for someone with: \\n \\n \\n  Extensive Knowledge of Data Platform (Data Lakes/Data Warehouses) Tools – At least 13+ years experience with modernized stack of Data platform capabilities across some combination of AWS, Databricks, Azure, etc. and how to best leverage to create delightful user experiences in a scalable and cost-efficient manner. \\n  Extensive Knowledge of Self-Service BI (SSBI) Tools – At least 10+ years experience with modernized stack of SSBI tools (PowerBI, LookerBI, etc.) and how to best leverage to create delightful customer experiences. \\n  System Knowledge – At least 8+ years of experience with quickly learning enterprise-wide systems (Oracle, SFDC, EDW) and their relationship; databases; modeling software. \\n  Operational Leader- At least 10+ years of experience successfully in leading and implementing improvements. Experience in change management roles including in managing large complex cross-company transformation programs, while defining clear strategy to achieve vision, milestones, and measuring progress for each major milestone; must have strong financial acumen. \\n  Communication – Must have 13+ years of experience interacting and persuading individuals in senior-level positions, with advanced writing and communication skills, as well as actively participating in meetings and group discussions with individuals from various functions across the organization (e.g., IT, Finance, Product). \\n  Analytical – Must have 8+ years of experience synthesizing information from cross functional areas to draw insights and actionable recommendations (e.g., structure the problem, collect data, identify issues, and present results). Individual should have strong quantitative skills with a portion of their background familiar with business case analysis and corporate data analysis. \\n \\n \\n  Preferred experience:',\n",
       "  1),\n",
       " ('Multi-Cloud Data Platform Experience – Ideally, candidate has run multi-cloud environments, in addition to exposure to multiple cloud providers.\\n  \\n \\n   Financial – Ideally, candidate will have 5+ years experience with or knowledge of corporate finance and accounting.\\n  \\n \\n \\n  Why work for us?',\n",
       "  0),\n",
       " (\"** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \\n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \\n Responsibilities: \\n \\n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \\n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \\n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \\n Maintain data pipelines & data systems\",\n",
       "  1),\n",
       " ('Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \\n \\n Requirements: \\n \\n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \\n Experienced with AWS services and AWS-native CI/CD tools \\n Experienced designing data pipelines & data solutions \\n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy',\n",
       "  1),\n",
       " ('Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \\n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \\n Ability to work independently and integrate with other team members \\n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \\n Excellent communication skills (written and verbal) \\n \\n Our Vetting Process \\n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position:',\n",
       "  1),\n",
       " ('Application (5 minutes) \\n Online Assessment (40 minutes) \\n Initial Phone Interview (30-45 minutes) \\n 2-3 Interviews with the Client \\n Job Offer! \\n \\n Job Type: Full-time',\n",
       "  0),\n",
       " ('Pay: $130,000.00 - $160,000.00 per year \\n Benefits: \\n \\n 401(k) \\n Dental insurance \\n Health insurance \\n \\n Schedule:',\n",
       "  0),\n",
       " (\"** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \\n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \\n Responsibilities: \\n \\n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \\n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \\n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \\n Maintain data pipelines & data systems\",\n",
       "  1),\n",
       " ('Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \\n \\n Requirements: \\n \\n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \\n Experienced with AWS services and AWS-native CI/CD tools \\n Experienced designing data pipelines & data solutions \\n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy',\n",
       "  1),\n",
       " ('Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \\n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \\n Ability to work independently and integrate with other team members \\n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \\n Excellent communication skills (written and verbal) \\n \\n Our Vetting Process \\n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position:',\n",
       "  1),\n",
       " ('Application (5 minutes) \\n Online Assessment (40 minutes) \\n Initial Phone Interview (30-45 minutes) \\n 2-3 Interviews with the Client \\n Job Offer! \\n \\n Job Type: Full-time',\n",
       "  0),\n",
       " ('Pay: $130,000.00 - $160,000.00 per year \\n Benefits: \\n \\n 401(k) \\n Dental insurance \\n Health insurance \\n \\n Schedule:',\n",
       "  0),\n",
       " ('About the job \\n \\n \\n \\n Alignment to Data Strategy Vision: \\n \\n  Senior-level client and team analytics leader and problem-solver. \\n  Focus on data strategy – aligning data resources and collection and analytics methods to client goals and questions. \\n  Team leadership – lead and mentor resources to scale team engagement and support small- and large-scale client loyalty engagements from initiation to data-driven loyalty. \\n  Build strong client relationships – able to discuss data and analytics in client-accessible ways, empowering the client to socialize findings confidently. \\n  Successful track record delivering actionable insights and recommendations to clients from user-level, behavioral data. \\n  Create client growth by proactively identifying P&LS-addressable opportunities from data, analysis, and client context. \\n  Orchestrate and delegate analysis efforts to offshore resources, ensuring that the solution addresses the problem/question appropriately and reliably.',\n",
       "  0),\n",
       " ('Document and drive data requirements to execute data-driven marketing programs (targeting, personalization, and relevance). \\n  Experienced to discuss and guide the choice of analytical approaches to achieve objectives and outcomes. \\n  Support development of new capabilities (AI, Emotion). \\n \\n \\n  Responsibilities: \\n \\n  Create client growth by proactively identifying and defining data-focused solutions supporting client objectives - build them into blueprints, measurement plans, pitches, and strategy decks. \\n  Lead analytics and measurement for Strategic accounts, leveraging and integrating the LXM approach. \\n  Develop and/or oversee measurement plans and strategies to provide program insights, address learning objectives and improve the effectiveness of the program. \\n  Write and/or review briefs to overseas resources, defining and specifying requirements for Data Science projects (predictive analytics, ML, AI) and lead/oversee the delivery of solutions from the results. \\n  Partner with external analytics stakeholders to confidently drive scalable solutions to address a client challenges (i.e., defining an approach, shaping the methods to apply to specific use case). \\n  Provide thought leadership and act as the subject matter expert in the identification and deployment of data-driven solutions.',\n",
       "  0),\n",
       " ('Translate unclear “draft” learning objectives into clear, executable data analysis that addresses causality as well as connection. \\n  Present findings to clients clearly and cogently, with a focus on delivering understanding that the client can confidently translate to their teams, peers and leadership. \\n  Where needed, apply statistical concepts and techniques to analyze experiments and predict user behavior and behavioral drivers for accuracy and client alignment. \\n \\n \\n \\n \\n \\n \\n Qualifications',\n",
       "  0),\n",
       " ('8+ years’ experience in data science or analytics roles delivering reporting, data-driven insights, predictive modeling, and recommendations to internal or external clients. \\n  Record of excellence developing integrated long-term analytics programs for clients, based on goals and business model. \\n  Master’s (preferred) degree in a quantitative field (STEM or quantitative social sciences), or STEM Bachelor’s degree with MBA. \\n  Demonstrated strong presentation and storytelling with data skills. Be able to deliver clear value from effort while simplifying the complexity for business audience while partnering with other disciplines to deliver on recommendations. \\n  Ability to confidently query, manipulate, extract and analyze data using Python, R and/or SQL to derive strong, supportable data-driven conclusions. \\n  Experience with cloud-based big data platforms – e.g., Redshift, Big Query, Teradata. \\n  Experience using Machine Learning and statistical data analysis tool/libraries such as NumPy, SciPy, scikit-learn, or equivalent to drive effective predictive modeling using Regression, Clustering, Scoring. \\n  Proficient on querying and reducing data from large database repositories and/or flat files and combining those data to generate and extend dimensional analysis and modeling. \\n  Knowledge of the digital marketing ecosystem and fluency with common ad tech platforms and technologies. \\n  Measurement experience across multiple marketing domains (e.g., media and CRM). \\n  Experimental design and offer testing – both theory and practice. \\n  Experience and competence with Power BI or other front-end visualization tools. \\n  Strong and demonstrated strategic thinking and business acumen.',\n",
       "  1),\n",
       " ('Strong organizational skills – task prioritization, effective time management, meeting facilitation, conflict resolution, and risk identification/mitigation. \\n \\n \\n \\n \\n \\n Additional Information \\n \\n \\n Employees from diverse or underrepresented backgrounds encouraged to apply.  Dentsu (the \"Company\") is committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee of the Company, on the basis of age, sex, sexual orientation, race, color, creed, religion, ethnicity, national origin, alienage or citizenship, disability, marital status, veteran or military status, genetic information, or any other legally-recognized protected basis under federal, state or local laws, regulations or ordinances. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and/or certain state or local laws. A reasonable accommodation is a change in the way things are normally done that will ensure an equal employment opportunity without imposing an undue hardship on the Company. Please contact recruiting@dentsuaegis.com if you need assistance completing any forms or to otherwise participate in the application process or to request or discuss an accommodation in connection with a job at the Company to which you are applying. \\n  The anticipated salary range for this position is $136,000-$219,650. Salary is based on a range of factors that include relevant experience, knowledge, skills, other job-related qualifications, and geography. \\n  #LI-MZ1',\n",
       "  0),\n",
       " ('Founded in 2003, IT Concepts’ core values – customer-centricity, teamwork, driven to deliver, innovation, and integrity – ensure we work together to be the best, realize objectives, and make a positive impact in our communities. We intentionally created and sustain our ITC culture that embraces change, experimentation, continuous learning, and improvement. We bring our design thinking problem solving approach that challenges assumptions, prioritizes curiosity, and invites complexity to deliver innovative, efficient, and effective solutions. As we continue to grow in the support of our government customers, we are looking for driven and innovative individuals to join our team. \\n  IT Concepts is seeking to hire a qualified Sr. Functional/Business Analyst to support our DOC customer. The Salesforce platform and application software development will support ITA’s Office of the Chief Information Officer (OCIO) in the delivery and maintenance of the mission critical Salesforce platform and applications. As a Sr. Functional/Business Analyst you will be part of a team helping DOC, ITA achieve mission-driven success. \\n  Responsibilities: \\n \\n  Collaborate with stakeholders and the development team, analyzing business and user needs to capture and refine system requirements and develop user stories  \\n \\n \\n Collaborate with stakeholders and team members to provide system requirement refinement, user story confirmation,  \\n Translate business requirements into solutions in the Salesforce environment.  \\n Understand the impacts of business requirements on the configuration, processes, and workflows within Salesforce.  \\n Collaborate on UAT test scenarios, developing test scripts, UI/UX testing (as needed) and conducting testing and verification exercises.  \\n Lead the development and monitoring of user acceptance criteria.',\n",
       "  0),\n",
       " ('Ensuring high-quality deliveries of code through the development and maintenance of test cases, for initial validation of new features as well as for existing features updated due to requirements/environmental changes.  \\n Performing functional testing, regression testing and keep track of all the new developments.  \\n Producing the evaluated test/defects reports and take part in software walk through.  \\n Working alongside a team of Business Analysts, Business stakeholders and developers to deliver Agile solutions.  \\n Participating in the product increment planning and document test plan and test scenarios in collaboration with the product owner, product analysts and engineers.  \\n \\n \\n Conducting peer reviews of test cases and test documents.  \\n \\n Requirements \\n \\n Bachelor’s degree in Computer Science, Mathematics, Information Technology or related field  \\n Experience supporting Salesforce platform and application development efforts for federal government.  \\n 4+ years of experience with requirements analysis, solutioning, functional/technical design documentation, and training',\n",
       "  1),\n",
       " ('4+ years of Salesforce Business Analyst experience, providing functional support for the development of Salesforce applications.  \\n Experience using common enterprise agile planning tools, such as Jira.  \\n Proven experience working in Agile/Scrum development methodologies.  \\n Strong ability to guide and work with cross-functional teams.  \\n Experience supporting federal IT projects.  \\n \\n \\n Excellent at identifying and solving problems.  \\n Must be a U.S Citizen  \\n Required Trailhead Certifications:',\n",
       "  1),\n",
       " ('Salesforce Business Analyst; or  \\n Salesforce Certified Administrator  \\n \\n \\n \\n Preferred :  \\n \\n Experience being part of Salesforce implementations at the Department of Commerce  \\n \\n Benefits \\n  The Company   \\n We believe in generating success collaboratively, enabling long-term mission success, and building trust for the next challenge. With you as our partner, let’s solve challenges, think innovatively, and maximize impact. As a valued member of our ITC community, you have the unique opportunity to work in a diverse range of technology and business career paths, all while supporting our nation and delivering innovative technology solutions. We are a close community of experts that pride ourselves on creating an environment defined by teamwork, dedication, and excellence.  \\n We hold three ISO certifications (27001:2013, 20000-1:2011, 9001:2015) and two CMMI ML 3 ratings (DEV and SVC).  \\n Industry Recognition',\n",
       "  0),\n",
       " (\"Growth | Inc 5000’s Fastest Growing Private Companies, DC Metro List Fastest Growing; Washington Business Journal: Fastest Growing Companies, Top Performing Small Technology Companies in Greater D.C.  \\n Culture | Northern Virginia Technology Council Tech 100 Honoree; Virginia Best Place to Work; Washington Business Journal: Best Places to Work, Corporate Diversity Index Winner – Mid-Size Companies, Companies Owned by People of Color; Department of Labor’s HireVets for our work helping veterans transition; SECAF Award of Excellence finalist; Victory Military Friendly Brand; Virginia Values Veterans (V3); Cystic Fibrosis Foundation Corporate Breath Award  \\n Benefits   \\n We offer great benefits – Competitive Paid Time Off, Medical, Dental and Vision Insurance, Identity Theft Protection, Legal Resources Coverage, 401(k) with company matching with NO vesting period. ITC Health benefits have $0 premium for certain plans as an employee.  \\n We invest in our employees – Every employee is eligible for education reimbursement for certifications, degrees, or professional development. Reimbursement amount may fluctuate due to IRS limitations. We want you to grow as an expert and a leader and offer flexibility for you to take a course, complete a certification, or other professional growth and networking. We are committed to supporting your curiosity and sustaining a culture that prioritizes commitment to continuous professional development.  \\n We work hard, we play hard. ITC is committed to incorporating fun into every day. We dedicate funds for activities – virtual and in-person – e.g., we have free tickets to Nationals games available upon employee request; we host happy hours, holiday events, fitness events, and annual celebrations. In alignment with our commitment to our communities, we host and attend charity galas/events. We believe in appreciating your commitment and building a positive workspace for you to be creative, innovative, and happy.  \\n AAEO & VEVRAA   \\n IT Concepts is an Affirmative Action/Equal Opportunity employer and a VEVRAA (Vietnam Era Veterans' Readjustment Assistance Act) Federal Contractor. As such, any personnel decisions (hire, promotion, job status, etc.) on applicants and/or employees are based on merit, qualifications, competence and business needs, not on race, color, citizenship status, national origin, ancestry, sexual orientation, gender identity, age, religion, creed, physical or mental disability, pregnancy, childbirth or related medical condition, genetic information of the employee or family member of the employee, marital status, veteran status, political affiliation, or any other factor protected by federal, state or local law.  \\n IT Concepts maintains a strong commitment to compliance with VEVRAA and other applicable federal, state, and local laws governing equal employment opportunity. We have developed comprehensive policies and procedures to ensure that our hiring practices align with these requirements.  \\n As a part of our VEVRAA compliance efforts, [Company Name] has established an affirmative action plan that outlines our commitment to the recruitment, hiring, and advancement of protected veterans. This plan is regularly reviewed and updated to ensure its effectiveness.  \\n We encourage protected veterans to self-identify during the application process. This information is strictly confidential and will only be used for reporting and compliance purposes as required by law. Providing this information is voluntary, and it will not impact your eligibility for employment.  \\n Our commitment to equal employment opportunity extends beyond legal compliance. We are dedicated to fostering an inclusive workplace where all employees, including protected veterans, are treated with dignity, respect, and fairness.  \\n How to Apply   \\n To apply to IT Concept Position- Please click on the: “Apply for this Job” button at the bottom of this Job Description or the button at the top: “Application.” You can upload your resume and complete all the application steps. You must submit the application for IT Concepts to receive. If you need alternative application methods, please email careers@useitc.com and request assistance.\",\n",
       "  0),\n",
       " ('Responsibilities: \\n \\n Creating and maintaining required documentation for projects including business requirements, design documents, and test scripts. \\n Conduct business process analysis through input from management, managers, supervisors, and subject matter experts. \\n Facilitate JAD sessions for creating design documents and system specifications for applications. \\n Involvement in managing functional and non-functional Business Requirements and new requested requirements. \\n \\n Requirements:',\n",
       "  0),\n",
       " ('Master’s degree or bachelor’s degree in Business, Management, Science, Technology, Engineering, or any relevant. \\n Good understanding of business requirements gathering, business process mapping, functional design, and documentation. \\n Knowledge in creating BRD, FRS documents, URS, and CR documents for system application development. \\n Knowledge of the full SDLC methodologies, frameworks, and analysis to compare As-Is with To-Be business processes. \\n \\n Job Type: Full-time \\n Pay: $65,000.00 - $75,000.00 per year',\n",
       "  0),\n",
       " ('Benefits: \\n \\n 401(k) \\n Green card sponsorship \\n Health insurance \\n Health savings account \\n Visa sponsorship',\n",
       "  0),\n",
       " ('Compensation package: \\n \\n Bonus opportunities \\n Hourly pay \\n Performance bonus \\n \\n Experience level:',\n",
       "  0),\n",
       " ('No experience needed \\n \\n Schedule: \\n \\n 8 hour shift \\n Monday to Friday \\n \\n Education:',\n",
       "  0),\n",
       " ('Qlarant is a not-for-profit corporation that partners with public and private sectors to create high quality, safe, and efficient delivery of health care and human services programs. We have multiple lines of business including utilization review, managed care organization quality review, and quality assurance for programs serving individuals with developmental disabilities. Qlarant is also a national leader in fighting fraud, waste and abuse for large organizations across the country. In addition, our Foundation provides grant opportunities to those with programs for under-served communities.\\n  \\n \\n   Are you seeking to begin or grow your career in healthcare data analysis? Qlarant has the perfect opportunity for a motivated candidate to join our PPI MEDIC data analytics team! This position requires strong analytical skills, knowledge of SAS and SQL (preferred) and/or other statistical programming (R, Python) and database languages and a willingness to learn and contribute.\\n  \\n \\n \\n \\n   This is a remote position but could be located in our Easton or Baltimore, MD offices if preferred. We offer opportunities for advancement, a collaborative and inclusive work environment and a very competitive benefits package.',\n",
       "  1),\n",
       " ('In this role you will be a part of a dedicated and talented data analytics team focused on making a positive difference in the future of our nation’s healthcare programs. The PPI MEDIC supports the Centers for Medicare and Medicaid’s fraud, waste and abuse monitoring efforts in the Medicare Parts C and D programs. The primary goal of the PPI MEDIC is to analyze Parts C and Part D data, conduct audits of plans, provide outreach and education to plan sponsors, and ensure compliance with regulatory requirements of Medicare Parts C and D.\\n  \\n \\n \\n \\n   The Data Analyst III is an entry level professional position that performs study design, data analysis, and report preparation. Studies originate from preliminary data analysis (trends), literature review, experience and expertise of the team, and mandated projects. Data analysis, including data preparation and presentation of findings is performed in conjunction with other analysts. Reports are drafted by teams with leadership of Scientists.\\n  \\n  Essential Duties and Responsibilities  include the following. Other duties may be assigned. \\n \\n Trend data to identify potential opportunities (e.g., variances, significant outliers, percentile ranked groups) for quality improvement or focused investigations. \\n Aid in design data analysis strategies to identify potential areas for quality improvement or focused investigation. \\n Analyze data, draw conclusions, and summarize into quality indicator values \\n Develop tabular and graphical presentations of data, which clearly and concisely illustrate current levels of care.',\n",
       "  0),\n",
       " ('Populate tabular and graphical presentation of data. \\n Contribute to the development of interventions (i.e., develop educational materials for doctors and nurses) which will improve healthcare processes and outcomes. \\n Facilitate design re-measurement strategies (after intervention) of healthcare processes and outcomes to effectively quantify impact of interventions for improvement. \\n Analyze re-measurement data and summarize into quality indicator values. \\n Support development of reports concerning all of the above. \\n May mentor junior Data Analysts in technical aspects of their work. \\n Assist in preparing findings for publishing in peer reviewed journals. \\n Familiar with commonly used concepts, practices and procedures, relying on instructions and pre-established guidelines to perform the functions of the job. \\n \\n Supervisory Responsibilities \\n  This job has no supervisory responsibilities.',\n",
       "  0),\n",
       " ('Required Skills\\n  \\n To perform the job successfully, an individual should demonstrate the following competencies: \\n \\n Analytical  - Synthesizes complex or diverse information; Collects and researches data; Uses intuition and experience to complement data. \\n Problem Solving  - Identifies and resolves problems in a timely manner; Gathers and analyzes information skillfully; Develops alternative solutions. \\n Judgment  - Exhibits sound and accurate judgment; Supports and explains reasoning for decisions. \\n \\n   Other Skills and Abilities \\n \\n To perform this job successfully, an individual should have fluency in SAS or other statistical programming software and MS Office. \\n Ability to work independently and in teams. \\n Must possess familiarity with statistical and epidemiological methodologies, and automation techniques for analytic tasks.',\n",
       "  1),\n",
       " (\"Ability to work with highly sensitive information while preserving the confidentiality of the information. \\n Working knowledge of healthcare systems, Medicare/Medicaid preferred, healthcare databases and coding systems. \\n \\n \\n \\n  Required Experience\\n  \\n \\n Bachelor's degree (BA or BS) in Statistics, Biostatistics, Epidemiology, Public Health or related discipline (e.g., Economics, Mathematics, Engineering, Computer Science) required. \\n Minimum of 2 years of hands-on experience with SAS and SQL. Other statistical programming work experience, or demonstrated combination of education and experience (Masters degree and project/internship experience using SAS and SQL) will be considered. \\n 6 months experience with health related analytic research and quality improvement methodology (ISO, CQI, TQM, Six Sigma, Lean, Etc.) preferred. \\n Intermediate or better ability to utilize Microsoft Office to include Excel, Word and Outlook.\",\n",
       "  1),\n",
       " (\"Claims Analyst I with AllCare Health in Grants Pass, Oregon \\n PLEASE NOTE:  this is a telecommute/remote position - however, due to our pay practices, qualified candidates MUST reside within one of the following states: OR, WA, CA, NV, AK, ID, MT, CO, UT, NM, TX, KS, IN, TN, SC, VA, GA and NY. While candidates need to live or relocate to one of the aforementioned states, working remotely will require reliable broadband internet and personal cell phone service. Telecommute/remote work may include working day-to-day operations during AllCare Health’s standard business hours (Pacific Standard Time). \\n Summary \\n This position is responsible for working closely with the Claims Processing Manager and the Claims and Encounter Data Analyst along with other staff to review, monitor, audit and adjudicate claims. \\n Essential Duties \\n 1. Accurately and efficiently adjudicates all claims applying benefits and pricing according to company policy, contract language and Medicare/Medicaid. \\n 2. Evaluates pending claims to identify errors and takes appropriate action. \\n 3. Take calls from Billing offices or Providers who have questions regarding claims status or processing. \\n 4. Respectfully provides communication to billing offices and providers who may need guidance and education on billing processes. \\n Education & Experience \\n \\n Associate's degree (A. A.) or equivalent from two-year College or technical school; or one to three years related experience and/or training; or equivalent combination of education and experience is required.\",\n",
       "  0),\n",
       " ('Knowledge of medical terminology, ICD-10CM, ICD10PCS, Current Procedural Terminology, Healthcare Common Procedure Coding Systems, CMS1500 and UB04 and other claim forms is required. \\n EZ-Cap experience preferred. \\n PACE knowledge is preferred. \\n \\n Certificates, Licenses and/or Registrations \\n \\n A Certified Professional Coder certificate is required or to be obtained within 12 months of hire date. \\n \\n Company Overview \\n AllCare offers competitive wages, an excellent benefit package that includes a 401k retirement, wellness program, and telecommute and flexible work schedule programs. \\n Since 2016, AllCare Health has been a Certified B Corp®. As a Certified B Corp®, AllCare Health considers its impact on society and the environment during the business decision-making process, and has long recognized the real value in social, economic, and environmental concerns of its stakeholders, including its employees, customers, and community members. \\n Grants Pass is located in Southern Oregon on the Rogue River and is surrounded by mountains and breathtaking views. The community is ideal for families and outdoor enthusiasts, with a temperate climate and easy access to outdoor recreation, wineries, outdoor concerts, the Ashland Shakespeare Festival, and much more.',\n",
       "  0),\n",
       " ('AllCare Health: https://www.allcarehealth.com/ \\n B Lab: https://bcorporation.net/about-b-corps \\n Job Type: Full-time \\n Pay: $20.00 - $23.00 per hour \\n Benefits: \\n \\n 401(k) \\n Dental insurance \\n Employee assistance program \\n Flexible schedule \\n Health insurance \\n Life insurance',\n",
       "  0),\n",
       " ('Paid time off \\n Professional development assistance \\n Referral program \\n Tuition reimbursement \\n Vision insurance \\n \\n Schedule: \\n \\n Monday to Friday \\n \\n Work setting:',\n",
       "  0),\n",
       " ('Remote \\n \\n Education: \\n \\n Associate (Preferred) \\n \\n Experience: \\n \\n EZ-Cap: 1 year (Preferred) \\n claims: 1 year (Preferred) \\n medical coding: 1 year (Preferred)',\n",
       "  0),\n",
       " ('We are Cognizant Artificial Intelligence \\n  Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. However, clients need new business models built from analyzing customers and business operations at every angle to really understand them. \\n  With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks \\n  * You must be legally authorized to work in United States without the need of employer sponsorship, now or at any time in the future * \\n \\n  This is a remote position open to any qualified applicant in the United States \\n \\n \\n Job Title: Azure BI Engineer \\n  Experience: 10 to 14 years \\n \\n Must Have Skills \\n \\n Databricks \\n \\n \\n Azure Analysis Services',\n",
       "  1),\n",
       " ('MS Power BI \\n \\n Good To Have Skills \\n \\n Azure Storage \\n \\n \\n Azure DW \\n \\n \\n Azure Data Factory \\n \\n Responsibilities \\n \\n Coordinate and oversee MS-BI Development by offshore (India) team members and distribution schedule to ensure timely delivery to customers. \\n Ensuring the highest quality for every project/request. Responsible for error resolution, follow up and performance metrics monitoring.  \\n Convert concepts to technical architecture, design, and implementation.',\n",
       "  1),\n",
       " ('Design and build BI solutions with an emphasis on performance, scalability, and high reliability. \\n Design and Develop BI solutions using Microsoft  Power BI , MS SQL \\n Develop data curation pipelines for reporting needs using  ADLS/ADF  and  Azure Databricks (Spark & SQL) \\n Design and Build Model using  Azure Analysis Services (AAS) \\n Manage creation of comprehensive workflows for the production and distribution of assigned reports, document reporting processes and procedures. \\n Contribute to building a team of top-performing data technology professionals. \\n Unit testing, Support System Testing and UAT  \\n Code optimization / tuning under guidance from Architects \\n Perform peer reviews. \\n Excellent problem-solving skills. \\n Self-motivated, takes initiative to identify, communicate, and resolve potential issues \\n \\n Experience \\n  Experience in MS-BI Development using Power BI MS SQL  Experience in data curation using - ADLS Data Bricks (Spark & SQL)  Exposure to Healthcare Payer BI implementation is an added plus \\n \\n  Salary and Other Compensation :',\n",
       "  1),\n",
       " ('The annual salary for this position is depending on experience and other qualifications of the successful candidate.\\n  \\n \\n   This position is also eligible for Cognizant’s discretionary annual incentive program, based on performance and subject to the terms of Cognizant’s applicable plans.\\n  \\n \\n \\n \\n  Benefits : Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:\\n  \\n \\n Medical/Dental/Vision/Life Insurance \\n Paid holidays plus Paid Time Off \\n 401(k) plan and contributions \\n Long-term/Short-term Disability \\n Paid Parental Leave \\n Employee Stock Purchase Plan',\n",
       "  0),\n",
       " (\"Disclaimer:  The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.\\n  \\n \\n \\n \\n  #LIMO1 #CB #Ind123 \\n \\n  Employee Status :  Full Time Employee \\n  Shift :  Day Job \\n  Travel :  No \\n  Job Posting :  Sep 08 2023 \\n \\n \\n  About Cognizant \\n  Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.\\n  \\n  Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.\",\n",
       "  0),\n",
       " (\"** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \\n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \\n Responsibilities: \\n \\n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \\n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \\n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \\n Maintain data pipelines & data systems\",\n",
       "  1),\n",
       " ('Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \\n \\n Requirements: \\n \\n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \\n Experienced with AWS services and AWS-native CI/CD tools \\n Experienced designing data pipelines & data solutions \\n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy',\n",
       "  1),\n",
       " ('Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \\n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \\n Ability to work independently and integrate with other team members \\n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \\n Excellent communication skills (written and verbal) \\n \\n Our Vetting Process \\n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position:',\n",
       "  1),\n",
       " ('Application (5 minutes) \\n Online Assessment (40 minutes) \\n Initial Phone Interview (30-45 minutes) \\n 2-3 Interviews with the Client \\n Job Offer! \\n \\n Job Type: Full-time',\n",
       "  0),\n",
       " ('Pay: $130,000.00 - $160,000.00 per year \\n Benefits: \\n \\n 401(k) \\n Dental insurance \\n Health insurance \\n \\n Schedule:',\n",
       "  0),\n",
       " (\"** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \\n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \\n Responsibilities: \\n \\n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \\n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \\n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \\n Maintain data pipelines & data systems\",\n",
       "  1),\n",
       " ('Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \\n \\n Requirements: \\n \\n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \\n Experienced with AWS services and AWS-native CI/CD tools \\n Experienced designing data pipelines & data solutions \\n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy',\n",
       "  1),\n",
       " ('Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \\n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \\n Ability to work independently and integrate with other team members \\n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \\n Excellent communication skills (written and verbal) \\n \\n Our Vetting Process \\n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position:',\n",
       "  1),\n",
       " ('Application (5 minutes) \\n Online Assessment (40 minutes) \\n Initial Phone Interview (30-45 minutes) \\n 2-3 Interviews with the Client \\n Job Offer! \\n \\n Job Type: Full-time',\n",
       "  0),\n",
       " ('Pay: $130,000.00 - $160,000.00 per year \\n Benefits: \\n \\n 401(k) \\n Dental insurance \\n Health insurance \\n \\n Schedule:',\n",
       "  0),\n",
       " (\"About Pinterest : \\n  Millions of people across the world come to Pinterest to find new ideas every day. It's where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you'll be challenged to take on work that upholds this mission and pushes Pinterest forward. You'll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet. \\n  Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences. \\n  Our new progressive work model is called PinFlex, a term that's uniquely Pinterest to describe our flexible approach to living and working. Visit our  PinFlex  landing page to learn more. \\n \\n  The Advanced Technologies Group (ATG) is Pinterest's advanced machine learning team. It keeps Pinterest at the forefront of machine learning technology across multiple application areas including recommendations, ranking, content understanding, and more. It is a high impact applied team that works horizontally across the company on state of the art AI and ML and works on directly bringing that technology to the product in collaboration with product engineering teams. The team publishes its work in applied research conferences, but the main contribution of the team's work is to drive top line metric impact across the company for our 400M+ monthly active users. \\n  What you'll do:\",\n",
       "  0),\n",
       " (\"Lead projects that involve developing and deploying state of the art (and beyond) ML models in production systems across the company at scale for hundreds of millions of users. \\n Help to define and drive forward looking ML strategy for the team and across the company. \\n Collaborate with other engineering teams (infrastructure, user modeling, content understanding) to leverage their platforms and signals and work with them to collaborate on the adoption and evaluation of new technologies. \\n Mentor junior engineers on the ATG and partner teams and help to uplevel ML talent across the company. \\n \\n What we're looking for: \\n \\n Experience with state of the art ML modeling techniques and approaches like transformers, self supervised pre-training, generative modeling, LLMs, etc.\",\n",
       "  1),\n",
       " ('Experience with large scale data processing (e.g. Hive, Scalding, Spark, Hadoop, Map-reduce) \\n Hands-on experience training and applying models at scale using deep learning frameworks like PyTorch or Tensorflow. Successful candidates in this role need to be able to build bridge state of the art approaches to real world impact. \\n 8+ years working experience in the engineering teams that build large-scale ML-driven user-facing products \\n 3+ years experience leading cross-team engineering efforts. \\n Strong execution skills in project management \\n Masters or PhD in Comp Sci or related fields \\n Understanding of an object-oriented programming language (Java, C++, Python)',\n",
       "  1),\n",
       " ('Desired skills: \\n \\n Experience in working on, backend and ML systems for large-scale user-facing products, and have a good understanding of how they all work. \\n Experience in closely collaborating with other engineering teams to ship new ML technologies to improve recommendation, content understanding, and ranking systems at scale. \\n \\n This position is not eligible for relocation assistance. \\n  #LI-SA1 \\n  #LI-REMOTE',\n",
       "  0),\n",
       " ('At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. \\n  Information regarding the culture at Pinterest and benefits available for this position can be found  here . \\n \\n  US based applicants only \\n \\n\\t$221,000—$455,000 USD',\n",
       "  0),\n",
       " ('Note: This is not open to C2C! \\n Job Description: Reviews, analyzes, and evaluates business systems and user needs. Formulates systems to parallel overall business strategies. Writes detailed description of user needs, program functions, and steps required to develop or modify computer programs. Relies on experience and judgment to plan and accomplish goals. Performs a variety of complicated tasks. May report directly to a project lead or manager. Basic Requirements: \\n \\n 2+ years of business analyst experience \\n Bachelor’s degree or Master’s degree \\n Familiar with relational database concepts, and client-server concepts. \\n A wide degree of creativity and latitude is expected. \\n Data analytics reports experience',\n",
       "  0),\n",
       " ('Proven good data analytics skills \\n Advanced excel skills good communication \\n Can take initiative and not wait \\n SQL/writing experience \\n Only candidates available and ready to work directly as NJAI/Genesis10 employees will be considered for this position. \\n \\n Compensation : $34.00 per hour \\n If you have the described qualifications and are interested in this exciting opportunity, apply today!',\n",
       "  1),\n",
       " ('Job Types: Full-time, Contract \\n Pay: Up to $34.00 per hour \\n Benefits: \\n \\n 401(k) \\n Dental insurance \\n Health insurance \\n Paid time off',\n",
       "  0),\n",
       " ('Vision insurance \\n \\n Experience level: \\n \\n 2 years \\n \\n Schedule:', 0),\n",
       " ('8 hour shift \\n \\n Experience: \\n \\n business analyst: 2 years (Preferred) \\n SQL: 2 years (Preferred) \\n \\n License/Certification:',\n",
       "  1),\n",
       " ('Company Description\\n   It all started with an idea at Block in 2013. Initially built to take the pain out of peer-to-peer payments, Cash App has gone from a simple product with a single purpose to a dynamic ecosystem, developing unique financial products, including Afterpay/Clearpay, to provide a better way to send, spend, invest, borrow and save to our 47 million monthly active customers. We want to redefine the world’s relationship with money to make it more relatable, instantly available, and universally accessible.\\tToday, Cash App has thousands of employees working globally across office and remote locations, with a culture geared toward innovation, collaboration and impact. We’ve been a distributed team since day one, and many of our roles can be done remotely from the countries where Cash App operates. No matter the location, we tailor our experience to ensure our employees are creative, productive, and happy.\\tCheck out our locations, benefits, and more at cash.app/careers. \\n \\n \\n \\n Job Description\\n   The Financial Crimes Technology Data team at Cash App finds and reports financial crimes activity on Cash App. We work globally with partners in business, engineering, counsel and product to guarantee we are providing a safe user experience for our customers while minimizing or eliminating bad activity on our platform. \\n  We are leveraging Machine Learning as an integral part of our toolkit to fulfill our mission. As Cash App scales, we are monitoring billions of dollars in transactions across traditional payment and blockchain networks. We uncover and put an end to money laundering, fraud, and illegal activities before they impact our users. We are looking for senior and staff MLEs that can integrate vertically into the ML sub-team and be focused on building/enhancing tools, libraries, frameworks, developer environments, etc. for ML modeling workflows. \\n  This is an IC role, but the Staff level has additional leadership responsibilities that include envisioning, owning, and driving strategic roadmaps & priorities to completion by collaborating with cross functional stakeholders. The role may have the opportunity to progress into a people manager role in the future. \\n  You will:',\n",
       "  0),\n",
       " ('Design and build services and tooling that support our ML and Data Science sub-teams \\n  Be the MLOps lead and facilitate modelers on the team by unblocking access to the infrastructure/tools necessary for development & production work \\n  Develop prototypes and partner with ML modelers to encourage adoption of new tools and technologies \\n  Proactively identify new opportunities and future needs of our ML teams \\n  Lead by example by applying ML and engineering best practices \\n  Stay on top of changing ML infrastructure and libraries at Block and advocate/educate the team about new developments by sharing resources, demos and PoCs \\n  Join a new, small, and growing team and have a significant impact on influencing team culture and direction',\n",
       "  0),\n",
       " ('Qualifications\\n  \\n \\n  4-6+ years of combined Machine Learning and Engineering industry experience \\n  A graduate degree in computer science, data science, operations research, applied math, stats, physics, or a related technical field \\n  Familiarity with Linux/OS X command line, version control software (git), and general software development principles with a machine learning software development life-cycle orientation. \\n  Experience working with product, business, and engineering to prioritize, scope, design, and deploy ML models \\n  Familiarity with Python computing stack, MySQL, Snowflake, Airflow, Java/Go \\n  Hosted models for inference on public clouds like GCP, AWS and/or built micro-services to facilitate event based triggering, feature generation, model inference and downstream actioning. \\n \\n  Additional Information',\n",
       "  1),\n",
       " ('Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.\\tZone A: USD $202,500 - USD $247,500  Zone B: USD $192,400 - USD $235,200  Zone C: USD $182,300 - USD $222,800  Zone D: USD $172,200 - USD $210,400 \\n \\n  To find a location’s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information. \\n  Full-time employee benefits include the following: \\n \\n  Healthcare coverage (Medical, Vision and Dental insurance) \\n  Health Savings Account and Flexible Spending Account \\n  Retirement Plans including company match \\n  Employee Stock Purchase Program \\n  Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance \\n  Paid parental and caregiving leave',\n",
       "  0),\n",
       " (\"Paid time off (including 12 paid holidays) \\n  Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees) \\n  Learning and Development resources \\n  Paid Life insurance, AD&D, and disability benefits \\n  Additional Perks such as WFH reimbursements and free access to caregiving, legal, and discounted resources \\n \\n  These benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans. \\n  We’re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, veteran status, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class. \\n  We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible.  Want to learn more about what we’re doing to build a workplace that is fair and square? Check out our  I+D page . \\n  Additionally, we consider qualified applicants with criminal histories for employment on our team, assessing candidates in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance.\",\n",
       "  0),\n",
       " ('Description \\n This position is remote and can sit in the footprint of Providence in the states of WA, OR, TX, or MT. \\n The Data Engineer designs and builds extract code, data pipelines and transformations, data enrichment processes, provisioning layers, and secure transmission methods to support clinical and operational processes across all parts of the healthcare system. This person will place a priority on collaboration with meticulous source control and documentation, an emphasis on simple solutions to complex problems, the ability to extract very specific data in defined formats for secure transmission. Knowledge of healthcare data and experience with data extraction from an EMR essential. \\n Position requires a highly collaborative, analytical, and forward-thinking self-starter, who takes initiative and thrives in a fast-paced environments. \\n This is an exciting opportunity to be part of a team to explore data and create meaningful analysis for rest of providence and vendors involved. Providence teams are known for their dynamic, diverse, and inclusive culture, grounded in trust, hospitality, collaboration, and innovation. These are the core values that inspire our work, and what we are looking for in Data Engineer. \\n Providence caregivers are not simply valued – they’re invaluable. Join our team at Enterprise Information Services and thrive in our culture of patient-focused, whole-person care built on understanding, commitment, and mutual respect. Your voice matters here, because we know that to inspire and retain the best people, we must empower them. \\n Required Qualifications:',\n",
       "  0),\n",
       " (\"Bachelor's Degree in Computer Engineering, Computer Science, Mathematics, Engineering or equivalent education/experience \\n 1 year related experience; 1-3 years preferred \\n Healthcare data knowledge and experience \\n \\n Preferred Qualifications:\",\n",
       "  0),\n",
       " ('EPIC certification(s) \\n \\n The salary range listed for this position MIN:$43.39 to MAX:$70.03 per hour is based upon the primary work location Renton, WA posted. This position is remote. Salary range and offers are determined by internal pay equity and geographic cost of living differences. Salary range will vary from State and region. Salary max is limited to 75% range in order to continue to offer internal pay growth. We welcome open and transparent discussions on salary at Providence \\n Our best-in-class benefits are uniquely designed to support you and your family in staying well, growing professionally and achieving financial security. We take care of you, so you can focus on delivering our Mission of caring for everyone, especially the most vulnerable in our communities. \\n About Providence \\n At Providence, our strength lies in Our Promise of “Know me, care for me, ease my way.” Working at our family of organizations means that regardless of your role, we’ll walk alongside you in your career, supporting you so you can support others. We provide best-in-class benefits and we foster an inclusive workplace where diversity is valued, and everyone is essential, heard and respected. Together, our 120,000 caregivers (all employees) serve in over 50 hospitals, over 1,000 clinics and a full range of health and social services across Alaska, California, Montana, New Mexico, Oregon, Texas and Washington. As a comprehensive health care organization, we are serving more people, advancing best practices and continuing our more than 100-year tradition of serving the poor and vulnerable. \\n The amounts listed are the base pay range; additional compensation may be available for this role, such as shift differentials, standby/on-call, overtime, premiums, extra shift incentives, or bonus opportunities.',\n",
       "  0),\n",
       " ('Check out our benefits page for more information about our Benefits and Rewards. \\n Requsition ID:  218202  \\n Company:  Providence Jobs  \\n Job Category:  Development/Engineering  \\n Job Function:  Information Technology  \\n Job Schedule:  Full time  \\n Job Shift:',\n",
       "  0),\n",
       " ('Career Track:   \\n Department:  4011 SS IS HI DP 3  \\n Address:  WA Renton 2001 Lind Ave SW  \\n Work Location:  Providence Southgate 2-Renton  \\n Pay Range:  $43.39 - $70.03  \\n The amounts listed are the base pay range; additional compensation may be available for this role, such as shift differentials, standby/on-call, overtime, premiums, extra shift incentives, or bonus opportunities.  \\n Check out our benefits page for more information about our Benefits and Rewards.',\n",
       "  0),\n",
       " (\"LivePerson (NASDAQ: LPSN) is a global leader in trustworthy and equal AI for business. Hundreds of the world's leading brands — including HSBC, Chipotle, and Virgin Media — use our Conversational Cloud platform to engage with millions of consumers safely and responsibly. We power a billion conversational interactions every month, providing a uniquely rich data set and safety tools to unlock the power of Generative AI and Large Language Models for better business outcomes. \\n  At LivePerson, we foster an inclusive workplace culture that encourages meaningful connection, collaboration, and innovation. Every mind is invited to ask questions and actively seek new ways to achieve success and reach their full potential. We operate as one with a growth mindset. This means spotting opportunities, solving ambiguities and seeking effective solutions to challenges that make things better. \\n  Overview \\n  We are seeking a Principal Prompt engineer to join our team to work with various Large Language Model (LLM) technologies and providers. The successful candidate will be responsible for designing and testing prompts and prompting methods for a variety of use cases and models, ensuring that the output is accurate, relevant, and high-quality. \\n  If you are passionate about Natural Language Processing, Conversational AI, and have experience working with advanced language generation models, we encourage you to apply for this exciting opportunity to bring cutting-edge Generative AI technologies to customers around the world. \\n \\n \\n  You will: \\n \\n Develop, implement, and test prompting strategies for a variety of products over various LLMs.\",\n",
       "  1),\n",
       " ('Work with production engineers to deploy these prompts into production \\n Collaborate with other engineers, data scientists, and analysts to improve the accuracy and quality of LLM responses \\n Analyze internal and external feedback and behaviors to continuously improve the quality of prompts and language output \\n Stay up-to-date with the latest developments in Generative AI specifically, and Natural Language Processing and Machine Learning in general. \\n \\n \\n \\n  You have: \\n \\n 8 years of industry experience',\n",
       "  1),\n",
       " (\"Foundational knowledge of Natural Language Processing and Machine Learning \\n Experience working with Large Language Models such as GPT-3/4, ChatGPT, Claude, or CoHere \\n Proficiency in Python \\n Excellent analytical and problem-solving skills \\n Strong attention to detail and ability to manage multiple tasks simultaneously \\n Effective communication skills and ability to collaborate with cross-functional teams \\n Bachelor's degree\",\n",
       "  1),\n",
       " (\"Preferred qualifications \\n \\n Master's or Ph.D. degree in Computer Science or related field \\n Experience working with other Machine Learning models or approaches such as BERT or Reinforcement Learning \\n Familiarity with deep learning frameworks such as TensorFlow or PyTorch \\n Experience with large-scale data processing and analysis \\n Familiarity with cloud computing platforms such as GCP, AWS, or Azure \\n \\n Benefits:\",\n",
       "  1),\n",
       " ('he salary range for this role will be between $150,000 to $180,000. Final compensation will be determined by a variety of factors, including, but not limited to, your location, skills, experience, education, and/or certifications. During the phone screening, the recruiter will provide the location-specific salary range for this role. Regardless of your personal situation or where you are in the world, LivePerson offers comprehensive and great benefits programs to meet your needs: \\n \\n Health: medical, dental, vision and wellbeing.   \\n Time away: Public holidays and discretionary PTO package for flexible days off with manager approval.   \\n Financial: 401K, ESPP, Basic life and AD&D insurance, long-term and short-term disability   \\n Family: parental leave, maternity support, fertility services.   \\n Development: tuition reimbursement, native AI learning.   \\n Additional: 24/7 access to professional counselors, voluntary insurance coverage, exclusive perks and discounts.   \\n #LI-Remote',\n",
       "  0),\n",
       " ('Who we are \\n Cooperative Computing (C|C) is a digital enablement organization enabling organizations to effectively operate in the automated economy. The future of business is in maximizing relationships through the effective use of technology. With our clients, we discover, strategically engineer a digital strategy and enable these strategies through the implementation of best-in-class applications to achieve clients 10x growth. \\n Our performance culture is built through our team members, working together to help our clients succeed. We inspire growth with our team members in delivering fanatical and passionate client experiences, knowing effective technology is built with and for people. \\n Our Values: \\n \\n Be Fanatical and Passionate Delivering Superior Client Experiences  - It’s who we are! Our customers are the center of every idea, process, and decision we create in building sustainable relationships. We over communicate, over deliver & outperform ourselves every time \\n Growth is Contagious  - I grow, You grow, We all grow! \\n Be Innovative  - Looking at tomorrow today. We live outside our comfort zone; we ask difficult questions of ourselves; we take risks, and we are fearless to experiment and lead the way forward \\n Show Empathy & Be Honest  - Every single word spoken, or action performed for our Customers, Team Members, Partners & Stakeholders will be filled with kindness, candor and honesty',\n",
       "  0),\n",
       " (\"High Performance  - It’s not for everyone - Our culture is our team members. We make the lives of our fellow team members better by first recognizing “I” am a team member first. We measure our progress constantly to be a better version of ourselves with every new day \\n \\n Life at CC: \\n Life at CC is a fusion of ambition, recognition, and lifestyle, where your career takes flight. We champion a high-performance culture with top-tier compensation and flexible working models. With us, enjoy robust benefits, milestone celebrations, and unparalleled learning opportunities. We foster a vibrant community through dynamic team activities. Join CC - embark on a journey where every day is rewarding and growth is a guaranteed promise. \\n About the Role: \\n The Data Engineer position at our company entails joining a dedicated team of analytics professionals. The role is primarily focused on broadening and enhancing our data and data pipeline architecture while streamlining data flow and collection for multiple functional teams. The successful candidate will play a vital role in creating an environment that supports optimal data extraction, transformation, and loading from diverse data sources. \\n Mission: \\n The Data Engineer's mission is to ensure an optimal data pipeline architecture and gather complex data sets that meet the business's functional and non-functional requirements. Their goal is to identify, design, and implement internal process improvements, automate manual processes, enhance data delivery, and re-architect infrastructure for greater scalability. \\n Capabilities (Key Behaviors):\",\n",
       "  0),\n",
       " (\"The Data Engineer will exhibit the following capabilities: \\n \\n 4 years of experience in a Data Engineer role. \\n Bachelor's degree in Software Engineering/Computer Science or equivalent experience in a related field. \\n Capability to create and maintain optimal data pipeline architecture, and compile large, complex data sets that meet business requirements. \\n Profound knowledge of infrastructure for optimal data extraction, transformation, and loading from diverse data sources using SQL and AWS 'big data' technologies. \\n Ability to identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. \\n Skill to build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. \\n Competency to work with various teams to assist with data-related technical issues and support their data infrastructure needs.\",\n",
       "  1),\n",
       " ('Proficiency in performing root cause analysis on internal and external data and processes to address specific business questions and identify improvement opportunities. \\n Expertise in interpreting trends and patterns, conducting complex data analysis, and reporting on results. \\n \\n Results: \\n \\n Successful maintenance and improvement of the data pipeline architecture, leading to better management and use of complex data sets. \\n Effective implementation of process improvements, resulting in increased efficiency and scalability of the data infrastructure. \\n Constructive collaboration with various teams, leading to the resolution of data-related technical issues and better support for data infrastructure needs. \\n Accurate root cause analysis leading to the identification of areas for improvement and solutions to business queries.',\n",
       "  0),\n",
       " ('Insightful interpretation of trends and patterns, allowing for comprehensive data analysis and valuable reporting on results. \\n \\n Job Type: Full-time \\n Application Question(s): \\n \\n What ETL Tools have you used? \\n \\n Experience:',\n",
       "  0),\n",
       " ('Cape Fox is seeking a highly qualified Data Analyst to join our team in support of a government customer. Must be available to participate in phone calls and virtual meetings during work hours, with adequate technological resources (such as internet bandwidth) to do so. A data analyst gathers, cleans, and studies data sets to help solve problems. Be proficient in using financial systems and software applications (e.g., Microsoft Excel, Word, PowerPoint, SharePoint) for presentation of analysis and preparation of supporting documentation. Provide comprehensive financial analysis support services for the planning, execution, and monitoring of various Federal projects and support to the project management team. \\n This position is contingent upon award. \\n Core Duties: \\n Ensuring project plans follow financial regulations and support evidenced based project performance. \\n Support the project management team in responding to financial inquiries from stakeholders. \\n (Tracks fund execution, including commitments, obligations, unobligated balances, and expenditures for programs and provide reports as necessary. \\n Analyze project financial data, including expenditures, revenue, and variances against budgeted amounts.',\n",
       "  1),\n",
       " ('Provide financial insights and recommendations to optimize project resource allocation. \\n Identify potential schedule delays and recommend corrective actions to ensure timely obligation and drawdown of project allocations. \\n Collaborate with project managers to forecast project financial outcomes based on current performance. \\n Monitor project financial health and flag any financial risks or discrepancies to relevant stakeholders. \\n Assist in the preparation of financial reports for submission to stakeholders. \\n Support internal and external audits by providing financial data and documentation as needed. \\n Participate in project review meetings to discuss financial performance and strategies for improvement.',\n",
       "  0),\n",
       " ('Populates budget and financial information into executive level presentations. \\n Develop tracking and reporting mechanisms to facilitate project management and project tracking. \\n JOB REQUIREMENTS \\n \\n Possess a bachelor’s degree in a business, finance, accounting, or related field and 4 years of relevant experience OR possess 10+ years of relevant experience in lieu of a degree \\n Ability to analyze and interpret financial data, to interact effectively with others in a broad range of functional and technical backgrounds, and to handle pressures in a deadline-driven environment. \\n Ability to use a high degree of professional judgment and ingenuity in interpreting guidelines for application in day-to-day issues and analyses.',\n",
       "  0),\n",
       " ('Ability to perform problem solving of a precedent-setting nature. \\n Must be able to pass a Tier 1 Background Investigation \\n Remote Position \\n \\n Job Types: Full-time, Contract \\n Benefits:',\n",
       "  0),\n",
       " ('401(k) \\n Dental insurance \\n Health insurance \\n \\n Schedule: \\n \\n Monday to Friday',\n",
       "  0),\n",
       " (\"Introduction to Demandbase: \\n  Demandbase is the Smarter GTM™ company for B2B brands. We help marketing and sales teams overcome the disruptive data and technology fragmentation that inhibits insight and forces them to spam their prospects. We do this by injecting Account Intelligence into every step of the buyer journey, wherever our clients interact with customers, and by helping them orchestrate every action across systems and channels - through advertising, account-based experience, and sales motions. The result? You spot opportunities earlier, engage with them more intelligently, and close deals faster. \\n  As a company, we're as committed to growing careers as we are to building world-class technology. We invest heavily in people, our culture, and the community around us. We have offices in the San Francisco Bay Area, Seattle, and teams in the UK and India, and allow employees to work remotely.  We have also been   continuously recognized as one of the best places to work in the San Francisco Bay Area. \\n  We're committed to attracting, developing, retaining, and promoting a diverse workforce. By ensuring that every Demandbase employee is able to bring a diversity of talents to work, we're increasingly capable of living out our mission to transform how B2B goes to market. We encourage people from historically underrepresented backgrounds and all walks of life to apply.  Come grow with us at Demandbase! \\n  About the Role: \\n  The compensation range for this role is: $169,374 - $272,290 \\n  Are you a seasoned Data Scientist with a strong focus on AdTech, looking to take your career to the next level? We are seeking a Staff Data Scientist with extensive experience in AdTech to join our team. In this role, you will play a key role in driving innovation and optimizing advertising strategies, leveraging your deep knowledge of data science and AdTech to deliver impactful results for our customers. \\n  Key Responsibilities: \\n \\n \\n Product and Engineering:  Collaborate with our product and engineering teams to pinpoint the precise objectives of advertising campaigns and architect machine learning models that optimize these campaigns with precision.\",\n",
       "  0),\n",
       " (\"Account Ranking in B2B Buying Journey:  Spearhead the development of algorithms to rank accounts in the B2B buying journey, shaping the direction of targeted advertising campaigns. \\n \\n \\n Intent Taxonomy:  Create and refine a browsable taxonomy of intent signals, enabling advertisers to gain a deeper understanding of user intent. \\n \\n \\n What You'll Be Doing: \\n \\n Advanced Algorithm Development:  Drive the development of advanced machine learning algorithms that optimize advertising strategies and deliver immediate business impact on KPIs.\",\n",
       "  0),\n",
       " (\"Model Deployment:  End-to-end, fullstack ownership of ML models. Take charge of building, testing, and deploying custom ML/AI models and algorithms on extensive datasets, establishing rigorous processes for monitoring and analyzing their performance in production environments. \\n Effective Communication:  Excel in effectively communicating complex data science methods, statistical findings, and algorithmic insights to technical and non-technical stakeholders. \\n Championing Innovation:  Stay at the forefront of AdTech and data science, actively leading innovation initiatives within the organization. \\n \\n What We're Looking For: \\n \\n Experience:  A minimum of 5 years of robust data science experience in AdTech. \\n Educational Background:  A degree in Statistics, Computer Science, Machine Learning, Mathematics, Computational Psychology, Operational Research, Physics, or a related field. \\n Project Leadership:  A strong track record of leading greenfield projects from conceptualization to production release. \\n Analytical Skills:  Proficiency in using analytical and database tools such as Jupyter notebooks, Hive, SQL, and No-SQL databases. \\n Coding Expertise:  Demonstrated ability to write clean and high-performance code in Python.\",\n",
       "  1),\n",
       " ('AI/ML Proficiency:  Proficiency in one or more of the following AI/ML technologies is advantageous: TensorFlow, scikit-learn, Spark MLlib, Bigquery Machine Learning, large-scale datasets, Natural Language Processing, and Graph algorithms. \\n \\n Nice to Have Skills: \\n \\n Team Leadership:  Previous experience in leading teams and managing complex projects. \\n Cloud Competency:  Familiarity with Google Cloud or AWS cloud platforms. \\n \\n Desired Qualities: \\n \\n Ownership Mentality:  Willingness to take full ownership of end-to-end machine learning products you develop or contribute to. \\n Self-Driven:  A highly self-motivated individual with the ability to operate with significant autonomy.',\n",
       "  1),\n",
       " ('Problem-Solving Acumen:  Proficiency in breaking down complex problems and delivering innovative solutions. \\n Goal-Focused:  An ability to maintain a sharp focus on project goals without getting lost in the minutiae. \\n Data Enthusiast:  A passion for data, metrics, and using them to drive the success of projects. \\n \\n As a Staff Data Scientist specializing in AdTech, you have the opportunity to make a significant impact in the ever-evolving landscape of data-driven advertising. Join our dynamic team today and take on a role that offers both challenges and rewards in the world of Advertising. Apply now to seize this exciting opportunity to contribute to the future of data-driven advertising. \\n  Benefits: \\n  Our benefits include options for up to 100% paid Medical and Vision premiums for employees, flexible PTO policy, no internal meeting Fridays, Modern Health mental wellness platform, and 11 paid holidays and 2 additional weeks where all Demandbase employees take off (the week of July 4th and the week of Thanksgiving). Plus 401(k), short-term/long-term disability, life insurance, and all those good things. \\n  Our Commitment to Diversity, Equity, and Inclusion at Demandbase \\n  At Demandbase, we believe in creating a workplace culture that values and celebrates diversity in all its forms. We recognize that everyone brings unique experiences, perspectives, and identities to the table, and we are committed to building a community where everyone feels valued, respected, and supported. Discrimination of any kind is not tolerated, and we strive to ensure that every individual has an equal opportunity to succeed and grow, regardless of their gender identity, sexual orientation, disability, race, ethnicity, background, marital status, genetic information, education level, veteran status, national origin, or any other protected status. We do not automatically disqualify applicants with criminal records and will consider each applicant on a case-by-case basis. \\n  We recognize that not all candidates will have every skill or qualification listed in this job description. If you feel you have the level of experience to be successful in the role, we encourage you to apply! \\n  We acknowledge that true diversity and inclusion require ongoing effort, and we are committed to doing the work required to make our workplace a safe and equitable space for all. Join us in building a community where we can learn from each other, celebrate our differences, and work together.',\n",
       "  0),\n",
       " ('Duration : 4 months \\n \\n  Job Description: \\n \\n \\n \\n  Role Description: Software Engineers apply the principles of software engineering to the design, development, maintenance, testing, and evaluation of the software and systems work to the specified requirements.',\n",
       "  0),\n",
       " (\"This role is focused on the support and management of the enterprise's CMDB. Primary work will include the support of incidents and requests related to the CMDB, as well as some analysis/development in managing the health of the CMDB. \\n \\n \\n  Job description:\",\n",
       "  0),\n",
       " (\"Apply current principles of software engineering to design, develop, maintain, configure and test software and systems to ensure they work to the specified requirements \\n  Performs incident triage, including determining potential impact and identifying remediation's \\n  Analyzes current systems & integrations, and works establish automation where possible \\n  Using software development skills & knowledge, fully support the Configuration Management practices with the enterprise, including work with the Configuration Management Database (CMDB), Configuration Items (CI's), and infrastructure & application discovery; all of this needs to be done with the Service Management framework using ITIL practices \\n  Build strong relationships with peers and Customers across functions through collaborative engineering operations and initiatives\",\n",
       "  0),\n",
       " ('Skills Required: \\n \\n \\n \\n  Can performs a range of assignments related to Service Management, especially Configuration Management and Configuration Management Database (CMDB) \\n  Knowledge and understanding of software engineering architectures, system/software designs, and system deployments \\n  Knowledge and understanding of System Administration (configurations, installations, patch management, server maintenance, etc.) and Network Management (firewalls, proxies, IP management, routing, DNS).',\n",
       "  0),\n",
       " ('Knowledge and understanding of integration and communication protocols between applications, databases, and technology platforms. \\n  Knowledge and understanding of coding principles developing independent modules based on the given specs in choice of scripts (Powershell, Shell, Perl), and hand-on experience with programming language like Java, Python, etc. \\n  Uses prescribed guidelines or policies in analyzing situations \\n  Works well within a team environment, and can work independently to get work accomplished when provide generally information about the solutions needed (requiring a moderate level of guidance and direction on a regular basis) \\n \\n \\n  The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.',\n",
       "  1),\n",
       " (\"** This is a direct-hire opportunity with our client and is fully remote (indefinitely). Candidates must be able to work in the US without sponsorship.** \\n Our client is looking add a  Senior Data Engineer  to their team to build strong data and ETL infrastructure in building 2.0 of their platform in  AWS . You will provide/direct technical expertise in the design, development, implementation, and testing of this new platform. You will also participate in and/or direct major deliverables of projects through all aspects of the data lifecycle. \\n Responsibilities: \\n \\n Lead the ETL efforts to create platform 2.0 using multiple technologies create/implement data processing frameworks and analytical infrastructure. \\n Define, design, and implement data integration, management, storage, consumption, backup, and recovery solutions that ensure the high performance of the organization's enterprise data. \\n Integrate new data sources into existing backend, ensuring interoperability and integrity with the current platform flows. \\n Maintain data pipelines & data systems\",\n",
       "  1),\n",
       " ('Design, build, and automate solutions utilizing AWS services not limited to IAM, Glue, Lambda, S3, Athena, SNS, SQS, DynamoDB, RDS, EMR, ECS, Route 53 and Redshift. \\n \\n Requirements: \\n \\n B.S. in relevant technical degree with minimum 5+ years Data Engineering experience \\n Experienced with AWS services and AWS-native CI/CD tools \\n Experienced designing data pipelines & data solutions \\n Experience in tools like Jenkins, GIT and/or AWS CodePipeline/CodeBuild/CodeDeploy',\n",
       "  1),\n",
       " ('Experienced with Python, PostgreSQL, and writing complex SQL queries and analysis of data correlations \\n Experience building data-driven unit test suites for data platforms and modelling highly dimensional data \\n Ability to work independently and integrate with other team members \\n Project management skills, to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform \\n Excellent communication skills (written and verbal) \\n \\n Our Vetting Process \\n At Emergent Software, we work hard to find Data Engineers who are the right fit for our clients. Here are the steps of our vetting process for this position:',\n",
       "  1),\n",
       " ('Application (5 minutes) \\n Online Assessment (40 minutes) \\n Initial Phone Interview (30-45 minutes) \\n 2-3 Interviews with the Client \\n Job Offer! \\n \\n Job Type: Full-time',\n",
       "  0),\n",
       " ('Pay: $130,000.00 - $160,000.00 per year \\n Benefits: \\n \\n 401(k) \\n Dental insurance \\n Health insurance \\n \\n Schedule:',\n",
       "  0),\n",
       " (\"Our Success Formula: Global Network – Local Service. \\n \\n \\n \\n  CW Bearing USA, Inc. is a medium-sized manufacturing company poised for growth over the next few years. We are a globally operational, privately-owned company based in Ningbo, on China's east coast. The company was founded in 1984 by Hu Xiangen.\\n  \\n \\n \\n  We specialize in the development and production of a wide range of premium quality ball bearings for electrical motors, gearboxes, power tools and the automotive industry. We also specialize in ball nuts and ball screws for automotive and industrial customers.\\n  \\n \\n \\n  CIXING GROUP CO. LTD. is one of the 10 largest producers of ball bearings in China. We possess a worldwide sales and distribution network and have offices in Asia, Europe, North America and Latin America.\\n  \\n \\n \\n  Our ultra-modern production facilities enable us to serve both global customers and mid-sized companies. Regardless of the size of the delivery – small lots or serial production – we never lose sight of our customers' requirements. The establishment of good working partnerships is our highest priority.\\n  \\n \\n \\n We offer competitive wages, excellent benefits, 401k with annual employer match, flexible schedules (for applicable positions) clean temperature-controlled manufacturing facility, and much more! \\n \\n \\n \\n  We are currently seeking:\",\n",
       "  0),\n",
       " ('Advanced Manufacturing Engineer – CNC Machining & Grinding \\n \\n \\n Reports to Engineering Manager \\n \\n \\n Classification: Salaried Exempt \\n \\n \\n Work From Home Status: Does Not Apply \\n \\n \\n \\n  Job Summary:\\n  \\n \\n  The Manufacturing Engineer will develop turning & milling processes by studying product requirements; researching, designing, modifying, and testing manufacturing methods and equipment; conferring with equipment vendors.\\n  \\n \\n  Supervisory Responsibilities:\\n  \\n \\n  None\\n  \\n \\n \\n  Duties & Responsibilities:',\n",
       "  0),\n",
       " (\"Develops grinding, machining & assembly processes by studying product requirements; researching, designing, modifying, and testing manufacturing methods and equipment; conferring with equipment vendors. \\n Develops and maintains CNC programs for grinding, turning/milling & honing/super finishing machines. \\n Help senior engineers through complete APQP process, installs and initial start-up of equipment. \\n Creates and maintains process routings, process drawings, CNC programs, and tooling lists. \\n Improves manufacturing efficiency by analyzing and planning workflow, space requirements, and equipment layout. \\n Improves grinding efficiency and cycle time by redesigning tooling/fixtures, develop new grinding wheels and dressing wheels by researching, testing various feed rates/depth of cuts, and taking direction from senior engineers. \\n Develop plant layouts, work instructions, process flow layout, set-up procedures while adhering to safety, quality and productivity procedures and policies. \\n Handle automation with robotic materials and parts. \\n Development and management of process FMEA's and other process documentation. Troubleshooting of processes and equipment. \\n Develop, monitor, and report production measurables and productivity incentive plans. Support senior engineers and or lead refurbishing and repurposing of equipment. \\n Use Solidworks to create process models/process drawings. \\n Create ballooned drawings & inspection standards for part manufacturing. \\n Develop standard procedures to improve manufacturing processes and to reduce overall cost. \\n Communicate with customers & suppliers to understand and determine product specifications. \\n Assures product and process quality by designing testing methods; testing finished product and process capabilities; establishing standards; confirming manufacturing processes. \\n Work with product development team to manufacture prototypes and work with product engineers to assure manufacturability. \\n Ensure the consideration of internal process standards and lessons learned, make sure that experiences and lessons learned are communicated back to the organization. \\n Completes design and development projects by training and guiding technicians. \\n Complies to All safety regulations outlined by the company, customer, and government regulations. \\n Communicates and interacts with all departments to achieve company goals and initiatives. \\n Travel 5-10% \\n \\n \\n \\n  Required Skills & Abilities:\\n  \\n \\n Knowledge of Six Sigma Concepts preferred\",\n",
       "  0),\n",
       " (\"Thorough understanding of setting up processes and BOM's \\n Familiarity with automotive specifications and GD&T \\n Ability to communicate with all levels of personnel including customers & suppliers \\n Experience in bearings, half-shaft components and power steering components manufacturing a plus \\n Experience working in a mass production environment with tight tolerances. \\n Hands on experience using Solid works, AutoCAD, or Inventor \\n Bilingual: English & Chinese (a plus) \\n \\n \\n \\n  Education & Experience:\\n  \\n \\n Bachelor's Degree in Manufacturing Engineering, Industrial Engineering, Production Engineering, or a related field preferred \\n 2+ years of experience in centerless grinding, automated assembly processes, and CNC machining processes \\n 3+ years of work experience in automotive manufacturing environment (excluding internships). \\n 5+ years experience with APQP, SPC, FMEA, & 8D Problem Solving \\n 5+ years experience planning, creating and interpreting timelines \\n 5-10 years experience process design and development including materials, drawings, industry specifications, equipment, tooling, gaging and testing \\n 5-10 years experience product and process validation including industry standard requirements \\n \\n \\n \\n  Physical Requirements\\n  \\n \\n  Positions at CW may require the following: alternating between sitting and standing; climbing stairs, ladders, scaffolding or ramps; crouching/stooping; driving; near/far visual acuity; fine motor manipulation; gross motor manipulation; hearing; keyboarding; kneeling; lifting/carrying; moving objects; peripheral visual acuity; pushing or pulling; reaching overhead or below; repetitive task performance; sitting; speaking; standing; using foot or leg controls; walking.\",\n",
       "  0),\n",
       " ('Environmental Requirements\\n  \\n \\n  Our manufacturing facility is a climate-controlled environment. There is no long-term exposure to dangerously loud noise or extreme temperatures.\\n  \\n \\n \\n  Cognitive Requirements\\n  \\n \\n  Cognitive abilities include executing tasks independently; learning and/or memorizing tasks; maintaining concentration/focus on tasks; working with/in close proximity to, other people.\\n  \\n \\n \\n  Attendance Requirements\\n  \\n \\n  All hourly team members are expected to report to work and be ready to work at their scheduled start time. It is required that they adhere to and follow the attendance policy(ies).\\n  \\n \\n \\n  All salaried team members are expected to perform their duties/responsibilities in a timely manner. Although they may not have set schedules, they are expected to either be at work on a schedule agreed upon by their manager or be available via phone/text/Teams if working remotely (Remote work does not apply to CWM positions).\\n  \\n \\n \\n All CW entities are equal opportunity employers.',\n",
       "  0),\n",
       " ('Field Aerospace is a leading aerospace company dedicated to providing cutting-edge aircraft modifications and support to the US military and allied nations. With over 75 years of industry experience, we take pride in our commitment to innovation, technical excellence, and delivering solutions that empower missions and support our valued clients. \\n  General Duties: \\n  We are seeking a dynamic and enthusiastic Salesforce Intern to join our team. You will assist in supporting optimizing our use of Salesforce, developing dashboards, building reports, cleaning our data, develop training documentation all aimed at providing business incites based on data for our business development teams. This internship is an excellent opportunity to gain practical experience in Salesforce and a dynamic aerospace company. \\n  Essential Job Functions: \\n \\n  Conduct research into how the system can be leveraged to provide greater value. \\n  Support teams in troubleshooting issues with the system. \\n  Create reporting and provide analysis of business development data. \\n  Perform data cleansing exercises to help standardize the data which exists in the system.',\n",
       "  0),\n",
       " ('Actively contribute to the value derived from the tool by creating documentation for end users and putting together trainings. \\n \\n  Skills and Experience: \\n \\n  Strong written and verbal communication skills, with attention to detail. \\n  Familiarity with business systems (Salesforce is a bonus). \\n  Proficient in Microsoft Office suite (Word, Excel, PowerPoint). \\n  Creative thinker with the ability to generate and implement innovative ideas. \\n  Self-motivated, proactive, and able to work independently as well as collaboratively.',\n",
       "  1),\n",
       " ('Strong organizational and time management skills, capable of handling multiple tasks efficiently. \\n \\n  Competencies: \\n \\n  Integrity \\n  Excellence \\n  Collaboration and Team Work \\n  Communicating Orally and in Writing \\n  Adaptability',\n",
       "  0),\n",
       " ('Initiative \\n  Organization and Planning \\n  Education \\n  Exercising Self-Control and Being Resilient \\n  Interpersonal Skills \\n \\n \\n  Education:',\n",
       "  0),\n",
       " ('Currently pursuing a degree in Business, Information Systems, Computer Science, Data Science, or a related field. \\n \\n  Physical Requirements: \\n  The physical demands described herein are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. \\n  While performing the duties of this job, the employee is regularly required to talk and hear, stand and sit most of the time. The employee is periodically required to use hands to finger, handle, or feel objects, tools, or controls and to climb/ascend ladders using feet and legs to balance. The employee is occasionally required to walk; reach with hands and arms; stoop, kneel, crouch or crawl. \\n  The employee may occasionally lift and move up to 10 pounds. Specific vision abilities required by this job include close vision, peripheral vision, depth perception and the ability to adjust focus. \\n  The employee is subject to environmental conditions. Protection from weather conditions but not necessarily from temperature changes. \\n  Additional Notes: \\n  In order to comply with Export Control Laws and the NISPOM, we must secure all governmental approvals that are required to authorize our workforce to work on our defense and government programs. To ensure we comply with these regulations in a manner that does not violate our equal opportunity employment/non-discrimination compliance obligations, Field maintains the following recruitment policy:',\n",
       "  0),\n",
       " ('Title: Business Analyst (Healthcare) \\n Location: 100% Remote (prefer candidates local to Bedford MA, NYC, Irving TX, Naperville IL, or Omaha NE) \\n Duration: 6-month Temp-to-Perm \\n Supplier Notes: \\n This is a heavily customer facing role and requires strong communication skills, strong understanding of healthcare claims processes, and a strong technical communication skill. \\n Requirements:',\n",
       "  0),\n",
       " ('2+ years’ experience in analysis and design on software development projects in analyst role (preferably within a Scrum Agile team). \\n Knowledge of Healthcare industry and understanding all major workflows associated with healthcare claim processing. \\n Ability to effectively present information and respond to questions from internal and external customers. \\n Excellent verbal and written communication skills. \\n Experience in SAFe Agile preferred. \\n TDD/BDD Development preferred.',\n",
       "  1),\n",
       " ('Job Types: Contract, Temporary \\n Salary: $50.00 - $60.00 per hour \\n Benefits: \\n \\n Dental insurance \\n Health insurance',\n",
       "  0),\n",
       " ('Life insurance \\n Vision insurance \\n \\n Experience level: \\n \\n 2 years',\n",
       "  0),\n",
       " ('Schedule: \\n \\n 8 hour shift \\n \\n Experience:', 0),\n",
       " ('SAP Success Factors Data Analyst \\n Hybrid- Nashville, TN \\n 3 month contract (possibility of extension or full time) \\n $38-41/hr \\n Work Auth:  US CITIZEN OR GC HOLDER ONLY (NO C2C OR SPONSORSHIP) \\n Responsibilities: \\n \\n Meet with business stakeholders to document reporting requirements for HR in SuccessFactors \\n Create report design/wireframe and identify gaps if any from requirements / current reporting solution \\n Work with Reporting Team to build reports and test',\n",
       "  0),\n",
       " ('Assist functional team with troubleshooting functional/reporting gaps at the process and/or field level \\n Test resolutions for defects and document findings in the tracking system \\n Work with business users for UAT, troubleshoot issues with reports, and determine data discrepancies \\n Support Report rollout to the user community, through communication and participation in OCM activities \\n \\n Requirements: \\n \\n Has SAP SuccessFactors Employee Central modules knowledge \\n Experience reporting in SAP Success Factors \\n Experience with developing reports in SAP Analytic Cloud',\n",
       "  1),\n",
       " ('Experience with reporting, including table, dashboard, canvas, and story reporting \\n Develop reporting requirements for business or HR requests \\n Build ORD (Report Center) reports from SF to meet the reporting requests \\n Hands-on with Static and runtime filters \\n \\n Preferred: \\n \\n Experience with Databases / Data Lake (ex: Snowflake) \\n Experience writing test scripts \\n Experience writing system and end-user documentation** This will start as a 3 month contract with extensions and possibility of permanent',\n",
       "  1),\n",
       " ('Job Types: Full-time, Contract \\n Pay: $38.00 - $41.00 per hour \\n Benefits: \\n \\n Dental insurance \\n Health insurance \\n Vision insurance \\n \\n Experience level:',\n",
       "  0),\n",
       " ('2 years \\n \\n Schedule: \\n \\n 8 hour shift \\n Monday to Friday \\n \\n Experience:',\n",
       "  0),\n",
       " ('The telehealth Data Analyst (Operations Research Analyst) known as a VMC Decision Science Analyst, elicits, analyzes, validates, and documents virtual healthcare data and information to support the business needs of stakeholders including the DHA and MTFs. The Decision Science Analyst will develop metrics driven by information requirements and supported by data to include statistical analysis, research design and decision support to improve decision making and business practices at all levels of the MHS. \\n \\n  The contractor shall provide healthcare analysis in support of senior level working groups involved in a variety of work associated with VMC and DHA trategic business and organizational planning, corporate metrics and benchmarking, patient satisfaction and project implementation matters.  \\n The contractor shall identify, develop, and execute \"think tank\" studies on a variety of health systems  \\n The contractor shall coordinate with VMC leadership staff offices for input on key study topics and develops data requirements and analysis for study.  \\n The contractor shall assess current best business practices and industry literature to identify future health systems requirements and to improve current operations.',\n",
       "  0),\n",
       " ('The contractor shall provide recommendations on policies and programs to address major health strategy and policy issues impacting on the MHS, develop alternatives, and recommend courses of action for the VMC.  \\n The contractor shall assist in the communication of DHA policies to the field by providing explanations of the analysis upon which the policies are based.  \\n The contractor shall attend meetings as necessary to represent VMC and DHA interests.  \\n The contractor shall implement Congress, DoD, and DHA staff approved concept plans.  \\n The contractor shall prepare Director approved documents for approval of concepts at the appropriate approval level. \\n  The contractor shall review healthcare industry standards to identify benchmarks and best practices for incorporation into VMC and DHA enterprise operations.',\n",
       "  0),\n",
       " ('The contractor shall retrieve corporate performance data from a variety of MHS data query tools to include the Military Health System (MHS) MDR – Medical Data Repository, MHS Mart (M2) (MDR, M2, the TRICARE Operations Center (TOC), and other MHS systems. \\n  The contractor shall develop, maintain, and enhance decision support tools and metrics used for senior level decision making.  \\n The contractor shall apply statistical analysis to operational data to provide decision support and program analysis. \\n  The contractor shall participate in health policy planning and project implementation efforts.  \\n The contractor shall evaluate, assess, and analyze all resource, workload, and customer satisfaction data.  \\n The contractor shall provide alternatives and recommend courses of action after validating the estimates, projections, forecasts, and conclusions of the studies.',\n",
       "  0),\n",
       " ('The contractor shall participate on multifunctional and MHS teams to oversee the accomplishment of long-term implementation plans.  \\n The contractor shall coordinate with MHS, DHA, and MTFs to facilitate initiatives and program analysis. \\n  The contractor shall furnish technical assistance, recommend priorities, and target dates, coordinate work to avoid delay and overlapping assignments, review decisions, reports, correspondence, etc.  \\n The contractor shall apply knowledge and understanding of all MHS components to solve difficult and obscure problems in performance planning, develop new approaches for use by the field, and to negotiate with other MHS organizations on multiple areas of collaboration. \\n  The contractor shall formulate and present recommendations and implementation strategies for significant changes to standard policies and procedures. \\n  The contractor shall develop business models and solutions for healthcare performance planning, realignments, resource management, and overall decision support.',\n",
       "  0),\n",
       " (\"The contractor shall perform data extraction and manipulation in Statistical Analysis Systems (SAS) (commercial software product) and Business Objects using multiple MHS data systems to include the M2, MDR, Expense Accounting System version 4 (EASIV), Defense Medical Human Resources System - Internet (DMHRS-I), and others. \\n \\n  Qualifications \\n \\n  A bachelor's degree in business, project management or clinical environment is required.  \\n Comprehensive knowledge of the missions, organizations, structure, programs, data systems, and requirements of healthcare delivery systems for the military departments and the MHS to apply and develop practices, concepts and standards that lead to emerging strategy for the success and progression of VH in the MHS.\",\n",
       "  1),\n",
       " (\"MLOps Data Engineer/Senior Software Engineer \\n  Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand new methodologies, tools, statistical methods and models. What’s more, we are in collaboration with leading academics, industry experts and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data. \\n  Be a part of a team that’s ensuring Dell Technologies' product integrity and customer satisfaction. Our MLOps team turns business requirements into technology solutions by designing, coding, and testing/debugging applications, as well as documenting procedures for use and constantly seeking quality improvements. \\n  Join us as a  Data Engineer/Senior Software Engineer  on our  MLOps  team to work remotely in  Panama or Brazil  to do the best work of your career and make a profound social impact. \\n  What you’ll achieve\",\n",
       "  0),\n",
       " ('As an MLOps Data Engineer/Senior Software Engineer, you will deliver products and improvements for a changing world. Working at the cutting edge, you will craft and develop software for platforms, peripherals, applications, and diagnostics — all with the most sophisticated technologies, tools, software engineering methodologies, and partnerships. \\n  What you’ll do: \\n  Feature engineering: Stitching together and aggregating multiple large data sets working with the data scientist to the desired format. You will need to optimize both queries and architecture to support big data sets. \\n  Data pipelining: Once the initial dataset is prepared, it is normally run through a further pipeline to prepare it for modeling. Here you will create features that make Machine Learning/Artificial algorithms work (e.g. translating the text into category variables). Expect to work with datasets with billions of rows and thousands of columns. Python/R is usually the preferred language, some larger tasks require Spark. \\n  Define standards for how data pipeline should be defined, designed and documented. Create pipeline definition that can be run in any runtime environment from day 1. Document pipeline and share best practices and knowledge with entire DataOps organization.',\n",
       "  1),\n",
       " ('Take the first step towards your dream career.  \\n Every Dell Technologies team member brings something unique to the table. Here’s what we are looking for with this role:  \\n Essential Requirements \\n \\n  Engineering Degree in Computer Science/Engineering, or equivalent professional experience',\n",
       "  0),\n",
       " ('Experience with big data technology (Hadoop, Apache Spark, etc.) and data migration from relational database to big data technologies \\n  Proficiency in data modelling, data optimization for both relational and non-relational databases (Oracle, MySQL, SqlServer, Mongo, Cassandra, Couchbase, Hadoop, Redis ) \\n  The primary skillset of building and maintaining complex ETL orchestrating pipelines preferably using Airflow \\n  Ability to work with varied data infrastructures – including relational databases, column stores, NoSQL databases, and file-based storage solutions. Exposure to machine learning or machine learning pipelines is a plus. Ability to set up containerized services using Kubernetes and Docker.',\n",
       "  1),\n",
       " ('Desirable Requirements \\n \\n  Experience with building and designing enterprise data pipelines with various levels of priority, concurrency, and versioning \\n  Experience with dbt, OpenMetadata, data catalogues and data lineage tools',\n",
       "  1),\n",
       " ('Who we are \\n  We believe that each of us has the power to make an impact. That’s why we put our team members at the center of everything we do. If you’re looking for an opportunity to grow your career with some of the best minds and most advanced tech in the industry, we’re looking for you. \\n  Dell Technologies is a unique family of businesses that helps individuals and organizations transform how they work, live and play. Join us to build a future that works for everyone because Progress Takes All of Us. \\n  Application closing date:  September 15th - 2023 \\n  Dell Technologies is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment.',\n",
       "  0)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vals = [textstr for textstr, label in list(eval(data))] #212 labelled paragraphs rn\n",
    "y_vals = [int(label.strip()) for textstr, label in list(eval(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About 38.0% of our labeled data contain tech names, leaving about 62.0% which do not.\n"
     ]
    }
   ],
   "source": [
    "print(f\"About {round(y_vals.count(1)/len(y_vals),2)*100}% of our labeled data contain tech names, leaving about {round(y_vals.count(0)/len(y_vals),2)*100}% which do not.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize/preprocess text using spacy\n",
    "processed = [\" \".join([token.lemma_ for token in nlp(paragraph)]) for paragraph in X_vals]\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed, y_vals, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000) #Small # features, shouldn't need to set max_features here I don't think?\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8372093023255814\n"
     ]
    }
   ],
   "source": [
    "acc = clf.score(X_test_tfidf, y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check a confusion matrix to see results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a roughly 84% accurate binary classifier for if a 'paragraph' contains tech words or not.  The confusion matrix seems to show that it isn't too biased one way or the other for incorrect labels, though our labelled data is biased overall towards non-tech paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that this classifier is good enough.  We now want to feed in every job desc we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_techs(text, vectorizer, clf, n=5):\n",
    "    original_text = text\n",
    "    \n",
    "    # Don't use split function as we don't want the output I use to label them manually.\n",
    "    splits = text.count(\"\\n\")//n\n",
    "    split_text = re.findall(\"\\n\".join([\"[^\\n]+\"]*splits), original_text)\n",
    "    processed = [\" \".join([token.lemma_ for token in nlp(para)]) for para in split_text]\n",
    "    \n",
    "    transformed = vectorizer.transform(processed)\n",
    "    pred_vals = clf.predict(transformed)\n",
    "    \n",
    "    zipped_paras = list(zip(split_text, pred_vals))\n",
    "    # return(zipped_paras)\n",
    "    return \" \".join([text for (text, label) in zipped_paras if label == 1])\n",
    "    \n",
    "#input the full job desc, split it into the 5ths, check each 5th if it contains tech with our classifier, if it does then we keep the original and join with others that do\n",
    "#if it doesn't, we get rid of that paragraph.\n",
    "# want to keep original text not just vectorized.\n",
    "# will do something like:\n",
    "# for text in job_desc_list:\n",
    "    # with_techs_text = check-for_techs(text)\n",
    "    # get_techs_from_openai(with_techs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_jds = []\n",
    "for jd in job_desc_list:\n",
    "    shortened_jds.append(check_for_techs(jd, tfidf_vectorizer, clf, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n  Possesses either an undergraduate or Master's degree in a quantitative field (e.g. mathematics, finance, statistics, or similar) or confirmed experience within data science and analytics \\n  3+ years of work experience involving quantitative data analysis and complex problem solving \\n  Excellent communication skills with the ability to distill complex issues and detailed analysis into simple, structured frameworks with concrete action plans \\n  Experience building statistical models to yield insights from complex user journeys. Experience in maintaining and developing production-grade models is a must. \\n  Strong proficiency in Python and/or another programming language; experience using SQL, Tableau, Excel and Airflow. \\n  Experience in experimentation methodologies, causal inferences and pricing \\n  Strong product sense \\n \""
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortened_jds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our model and vectorizer\n",
    "# with open(\"job_desc_classifier_v1.0.pkl\", \"wb\") as model_file:\n",
    "#     pickle.dump(clf, model_file)\n",
    "# with open(\"job_desc_tfidf_vectorizerv1.0.pkl\", \"wb\") as vect_file:\n",
    "#     pickle.dump(tfidf_vectorizer, vect_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To re-use the saved model:\n",
    "with open(\"job_desc_classifier_v1.0.pkl\", \"rb\") as model_file:\n",
    "    clf = pickle.load(model_file)\n",
    "with open(\"job_desc_tfidf_vectorizerv1.0.pkl\", \"rb\") as vect_file:\n",
    "    tfidf_vectorizer = pickle.load(vect_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"openai_api_key\")\n",
    "example_text_1 = os.getenv(\"example_text_1\")\n",
    "example_text_2 = os.getenv(\"example_text_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally from getTechs.py\n",
    "def get_techs(text):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\", \"content\":\"You identify specific technology names from job descriptions.  Respond only with a list of the names of the specific technologies.\"},\n",
    "            {\"role\":\"user\", \"content\":f\"Identify the specific technologies in the following text and respond with them in a list: {example_text_1}\"},\n",
    "            {\"role\":\"assistant\", \"content\":\"llama 2, python, pytorch\"},\n",
    "            {\"role\":\"user\", \"content\":f\"Identify the specific technologies in the following text: {example_text_2}\"},\n",
    "            {\"role\":\"assistant\", \"content\":\"python, spark, sql, snowflake, tableau, aws, azure, power bi\"},\n",
    "            {\"role\":\"user\", \"content\":f\"Identify the specific technologies in the following text: {text}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_json(dict, filepath):\n",
    "    with open(filepath, \"w\") as out:\n",
    "        json.dump(dict, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally from getTechs.py\n",
    "# Modified to remove counter, use to just get my spacy training data.\n",
    "\n",
    "### Does the original json file need to be closed?  I'm unsure how it works when renaming then dumping.  The new file is definitely closed as it's in the with open() command\n",
    "### Original file may not be? Think about and fix if necessary\n",
    "def print_attempt_number(retry_state):\n",
    "    print(f\"Retrying: {retry_state.attempt_number}...\")\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6), after=print_attempt_number)\n",
    "def get_tech_list(filepath):\n",
    "    f = open(filepath)\n",
    "    data = json.load(f)\n",
    "    for key in list(data.keys()):\n",
    "        job_desc = data[key]['desc']\n",
    "        response = get_techs(job_desc) # keep whole object in case I want metadata from it later\n",
    "        response_list = [x.lower() for x in response[\"choices\"][0][\"message\"][\"content\"].split(\", \")]\n",
    "        data[key]['techs'] = response_list \n",
    "    # dict_to_json originally from jobSearch.py, use to indicate which files have already been parsed\n",
    "    # Right now saves file separately, keeping original.  When done with testing will want it to overwrite.  Then it won't matter if the original is saved or not as we'll delete it anyways.\n",
    "    dict_to_json(data, f\"p-{filepath}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in os.listdir(\"data\"):\n",
    "#     if filename.startswith(\"p-\"): #already been parsed\n",
    "#         continue\n",
    "#     else:\n",
    "#         filepath = fr\"data/{filename}\"\n",
    "#         get_tech_list(filename) # remember this saves the files separately, keeping the original for now.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above will put the openai decided techs into a list in data[job_id]['techs'].  Can now use data itself for training spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x1b20bea8af0 state=finished raised RateLimitError>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Zade\\Desktop\\PythonStuff\\jobReqs\\jobReqs\\.venv\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Zade\\Desktop\\PythonStuff\\jobReqs\\jobReqs\\trainspacy.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m job_desc \u001b[39m=\u001b[39m data[key][\u001b[39m'\u001b[39m\u001b[39mdesc\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m response \u001b[39m=\u001b[39m get_techs(job_desc) \u001b[39m# keep whole object in case I want metadata from it later\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m response_list \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "\u001b[1;32mc:\\Users\\Zade\\Desktop\\PythonStuff\\jobReqs\\jobReqs\\trainspacy.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_techs\u001b[39m(text):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39mYou identify specific technology names from job descriptions.  Respond only with a list of the names of the specific technologies.\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mIdentify the specific technologies in the following text and respond with them in a list: \u001b[39;49m\u001b[39m{\u001b[39;49;00mexample_text_1\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m},\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39massistant\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39mllama 2, python, pytorch\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mIdentify the specific technologies in the following text: \u001b[39;49m\u001b[39m{\u001b[39;49;00mexample_text_2\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m},\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39massistant\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39mpython, spark, sql, snowflake, tableau, aws, azure, power bi\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mIdentify the specific technologies in the following text: \u001b[39;49m\u001b[39m{\u001b[39;49;00mtext\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Zade\\Desktop\\PythonStuff\\jobReqs\\jobReqs\\.venv\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Zade\\Desktop\\PythonStuff\\jobReqs\\jobReqs\\.venv\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    138\u001b[0m (\n\u001b[0;32m    139\u001b[0m     deployment_id,\n\u001b[0;32m    140\u001b[0m     engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m )\n\u001b[1;32m--> 153\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m     url,\n\u001b[0;32m    156\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m     request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    163\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m     \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Zade\\Desktop\\PythonStuff\\jobReqs\\jobReqs\\.venv\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    288\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m     method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m     url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m     request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m )\n\u001b[1;32m--> 298\u001b[0m resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\Zade\\Desktop\\PythonStuff\\jobReqs\\jobReqs\\.venv\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    705\u001b[0m         ),\n\u001b[0;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Zade\\Desktop\\PythonStuff\\jobReqs\\jobReqs\\.venv\\lib\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-H77pLY0lAaZjBOoJznR3V5AN on tokens per min. Limit: 90000 / min. Current: 86754 / min. Contact us through our help center at help.openai.com if you continue to have issues.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Zade\\Desktop\\PythonStuff\\jobReqs\\jobReqs\\trainspacy.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Test with only the first json now\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Hit error with rate limits on openai api\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# https://platform.openai.com/docs/guides/rate-limits/error-mitigation\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Zade/Desktop/PythonStuff/jobReqs/jobReqs/trainspacy.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m get_tech_list(\u001b[39mfr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdata/07-09-23-q-data+science-data+analysis-data+engineer-mle-machine+learning-mlops-l-remote.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Zade\\Desktop\\PythonStuff\\jobReqs\\jobReqs\\.venv\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(f, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\Zade\\Desktop\\PythonStuff\\jobReqs\\jobReqs\\.venv\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Zade\\Desktop\\PythonStuff\\jobReqs\\jobReqs\\.venv\\lib\\site-packages\\tenacity\\__init__.py:326\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreraise:\n\u001b[0;32m    325\u001b[0m         \u001b[39mraise\u001b[39;00m retry_exc\u001b[39m.\u001b[39mreraise()\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m retry_exc \u001b[39mfrom\u001b[39;00m \u001b[39mfut\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m()\n\u001b[0;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait:\n\u001b[0;32m    329\u001b[0m     sleep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait(retry_state)\n",
      "\u001b[1;31mRetryError\u001b[0m: RetryError[<Future at 0x1b20bea8af0 state=finished raised RateLimitError>]"
     ]
    }
   ],
   "source": [
    "# Test with only the first json now\n",
    "# Hit error with rate limits on openai api\n",
    "# https://platform.openai.com/docs/guides/rate-limits/error-mitigation\n",
    "get_tech_list(fr\"data/07-09-23-q-data+science-data+analysis-data+engineer-mle-machine+learning-mlops-l-remote.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
