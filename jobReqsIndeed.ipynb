{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't work, 403 forbidden response.\n",
    "# HEADERS = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\",\n",
    "#     \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "#     \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "#     \"Connection\": \"keep-alive\",\n",
    "#     \"Accept-Language\": \"en-US,en;q=0.9,lt;q=0.8,et;q=0.7,de;q=0.6\",\n",
    "# }\n",
    "\n",
    "# response = httpx.get(\"https://www.indeed.com/jobs?q=data+science&l=remote&vjk=96bf68966c7569d7\", header=HEADERS)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.FirefoxOptions()\n",
    "driver = webdriver.Firefox(options=options)\n",
    "driver.get(\"https://www.indeed.com/jobs?q=data+science&l=remote\")\n",
    "time.sleep(1)\n",
    "\n",
    "soup = bs(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find(id=\"jobsearch-Main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_elements = results.find_all(\"div\", class_=\"slider_item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyppeteer import launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    browser = await launch()\n",
    "    page = await browser.newPage()\n",
    "    await page.goto(\"https://www.indeed.com/jobs?q=data+science&l=remote\")\n",
    "    html = await page.content()\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m     temp_results \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mjobDescriptionText\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m     \u001b[39m# print(d[link_element_id]['title'], d[link_element_id]['company'])\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     d[link_element_id][\u001b[39m'\u001b[39m\u001b[39mdesc\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m temp_results\u001b[39m.\u001b[39;49mtext\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m     28\u001b[0m \u001b[39m# print(title_element.text.strip())\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m# print(company_element.text.strip())\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# print(location_element.text.strip())\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39m# print(\"\\n\")\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m# time.sleep(10)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "basePath = \"https://www.indeed.com\"\n",
    "d = {}\n",
    "# Currently using link_element_id as unique ID.  I am fairly sure it's unique - at least it has been from what I've seen so far.\n",
    "for job_element in job_elements[:1]:\n",
    "    link_element_id = job_element.find_all(\"a\")[0][\"id\"]\n",
    "\n",
    "    d[link_element_id] = {}\n",
    "    d[link_element_id][\"title\"] = job_element.find(\"h2\", class_=\"jobTitle\").text.strip()\n",
    "    d[link_element_id]['company'] = job_element.find(\"span\", class_=\"companyName\").text.strip()\n",
    "    d[link_element_id]['posted'] = job_element.find(\"span\", class_=\"date\").next_element.next_element.next_element\n",
    "    try:\n",
    "        d[link_element_id]['location'] = job_element.find(\"div\", class_=\"companyLocation\").text.strip()\n",
    "    except AttributeError: \n",
    "        d[link_element_id]['location'] = \"Not Listed\"\n",
    "    try:\n",
    "        d[link_element_id]['salary'] = job_element.find(\"div\", class_=\"metadata\").text.strip()\n",
    "    except AttributeError:\n",
    "        d[link_element_id]['salary'] = \"Not Listed\"\n",
    "    # date_element = job_element.find(\"div\", class_=\"visually-hidden\")  # not useful as-is.  figure out if this is possible to find at all\n",
    "\n",
    "    WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.XPATH, f'//*[@id=\"{link_element_id}\"]'))).click()\n",
    "\n",
    "    time.sleep(2)\n",
    "    temp_results = results.find(\"div\", id='jobDescriptionText')\n",
    "    # print(d[link_element_id]['title'], d[link_element_id]['company'])\n",
    "    d[link_element_id]['desc'] = temp_results.text.strip()\n",
    "\n",
    "# print(title_element.text.strip())\n",
    "# print(company_element.text.strip())\n",
    "# print(location_element.text.strip())\n",
    "# print(salary_element.text.strip())\n",
    "# print(jobsnippet_element.text.strip())\n",
    "# print(full_desc_element)\n",
    "# print(\"\\n\")\n",
    "# time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sj_1d2fe01649ff8b16': {'title': 'Sr. Architect – Data , AI, Information Architecture',\n",
       "  'company': 'The Travelers Companies, Inc.',\n",
       "  'posted': 'Posted 30+ days ago',\n",
       "  'location': 'Remote in Hartford, CT 06101',\n",
       "  'salary': '$130,600 - $215,400 a year',\n",
       "  'desc': \"It's fun to work in a company where people truly BELIEVE in what they're doing!\\nWe're committed to bringing passion and customer focus to the business.\\nJob purpose\\nCelink is rapidly building proprietary servicing system on AWS and automating manual servicing processes. Data is a critical part of our business and influences every aspect of the work that we do. The Data Management (DM) team is hiring a Data Scientist to focus on data analytical efforts. This role must ensure the successful completion of multiple statistical/analytical models that can be applied on Celink Data Lake that will provide insights for business operations.\\nAs a Data Scientist, you will analyze complex mortgage and servicing operations data sets to extract valuable insights that can assist in making data-driven business decisions. You will need to understand the analytical use case and collaborate with available resources to comprehend the available data and determine the most suitable statistical/analytical model for the job. You will perform feature engineering and model selection. Once you have tested the model and obtained the desired results, you will present them using visualization tools like Power BI and “productionalize” the model for automated insights.\\nThis is an excellent opportunity to join a fast-paced team and enhance your data scientist skills. The ideal candidate for this role should possess a solid background in data analysis, statistics, and machine learning, as well as exceptional problem-solving and communication abilities.\\nDuties and responsibilities\\n\\nGather requirements from stakeholders. Collaborate with multiple teams to identify business problems and develop data-driven solutions\\nPerform extensive data analysis to extract insights from large and complex data sets\\nDesign, develop and implement data models and algorithms\\nDevelop predictive models to forecast business trends and outcomes\\nUse data visualization tools to communicate findings to stakeholders in a clear and concise manner.\\nContinuously monitor and improve model performance\\nUnderstand and references technical designs and specifications to share with stakeholders\\nWill participate in ad-hoc deep dive sessions\\nWrite documentation in Confluence on the established analytical models, add analysis results on Epic pages\\nCommunicate with scrum masters to prioritized work from assigned Jira Epics\\nKeep up to date with the latest industry trends and techniques\\n\\nMinimum Qualifications\\nEducation \\n\\nBachelor's degree, preferably in Data Science, Mathematics or Statistics, is required, along with minimum of 5 years of applicable experience; or master’s degree along with a minimum of 3 years applicable experience.\\n\\nExperience and Training \\n\\nModeling experience with large amounts of data sets in Data Science and Machine Learning is required\\nExperience building predictive models\\nExperience working in a Data Scientist advanced data analytics role\\nExperience analyzing and communicating statistical analyses required\\nStrong programming skills in Python and Jupyter\\nProficient in SQL for data extraction and manipulation\\nExperience with machine learning frameworks such as TensorFlow or PyTorch and environments like Amazon SageMaker\\nFamiliarity with data visualization tools such as Power BI\\nExcellent communication and problem-solving skills\\nAbility to work in a fast-paced environment and manage multiple projects simultaneously\\nExperience and ability to work in environment with 2-week sprints with pre and post routines.\\nMust be successful working in fast-paced development cycles\\nExperience working with cloud-based platforms such as AWS\\nExcellent communication skills\\nExcellent analytical and critical thinking skills\\nProven experience in technical writing and requirements gathering\\nStrong knowledge of JIRA/Confluence\\nStrong knowledge of Microsoft Office applications\\nKnowledge of mortgage servicing data is a plus\\nTenacity to problem-solve using data and seeing the big-picture business needs\\nAttentive to details, methodical and rigorous. It bothers you if some data you own is incorrect\\nMortgage and finance knowledge strongly preferred\\n\\nWorking conditionsWork may be performed in a professional office or home office environment with no unusual hazards. This role routinely uses standard office equipment such as computers, phones, headsets, photocopiers, filing cabinets and fax machines.\\nEEO Statement\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.\\nJob Type: Full-time\\nPay: $80,000.00 - $105,000.00 per year\\nBenefits:\\n\\n401(k)\\n401(k) matching\\nDental insurance\\nFlexible spending account\\nHealth insurance\\nPaid time off\\nVision insurance\\n\\nSchedule:\\n\\n8 hour shift\\nDay shift\\nMonday to Friday\\n\\nApplication Question(s):\\n\\nStrong programming skills in Python and Jupyter\\n Proficient in SQL for data extraction and manipulation\\nMUST HAVE • Bachelor's degree, preferably in Data Science, Mathematics or Statistics, is required, along with minimum of 5 years of applicable experience; or master’s degree along with a minimum of 3 years applicable experience. \\nBackground in requirement gathering and writing\\nMUST HAVE Modeling experience with large amounts of data sets in Data Science and Machine Learning is required\\n• Experience with machine learning frameworks such as TensorFlow or PyTorch and environments like Amazon SageMaker\\n Familiarity with data visualization tools such as Power BI\\n\\nWork Location: Remote\"}}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I can get the full job desc, I will want to do the following:\n",
    "For each job, create a JSON object for it.  Each JSON object will have a structure like\n",
    "\n",
    "{ \"title\":job_title, \"company\":company_name, \"location\":job_location, \"salary\":job_salary, \"snippet\":job_desc_snippet, \"desc\":full_job_desc, \"tags\":search_terms_used_to_find_job}\n",
    "\n",
    "I want a way to go through and if the same job is repeated, add on to the tags in the proper json.  Can do this by instead building it as a dict, appending to the \"tags\" key so then search_terms_used_to_find_job is instead a list thereof, and the list is appended with the new search terms before being json-ified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to figure out if I can / how to get the date posted.\n",
    "\n",
    "Need to figure out if the job ID on indeed is unique.  If I can't figure it out, may need my own unique ID for each\n",
    "\n",
    "Probably remove the job desc snippet since I have full descs now\n",
    "\n",
    "Need to figure out how to go through all pages\n",
    "\n",
    "Need to add tags for search params\n",
    "\n",
    "Can use: https://scrapfly.io/blog/how-to-scrape-indeedcom/\n",
    "To see how to better optimize this search.  Both for making searching more dynamic as well as using async IO to speed it up"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
